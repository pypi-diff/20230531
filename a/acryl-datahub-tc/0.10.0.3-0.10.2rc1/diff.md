# Comparing `tmp/acryl-datahub-tc-0.10.0.3.tar.gz` & `tmp/acryl-datahub-tc-0.10.2rc1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist/acryl-datahub-tc-0.10.0.3.tar", last modified: Mon Mar 20 14:40:41 2023, max compression
+gzip compressed data, was "dist/acryl-datahub-tc-0.10.2rc1.tar", last modified: Wed May 31 19:14:29 2023, max compression
```

## Comparing `acryl-datahub-tc-0.10.0.3.tar` & `acryl-datahub-tc-0.10.2rc1.tar`

### file list

```diff
@@ -1,754 +1,634 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/
--rw-r--r--   0 runner    (1001) docker     (123)    14560 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    10040 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/README.md
--rw-r--r--   0 runner    (1001) docker     (123)      928 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/pyproject.toml
--rw-r--r--   0 runner    (1001) docker     (123)     2035 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (123)    26270 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/acryl_datahub_tc.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)    14560 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/acryl_datahub_tc.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    31052 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/acryl_datahub_tc.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/acryl_datahub_tc.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)     6434 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/acryl_datahub_tc.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/acryl_datahub_tc.egg-info/not-zip-safe
--rw-r--r--   0 runner    (1001) docker     (123)    32873 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/acryl_datahub_tc.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)       25 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/acryl_datahub_tc.egg-info/top_level.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/
--rw-r--r--   0 runner    (1001) docker     (123)      554 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      106 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/__main__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/circuit_breaker/
--rw-r--r--   0 runner    (1001) docker     (123)      268 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/circuit_breaker/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5324 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/circuit_breaker/assertion_circuit_breaker.py
--rw-r--r--   0 runner    (1001) docker     (123)     1471 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/circuit_breaker/circuit_breaker.py
--rw-r--r--   0 runner    (1001) docker     (123)     2908 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/circuit_breaker/operation_circuit_breaker.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/entities/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/entities/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/entities/corpgroup/
--rw-r--r--   0 runner    (1001) docker     (123)       63 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/entities/corpgroup/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2951 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/entities/corpgroup/corpgroup.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/entities/corpuser/
--rw-r--r--   0 runner    (1001) docker     (123)       60 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/entities/corpuser/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3639 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/entities/corpuser/corpuser.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/entities/datajob/
--rw-r--r--   0 runner    (1001) docker     (123)      116 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/entities/datajob/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4265 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/entities/datajob/dataflow.py
--rw-r--r--   0 runner    (1001) docker     (123)     7492 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/entities/datajob/datajob.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/entities/dataprocess/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/entities/dataprocess/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    14547 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/entities/dataprocess/dataprocess_instance.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/graphql/
--rw-r--r--   0 runner    (1001) docker     (123)      104 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/graphql/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2818 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/graphql/assertion.py
--rw-r--r--   0 runner    (1001) docker     (123)     1566 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/graphql/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     5116 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/api/graphql/operation.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/cli/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/cli/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2725 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/cli/check_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)    24236 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/cli/cli_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    16357 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/cli/delete_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)     7524 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/cli/docker_check.py
--rw-r--r--   0 runner    (1001) docker     (123)    33174 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/cli/docker_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)     1431 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/cli/get_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)    12955 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/cli/ingest_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)      888 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/cli/json_file.py
--rw-r--r--   0 runner    (1001) docker     (123)    12776 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/cli/lite_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)    16454 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/cli/migrate.py
--rw-r--r--   0 runner    (1001) docker     (123)     8936 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/cli/migration_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     3067 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/cli/put_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)     2448 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/cli/state_cli.py
--rw-r--r--   0 runner    (1001) docker     (123)      489 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/cli/telemetry.py
--rw-r--r--   0 runner    (1001) docker     (123)     7352 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/cli/timeline_cli.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/configuration/
--rw-r--r--   0 runner    (1001) docker     (123)      114 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/configuration/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      934 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/configuration/_config_enum.py
--rw-r--r--   0 runner    (1001) docker     (123)    10553 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/configuration/common.py
--rw-r--r--   0 runner    (1001) docker     (123)     3250 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/configuration/config_loader.py
--rw-r--r--   0 runner    (1001) docker     (123)     5603 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/configuration/github.py
--rw-r--r--   0 runner    (1001) docker     (123)      369 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/configuration/import_resolver.py
--rw-r--r--   0 runner    (1001) docker     (123)     2105 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/configuration/kafka.py
--rw-r--r--   0 runner    (1001) docker     (123)      386 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/configuration/pattern_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)      660 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/configuration/pydantic_field_deprecation.py
--rw-r--r--   0 runner    (1001) docker     (123)     2337 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/configuration/source_common.py
--rw-r--r--   0 runner    (1001) docker     (123)     2363 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/configuration/time_window_config.py
--rw-r--r--   0 runner    (1001) docker     (123)      380 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/configuration/toml.py
--rw-r--r--   0 runner    (1001) docker     (123)      688 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/configuration/validate_field_removal.py
--rw-r--r--   0 runner    (1001) docker     (123)     1705 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/configuration/validate_field_rename.py
--rw-r--r--   0 runner    (1001) docker     (123)      867 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/configuration/validate_host_port.py
--rw-r--r--   0 runner    (1001) docker     (123)      301 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/configuration/yaml.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/emitter/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/emitter/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      292 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/emitter/aspect.py
--rw-r--r--   0 runner    (1001) docker     (123)     5645 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/emitter/kafka_emitter.py
--rw-r--r--   0 runner    (1001) docker     (123)    13734 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/emitter/mce_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     8174 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/emitter/mcp.py
--rw-r--r--   0 runner    (1001) docker     (123)     9187 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/emitter/mcp_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     2523 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/emitter/mcp_patch_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)      524 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/emitter/request_helper.py
--rw-r--r--   0 runner    (1001) docker     (123)    11320 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/emitter/rest_emitter.py
--rw-r--r--   0 runner    (1001) docker     (123)     3652 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/emitter/serialization_helper.py
--rw-r--r--   0 runner    (1001) docker     (123)     7374 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/entrypoints.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      449 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/closeable.py
--rw-r--r--   0 runner    (1001) docker     (123)      886 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/committable.py
--rw-r--r--   0 runner    (1001) docker     (123)     2671 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/common.py
--rw-r--r--   0 runner    (1001) docker     (123)     3571 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/decorators.py
--rw-r--r--   0 runner    (1001) docker     (123)     2013 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/ingestion_job_checkpointing_provider_base.py
--rw-r--r--   0 runner    (1001) docker     (123)      608 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/pipeline_run_listener.py
--rw-r--r--   0 runner    (1001) docker     (123)     6346 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/registry.py
--rw-r--r--   0 runner    (1001) docker     (123)     4752 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/report.py
--rw-r--r--   0 runner    (1001) docker     (123)      994 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/report_helpers.py
--rw-r--r--   0 runner    (1001) docker     (123)     4491 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/sink.py
--rw-r--r--   0 runner    (1001) docker     (123)     6074 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/source.py
--rw-r--r--   0 runner    (1001) docker     (123)      582 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/transform.py
--rw-r--r--   0 runner    (1001) docker     (123)     3316 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/workunit.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/extractor/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/extractor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      342 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/extractor/extractor_registry.py
--rw-r--r--   0 runner    (1001) docker     (123)     3701 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/extractor/mce_extractor.py
--rw-r--r--   0 runner    (1001) docker     (123)    13147 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/extractor/protobuf_util.py
--rw-r--r--   0 runner    (1001) docker     (123)    21985 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/extractor/schema_util.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/glossary/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/glossary/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6189 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/glossary/classification_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     2675 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/glossary/classifier.py
--rw-r--r--   0 runner    (1001) docker     (123)      307 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/glossary/classifier_registry.py
--rw-r--r--   0 runner    (1001) docker     (123)     4813 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/glossary/datahub_classifier.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/graph/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/graph/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    18430 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/graph/client.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/reporting/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/reporting/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8506 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/reporting/datahub_ingestion_run_summary_provider.py
--rw-r--r--   0 runner    (1001) docker     (123)     1576 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/reporting/file_reporter.py
--rw-r--r--   0 runner    (1001) docker     (123)      445 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/reporting/reporting_provider_registry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/run/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/run/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1474 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/run/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    22461 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/run/pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     3902 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/run/pipeline_config.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/sink/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/sink/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      591 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/sink/console.py
--rw-r--r--   0 runner    (1001) docker     (123)     2838 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/sink/datahub_kafka.py
--rw-r--r--   0 runner    (1001) docker     (123)     1991 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/sink/datahub_lite.py
--rw-r--r--   0 runner    (1001) docker     (123)     8138 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/sink/datahub_rest.py
--rw-r--r--   0 runner    (1001) docker     (123)     2743 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/sink/file.py
--rw-r--r--   0 runner    (1001) docker     (123)      593 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/sink/sink_registry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8134 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/aws_common.py
--rw-r--r--   0 runner    (1001) docker     (123)    48270 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/glue.py
--rw-r--r--   0 runner    (1001) docker     (123)     8485 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/path_spec.py
--rw-r--r--   0 runner    (1001) docker     (123)     3884 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/s3_boto_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     1694 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/s3_util.py
--rw-r--r--   0 runner    (1001) docker     (123)     3809 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/sagemaker.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/sagemaker_processors/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/sagemaker_processors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1554 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/sagemaker_processors/common.py
--rw-r--r--   0 runner    (1001) docker     (123)    10383 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/sagemaker_processors/feature_groups.py
--rw-r--r--   0 runner    (1001) docker     (123)    10165 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/sagemaker_processors/job_classes.py
--rw-r--r--   0 runner    (1001) docker     (123)    34904 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/sagemaker_processors/jobs.py
--rw-r--r--   0 runner    (1001) docker     (123)     9290 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/sagemaker_processors/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)    19207 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/sagemaker_processors/models.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/azure/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/azure/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3706 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/azure/azure_common.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/bigquery_v2/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/bigquery_v2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    49995 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/bigquery_v2/bigquery.py
--rw-r--r--   0 runner    (1001) docker     (123)    21982 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/bigquery_v2/bigquery_audit.py
--rw-r--r--   0 runner    (1001) docker     (123)    12539 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/bigquery_v2/bigquery_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     3630 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/bigquery_v2/bigquery_report.py
--rw-r--r--   0 runner    (1001) docker     (123)    17574 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/bigquery_v2/bigquery_schema.py
--rw-r--r--   0 runner    (1001) docker     (123)     1640 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/bigquery_v2/common.py
--rw-r--r--   0 runner    (1001) docker     (123)    32694 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/bigquery_v2/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)    12650 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/bigquery_v2/profiler.py
--rw-r--r--   0 runner    (1001) docker     (123)    34983 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/bigquery_v2/usage.py
--rw-r--r--   0 runner    (1001) docker     (123)    13215 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/confluent_schema_registry.py
--rw-r--r--   0 runner    (1001) docker     (123)    26242 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/csv_enricher.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/dbt/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/dbt/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    12966 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/dbt/dbt_cloud.py
--rw-r--r--   0 runner    (1001) docker     (123)    56180 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/dbt/dbt_common.py
--rw-r--r--   0 runner    (1001) docker     (123)    17412 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/dbt/dbt_core.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/delta_lake/
--rw-r--r--   0 runner    (1001) docker     (123)       71 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/delta_lake/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3356 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/delta_lake/config.py
--rw-r--r--   0 runner    (1001) docker     (123)     1923 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/delta_lake/delta_lake_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)      467 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/delta_lake/report.py
--rw-r--r--   0 runner    (1001) docker     (123)    12467 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/delta_lake/source.py
--rw-r--r--   0 runner    (1001) docker     (123)     1068 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/demo_data.py
--rw-r--r--   0 runner    (1001) docker     (123)    17210 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/elastic_search.py
--rw-r--r--   0 runner    (1001) docker     (123)    14671 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/feast.py
--rw-r--r--   0 runner    (1001) docker     (123)    15277 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/file.py
--rw-r--r--   0 runner    (1001) docker     (123)    43770 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/ge_data_profiler.py
--rw-r--r--   0 runner    (1001) docker     (123)     8586 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/ge_profiling_config.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/git/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/git/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2564 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/git/git_import.py
--rw-r--r--   0 runner    (1001) docker     (123)     2010 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/glue_profiling_config.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/iceberg/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/iceberg/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    19431 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/iceberg/iceberg.py
--rw-r--r--   0 runner    (1001) docker     (123)     8221 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/iceberg/iceberg_common.py
--rw-r--r--   0 runner    (1001) docker     (123)     9563 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/iceberg/iceberg_profiler.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/identity/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/identity/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    27600 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/identity/azure_ad.py
--rw-r--r--   0 runner    (1001) docker     (123)    30328 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/identity/okta.py
--rw-r--r--   0 runner    (1001) docker     (123)    17456 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/kafka.py
--rw-r--r--   0 runner    (1001) docker     (123)    47922 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/kafka_connect.py
--rw-r--r--   0 runner    (1001) docker     (123)      321 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/kafka_schema_registry_base.py
--rw-r--r--   0 runner    (1001) docker     (123)    17786 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/ldap.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/looker/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/looker/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    42243 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/looker/looker_common.py
--rw-r--r--   0 runner    (1001) docker     (123)     7480 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/looker/looker_lib_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (123)     2202 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/looker/looker_query_model.py
--rw-r--r--   0 runner    (1001) docker     (123)    54007 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/looker/looker_source.py
--rw-r--r--   0 runner    (1001) docker     (123)    24029 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/looker/looker_usage.py
--rw-r--r--   0 runner    (1001) docker     (123)    75978 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/looker/lookml_source.py
--rw-r--r--   0 runner    (1001) docker     (123)    22826 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/metabase.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/metadata/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/metadata/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    16861 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/metadata/business_glossary.py
--rw-r--r--   0 runner    (1001) docker     (123)     8676 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/metadata/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)      326 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/metadata_common.py
--rw-r--r--   0 runner    (1001) docker     (123)    30025 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/mode.py
--rw-r--r--   0 runner    (1001) docker     (123)    17163 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/mongodb.py
--rw-r--r--   0 runner    (1001) docker     (123)    43240 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/nifi.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    13322 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/openapi.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    13485 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/openapi_parser.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi/
--rw-r--r--   0 runner    (1001) docker     (123)       76 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8148 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi/config.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi/m_query/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi/m_query/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1050 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi/m_query/data_classes.py
--rw-r--r--   0 runner    (1001) docker     (123)     1586 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi/m_query/native_sql_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     2331 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi/m_query/parser.py
--rw-r--r--   0 runner    (1001) docker     (123)    24168 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi/m_query/resolver.py
--rw-r--r--   0 runner    (1001) docker     (123)     4809 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi/m_query/tree_function.py
--rw-r--r--   0 runner    (1001) docker     (123)     1052 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi/m_query/validator.py
--rw-r--r--   0 runner    (1001) docker     (123)    17280 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi/powerbi-lexical-grammar.rule
--rw-r--r--   0 runner    (1001) docker     (123)    31348 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi/powerbi.py
--rw-r--r--   0 runner    (1001) docker     (123)    36426 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi/proxy.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi_report_server/
--rw-r--r--   0 runner    (1001) docker     (123)      324 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi_report_server/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3689 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi_report_server/constants.py
--rw-r--r--   0 runner    (1001) docker     (123)    20069 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi_report_server/report_server.py
--rw-r--r--   0 runner    (1001) docker     (123)    11684 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi_report_server/report_server_domain.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/profiling/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/profiling/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1426 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/profiling/common.py
--rw-r--r--   0 runner    (1001) docker     (123)    20480 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/pulsar.py
--rw-r--r--   0 runner    (1001) docker     (123)    32287 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/redash.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/s3/
--rw-r--r--   0 runner    (1001) docker     (123)       56 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/s3/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5057 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/s3/config.py
--rw-r--r--   0 runner    (1001) docker     (123)     3999 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/s3/data_lake_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    20777 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/s3/profiling.py
--rw-r--r--   0 runner    (1001) docker     (123)      466 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/s3/report.py
--rw-r--r--   0 runner    (1001) docker     (123)    31119 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/s3/source.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sagemaker_processors/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sagemaker_processors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    27038 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/salesforce.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/schema_inference/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/schema_inference/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      563 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/schema_inference/avro.py
--rw-r--r--   0 runner    (1001) docker     (123)      379 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/schema_inference/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     2229 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/schema_inference/csv_tsv.py
--rw-r--r--   0 runner    (1001) docker     (123)     2103 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/schema_inference/json.py
--rw-r--r--   0 runner    (1001) docker     (123)     5760 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/schema_inference/object.py
--rw-r--r--   0 runner    (1001) docker     (123)     3426 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/schema_inference/parquet.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/snowflake/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/snowflake/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1581 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/snowflake/constants.py
--rw-r--r--   0 runner    (1001) docker     (123)     7021 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/snowflake/snowflake_config.py
--rw-r--r--   0 runner    (1001) docker     (123)    29016 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/snowflake/snowflake_lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)     7240 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/snowflake/snowflake_profiler.py
--rw-r--r--   0 runner    (1001) docker     (123)    24377 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/snowflake/snowflake_query.py
--rw-r--r--   0 runner    (1001) docker     (123)     3076 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/snowflake/snowflake_report.py
--rw-r--r--   0 runner    (1001) docker     (123)    19227 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/snowflake/snowflake_schema.py
--rw-r--r--   0 runner    (1001) docker     (123)     6141 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/snowflake/snowflake_tag.py
--rw-r--r--   0 runner    (1001) docker     (123)    18630 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/snowflake/snowflake_usage_v2.py
--rw-r--r--   0 runner    (1001) docker     (123)    10596 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/snowflake/snowflake_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    62463 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/snowflake/snowflake_v2.py
--rw-r--r--   0 runner    (1001) docker     (123)     1043 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/source_registry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9483 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/athena.py
--rw-r--r--   0 runner    (1001) docker     (123)    25323 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/clickhouse.py
--rw-r--r--   0 runner    (1001) docker     (123)     2345 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/druid.py
--rw-r--r--   0 runner    (1001) docker     (123)     1342 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/hana.py
--rw-r--r--   0 runner    (1001) docker     (123)     6458 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/hive.py
--rw-r--r--   0 runner    (1001) docker     (123)      737 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/mariadb.py
--rw-r--r--   0 runner    (1001) docker     (123)    10975 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/mssql.py
--rw-r--r--   0 runner    (1001) docker     (123)     2650 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/mysql.py
--rw-r--r--   0 runner    (1001) docker     (123)     2316 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/oauth_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)     6369 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/oracle.py
--rw-r--r--   0 runner    (1001) docker     (123)     8431 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/postgres.py
--rw-r--r--   0 runner    (1001) docker     (123)     3608 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/presto.py
--rw-r--r--   0 runner    (1001) docker     (123)    33194 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/presto_on_hive.py
--rw-r--r--   0 runner    (1001) docker     (123)    45578 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/redshift.py
--rw-r--r--   0 runner    (1001) docker     (123)    45704 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/sql_common.py
--rw-r--r--   0 runner    (1001) docker     (123)     6941 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/sql_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     2933 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/sql_generic.py
--rw-r--r--   0 runner    (1001) docker     (123)     7067 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/sql_generic_profiler.py
--rw-r--r--   0 runner    (1001) docker     (123)     9991 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/sql_types.py
--rw-r--r--   0 runner    (1001) docker     (123)     6395 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/sql_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    10553 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/trino.py
--rw-r--r--   0 runner    (1001) docker     (123)     4937 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/two_tier_sql_source.py
--rw-r--r--   0 runner    (1001) docker     (123)    53123 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/vertica.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8797 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state/checkpoint.py
--rw-r--r--   0 runner    (1001) docker     (123)     4220 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state/entity_removal_state.py
--rw-r--r--   0 runner    (1001) docker     (123)      512 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state/profiling_state.py
--rw-r--r--   0 runner    (1001) docker     (123)     4209 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state/profiling_state_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     5544 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state/redundant_run_skip_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)      143 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state/sql_common_state.py
--rw-r--r--   0 runner    (1001) docker     (123)    13120 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state/stale_entity_removal_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)    16871 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state/stateful_ingestion_base.py
--rw-r--r--   0 runner    (1001) docker     (123)      463 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state/usage_common_state.py
--rw-r--r--   0 runner    (1001) docker     (123)     1271 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state/use_case_handler.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state_provider/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state_provider/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5368 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state_provider/datahub_ingestion_checkpointing_provider.py
--rw-r--r--   0 runner    (1001) docker     (123)      501 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state_provider/state_provider_registry.py
--rw-r--r--   0 runner    (1001) docker     (123)    14155 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/superset.py
--rw-r--r--   0 runner    (1001) docker     (123)    68206 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/tableau.py
--rw-r--r--   0 runner    (1001) docker     (123)    14785 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/tableau_common.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/unity/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/unity/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2907 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/unity/config.py
--rw-r--r--   0 runner    (1001) docker     (123)    11317 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/unity/proxy.py
--rw-r--r--   0 runner    (1001) docker     (123)      585 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/unity/report.py
--rw-r--r--   0 runner    (1001) docker     (123)    18273 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/unity/source.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/usage/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/usage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9853 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/usage/clickhouse_usage.py
--rw-r--r--   0 runner    (1001) docker     (123)    15330 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/usage/redshift_usage.py
--rw-r--r--   0 runner    (1001) docker     (123)    10159 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/usage/starburst_trino_usage.py
--rw-r--r--   0 runner    (1001) docker     (123)     5931 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/usage/usage_common.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_config/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_config/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1654 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_config/bigquery.py
--rw-r--r--   0 runner    (1001) docker     (123)     1661 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_config/csv_enricher.py
--rw-r--r--   0 runner    (1001) docker     (123)     6099 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_config/pulsar.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_config/sql/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_config/sql/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5582 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_config/sql/bigquery.py
--rw-r--r--   0 runner    (1001) docker     (123)    19502 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_config/sql/snowflake.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_config/usage/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_config/usage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7841 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_config/usage/bigquery_usage.py
--rw-r--r--   0 runner    (1001) docker     (123)      565 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_config/usage/snowflake_usage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_report/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_report/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1037 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_report/pulsar.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_report/sql/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_report/sql/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1864 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_report/sql/bigquery.py
--rw-r--r--   0 runner    (1001) docker     (123)     1512 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_report/sql/snowflake.py
--rw-r--r--   0 runner    (1001) docker     (123)      229 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_report/time_window.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_report/usage/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_report/usage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1374 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_report/usage/bigquery_usage.py
--rw-r--r--   0 runner    (1001) docker     (123)      780 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_report/usage/snowflake_usage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3420 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/add_dataset_browse_path.py
--rw-r--r--   0 runner    (1001) docker     (123)     6849 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/add_dataset_ownership.py
--rw-r--r--   0 runner    (1001) docker     (123)     5605 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/add_dataset_properties.py
--rw-r--r--   0 runner    (1001) docker     (123)     5664 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/add_dataset_schema_tags.py
--rw-r--r--   0 runner    (1001) docker     (123)     6239 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/add_dataset_schema_terms.py
--rw-r--r--   0 runner    (1001) docker     (123)     4911 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/add_dataset_tags.py
--rw-r--r--   0 runner    (1001) docker     (123)     5707 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/add_dataset_terms.py
--rw-r--r--   0 runner    (1001) docker     (123)    11200 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/base_transformer.py
--rw-r--r--   0 runner    (1001) docker     (123)     6187 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/dataset_domain.py
--rw-r--r--   0 runner    (1001) docker     (123)     1598 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/dataset_transformer.py
--rw-r--r--   0 runner    (1001) docker     (123)     1323 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/mark_dataset_status.py
--rw-r--r--   0 runner    (1001) docker     (123)     1218 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/remove_dataset_ownership.py
--rw-r--r--   0 runner    (1001) docker     (123)     1401 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/transform_registry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/integrations/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/integrations/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/integrations/great_expectations/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/integrations/great_expectations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    34503 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/integrations/great_expectations/action.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/lite/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/lite/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    32647 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/lite/duckdb_lite.py
--rw-r--r--   0 runner    (1001) docker     (123)     2846 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/lite/lite_local.py
--rw-r--r--   0 runner    (1001) docker     (123)      292 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/lite/lite_registry.py
--rw-r--r--   0 runner    (1001) docker     (123)     1949 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/lite/lite_server.py
--rw-r--r--   0 runner    (1001) docker     (123)     4503 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/lite/lite_util.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/
--rw-r--r--   0 runner    (1001) docker     (123)      197 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/events/
--rw-r--r--   0 runner    (1001) docker     (123)      257 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/events/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/access/
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/access/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/access/token/
--rw-r--r--   0 runner    (1001) docker     (123)      277 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/access/token/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/assertion/
--rw-r--r--   0 runner    (1001) docker     (123)     1589 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/assertion/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/chart/
--rw-r--r--   0 runner    (1001) docker     (123)      807 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/chart/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/common/
--rw-r--r--   0 runner    (1001) docker     (123)     3456 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/common/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/common/fieldtransformer/
--rw-r--r--   0 runner    (1001) docker     (123)      355 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/common/fieldtransformer/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/container/
--rw-r--r--   0 runner    (1001) docker     (123)      469 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/container/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/dashboard/
--rw-r--r--   0 runner    (1001) docker     (123)      615 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/dashboard/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/
--rw-r--r--   0 runner    (1001) docker     (123)      828 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/azkaban/
--rw-r--r--   0 runner    (1001) docker     (123)      253 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/azkaban/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/datahub/
--rw-r--r--   0 runner    (1001) docker     (123)      535 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/datahub/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/dataplatform/
--rw-r--r--   0 runner    (1001) docker     (123)      341 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/dataplatform/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/dataplatforminstance/
--rw-r--r--   0 runner    (1001) docker     (123)      300 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/dataplatforminstance/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/dataprocess/
--rw-r--r--   0 runner    (1001) docker     (123)     1317 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/dataprocess/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/dataset/
--rw-r--r--   0 runner    (1001) docker     (123)     2204 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/dataset/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/domain/
--rw-r--r--   0 runner    (1001) docker     (123)      326 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/domain/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/events/
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/events/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/events/metadata/
--rw-r--r--   0 runner    (1001) docker     (123)      241 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/events/metadata/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/execution/
--rw-r--r--   0 runner    (1001) docker     (123)      734 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/execution/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/glossary/
--rw-r--r--   0 runner    (1001) docker     (123)      460 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/glossary/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/identity/
--rw-r--r--   0 runner    (1001) docker     (123)     1443 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/identity/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/ingestion/
--rw-r--r--   0 runner    (1001) docker     (123)      556 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/ingestion/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/key/
--rw-r--r--   0 runner    (1001) docker     (123)     3787 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/key/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/query/
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/query/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/query/filter/
--rw-r--r--   0 runner    (1001) docker     (123)      491 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/query/filter/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/snapshot/
--rw-r--r--   0 runner    (1001) docker     (123)     2317 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/snapshot/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/ml/
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/ml/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/ml/metadata/
--rw-r--r--   0 runner    (1001) docker     (123)     3151 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/ml/metadata/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/mxe/
--rw-r--r--   0 runner    (1001) docker     (123)      932 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/mxe/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/notebook/
--rw-r--r--   0 runner    (1001) docker     (123)      860 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/notebook/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/platform/
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/platform/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/platform/event/
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/platform/event/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/platform/event/v1/
--rw-r--r--   0 runner    (1001) docker     (123)      342 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/platform/event/v1/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/policy/
--rw-r--r--   0 runner    (1001) docker     (123)      876 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/policy/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/post/
--rw-r--r--   0 runner    (1001) docker     (123)      477 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/post/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/retention/
--rw-r--r--   0 runner    (1001) docker     (123)      561 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/retention/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/schema/
--rw-r--r--   0 runner    (1001) docker     (123)     2822 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/schema/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/secret/
--rw-r--r--   0 runner    (1001) docker     (123)      264 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/secret/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/settings/
--rw-r--r--   0 runner    (1001) docker     (123)      161 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/settings/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/settings/global/
--rw-r--r--   0 runner    (1001) docker     (123)      370 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/settings/global/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/step/
--rw-r--r--   0 runner    (1001) docker     (123)      288 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/step/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/tag/
--rw-r--r--   0 runner    (1001) docker     (123)      249 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/tag/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/telemetry/
--rw-r--r--   0 runner    (1001) docker     (123)      261 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/telemetry/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/test/
--rw-r--r--   0 runner    (1001) docker     (123)      670 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/test/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/timeseries/
--rw-r--r--   0 runner    (1001) docker     (123)      596 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/timeseries/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/upgrade/
--rw-r--r--   0 runner    (1001) docker     (123)      380 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/upgrade/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/usage/
--rw-r--r--   0 runner    (1001) docker     (123)      561 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/usage/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/view/
--rw-r--r--   0 runner    (1001) docker     (123)      457 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/view/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)   407120 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schema.avsc
--rw-r--r--   0 runner    (1001) docker     (123)   782790 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schema_classes.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/
--rw-r--r--   0 runner    (1001) docker     (123)    11363 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/AssertionInfo.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      592 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/AssertionKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     8820 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/AssertionRunEvent.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      654 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/BrowsePaths.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     2769 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/CaveatsAndRecommendations.avsc
--rw-r--r--   0 runner    (1001) docker     (123)    10317 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/ChartInfo.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      972 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/ChartKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      863 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/ChartQuery.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     5331 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/ChartUsageStatistics.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      741 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/Container.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      794 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/ContainerKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     3204 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/ContainerProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1290 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/CorpGroupEditableInfo.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     4414 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/CorpGroupInfo.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      752 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/CorpGroupKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      980 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/CorpUserCredentials.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     2473 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/CorpUserEditableInfo.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     3444 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/CorpUserInfo.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      921 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/CorpUserKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1699 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/CorpUserSettings.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     2348 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/CorpUserStatus.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1317 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/Cost.avsc
--rw-r--r--   0 runner    (1001) docker     (123)    10755 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DashboardInfo.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1027 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DashboardKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     7139 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DashboardUsageStatistics.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     3047 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataFlowInfo.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      958 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataFlowKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1606 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataHubAccessTokenInfo.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      483 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataHubAccessTokenKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     2736 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataHubIngestionSourceInfo.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      526 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataHubIngestionSourceKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     7539 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataHubPolicyInfo.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      651 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataHubPolicyKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1413 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataHubRetentionConfig.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      660 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataHubRetentionKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      687 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataHubRoleInfo.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      492 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataHubRoleKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      441 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataHubSecretKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     2805 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataHubSecretValue.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      457 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataHubStepStateKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     2327 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataHubStepStateProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      434 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataHubUpgradeKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      476 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataHubUpgradeRequest.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      636 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataHubUpgradeResult.avsc
--rw-r--r--   0 runner    (1001) docker     (123)    10119 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataHubViewInfo.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      424 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataHubViewKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     5490 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataJobInfo.avsc
--rw-r--r--   0 runner    (1001) docker     (123)    13272 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataJobInputOutput.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1112 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataJobKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     2564 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataPlatformInfo.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1018 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataPlatformInstance.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      780 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataPlatformInstanceKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1468 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataPlatformInstanceProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      461 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataPlatformKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1448 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataProcessInfo.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      799 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataProcessInstanceInput.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      808 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataProcessInstanceKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      797 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataProcessInstanceOutput.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     3594 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataProcessInstanceProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     2098 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataProcessInstanceRelationships.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     5739 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataProcessInstanceRunEvent.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1930 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DataProcessKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     5155 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DatahubIngestionCheckpoint.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     8907 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DatahubIngestionRunSummary.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1236 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DatasetDeprecation.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     2526 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DatasetKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     9088 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DatasetProfile.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     4021 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DatasetProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     5197 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DatasetUpstreamLineage.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     6385 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DatasetUsageStatistics.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1033 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/Deprecation.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      586 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DomainKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     2735 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DomainProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      756 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/Domains.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     3715 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/EditableChartProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      603 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/EditableContainerProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     3730 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/EditableDashboardProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     3726 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/EditableDataFlowProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     3724 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/EditableDataJobProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     3749 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/EditableDatasetProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      540 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/EditableMLFeatureProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      560 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/EditableMLFeatureTableProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      557 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/EditableMLModelGroupProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      534 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/EditableMLModelProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      451 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/EditableMLPrimaryKeyProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     3757 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/EditableNotebookProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)    11239 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/EditableSchemaMetadata.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      401 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/Embed.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     3580 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/EntityChangeEvent.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1957 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/EthicalConsiderations.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1615 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/EvaluationData.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     2229 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/ExecutionRequestInput.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      584 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/ExecutionRequestKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     2163 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/ExecutionRequestResult.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     2463 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/ExecutionRequestSignal.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     4728 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/Filter.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      993 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/GlobalSettingsInfo.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      563 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/GlobalSettingsKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1762 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/GlobalTags.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1439 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/GlossaryNodeInfo.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      559 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/GlossaryNodeKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     2511 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/GlossaryRelatedTerms.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     2755 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/GlossaryTermInfo.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      691 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/GlossaryTermKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     3345 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/GlossaryTerms.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      509 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/GroupMembership.avsc
--rw-r--r--   0 runner    (1001) docker     (123)    23357 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/InputFields.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     3593 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/InstitutionalMemory.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1295 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/IntendedUse.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      717 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/InviteToken.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      430 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/InviteTokenKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      774 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/MLFeatureKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     4269 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/MLFeatureProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      968 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/MLFeatureTableKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1881 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/MLFeatureTableProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      833 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/MLHyperParam.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      804 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/MLMetric.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1925 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/MLModelDeploymentKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     2976 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/MLModelDeploymentProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     2608 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/MLModelFactorPrompts.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1995 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/MLModelGroupKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1587 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/MLModelGroupProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     2027 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/MLModelKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     7899 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/MLModelProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      797 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/MLPrimaryKeyKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     4120 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/MLPrimaryKeyProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)   322048 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/MetadataChangeEvent.avsc
--rw-r--r--   0 runner    (1001) docker     (123)    10577 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/MetadataChangeLog.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     8224 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/MetadataChangeProposal.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      690 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/Metrics.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      540 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/NativeGroupMembership.avsc
--rw-r--r--   0 runner    (1001) docker     (123)    11920 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/NotebookContent.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     5485 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/NotebookInfo.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1121 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/NotebookKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     6409 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/Operation.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      949 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/Origin.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     7723 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/Ownership.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1460 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/PlatformEvent.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     4040 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/PostInfo.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      457 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/PostKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      960 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/QuantitativeAnalyses.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      513 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/RoleMembership.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      736 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/SchemaFieldKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)    30183 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/SchemaMetadata.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      794 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/Siblings.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1236 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/SourceCode.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      522 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/Status.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      654 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/SubTypes.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      609 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/TagKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      821 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/TagProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      408 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/TelemetryClientId.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      493 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/TelemetryKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1533 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/TestInfo.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      421 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/TestKey.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     2049 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/TestResults.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1970 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/TrainingData.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     9068 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/UpstreamLineage.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     4205 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/UsageAggregation.avsc
--rw-r--r--   0 runner    (1001) docker     (123)     1096 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/VersionInfo.avsc
--rw-r--r--   0 runner    (1001) docker     (123)      701 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/ViewProperties.avsc
--rw-r--r--   0 runner    (1001) docker     (123)    19709 2023-03-20 14:40:39.000000 acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/py.typed
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/specific/
--rw-r--r--   0 runner    (1001) docker     (123)       88 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/specific/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5940 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/specific/dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/telemetry/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/telemetry/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      967 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/telemetry/stats.py
--rw-r--r--   0 runner    (1001) docker     (123)    11734 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/telemetry/telemetry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/upgrade/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/upgrade/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    15058 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/upgrade/upgrade.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      444 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/_markupsafe_compat.py
--rw-r--r--   0 runner    (1001) docker     (123)     3146 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/bigquery_sql_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     1138 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/checkpoint_state_util.py
--rw-r--r--   0 runner    (1001) docker     (123)      459 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/config_clean.py
--rw-r--r--   0 runner    (1001) docker     (123)      468 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/dedup_list.py
--rw-r--r--   0 runner    (1001) docker     (123)      645 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/delayed_iter.py
--rw-r--r--   0 runner    (1001) docker     (123)    10998 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/hive_schema_to_avro.py
--rw-r--r--   0 runner    (1001) docker     (123)     6432 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/logging_manager.py
--rw-r--r--   0 runner    (1001) docker     (123)     4614 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/lossy_collections.py
--rw-r--r--   0 runner    (1001) docker     (123)    10545 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     1564 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/memory_footprint.py
--rw-r--r--   0 runner    (1001) docker     (123)     3933 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/memory_leak_detector.py
--rw-r--r--   0 runner    (1001) docker     (123)      598 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/parsing_util.py
--rw-r--r--   0 runner    (1001) docker     (123)     1097 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/perf_timer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/registries/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/registries/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2450 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/registries/domain_registry.py
--rw-r--r--   0 runner    (1001) docker     (123)      766 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/sample_data.py
--rw-r--r--   0 runner    (1001) docker     (123)      698 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/server_config_util.py
--rw-r--r--   0 runner    (1001) docker     (123)     3597 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/source_helpers.py
--rw-r--r--   0 runner    (1001) docker     (123)      871 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/sql_formatter.py
--rw-r--r--   0 runner    (1001) docker     (123)     6522 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/sql_lineage_parser_impl.py
--rw-r--r--   0 runner    (1001) docker     (123)     5961 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/sql_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)      456 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/sql_parser_base.py
--rw-r--r--   0 runner    (1001) docker     (123)    14786 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/sqlalchemy_query_combiner.py
--rw-r--r--   0 runner    (1001) docker     (123)     1990 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/sqllineage_patch.py
--rw-r--r--   0 runner    (1001) docker     (123)     1218 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/stats_collections.py
--rw-r--r--   0 runner    (1001) docker     (123)      601 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/tee_io.py
--rw-r--r--   0 runner    (1001) docker     (123)      358 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/time.py
--rw-r--r--   0 runner    (1001) docker     (123)      970 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/type_annotations.py
--rw-r--r--   0 runner    (1001) docker     (123)      172 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/url_util.py
--rw-r--r--   0 runner    (1001) docker     (123)      984 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urn_encoder.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1317 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/corp_group_urn.py
--rw-r--r--   0 runner    (1001) docker     (123)     1307 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/corpuser_urn.py
--rw-r--r--   0 runner    (1001) docker     (123)     2603 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/data_flow_urn.py
--rw-r--r--   0 runner    (1001) docker     (123)     1744 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/data_job_urn.py
--rw-r--r--   0 runner    (1001) docker     (123)     1097 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/data_platform_urn.py
--rw-r--r--   0 runner    (1001) docker     (123)     1587 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/data_process_instance_urn.py
--rw-r--r--   0 runner    (1001) docker     (123)     3877 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/dataset_urn.py
--rw-r--r--   0 runner    (1001) docker     (123)     1292 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/domain_urn.py
--rw-r--r--   0 runner    (1001) docker     (123)       98 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/error.py
--rw-r--r--   0 runner    (1001) docker     (123)     1533 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/notebook_urn.py
--rw-r--r--   0 runner    (1001) docker     (123)     1247 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/tag_urn.py
--rw-r--r--   0 runner    (1001) docker     (123)     5447 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/urn.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/
--rw-r--r--   0 runner    (1001) docker     (123)     1073 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      291 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/_airflow_compat.py
--rw-r--r--   0 runner    (1001) docker     (123)      844 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/_airflow_shims.py
--rw-r--r--   0 runner    (1001) docker     (123)     3564 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/_lineage_core.py
--rw-r--r--   0 runner    (1001) docker     (123)    12817 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/_plugin.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/client/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/client/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    19518 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/client/airflow_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)      994 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/entities.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/example_dags/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/example_dags/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1319 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/example_dags/generic_recipe_sample_dag.py
--rw-r--r--   0 runner    (1001) docker     (123)     1348 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/example_dags/lineage_backend_demo.py
--rw-r--r--   0 runner    (1001) docker     (123)     1414 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/example_dags/lineage_backend_taskflow_demo.py
--rw-r--r--   0 runner    (1001) docker     (123)     2283 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/example_dags/lineage_emission_dag.py
--rw-r--r--   0 runner    (1001) docker     (123)     1922 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/example_dags/mysql_sample_dag.py
--rw-r--r--   0 runner    (1001) docker     (123)     3234 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/example_dags/snowflake_sample_dag.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/hooks/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/hooks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6980 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/hooks/datahub.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/lineage/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/lineage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3286 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/lineage/datahub.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-20 14:40:41.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/operators/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/operators/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1848 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/operators/datahub.py
--rw-r--r--   0 runner    (1001) docker     (123)     2900 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/operators/datahub_assertion_operator.py
--rw-r--r--   0 runner    (1001) docker     (123)     2903 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/operators/datahub_assertion_sensor.py
--rw-r--r--   0 runner    (1001) docker     (123)     3338 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/operators/datahub_operation_operator.py
--rw-r--r--   0 runner    (1001) docker     (123)     3606 2023-03-20 14:38:00.000000 acryl-datahub-tc-0.10.0.3/src/datahub_provider/operators/datahub_operation_sensor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/
+-rw-r--r--   0 runner    (1001) docker     (123)    15538 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    10870 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)      928 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (123)     2189 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)    27001 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/acryl_datahub_tc.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)    15538 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/acryl_datahub_tc.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    24526 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/acryl_datahub_tc.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/acryl_datahub_tc.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     6651 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/acryl_datahub_tc.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/acryl_datahub_tc.egg-info/not-zip-safe
+-rw-r--r--   0 runner    (1001) docker     (123)    33655 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/acryl_datahub_tc.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       25 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/acryl_datahub_tc.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      106 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/__main__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/circuit_breaker/
+-rw-r--r--   0 runner    (1001) docker     (123)      268 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/circuit_breaker/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5324 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/circuit_breaker/assertion_circuit_breaker.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1471 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/circuit_breaker/circuit_breaker.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2908 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/circuit_breaker/operation_circuit_breaker.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/corpgroup/
+-rw-r--r--   0 runner    (1001) docker     (123)       63 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/corpgroup/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9873 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/corpgroup/corpgroup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/corpuser/
+-rw-r--r--   0 runner    (1001) docker     (123)       60 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/corpuser/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5889 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/corpuser/corpuser.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/datajob/
+-rw-r--r--   0 runner    (1001) docker     (123)      116 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/datajob/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4265 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/datajob/dataflow.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7492 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/datajob/datajob.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/dataprocess/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/dataprocess/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14547 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/dataprocess/dataprocess_instance.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/graphql/
+-rw-r--r--   0 runner    (1001) docker     (123)      104 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/graphql/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2818 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/graphql/assertion.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1566 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/graphql/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5116 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/api/graphql/operation.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2725 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/check_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24292 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/cli_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16533 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/delete_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7362 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/docker_check.py
+-rw-r--r--   0 runner    (1001) docker     (123)    34107 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/docker_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1431 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/get_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13132 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/ingest_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)      888 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/json_file.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12786 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/lite_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16454 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/migrate.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8936 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/migration_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3067 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/put_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5509 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/quickstart_versioning.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/specific/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/specific/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1275 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/specific/file_loader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1966 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/specific/group_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1874 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/specific/user_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1819 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/state_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)      489 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/telemetry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7352 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/cli/timeline_cli.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/
+-rw-r--r--   0 runner    (1001) docker     (123)      114 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      934 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/_config_enum.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10548 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/common.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3770 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/config_loader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5594 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/git.py
+-rw-r--r--   0 runner    (1001) docker     (123)      369 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/import_resolver.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2105 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/kafka.py
+-rw-r--r--   0 runner    (1001) docker     (123)      386 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/pattern_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)      768 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/pydantic_field_deprecation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2200 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/source_common.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2363 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/time_window_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)      380 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/toml.py
+-rw-r--r--   0 runner    (1001) docker     (123)      688 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/validate_field_removal.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1848 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/validate_field_rename.py
+-rw-r--r--   0 runner    (1001) docker     (123)      867 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/validate_host_port.py
+-rw-r--r--   0 runner    (1001) docker     (123)      301 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/yaml.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      292 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/aspect.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5761 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/kafka_emitter.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14740 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/mce_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8579 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/mcp.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9187 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/mcp_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2523 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/mcp_patch_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)      706 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/request_helper.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11252 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/rest_emitter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3652 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/serialization_helper.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6799 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/entrypoints.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      449 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/closeable.py
+-rw-r--r--   0 runner    (1001) docker     (123)      886 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/committable.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2753 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/common.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3428 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/decorators.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1870 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/ingestion_job_checkpointing_provider_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)      608 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/pipeline_run_listener.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7150 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4752 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/report.py
+-rw-r--r--   0 runner    (1001) docker     (123)      994 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/report_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4503 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/sink.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6074 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/source.py
+-rw-r--r--   0 runner    (1001) docker     (123)      582 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/transform.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3316 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/workunit.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/extractor/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/extractor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      342 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/extractor/extractor_registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1454 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/extractor/json_ref_patch.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24286 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/extractor/json_schema_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3649 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/extractor/mce_extractor.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13147 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/extractor/protobuf_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21985 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/extractor/schema_util.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/glossary/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/glossary/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6189 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/glossary/classification_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2675 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/glossary/classifier.py
+-rw-r--r--   0 runner    (1001) docker     (123)      307 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/glossary/classifier_registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4813 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/glossary/datahub_classifier.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/graph/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/graph/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20384 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/graph/client.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/reporting/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/reporting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8506 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/reporting/datahub_ingestion_run_summary_provider.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1576 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/reporting/file_reporter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      310 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/reporting/reporting_provider_registry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/run/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/run/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1474 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/run/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24179 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/run/pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3920 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/run/pipeline_config.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      557 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/blackhole.py
+-rw-r--r--   0 runner    (1001) docker     (123)      591 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/console.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2838 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/datahub_kafka.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1991 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/datahub_lite.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8138 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/datahub_rest.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2743 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/file.py
+-rw-r--r--   0 runner    (1001) docker     (123)      490 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/sink_registry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8114 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/aws_common.py
+-rw-r--r--   0 runner    (1001) docker     (123)    47697 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/glue.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8485 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/path_spec.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3892 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/s3_boto_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1694 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/s3_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3809 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1554 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/common.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10383 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/feature_groups.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10165 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/job_classes.py
+-rw-r--r--   0 runner    (1001) docker     (123)    35124 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/jobs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9290 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19207 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/models.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/azure/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/azure/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3706 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/azure/azure_common.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    51101 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/bigquery.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24168 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/bigquery_audit.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12893 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/bigquery_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4316 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/bigquery_report.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23505 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/bigquery_schema.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1640 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/common.py
+-rw-r--r--   0 runner    (1001) docker     (123)    28643 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12225 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/profiler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    36592 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/usage.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/common/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/common/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      980 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/common/subtypes.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16833 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/confluent_schema_registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26249 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/csv_enricher.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/dbt/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/dbt/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12796 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/dbt/dbt_cloud.py
+-rw-r--r--   0 runner    (1001) docker     (123)    56247 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/dbt/dbt_common.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17244 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/dbt/dbt_core.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/delta_lake/
+-rw-r--r--   0 runner    (1001) docker     (123)       71 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/delta_lake/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3342 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/delta_lake/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2006 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/delta_lake/delta_lake_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)      467 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/delta_lake/report.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12467 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/delta_lake/source.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1228 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/demo_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17398 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/elastic_search.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14671 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/feast.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16770 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/file.py
+-rw-r--r--   0 runner    (1001) docker     (123)    44577 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/ge_data_profiler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8830 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/ge_profiling_config.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/git/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/git/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2563 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/git/git_import.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2010 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/glue_profiling_config.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/iceberg/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/iceberg/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19281 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/iceberg/iceberg.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7823 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/iceberg/iceberg_common.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9563 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/iceberg/iceberg_profiler.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/identity/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/identity/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29532 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/identity/azure_ad.py
+-rw-r--r--   0 runner    (1001) docker     (123)    32064 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/identity/okta.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17692 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/kafka.py
+-rw-r--r--   0 runner    (1001) docker     (123)    46297 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/kafka_connect.py
+-rw-r--r--   0 runner    (1001) docker     (123)      321 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/kafka_schema_registry_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17596 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/ldap.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    43834 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/looker_common.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7480 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/looker_lib_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2202 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/looker_query_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)    53361 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/looker_source.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24029 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/looker_usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    81876 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/lookml_source.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22826 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/metabase.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/metadata/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/metadata/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16789 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/metadata/business_glossary.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6717 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/metadata/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    30160 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/mode.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17143 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/mongodb.py
+-rw-r--r--   0 runner    (1001) docker     (123)    43231 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/nifi.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    13322 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/openapi.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    13526 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/openapi_parser.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/
+-rw-r--r--   0 runner    (1001) docker     (123)       76 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13454 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2265 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/dataplatform_instance_resolver.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1147 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/data_classes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1578 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/native_sql_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3204 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29227 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/resolver.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6111 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/tree_function.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1052 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/validator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17764 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/powerbi-lexical-grammar.rule
+-rw-r--r--   0 runner    (1001) docker     (123)    35205 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/powerbi.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/rest_api_wrapper/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/rest_api_wrapper/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4290 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/rest_api_wrapper/data_classes.py
+-rw-r--r--   0 runner    (1001) docker     (123)    27836 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/rest_api_wrapper/data_resolver.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13689 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/rest_api_wrapper/powerbi_api.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi_report_server/
+-rw-r--r--   0 runner    (1001) docker     (123)      324 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi_report_server/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3689 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi_report_server/constants.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20049 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi_report_server/report_server.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11684 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi_report_server/report_server_domain.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/profiling/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/profiling/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1426 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/profiling/common.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20413 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/pulsar.py
+-rw-r--r--   0 runner    (1001) docker     (123)    32287 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redash.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      362 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/common.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5163 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17491 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5607 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/profile.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17388 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/query.py
+-rw-r--r--   0 runner    (1001) docker     (123)    34186 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/redshift.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12416 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/redshift_schema.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1389 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/report.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15002 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/usage.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/s3/
+-rw-r--r--   0 runner    (1001) docker     (123)       56 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/s3/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5167 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/s3/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4126 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/s3/data_lake_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20777 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/s3/profiling.py
+-rw-r--r--   0 runner    (1001) docker     (123)      466 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/s3/report.py
+-rw-r--r--   0 runner    (1001) docker     (123)    32051 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/s3/source.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sagemaker_processors/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sagemaker_processors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    27170 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/salesforce.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15812 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema/json_schema.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      563 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/avro.py
+-rw-r--r--   0 runner    (1001) docker     (123)      379 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2229 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/csv_tsv.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2103 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/json.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5760 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/object.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3426 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/parquet.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1581 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/constants.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7116 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29016 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_lineage_legacy.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21942 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_lineage_v2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6966 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_profiler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    33575 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_query.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3665 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_report.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19448 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_schema.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6141 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_tag.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18561 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_usage_v2.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10596 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    62420 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_v2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1313 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/source_registry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9739 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/athena.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25177 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/clickhouse.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2346 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/druid.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1342 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/hana.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6454 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/hive.py
+-rw-r--r--   0 runner    (1001) docker     (123)      737 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/mariadb.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10973 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/mssql.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2650 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/mysql.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2316 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/oauth_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6923 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/oracle.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10476 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/postgres.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3606 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/presto.py
+-rw-r--r--   0 runner    (1001) docker     (123)    33112 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/presto_on_hive.py
+-rw-r--r--   0 runner    (1001) docker     (123)    45775 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/redshift.py
+-rw-r--r--   0 runner    (1001) docker     (123)    44223 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/sql_common.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7105 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/sql_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2731 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/sql_generic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6881 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/sql_generic_profiler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11475 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/sql_types.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7629 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/sql_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10551 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/trino.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4936 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/two_tier_sql_source.py
+-rw-r--r--   0 runner    (1001) docker     (123)    53109 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/vertica.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8797 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4220 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/entity_removal_state.py
+-rw-r--r--   0 runner    (1001) docker     (123)      512 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/profiling_state.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4209 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/profiling_state_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5544 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/redundant_run_skip_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)      143 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/sql_common_state.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13129 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/stale_entity_removal_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15642 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/stateful_ingestion_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)      463 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/usage_common_state.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1271 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/use_case_handler.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state_provider/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state_provider/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5226 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state_provider/datahub_ingestion_checkpointing_provider.py
+-rw-r--r--   0 runner    (1001) docker     (123)      401 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state_provider/state_provider_registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14641 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/superset.py
+-rw-r--r--   0 runner    (1001) docker     (123)    91756 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/tableau.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15651 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/tableau_common.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2284 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/tableau_constant.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/unity/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/unity/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3164 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/unity/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11625 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/unity/proxy.py
+-rw-r--r--   0 runner    (1001) docker     (123)      585 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/unity/report.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19523 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/unity/source.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/usage/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/usage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9827 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/usage/clickhouse_usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15306 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/usage/redshift_usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10155 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/usage/starburst_trino_usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6281 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/usage/usage_common.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1654 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/bigquery.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1688 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/csv_enricher.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5615 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/pulsar.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/sql/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/sql/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16134 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/sql/snowflake.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/usage/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/usage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7702 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/usage/bigquery_usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)      565 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/usage/snowflake_usage.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1037 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/pulsar.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/sql/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/sql/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1864 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/sql/bigquery.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1331 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/sql/snowflake.py
+-rw-r--r--   0 runner    (1001) docker     (123)      229 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/time_window.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/usage/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/usage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1374 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/usage/bigquery_usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)      780 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/usage/snowflake_usage.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3420 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_browse_path.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6849 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_ownership.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5605 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_properties.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5664 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_schema_tags.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6239 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_schema_terms.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4911 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_tags.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5707 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_terms.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10623 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/base_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6241 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/dataset_domain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1598 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/dataset_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1323 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/mark_dataset_status.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1218 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/remove_dataset_ownership.py
+-rw-r--r--   0 runner    (1001) docker     (123)      251 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/transform_registry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/integrations/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/integrations/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/integrations/great_expectations/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/integrations/great_expectations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    34843 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/integrations/great_expectations/action.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/lite/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/lite/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    32549 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/lite/duckdb_lite.py
+-rw-r--r--   0 runner    (1001) docker     (123)      157 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/lite/duckdb_lite_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2846 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/lite/lite_local.py
+-rw-r--r--   0 runner    (1001) docker     (123)      286 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/lite/lite_registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1949 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/lite/lite_server.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4503 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/lite/lite_util.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/
+-rw-r--r--   0 runner    (1001) docker     (123)      197 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/events/
+-rw-r--r--   0 runner    (1001) docker     (123)      257 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/events/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/access/
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/access/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/access/token/
+-rw-r--r--   0 runner    (1001) docker     (123)      277 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/access/token/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/assertion/
+-rw-r--r--   0 runner    (1001) docker     (123)     1589 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/assertion/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/chart/
+-rw-r--r--   0 runner    (1001) docker     (123)      807 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/chart/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/common/
+-rw-r--r--   0 runner    (1001) docker     (123)     3456 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/common/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/common/fieldtransformer/
+-rw-r--r--   0 runner    (1001) docker     (123)      355 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/common/fieldtransformer/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/container/
+-rw-r--r--   0 runner    (1001) docker     (123)      469 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/container/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dashboard/
+-rw-r--r--   0 runner    (1001) docker     (123)      615 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dashboard/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/
+-rw-r--r--   0 runner    (1001) docker     (123)      828 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/azkaban/
+-rw-r--r--   0 runner    (1001) docker     (123)      253 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/azkaban/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/datahub/
+-rw-r--r--   0 runner    (1001) docker     (123)      535 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/datahub/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dataplatform/
+-rw-r--r--   0 runner    (1001) docker     (123)      341 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dataplatform/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dataplatforminstance/
+-rw-r--r--   0 runner    (1001) docker     (123)      300 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dataplatforminstance/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dataprocess/
+-rw-r--r--   0 runner    (1001) docker     (123)     1317 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dataprocess/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dataset/
+-rw-r--r--   0 runner    (1001) docker     (123)     2204 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dataset/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/domain/
+-rw-r--r--   0 runner    (1001) docker     (123)      326 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/domain/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/events/
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/events/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/events/metadata/
+-rw-r--r--   0 runner    (1001) docker     (123)      241 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/events/metadata/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/execution/
+-rw-r--r--   0 runner    (1001) docker     (123)      734 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/execution/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/glossary/
+-rw-r--r--   0 runner    (1001) docker     (123)      460 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/glossary/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/identity/
+-rw-r--r--   0 runner    (1001) docker     (123)     1443 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/identity/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/ingestion/
+-rw-r--r--   0 runner    (1001) docker     (123)      556 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/ingestion/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/key/
+-rw-r--r--   0 runner    (1001) docker     (123)     3859 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/key/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/query/
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/query/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/query/filter/
+-rw-r--r--   0 runner    (1001) docker     (123)      491 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/query/filter/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/snapshot/
+-rw-r--r--   0 runner    (1001) docker     (123)     2317 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/snapshot/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/ml/
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/ml/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/ml/metadata/
+-rw-r--r--   0 runner    (1001) docker     (123)     3151 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/ml/metadata/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/mxe/
+-rw-r--r--   0 runner    (1001) docker     (123)      932 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/mxe/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/notebook/
+-rw-r--r--   0 runner    (1001) docker     (123)      860 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/notebook/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/platform/
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/platform/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/platform/event/
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/platform/event/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/platform/event/v1/
+-rw-r--r--   0 runner    (1001) docker     (123)      342 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/platform/event/v1/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/policy/
+-rw-r--r--   0 runner    (1001) docker     (123)      876 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/policy/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/post/
+-rw-r--r--   0 runner    (1001) docker     (123)      477 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/post/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/query/
+-rw-r--r--   0 runner    (1001) docker     (123)      679 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/query/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/retention/
+-rw-r--r--   0 runner    (1001) docker     (123)      561 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/retention/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/schema/
+-rw-r--r--   0 runner    (1001) docker     (123)     2822 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/schema/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/secret/
+-rw-r--r--   0 runner    (1001) docker     (123)      264 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/secret/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/settings/
+-rw-r--r--   0 runner    (1001) docker     (123)      161 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/settings/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/settings/global/
+-rw-r--r--   0 runner    (1001) docker     (123)      370 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/settings/global/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/step/
+-rw-r--r--   0 runner    (1001) docker     (123)      288 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/step/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/tag/
+-rw-r--r--   0 runner    (1001) docker     (123)      249 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/tag/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/telemetry/
+-rw-r--r--   0 runner    (1001) docker     (123)      261 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/telemetry/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/test/
+-rw-r--r--   0 runner    (1001) docker     (123)      670 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/test/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/timeseries/
+-rw-r--r--   0 runner    (1001) docker     (123)      596 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/timeseries/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/upgrade/
+-rw-r--r--   0 runner    (1001) docker     (123)      380 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/upgrade/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/usage/
+-rw-r--r--   0 runner    (1001) docker     (123)      561 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/usage/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/view/
+-rw-r--r--   0 runner    (1001) docker     (123)      457 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/view/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)   421524 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/schema.avsc
+-rw-r--r--   0 runner    (1001) docker     (123)   788555 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/schema_classes.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/schemas/
+-rw-r--r--   0 runner    (1001) docker     (123)   333312 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/schemas/MetadataChangeEvent.avsc
+-rw-r--r--   0 runner    (1001) docker     (123)     8438 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/schemas/MetadataChangeProposal.avsc
+-rw-r--r--   0 runner    (1001) docker     (123)      540 2023-05-31 19:14:26.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/schemas/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/py.typed
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/specific/
+-rw-r--r--   0 runner    (1001) docker     (123)       88 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/specific/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      994 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/specific/custom_properties.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7642 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/specific/dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/telemetry/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/telemetry/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      967 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/telemetry/stats.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11734 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/telemetry/telemetry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/upgrade/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/upgrade/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15375 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/upgrade/upgrade.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      444 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/_markupsafe_compat.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3147 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/bigquery_sql_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1138 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/checkpoint_state_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)      459 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/config_clean.py
+-rw-r--r--   0 runner    (1001) docker     (123)      468 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/dedup_list.py
+-rw-r--r--   0 runner    (1001) docker     (123)      645 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/delayed_iter.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15335 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/file_backed_collections.py
+-rw-r--r--   0 runner    (1001) docker     (123)      261 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/global_warning_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10998 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/hive_schema_to_avro.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6609 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/logging_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4614 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/lossy_collections.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10590 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1564 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/memory_footprint.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3933 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/memory_leak_detector.py
+-rw-r--r--   0 runner    (1001) docker     (123)      598 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/parsing_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1097 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/perf_timer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/registries/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/registries/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2452 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/registries/domain_registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)      766 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sample_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)      698 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/server_config_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4779 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/source_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (123)      871 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sql_formatter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6522 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sql_lineage_parser_impl.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5814 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sql_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)      456 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sql_parser_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14786 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sqlalchemy_query_combiner.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1990 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sqllineage_patch.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1390 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/stats_collections.py
+-rw-r--r--   0 runner    (1001) docker     (123)      601 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/tee_io.py
+-rw-r--r--   0 runner    (1001) docker     (123)      358 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/time.py
+-rw-r--r--   0 runner    (1001) docker     (123)      970 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/type_annotations.py
+-rw-r--r--   0 runner    (1001) docker     (123)      172 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/url_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)      984 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urn_encoder.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1317 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/corp_group_urn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1307 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/corpuser_urn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2847 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/data_flow_urn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1744 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/data_job_urn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1097 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/data_platform_urn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1587 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/data_process_instance_urn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3877 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/dataset_urn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1292 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/domain_urn.py
+-rw-r--r--   0 runner    (1001) docker     (123)       98 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/error.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1533 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/notebook_urn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1247 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/tag_urn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5447 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/urn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4176 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/urn_iter.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/
+-rw-r--r--   0 runner    (1001) docker     (123)     1073 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      291 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/_airflow_compat.py
+-rw-r--r--   0 runner    (1001) docker     (123)      844 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/_airflow_shims.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3564 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/_lineage_core.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12902 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/_plugin.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/client/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/client/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19475 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/client/airflow_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)      994 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/entities.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1319 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/generic_recipe_sample_dag.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1348 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/lineage_backend_demo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1414 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/lineage_backend_taskflow_demo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2283 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/lineage_emission_dag.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1922 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/mysql_sample_dag.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3234 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/snowflake_sample_dag.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/hooks/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/hooks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6980 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/hooks/datahub.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/lineage/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/lineage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3286 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/lineage/datahub.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 19:14:29.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/operators/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/operators/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1848 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/operators/datahub.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2900 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/operators/datahub_assertion_operator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2903 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/operators/datahub_assertion_sensor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3338 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/operators/datahub_operation_operator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3606 2023-05-31 19:12:13.000000 acryl-datahub-tc-0.10.2rc1/src/datahub_provider/operators/datahub_operation_sensor.py
```

### Comparing `acryl-datahub-tc-0.10.0.3/PKG-INFO` & `acryl-datahub-tc-0.10.2rc1/PKG-INFO`

 * *Files 4% similar despite different names*

```diff
@@ -1,23 +1,23 @@
 Metadata-Version: 2.1
 Name: acryl-datahub-tc
-Version: 0.10.0.3
+Version: 0.10.2rc1
 Summary: A CLI to work with DataHub metadata
 Home-page: https://datahubproject.io/
 License: Apache License 2.0
 Project-URL: Documentation, https://datahubproject.io/docs/
 Project-URL: Source, https://github.com/datahub-project/datahub
 Project-URL: Changelog, https://github.com/datahub-project/datahub/releases
 Description: # Introduction to Metadata Ingestion
         
         ## Integration Options
         
-        DataHub supports both **push-based** and **pull-based** metadata integration. 
+        DataHub supports both **push-based** and **pull-based** metadata integration.
         
-        Push-based integrations allow you to emit metadata directly from your data systems when metadata changes, while pull-based integrations allow you to "crawl" or "ingest" metadata from the data systems by connecting to them and extracting metadata in a batch or incremental-batch manner. Supporting both mechanisms means that you can integrate with all your systems in the most flexible way possible. 
+        Push-based integrations allow you to emit metadata directly from your data systems when metadata changes, while pull-based integrations allow you to "crawl" or "ingest" metadata from the data systems by connecting to them and extracting metadata in a batch or incremental-batch manner. Supporting both mechanisms means that you can integrate with all your systems in the most flexible way possible.
         
         Examples of push-based integrations include [Airflow](../docs/lineage/airflow.md), [Spark](../metadata-integration/java/spark-lineage/README.md), [Great Expectations](./integration_docs/great-expectations.md) and [Protobuf Schemas](../metadata-integration/java/datahub-protobuf/README.md). This allows you to get low-latency metadata integration from the "active" agents in your data ecosystem. Examples of pull-based integrations include BigQuery, Snowflake, Looker, Tableau and many others.
         
         This document describes the pull-based metadata ingestion system that is built into DataHub for easy integration with a wide variety of sources in your data stack.
         
         ## Getting Started
         
@@ -25,72 +25,76 @@
         
         Before running any metadata ingestion job, you should make sure that DataHub backend services are all running. You can either run ingestion via the [UI](../docs/ui-ingestion.md) or via the [CLI](../docs/cli.md). You can reference the CLI usage guide given there as you go through this page.
         
         ## Core Concepts
         
         ### Sources
         
+        Please see our [Integrations page](https://datahubproject.io/integrations) to browse our ingestion sources and filter on their features.
+        
         Data systems that we are extracting metadata from are referred to as **Sources**. The `Sources` tab on the left in the sidebar shows you all the sources that are available for you to ingest metadata from. For example, we have sources for [BigQuery](https://datahubproject.io/docs/generated/ingestion/sources/bigquery), [Looker](https://datahubproject.io/docs/generated/ingestion/sources/looker), [Tableau](https://datahubproject.io/docs/generated/ingestion/sources/tableau) and many others.
         
         #### Metadata Ingestion Source Status
         
         We apply a Support Status to each Metadata Source to help you understand the integration reliability at a glance.
         
         ![Certified](https://img.shields.io/badge/support%20status-certified-brightgreen): Certified Sources are well-tested & widely-adopted by the DataHub Community. We expect the integration to be stable with few user-facing issues.
         
         ![Incubating](https://img.shields.io/badge/support%20status-incubating-blue): Incubating Sources are ready for DataHub Community adoption but have not been tested for a wide variety of edge-cases. We eagerly solicit feedback from the Community to streghten the connector; minor version changes may arise in future releases.
         
-        ![Testing](https://img.shields.io/badge/support%20status-testing-lightgrey): Testing Sources are available for experiementation by DataHub Community members, but may change without notice. 
+        ![Testing](https://img.shields.io/badge/support%20status-testing-lightgrey): Testing Sources are available for experiementation by DataHub Community members, but may change without notice.
         
         ### Sinks
         
         Sinks are destinations for metadata. When configuring ingestion for DataHub, you're likely to be sending the metadata to DataHub over either the [REST (datahub-sink)](./sink_docs/datahub.md#datahub-rest) or the [Kafka (datahub-kafka)](./sink_docs/datahub.md#datahub-kafka) sink. In some cases, the [File](./sink_docs/file.md) sink is also helpful to store a persistent offline copy of the metadata during debugging.
         
         The default sink that most of the ingestion systems and guides assume is the `datahub-rest` sink, but you should be able to adapt all of them for the other sinks as well!
         
         ### Recipes
         
         A recipe is the main configuration file that puts it all together. It tells our ingestion scripts where to pull data from (source) and where to put it (sink).
         
         :::tip
-        Name your recipe with **.dhub.yaml** extension like *myrecipe.dhub.yaml* to use vscode or intellij as a recipe editor with autocomplete
+        Name your recipe with **.dhub.yaml** extension like _myrecipe.dhub.yaml_ to use vscode or intellij as a recipe editor with autocomplete
         and syntax validation.
         
         Make sure yaml plugin is installed for your editor:
+        
         - For vscode install [Redhat's yaml plugin](https://marketplace.visualstudio.com/items?itemName=redhat.vscode-yaml)
-        - For intellij install [official yaml plugin](https://plugins.jetbrains.com/plugin/13126-yaml
-        )
+        - For intellij install [official yaml plugin](https://plugins.jetbrains.com/plugin/13126-yaml)
         
         :::
         
         Since `acryl-datahub` version `>=0.8.33.2`, the default sink is assumed to be a DataHub REST endpoint:
+        
         - Hosted at "http://localhost:8080" or the environment variable `${DATAHUB_GMS_URL}` if present
-        - With an empty auth token or the environment variable `${DATAHUB_GMS_TOKEN}` if present. 
+        - With an empty auth token or the environment variable `${DATAHUB_GMS_TOKEN}` if present.
         
         Here's a simple recipe that pulls metadata from MSSQL (source) and puts it into the default sink (datahub rest).
         
         ```yaml
         # The simplest recipe that pulls metadata from MSSQL and puts it into DataHub
         # using the Rest API.
         source:
           type: mssql
           config:
             username: sa
             password: ${MSSQL_PASSWORD}
             database: DemoData
-        
         # sink section omitted as we want to use the default datahub-rest sink
         ```
         
         Running this recipe is as simple as:
+        
         ```shell
         datahub ingest -c recipe.dhub.yaml
         ```
         
         or if you want to override the default endpoints, you can provide the environment variables as part of the command like below:
+        
         ```shell
         DATAHUB_GMS_URL="https://my-datahub-server:8080" DATAHUB_GMS_TOKEN="my-datahub-token" datahub ingest -c recipe.dhub.yaml
         ```
         
         A number of recipes are included in the [examples/recipes](./examples/recipes) directory. For full info and context on each source and sink, see the pages described in the [table of plugins](../docs/cli.md#installing-plugins).
         
         > Note that one recipe file can only have 1 source and 1 sink. If you want multiple sources then you will need multiple recipe files.
@@ -143,66 +147,72 @@
         # Running ingestion with reporting to DataHub turned off
         datahub ingest -c ./examples/recipes/example_to_datahub_rest.dhub.yaml --no-default-report
         ```
         
         The reports include the recipe that was used for ingestion. This can be turned off by adding an additional section to the ingestion recipe.
         
         ```yaml
-        
         source:
-           # source configs
+          # source configs
         
         sink:
-           # sink configs
+          # sink configs
         
         # Add configuration for the datahub reporter
         reporting:
           - type: datahub
             config:
               report_recipe: false
         ```
         
-        
         ## Transformations
         
         If you'd like to modify data before it reaches the ingestion sinks  for instance, adding additional owners or tags  you can use a transformer to write your own module and integrate it with DataHub. Transformers require extending the recipe with a new section to describe the transformers that you want to run.
         
         For example, a pipeline that ingests metadata from MSSQL and applies a default "important" tag to all datasets is described below:
+        
         ```yaml
         # A recipe to ingest metadata from MSSQL and apply default tags to all tables
         source:
           type: mssql
           config:
             username: sa
             password: ${MSSQL_PASSWORD}
             database: DemoData
         
         transformers: # an array of transformers applied sequentially
           - type: simple_add_dataset_tags
             config:
               tag_urns:
                 - "urn:li:tag:Important"
-        
         # default sink, no config needed
         ```
         
         Check out the [transformers guide](./docs/transformer/intro.md) to learn more about how you can create really flexible pipelines for processing metadata using Transformers!
         
         ## Using as a library (SDK)
         
-        In some cases, you might want to construct Metadata events directly and use programmatic ways to emit that metadata to DataHub. In this case, take a look at the [Python emitter](./as-a-library.md) and the [Java emitter](../metadata-integration/java/as-a-library.md) libraries which can be called from your own code. 
+        In some cases, you might want to construct Metadata events directly and use programmatic ways to emit that metadata to DataHub. In this case, take a look at the [Python emitter](./as-a-library.md) and the [Java emitter](../metadata-integration/java/as-a-library.md) libraries which can be called from your own code.
         
         ### Programmatic Pipeline
+        
         In some cases, you might want to configure and run a pipeline entirely from within your custom Python script. Here is an example of how to do it.
-         - [programmatic_pipeline.py](./examples/library/programatic_pipeline.py) - a basic mysql to REST programmatic pipeline.
+        
+        - [programmatic_pipeline.py](./examples/library/programatic_pipeline.py) - a basic mysql to REST programmatic pipeline.
         
         ## Developing
         
         See the guides on [developing](./developing.md), [adding a source](./adding-source.md) and [using transformers](./docs/transformer/intro.md).
         
+        ## Compatibility
+        
+        DataHub server uses a 3 digit versioning scheme, while the CLI uses a 4 digit scheme. For example, if you're using DataHub server version 0.10.0, you should use CLI version 0.10.0.x, where x is a patch version.
+        We do this because we do CLI releases at a much higher frequency than server releases, usually every few days vs twice a month.
+        
+        For ingestion sources, any breaking changes will be highlighted in the [release notes](../docs/how/updating-datahub.md). When fields are deprecated or otherwise changed, we will try to maintain backwards compatibility for two server releases, which is about 4-6 weeks. The CLI will also print warnings whenever deprecated options are used.
         
 Platform: UNKNOWN
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Programming Language :: Python
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3 :: Only
 Classifier: Programming Language :: Python :: 3.7
@@ -242,14 +252,15 @@
 Provides-Extra: druid
 Provides-Extra: elasticsearch
 Provides-Extra: feast
 Provides-Extra: glue
 Provides-Extra: hana
 Provides-Extra: hive
 Provides-Extra: iceberg
+Provides-Extra: json-schema
 Provides-Extra: kafka
 Provides-Extra: kafka-connect
 Provides-Extra: ldap
 Provides-Extra: looker
 Provides-Extra: lookml
 Provides-Extra: metabase
 Provides-Extra: mode
@@ -262,15 +273,16 @@
 Provides-Extra: oracle
 Provides-Extra: postgres
 Provides-Extra: presto
 Provides-Extra: presto-on-hive
 Provides-Extra: pulsar
 Provides-Extra: redash
 Provides-Extra: redshift
-Provides-Extra: redshift-usage
+Provides-Extra: redshift-legacy
+Provides-Extra: redshift-usage-legacy
 Provides-Extra: s3
 Provides-Extra: sagemaker
 Provides-Extra: salesforce
 Provides-Extra: snowflake
 Provides-Extra: snowflake-beta
 Provides-Extra: sqlalchemy
 Provides-Extra: superset
```

### Comparing `acryl-datahub-tc-0.10.0.3/README.md` & `acryl-datahub-tc-0.10.2rc1/README.md`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # Introduction to Metadata Ingestion
 
 ## Integration Options
 
-DataHub supports both **push-based** and **pull-based** metadata integration. 
+DataHub supports both **push-based** and **pull-based** metadata integration.
 
-Push-based integrations allow you to emit metadata directly from your data systems when metadata changes, while pull-based integrations allow you to "crawl" or "ingest" metadata from the data systems by connecting to them and extracting metadata in a batch or incremental-batch manner. Supporting both mechanisms means that you can integrate with all your systems in the most flexible way possible. 
+Push-based integrations allow you to emit metadata directly from your data systems when metadata changes, while pull-based integrations allow you to "crawl" or "ingest" metadata from the data systems by connecting to them and extracting metadata in a batch or incremental-batch manner. Supporting both mechanisms means that you can integrate with all your systems in the most flexible way possible.
 
 Examples of push-based integrations include [Airflow](../docs/lineage/airflow.md), [Spark](../metadata-integration/java/spark-lineage/README.md), [Great Expectations](./integration_docs/great-expectations.md) and [Protobuf Schemas](../metadata-integration/java/datahub-protobuf/README.md). This allows you to get low-latency metadata integration from the "active" agents in your data ecosystem. Examples of pull-based integrations include BigQuery, Snowflake, Looker, Tableau and many others.
 
 This document describes the pull-based metadata ingestion system that is built into DataHub for easy integration with a wide variety of sources in your data stack.
 
 ## Getting Started
 
@@ -16,72 +16,76 @@
 
 Before running any metadata ingestion job, you should make sure that DataHub backend services are all running. You can either run ingestion via the [UI](../docs/ui-ingestion.md) or via the [CLI](../docs/cli.md). You can reference the CLI usage guide given there as you go through this page.
 
 ## Core Concepts
 
 ### Sources
 
+Please see our [Integrations page](https://datahubproject.io/integrations) to browse our ingestion sources and filter on their features.
+
 Data systems that we are extracting metadata from are referred to as **Sources**. The `Sources` tab on the left in the sidebar shows you all the sources that are available for you to ingest metadata from. For example, we have sources for [BigQuery](https://datahubproject.io/docs/generated/ingestion/sources/bigquery), [Looker](https://datahubproject.io/docs/generated/ingestion/sources/looker), [Tableau](https://datahubproject.io/docs/generated/ingestion/sources/tableau) and many others.
 
 #### Metadata Ingestion Source Status
 
 We apply a Support Status to each Metadata Source to help you understand the integration reliability at a glance.
 
 ![Certified](https://img.shields.io/badge/support%20status-certified-brightgreen): Certified Sources are well-tested & widely-adopted by the DataHub Community. We expect the integration to be stable with few user-facing issues.
 
 ![Incubating](https://img.shields.io/badge/support%20status-incubating-blue): Incubating Sources are ready for DataHub Community adoption but have not been tested for a wide variety of edge-cases. We eagerly solicit feedback from the Community to streghten the connector; minor version changes may arise in future releases.
 
-![Testing](https://img.shields.io/badge/support%20status-testing-lightgrey): Testing Sources are available for experiementation by DataHub Community members, but may change without notice. 
+![Testing](https://img.shields.io/badge/support%20status-testing-lightgrey): Testing Sources are available for experiementation by DataHub Community members, but may change without notice.
 
 ### Sinks
 
 Sinks are destinations for metadata. When configuring ingestion for DataHub, you're likely to be sending the metadata to DataHub over either the [REST (datahub-sink)](./sink_docs/datahub.md#datahub-rest) or the [Kafka (datahub-kafka)](./sink_docs/datahub.md#datahub-kafka) sink. In some cases, the [File](./sink_docs/file.md) sink is also helpful to store a persistent offline copy of the metadata during debugging.
 
 The default sink that most of the ingestion systems and guides assume is the `datahub-rest` sink, but you should be able to adapt all of them for the other sinks as well!
 
 ### Recipes
 
 A recipe is the main configuration file that puts it all together. It tells our ingestion scripts where to pull data from (source) and where to put it (sink).
 
 :::tip
-Name your recipe with **.dhub.yaml** extension like *myrecipe.dhub.yaml* to use vscode or intellij as a recipe editor with autocomplete
+Name your recipe with **.dhub.yaml** extension like _myrecipe.dhub.yaml_ to use vscode or intellij as a recipe editor with autocomplete
 and syntax validation.
 
 Make sure yaml plugin is installed for your editor:
+
 - For vscode install [Redhat's yaml plugin](https://marketplace.visualstudio.com/items?itemName=redhat.vscode-yaml)
-- For intellij install [official yaml plugin](https://plugins.jetbrains.com/plugin/13126-yaml
-)
+- For intellij install [official yaml plugin](https://plugins.jetbrains.com/plugin/13126-yaml)
 
 :::
 
 Since `acryl-datahub` version `>=0.8.33.2`, the default sink is assumed to be a DataHub REST endpoint:
+
 - Hosted at "http://localhost:8080" or the environment variable `${DATAHUB_GMS_URL}` if present
-- With an empty auth token or the environment variable `${DATAHUB_GMS_TOKEN}` if present. 
+- With an empty auth token or the environment variable `${DATAHUB_GMS_TOKEN}` if present.
 
 Here's a simple recipe that pulls metadata from MSSQL (source) and puts it into the default sink (datahub rest).
 
 ```yaml
 # The simplest recipe that pulls metadata from MSSQL and puts it into DataHub
 # using the Rest API.
 source:
   type: mssql
   config:
     username: sa
     password: ${MSSQL_PASSWORD}
     database: DemoData
-
 # sink section omitted as we want to use the default datahub-rest sink
 ```
 
 Running this recipe is as simple as:
+
 ```shell
 datahub ingest -c recipe.dhub.yaml
 ```
 
 or if you want to override the default endpoints, you can provide the environment variables as part of the command like below:
+
 ```shell
 DATAHUB_GMS_URL="https://my-datahub-server:8080" DATAHUB_GMS_TOKEN="my-datahub-token" datahub ingest -c recipe.dhub.yaml
 ```
 
 A number of recipes are included in the [examples/recipes](./examples/recipes) directory. For full info and context on each source and sink, see the pages described in the [table of plugins](../docs/cli.md#installing-plugins).
 
 > Note that one recipe file can only have 1 source and 1 sink. If you want multiple sources then you will need multiple recipe files.
@@ -134,59 +138,65 @@
 # Running ingestion with reporting to DataHub turned off
 datahub ingest -c ./examples/recipes/example_to_datahub_rest.dhub.yaml --no-default-report
 ```
 
 The reports include the recipe that was used for ingestion. This can be turned off by adding an additional section to the ingestion recipe.
 
 ```yaml
-
 source:
-   # source configs
+  # source configs
 
 sink:
-   # sink configs
+  # sink configs
 
 # Add configuration for the datahub reporter
 reporting:
   - type: datahub
     config:
       report_recipe: false
 ```
 
-
 ## Transformations
 
 If you'd like to modify data before it reaches the ingestion sinks  for instance, adding additional owners or tags  you can use a transformer to write your own module and integrate it with DataHub. Transformers require extending the recipe with a new section to describe the transformers that you want to run.
 
 For example, a pipeline that ingests metadata from MSSQL and applies a default "important" tag to all datasets is described below:
+
 ```yaml
 # A recipe to ingest metadata from MSSQL and apply default tags to all tables
 source:
   type: mssql
   config:
     username: sa
     password: ${MSSQL_PASSWORD}
     database: DemoData
 
 transformers: # an array of transformers applied sequentially
   - type: simple_add_dataset_tags
     config:
       tag_urns:
         - "urn:li:tag:Important"
-
 # default sink, no config needed
 ```
 
 Check out the [transformers guide](./docs/transformer/intro.md) to learn more about how you can create really flexible pipelines for processing metadata using Transformers!
 
 ## Using as a library (SDK)
 
-In some cases, you might want to construct Metadata events directly and use programmatic ways to emit that metadata to DataHub. In this case, take a look at the [Python emitter](./as-a-library.md) and the [Java emitter](../metadata-integration/java/as-a-library.md) libraries which can be called from your own code. 
+In some cases, you might want to construct Metadata events directly and use programmatic ways to emit that metadata to DataHub. In this case, take a look at the [Python emitter](./as-a-library.md) and the [Java emitter](../metadata-integration/java/as-a-library.md) libraries which can be called from your own code.
 
 ### Programmatic Pipeline
+
 In some cases, you might want to configure and run a pipeline entirely from within your custom Python script. Here is an example of how to do it.
- - [programmatic_pipeline.py](./examples/library/programatic_pipeline.py) - a basic mysql to REST programmatic pipeline.
+
+- [programmatic_pipeline.py](./examples/library/programatic_pipeline.py) - a basic mysql to REST programmatic pipeline.
 
 ## Developing
 
 See the guides on [developing](./developing.md), [adding a source](./adding-source.md) and [using transformers](./docs/transformer/intro.md).
 
+## Compatibility
+
+DataHub server uses a 3 digit versioning scheme, while the CLI uses a 4 digit scheme. For example, if you're using DataHub server version 0.10.0, you should use CLI version 0.10.0.x, where x is a patch version.
+We do this because we do CLI releases at a much higher frequency than server releases, usually every few days vs twice a month.
+
+For ingestion sources, any breaking changes will be highlighted in the [release notes](../docs/how/updating-datahub.md). When fields are deprecated or otherwise changed, we will try to maintain backwards compatibility for two server releases, which is about 4-6 weeks. The CLI will also print warnings whenever deprecated options are used.
```

### Comparing `acryl-datahub-tc-0.10.0.3/pyproject.toml` & `acryl-datahub-tc-0.10.2rc1/pyproject.toml`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/setup.cfg` & `acryl-datahub-tc-0.10.2rc1/setup.cfg`

 * *Files 22% similar despite different names*

```diff
@@ -1,32 +1,35 @@
 [flake8]
 max-complexity = 20
 ignore = 
 	E501,
 	D203,
 	W503,
-	E203
+	E203,
+	B019,
+	B008,
+	B006,
+	B007
 exclude = 
 	.git,
 	src/datahub/metadata,
 	venv,
 	.tox,
 	__pycache__
 per-file-ignores = 
 	__init__.py: F401
 ban-relative-imports = true
 
 [mypy]
 plugins = 
 	./tests/test_helpers/sqlalchemy_mypy_plugin.py,
 	pydantic.mypy
-exclude = ^(venv|build|dist)/
+exclude = ^(venv/|build/|dist/|examples/transforms/setup.py)
 ignore_missing_imports = yes
 namespace_packages = no
-implicit_optional = no
 strict_optional = yes
 check_untyped_defs = yes
 disallow_incomplete_defs = yes
 disallow_untyped_decorators = yes
 warn_unused_configs = yes
 disallow_untyped_defs = no
 
@@ -55,24 +58,26 @@
 disallow_untyped_defs = yes
 
 [mypy-datahub.utilities.*]
 disallow_untyped_defs = yes
 
 [tool:pytest]
 asyncio_mode = auto
-addopts = --cov=src --cov-report term-missing --cov-config setup.cfg --strict-markers
+addopts = --cov=src --cov-report= --cov-config setup.cfg --strict-markers
 markers = 
 	airflow: marks tests related to airflow (deselect with '-m not airflow')
 	slow_unit: marks tests to only run slow unit tests (deselect with '-m not slow_unit')
 	integration: marks tests to only run in integration (deselect with '-m "not integration"')
 	integration_batch_1: mark tests to only run in batch 1 of integration tests. This is done mainly for parallelisation (deselect with '-m not integration_batch_1')
 	slow_integration: marks tests that are too slow to even run in integration (deselect with '-m "not slow_integration"')
+	performance: marks tests that are sparingly run to measure performance (deselect with '-m "not performance"')
 testpaths = 
 	tests/unit
 	tests/integration
+	tests/performance
 
 [coverage:run]
 
 [coverage:paths]
 source = 
 	src
 	*/site-packages
```

### Comparing `acryl-datahub-tc-0.10.0.3/setup.py` & `acryl-datahub-tc-0.10.2rc1/setup.py`

 * *Files 4% similar despite different names*

```diff
@@ -33,32 +33,33 @@
     "click>=7.1.2",
     "click-default-group",
     "PyYAML",
     "toml>=0.10.0",
     "entrypoints",
     "docker",
     "expandvars>=0.6.5",
-    "avro-gen3==0.7.8",
+    "avro-gen3==0.7.10",
     # "avro-gen3 @ git+https://github.com/acryldata/avro_gen@master#egg=avro-gen3",
     "avro>=1.10.2,<1.11",
     "python-dateutil>=2.8.0",
-    "stackprinter>=0.2.6",
     "tabulate",
     "progressbar2",
     "termcolor>=1.0.0",
     "psutil>=5.8.0",
     "ratelimiter",
     "Deprecated",
     "humanfriendly",
     "packaging",
     "aiohttp<4",
     "cached_property",
     "ijson",
     "click-spinner",
     "requests_file",
+    "jsonref",
+    "jsonschema",
 }
 
 rest_common = {"requests", "requests_file"}
 
 kafka_common = {
     # The confluent_kafka package provides a number of pre-built wheels for
     # various platforms and architectures. However, it does not provide wheels
@@ -105,15 +106,15 @@
     "grpcio-tools>=1.44.0,<2",
 }
 
 sql_common = {
     # Required for all SQL sources.
     "sqlalchemy>=1.3.24, <2",
     # Required for SQL profiling.
-    "great-expectations>=0.15.12, <=0.15.41",
+    "great-expectations>=0.15.12, <=0.15.50",
     # scipy version restricted to reduce backtracking, used by great-expectations,
     "scipy>=1.7.2",
     # GE added handling for higher version of jinja2
     # https://github.com/great-expectations/great_expectations/pull/5382/files
     # datahub does not depend on traitlets directly but great expectations does.
     # https://github.com/ipython/traitlets/issues/741
     "traitlets<5.2.2",
@@ -133,28 +134,27 @@
 path_spec_common = {
     "parse>=1.19.0",
     "wcmatch",
 }
 
 looker_common = {
     # Looker Python SDK
-    "looker-sdk==22.2.1",
+    "looker-sdk==23.0.0",
     # This version of lkml contains a fix for parsing lists in
     # LookML files with spaces between an item and the following comma.
     # See https://github.com/joshtemple/lkml/issues/73.
     "lkml>=1.3.0b5",
     "sql-metadata==2.2.2",
     sqllineage_lib,
     "GitPython>2",
 }
 
 bigquery_common = {
-    "google-api-python-client",
     # Google cloud logging library
-    "google-cloud-logging<3.1.2",
+    "google-cloud-logging<=3.5.0",
     "google-cloud-bigquery",
     "more-itertools>=8.12.0",
 }
 
 clickhouse_common = {
     # Clickhouse 0.1.8 requires SQLAlchemy 1.3.x, while the newer versions
     # allow SQLAlchemy 1.4.x.
@@ -176,19 +176,20 @@
     # See https://github.com/snowflakedb/snowflake-sqlalchemy/issues/234 for why 1.2.5 is blocked.
     "snowflake-sqlalchemy>=1.2.4, !=1.2.5",
     # Because of https://github.com/snowflakedb/snowflake-sqlalchemy/issues/350 we need to restrict SQLAlchemy's max version.
     # Eventually we should just require snowflake-sqlalchemy>=1.4.3, but I won't do that immediately
     # because it may break Airflow users that need SQLAlchemy 1.3.x.
     "SQLAlchemy<1.4.42",
     # See https://github.com/snowflakedb/snowflake-connector-python/pull/1348 for why 2.8.2 is blocked
-    "snowflake-connector-python!=2.8.2",
+    # Cannot upgrade to 3.0.0 because of dependency on pyarrow>=10.0.1, conflicts with feast
+    "snowflake-connector-python!=2.8.2, <3.0.0",
     "pandas",
     "cryptography",
     "msal",
-    "acryl-datahub-classify>=0.0.4",
+    "acryl-datahub-classify==0.0.6",
     # spacy version restricted to reduce backtracking, used by acryl-datahub-classify,
     "spacy==3.4.3",
 }
 
 trino = {
     # Trino 0.317 broke compatibility with SQLAlchemy 1.3.24.
     # See https://github.com/trinodb/trino-python-client/issues/250.
@@ -245,15 +246,14 @@
         "fastapi",
         "uvicorn",
     },
     # Integrations.
     "airflow": {
         "apache-airflow >= 2.0.2",
         *rest_common,
-        *kafka_common,
     },
     "circuit-breaker": {
         "gql>=3.3.0",
         "gql[requests]>=3.3.0",
     },
     "great-expectations": sql_common | {sqllineage_lib},
     # Source plugins
@@ -284,15 +284,20 @@
     "dbt-cloud": {"requests"},
     "druid": sql_common | {"pydruid>=0.6.2"},
     # Starting with 7.14.0 python client is checking if it is connected to elasticsearch client. If its not it throws
     # UnsupportedProductError
     # https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/release-notes.html#rn-7-14-0
     # https://github.com/elastic/elasticsearch-py/issues/1639#issuecomment-883587433
     "elasticsearch": {"elasticsearch==7.13.4"},
-    "feast": {"feast~=0.29.0", "flask-openid>=1.3.0"},
+    "feast": {
+        "feast~=0.29.0",
+        "flask-openid>=1.3.0",
+        # typeguard 3.x, released on 2023-03-14, seems to cause issues with Feast.
+        "typeguard<3",
+    },
     "glue": aws_common,
     # hdbcli is supported officially by SAP, sqlalchemy-hana is built on top but not officially supported
     "hana": sql_common
     | {
         "sqlalchemy-hana>=0.5.0; platform_machine != 'aarch64' and platform_machine != 'arm64'",
         "hdbcli>=2.11.20; platform_machine != 'aarch64' and platform_machine != 'arm64'",
     },
@@ -306,14 +311,15 @@
         "databricks-dbapi",
         # Due to https://github.com/great-expectations/great_expectations/issues/6146,
         # we cannot allow 0.15.{23-26}. This was fixed in 0.15.27 by
         # https://github.com/great-expectations/great_expectations/pull/6149.
         "great-expectations != 0.15.23, != 0.15.24, != 0.15.25, != 0.15.26",
     },
     "iceberg": iceberg_common,
+    "json-schema": set(),
     "kafka": {*kafka_common, *kafka_protobuf},
     "kafka-connect": sql_common | {"requests", "JPype1"},
     "ldap": {"python-ldap>=2.4"},
     "looker": looker_common,
     "lookml": looker_common,
     "metabase": {"requests", sqllineage_lib},
     "mode": {"requests", sqllineage_lib, "tenacity>=8.0.1"},
@@ -327,31 +333,32 @@
     "oracle": sql_common | {"cx_Oracle"},
     "postgres": sql_common | {"psycopg2-binary", "GeoAlchemy2"},
     "presto": sql_common | trino | {"acryl-pyhive[hive]>=0.6.12"},
     "presto-on-hive": sql_common
     | {"psycopg2-binary", "acryl-pyhive[hive]>=0.6.12", "pymysql>=1.0.2"},
     "pulsar": {"requests"},
     "redash": {"redash-toolbelt", "sql-metadata", sqllineage_lib},
-    "redshift": sql_common | redshift_common,
-    "redshift-usage": sql_common | usage_common | redshift_common,
+    "redshift": sql_common | redshift_common | usage_common | {"redshift-connector"},
+    "redshift-legacy": sql_common | redshift_common,
+    "redshift-usage-legacy": sql_common | usage_common | redshift_common,
     "s3": {*s3_base, *data_lake_profiling},
     "sagemaker": aws_common,
     "salesforce": {"simple-salesforce"},
     "snowflake": snowflake_common | usage_common,
     "snowflake-beta": (
         snowflake_common | usage_common
     ),  # deprecated, but keeping the extra for backwards compatibility
     "sqlalchemy": sql_common,
     "superset": {
         "requests",
         "sqlalchemy",
         "great_expectations",
         "greenlet",
     },
-    "tableau": {"tableauserverclient>=0.17.0"},
+    "tableau": {"tableauserverclient>=0.17.0", sqllineage_lib},
     "trino": sql_common | trino,
     "starburst-trino-usage": sql_common | usage_common | trino,
     "nifi": {"requests", "packaging"},
     "powerbi": microsoft_common | {"lark[regex]==1.1.4", "sqlparse"},
     "powerbi-report-server": powerbi_report_server,
     "vertica": sql_common | {"vertica-sqlalchemy-dialect[vertica-python]==0.0.1"},
     "unity-catalog": databricks_cli | {"requests"},
@@ -397,18 +404,19 @@
     *framework_common,
     *mypy_stubs,
     *s3_base,
     # This is pinned only to avoid spurious errors in CI.
     # We should make an effort to keep it up to date.
     "black==22.12.0",
     "coverage>=5.1",
-    "flake8>=3.8.3",
+    "flake8>=3.8.3",  # DEPRECATION: Once we drop Python 3.7, we can pin to 6.x.
     "flake8-tidy-imports>=4.3.0",
+    "flake8-bugbear==23.3.12",
     "isort>=5.7.0",
-    "mypy==0.991",
+    "mypy==1.0.0",
     # pydantic 1.8.2 is incompatible with mypy 0.910.
     # See https://github.com/samuelcolvin/pydantic/pull/3175#issuecomment-995382910.
     "pydantic>=1.9.0",
     "pytest>=6.2.2",
     "pytest-asyncio>=0.16.0",
     "pytest-cov>=2.8.1",
     "pytest-docker>=1.0.1",
@@ -425,14 +433,15 @@
             "clickhouse",
             "clickhouse-usage",
             "delta-lake",
             "druid",
             "elasticsearch",
             "feast" if sys.version_info >= (3, 8) else None,
             "iceberg",
+            "json-schema",
             "ldap",
             "looker",
             "lookml",
             "glue",
             "mariadb",
             "okta",
             "oracle",
@@ -440,15 +449,16 @@
             "sagemaker",
             "kafka",
             "datahub-rest",
             "datahub-lite",
             "presto",
             "redash",
             "redshift",
-            "redshift-usage",
+            "redshift-legacy",
+            "redshift-usage-legacy",
             "s3",
             "snowflake",
             "tableau",
             "trino",
             "hive",
             "starburst-trino-usage",
             "powerbi",
@@ -512,14 +522,15 @@
         "druid = datahub.ingestion.source.sql.druid:DruidSource",
         "elasticsearch = datahub.ingestion.source.elastic_search:ElasticsearchSource",
         "feast = datahub.ingestion.source.feast:FeastRepositorySource",
         "glue = datahub.ingestion.source.aws.glue:GlueSource",
         "sagemaker = datahub.ingestion.source.aws.sagemaker:SagemakerSource",
         "hana = datahub.ingestion.source.sql.hana:HanaSource",
         "hive = datahub.ingestion.source.sql.hive:HiveSource",
+        "json-schema = datahub.ingestion.source.schema.json_schema:JsonSchemaSource",
         "kafka = datahub.ingestion.source.kafka:KafkaSource",
         "kafka-connect = datahub.ingestion.source.kafka_connect:KafkaConnectSource",
         "ldap = datahub.ingestion.source.ldap:LDAPSource",
         "looker = datahub.ingestion.source.looker.looker_source:LookerDashboardSource",
         "lookml = datahub.ingestion.source.looker.lookml_source:LookMLSource",
         "datahub-lineage-file = datahub.ingestion.source.metadata.lineage:LineageFileSource",
         "datahub-business-glossary = datahub.ingestion.source.metadata.business_glossary:BusinessGlossaryFileSource",
@@ -528,16 +539,17 @@
         "mssql = datahub.ingestion.source.sql.mssql:SQLServerSource",
         "mysql = datahub.ingestion.source.sql.mysql:MySQLSource",
         "mariadb = datahub.ingestion.source.sql.mariadb.MariaDBSource",
         "okta = datahub.ingestion.source.identity.okta:OktaSource",
         "oracle = datahub.ingestion.source.sql.oracle:OracleSource",
         "postgres = datahub.ingestion.source.sql.postgres:PostgresSource",
         "redash = datahub.ingestion.source.redash:RedashSource",
-        "redshift = datahub.ingestion.source.sql.redshift:RedshiftSource",
-        "redshift-usage = datahub.ingestion.source.usage.redshift_usage:RedshiftUsageSource",
+        "redshift = datahub.ingestion.source.redshift.redshift:RedshiftSource",
+        "redshift-legacy = datahub.ingestion.source.sql.redshift:RedshiftSource",
+        "redshift-usage-legacy = datahub.ingestion.source.usage.redshift_usage:RedshiftUsageSource",
         "snowflake = datahub.ingestion.source.snowflake.snowflake_v2:SnowflakeV2Source",
         "superset = datahub.ingestion.source.superset:SupersetSource",
         "tableau = datahub.ingestion.source.tableau:TableauSource",
         "openapi = datahub.ingestion.source.openapi:OpenApiSource",
         "metabase = datahub.ingestion.source.metabase:MetabaseSource",
         "trino = datahub.ingestion.source.sql.trino:TrinoSource",
         "starburst-trino-usage = datahub.ingestion.source.usage.starburst_trino_usage:TrinoUsageSource",
@@ -573,14 +585,15 @@
         "simple_add_dataset_properties = datahub.ingestion.transformer.add_dataset_properties:SimpleAddDatasetProperties",
         "pattern_add_dataset_schema_terms = datahub.ingestion.transformer.add_dataset_schema_terms:PatternAddDatasetSchemaTerms",
         "pattern_add_dataset_schema_tags = datahub.ingestion.transformer.add_dataset_schema_tags:PatternAddDatasetSchemaTags",
     ],
     "datahub.ingestion.sink.plugins": [
         "file = datahub.ingestion.sink.file:FileSink",
         "console = datahub.ingestion.sink.console:ConsoleSink",
+        "blackhole = datahub.ingestion.sink.blackhole:BlackHoleSink",
         "datahub-kafka = datahub.ingestion.sink.datahub_kafka:DatahubKafkaSink",
         "datahub-rest = datahub.ingestion.sink.datahub_rest:DatahubRestSink",
         "datahub-lite = datahub.ingestion.sink.datahub_lite:DataHubLiteSink",
     ],
     "datahub.ingestion.checkpointing_provider.plugins": [
         "datahub = datahub.ingestion.source.state_provider.datahub_ingestion_checkpointing_provider:DatahubIngestionCheckpointingProvider",
     ],
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/acryl_datahub_tc.egg-info/PKG-INFO` & `acryl-datahub-tc-0.10.2rc1/src/acryl_datahub_tc.egg-info/PKG-INFO`

 * *Files 4% similar despite different names*

```diff
@@ -1,23 +1,23 @@
 Metadata-Version: 2.1
 Name: acryl-datahub-tc
-Version: 0.10.0.3
+Version: 0.10.2rc1
 Summary: A CLI to work with DataHub metadata
 Home-page: https://datahubproject.io/
 License: Apache License 2.0
 Project-URL: Documentation, https://datahubproject.io/docs/
 Project-URL: Source, https://github.com/datahub-project/datahub
 Project-URL: Changelog, https://github.com/datahub-project/datahub/releases
 Description: # Introduction to Metadata Ingestion
         
         ## Integration Options
         
-        DataHub supports both **push-based** and **pull-based** metadata integration. 
+        DataHub supports both **push-based** and **pull-based** metadata integration.
         
-        Push-based integrations allow you to emit metadata directly from your data systems when metadata changes, while pull-based integrations allow you to "crawl" or "ingest" metadata from the data systems by connecting to them and extracting metadata in a batch or incremental-batch manner. Supporting both mechanisms means that you can integrate with all your systems in the most flexible way possible. 
+        Push-based integrations allow you to emit metadata directly from your data systems when metadata changes, while pull-based integrations allow you to "crawl" or "ingest" metadata from the data systems by connecting to them and extracting metadata in a batch or incremental-batch manner. Supporting both mechanisms means that you can integrate with all your systems in the most flexible way possible.
         
         Examples of push-based integrations include [Airflow](../docs/lineage/airflow.md), [Spark](../metadata-integration/java/spark-lineage/README.md), [Great Expectations](./integration_docs/great-expectations.md) and [Protobuf Schemas](../metadata-integration/java/datahub-protobuf/README.md). This allows you to get low-latency metadata integration from the "active" agents in your data ecosystem. Examples of pull-based integrations include BigQuery, Snowflake, Looker, Tableau and many others.
         
         This document describes the pull-based metadata ingestion system that is built into DataHub for easy integration with a wide variety of sources in your data stack.
         
         ## Getting Started
         
@@ -25,72 +25,76 @@
         
         Before running any metadata ingestion job, you should make sure that DataHub backend services are all running. You can either run ingestion via the [UI](../docs/ui-ingestion.md) or via the [CLI](../docs/cli.md). You can reference the CLI usage guide given there as you go through this page.
         
         ## Core Concepts
         
         ### Sources
         
+        Please see our [Integrations page](https://datahubproject.io/integrations) to browse our ingestion sources and filter on their features.
+        
         Data systems that we are extracting metadata from are referred to as **Sources**. The `Sources` tab on the left in the sidebar shows you all the sources that are available for you to ingest metadata from. For example, we have sources for [BigQuery](https://datahubproject.io/docs/generated/ingestion/sources/bigquery), [Looker](https://datahubproject.io/docs/generated/ingestion/sources/looker), [Tableau](https://datahubproject.io/docs/generated/ingestion/sources/tableau) and many others.
         
         #### Metadata Ingestion Source Status
         
         We apply a Support Status to each Metadata Source to help you understand the integration reliability at a glance.
         
         ![Certified](https://img.shields.io/badge/support%20status-certified-brightgreen): Certified Sources are well-tested & widely-adopted by the DataHub Community. We expect the integration to be stable with few user-facing issues.
         
         ![Incubating](https://img.shields.io/badge/support%20status-incubating-blue): Incubating Sources are ready for DataHub Community adoption but have not been tested for a wide variety of edge-cases. We eagerly solicit feedback from the Community to streghten the connector; minor version changes may arise in future releases.
         
-        ![Testing](https://img.shields.io/badge/support%20status-testing-lightgrey): Testing Sources are available for experiementation by DataHub Community members, but may change without notice. 
+        ![Testing](https://img.shields.io/badge/support%20status-testing-lightgrey): Testing Sources are available for experiementation by DataHub Community members, but may change without notice.
         
         ### Sinks
         
         Sinks are destinations for metadata. When configuring ingestion for DataHub, you're likely to be sending the metadata to DataHub over either the [REST (datahub-sink)](./sink_docs/datahub.md#datahub-rest) or the [Kafka (datahub-kafka)](./sink_docs/datahub.md#datahub-kafka) sink. In some cases, the [File](./sink_docs/file.md) sink is also helpful to store a persistent offline copy of the metadata during debugging.
         
         The default sink that most of the ingestion systems and guides assume is the `datahub-rest` sink, but you should be able to adapt all of them for the other sinks as well!
         
         ### Recipes
         
         A recipe is the main configuration file that puts it all together. It tells our ingestion scripts where to pull data from (source) and where to put it (sink).
         
         :::tip
-        Name your recipe with **.dhub.yaml** extension like *myrecipe.dhub.yaml* to use vscode or intellij as a recipe editor with autocomplete
+        Name your recipe with **.dhub.yaml** extension like _myrecipe.dhub.yaml_ to use vscode or intellij as a recipe editor with autocomplete
         and syntax validation.
         
         Make sure yaml plugin is installed for your editor:
+        
         - For vscode install [Redhat's yaml plugin](https://marketplace.visualstudio.com/items?itemName=redhat.vscode-yaml)
-        - For intellij install [official yaml plugin](https://plugins.jetbrains.com/plugin/13126-yaml
-        )
+        - For intellij install [official yaml plugin](https://plugins.jetbrains.com/plugin/13126-yaml)
         
         :::
         
         Since `acryl-datahub` version `>=0.8.33.2`, the default sink is assumed to be a DataHub REST endpoint:
+        
         - Hosted at "http://localhost:8080" or the environment variable `${DATAHUB_GMS_URL}` if present
-        - With an empty auth token or the environment variable `${DATAHUB_GMS_TOKEN}` if present. 
+        - With an empty auth token or the environment variable `${DATAHUB_GMS_TOKEN}` if present.
         
         Here's a simple recipe that pulls metadata from MSSQL (source) and puts it into the default sink (datahub rest).
         
         ```yaml
         # The simplest recipe that pulls metadata from MSSQL and puts it into DataHub
         # using the Rest API.
         source:
           type: mssql
           config:
             username: sa
             password: ${MSSQL_PASSWORD}
             database: DemoData
-        
         # sink section omitted as we want to use the default datahub-rest sink
         ```
         
         Running this recipe is as simple as:
+        
         ```shell
         datahub ingest -c recipe.dhub.yaml
         ```
         
         or if you want to override the default endpoints, you can provide the environment variables as part of the command like below:
+        
         ```shell
         DATAHUB_GMS_URL="https://my-datahub-server:8080" DATAHUB_GMS_TOKEN="my-datahub-token" datahub ingest -c recipe.dhub.yaml
         ```
         
         A number of recipes are included in the [examples/recipes](./examples/recipes) directory. For full info and context on each source and sink, see the pages described in the [table of plugins](../docs/cli.md#installing-plugins).
         
         > Note that one recipe file can only have 1 source and 1 sink. If you want multiple sources then you will need multiple recipe files.
@@ -143,66 +147,72 @@
         # Running ingestion with reporting to DataHub turned off
         datahub ingest -c ./examples/recipes/example_to_datahub_rest.dhub.yaml --no-default-report
         ```
         
         The reports include the recipe that was used for ingestion. This can be turned off by adding an additional section to the ingestion recipe.
         
         ```yaml
-        
         source:
-           # source configs
+          # source configs
         
         sink:
-           # sink configs
+          # sink configs
         
         # Add configuration for the datahub reporter
         reporting:
           - type: datahub
             config:
               report_recipe: false
         ```
         
-        
         ## Transformations
         
         If you'd like to modify data before it reaches the ingestion sinks  for instance, adding additional owners or tags  you can use a transformer to write your own module and integrate it with DataHub. Transformers require extending the recipe with a new section to describe the transformers that you want to run.
         
         For example, a pipeline that ingests metadata from MSSQL and applies a default "important" tag to all datasets is described below:
+        
         ```yaml
         # A recipe to ingest metadata from MSSQL and apply default tags to all tables
         source:
           type: mssql
           config:
             username: sa
             password: ${MSSQL_PASSWORD}
             database: DemoData
         
         transformers: # an array of transformers applied sequentially
           - type: simple_add_dataset_tags
             config:
               tag_urns:
                 - "urn:li:tag:Important"
-        
         # default sink, no config needed
         ```
         
         Check out the [transformers guide](./docs/transformer/intro.md) to learn more about how you can create really flexible pipelines for processing metadata using Transformers!
         
         ## Using as a library (SDK)
         
-        In some cases, you might want to construct Metadata events directly and use programmatic ways to emit that metadata to DataHub. In this case, take a look at the [Python emitter](./as-a-library.md) and the [Java emitter](../metadata-integration/java/as-a-library.md) libraries which can be called from your own code. 
+        In some cases, you might want to construct Metadata events directly and use programmatic ways to emit that metadata to DataHub. In this case, take a look at the [Python emitter](./as-a-library.md) and the [Java emitter](../metadata-integration/java/as-a-library.md) libraries which can be called from your own code.
         
         ### Programmatic Pipeline
+        
         In some cases, you might want to configure and run a pipeline entirely from within your custom Python script. Here is an example of how to do it.
-         - [programmatic_pipeline.py](./examples/library/programatic_pipeline.py) - a basic mysql to REST programmatic pipeline.
+        
+        - [programmatic_pipeline.py](./examples/library/programatic_pipeline.py) - a basic mysql to REST programmatic pipeline.
         
         ## Developing
         
         See the guides on [developing](./developing.md), [adding a source](./adding-source.md) and [using transformers](./docs/transformer/intro.md).
         
+        ## Compatibility
+        
+        DataHub server uses a 3 digit versioning scheme, while the CLI uses a 4 digit scheme. For example, if you're using DataHub server version 0.10.0, you should use CLI version 0.10.0.x, where x is a patch version.
+        We do this because we do CLI releases at a much higher frequency than server releases, usually every few days vs twice a month.
+        
+        For ingestion sources, any breaking changes will be highlighted in the [release notes](../docs/how/updating-datahub.md). When fields are deprecated or otherwise changed, we will try to maintain backwards compatibility for two server releases, which is about 4-6 weeks. The CLI will also print warnings whenever deprecated options are used.
         
 Platform: UNKNOWN
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Programming Language :: Python
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3 :: Only
 Classifier: Programming Language :: Python :: 3.7
@@ -242,14 +252,15 @@
 Provides-Extra: druid
 Provides-Extra: elasticsearch
 Provides-Extra: feast
 Provides-Extra: glue
 Provides-Extra: hana
 Provides-Extra: hive
 Provides-Extra: iceberg
+Provides-Extra: json-schema
 Provides-Extra: kafka
 Provides-Extra: kafka-connect
 Provides-Extra: ldap
 Provides-Extra: looker
 Provides-Extra: lookml
 Provides-Extra: metabase
 Provides-Extra: mode
@@ -262,15 +273,16 @@
 Provides-Extra: oracle
 Provides-Extra: postgres
 Provides-Extra: presto
 Provides-Extra: presto-on-hive
 Provides-Extra: pulsar
 Provides-Extra: redash
 Provides-Extra: redshift
-Provides-Extra: redshift-usage
+Provides-Extra: redshift-legacy
+Provides-Extra: redshift-usage-legacy
 Provides-Extra: s3
 Provides-Extra: sagemaker
 Provides-Extra: salesforce
 Provides-Extra: snowflake
 Provides-Extra: snowflake-beta
 Provides-Extra: sqlalchemy
 Provides-Extra: superset
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/acryl_datahub_tc.egg-info/entry_points.txt` & `acryl-datahub-tc-0.10.2rc1/src/acryl_datahub_tc.egg-info/entry_points.txt`

 * *Files 4% similar despite different names*

```diff
@@ -8,14 +8,15 @@
 datahub = datahub.ingestion.source.state_provider.datahub_ingestion_checkpointing_provider:DatahubIngestionCheckpointingProvider
 
 [datahub.ingestion.reporting_provider.plugins]
 datahub = datahub.ingestion.reporting.datahub_ingestion_run_summary_provider:DatahubIngestionRunSummaryProvider
 file = datahub.ingestion.reporting.file_reporter:FileReporter
 
 [datahub.ingestion.sink.plugins]
+blackhole = datahub.ingestion.sink.blackhole:BlackHoleSink
 console = datahub.ingestion.sink.console:ConsoleSink
 datahub-kafka = datahub.ingestion.sink.datahub_kafka:DatahubKafkaSink
 datahub-lite = datahub.ingestion.sink.datahub_lite:DataHubLiteSink
 datahub-rest = datahub.ingestion.sink.datahub_rest:DatahubRestSink
 file = datahub.ingestion.sink.file:FileSink
 
 [datahub.ingestion.source.plugins]
@@ -35,14 +36,15 @@
 elasticsearch = datahub.ingestion.source.elastic_search:ElasticsearchSource
 feast = datahub.ingestion.source.feast:FeastRepositorySource
 file = datahub.ingestion.source.file:GenericFileSource
 glue = datahub.ingestion.source.aws.glue:GlueSource
 hana = datahub.ingestion.source.sql.hana:HanaSource
 hive = datahub.ingestion.source.sql.hive:HiveSource
 iceberg = datahub.ingestion.source.iceberg.iceberg:IcebergSource
+json-schema = datahub.ingestion.source.schema.json_schema:JsonSchemaSource
 kafka = datahub.ingestion.source.kafka:KafkaSource
 kafka-connect = datahub.ingestion.source.kafka_connect:KafkaConnectSource
 ldap = datahub.ingestion.source.ldap:LDAPSource
 looker = datahub.ingestion.source.looker.looker_source:LookerDashboardSource
 lookml = datahub.ingestion.source.looker.lookml_source:LookMLSource
 mariadb = datahub.ingestion.source.sql.mariadb.MariaDBSource
 metabase = datahub.ingestion.source.metabase:MetabaseSource
@@ -57,16 +59,17 @@
 postgres = datahub.ingestion.source.sql.postgres:PostgresSource
 powerbi = datahub.ingestion.source.powerbi:PowerBiDashboardSource
 powerbi-report-server = datahub.ingestion.source.powerbi_report_server:PowerBiReportServerDashboardSource
 presto = datahub.ingestion.source.sql.presto:PrestoSource
 presto-on-hive = datahub.ingestion.source.sql.presto_on_hive:PrestoOnHiveSource
 pulsar = datahub.ingestion.source.pulsar:PulsarSource
 redash = datahub.ingestion.source.redash:RedashSource
-redshift = datahub.ingestion.source.sql.redshift:RedshiftSource
-redshift-usage = datahub.ingestion.source.usage.redshift_usage:RedshiftUsageSource
+redshift = datahub.ingestion.source.redshift.redshift:RedshiftSource
+redshift-legacy = datahub.ingestion.source.sql.redshift:RedshiftSource
+redshift-usage-legacy = datahub.ingestion.source.usage.redshift_usage:RedshiftUsageSource
 s3 = datahub.ingestion.source.s3:S3Source
 sagemaker = datahub.ingestion.source.aws.sagemaker:SagemakerSource
 salesforce = datahub.ingestion.source.salesforce:SalesforceSource
 snowflake = datahub.ingestion.source.snowflake.snowflake_v2:SnowflakeV2Source
 sqlalchemy = datahub.ingestion.source.sql.sql_generic:SQLAlchemyGenericSource
 starburst-trino-usage = datahub.ingestion.source.usage.starburst_trino_usage:TrinoUsageSource
 superset = datahub.ingestion.source.superset:SupersetSource
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/acryl_datahub_tc.egg-info/requires.txt` & `acryl-datahub-tc-0.10.2rc1/src/acryl_datahub_tc.egg-info/requires.txt`

 * *Files 26% similar despite different names*

```diff
@@ -1,2233 +1,2361 @@
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
-click-default-group
-toml>=0.10.0
-aiohttp<4
-pydantic!=1.10.3,>=1.5.1
-typing-inspect
-mypy_extensions>=0.4.3
-entrypoints
-termcolor>=1.0.0
-stackprinter>=0.2.6
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
+tabulate
+docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
 PyYAML
+click-spinner
+mypy_extensions>=0.4.3
+requests_file
 mixpanel>=4.9.0
-psutil>=5.8.0
-tabulate
-cached_property
+pydantic!=1.10.3,>=1.5.1
+ratelimiter
 humanfriendly
+jsonschema
+click-default-group
+toml>=0.10.0
+typing-inspect
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
+entrypoints
+Deprecated
 ijson
-docker
-click>=7.1.2
-click-spinner
-expandvars>=0.6.5
+termcolor>=1.0.0
 
 [:python_version < "3.8"]
 typing_extensions>=3.7.4.3
 
 [:python_version >= "3.8"]
 typing_extensions>=3.10.0.2
 
 [airflow]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
-requests
-click-default-group
-toml>=0.10.0
-aiohttp<4
-entrypoints
-termcolor>=1.0.0
-stackprinter>=0.2.6
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
-confluent_kafka>=1.5.0
-apache-airflow>=2.0.2
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-fastavro>=1.2.0
-
-[airflow:platform_system != "Darwin" and (platform_machine == "aarch64" or platform_machine == "arm64")]
-confluent_kafka<1.9.0
-
-[all]
-python-dateutil>=2.8.0
-pydruid>=0.6.2
-requests
-scipy>=1.7.2
-pyarrow>=6.0.1
-azure-identity==1.10.0
-google-cloud-logging<3.1.2
-networkx>=2.6.2
-boto3
-sqllineage==1.3.6
-looker-sdk==22.2.1
-spacy==3.4.3
-sqlalchemy<2,>=1.3.24
+avro-gen3==0.7.10
+PyYAML
+click-spinner
 requests_file
-trino[sqlalchemy]!=0.317,>=0.308
-moto[s3]
-google-cloud-datacatalog-lineage==0.2.0
-clickhouse-sqlalchemy>=0.1.8
-snowflake-sqlalchemy!=1.2.5,>=1.2.4
-databricks-dbapi
-databricks-cli==0.17.3
+jsonschema
+click-default-group
+ratelimiter
 humanfriendly
+toml>=0.10.0
+requests
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
+entrypoints
+Deprecated
 ijson
-psycopg2-binary
-sqlparse
-expandvars>=0.6.5
-fastavro>=1.2.0
-acryl-iceberg-legacy==0.0.4
+apache-airflow>=2.0.2
+termcolor>=1.0.0
+
+[all]
+pymysql>=1.0.2
+more-itertools>=8.12.0
+wcmatch
+sql_metadata
+google-cloud-datacatalog-lineage==0.2.0
+great_expectations
+GeoAlchemy2
 requests_ntlm
-traitlets<5.2.2
+acryl-pyhive[hive]>=0.6.13
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+PyAthena[sqlalchemy]==2.4.1
+clickhouse-sqlalchemy>=0.1.8
+azure-identity==1.10.0
 JPype1
-pyspark==3.0.3
-great-expectations!=0.15.23,!=0.15.24,!=0.15.25,!=0.15.26
 toml>=0.10.0
-sql_metadata
-aiohttp<4
-python-ldap>=2.4
-sql-metadata
-stackprinter>=0.2.6
-sqlalchemy
-avro-gen3==0.7.8
-simple-salesforce
-grpcio<2,>=1.44.0
+requests
+moto[s3]
+scipy>=1.7.2
 psutil>=5.8.0
-ujson>=5.2.0
-sqlalchemy-pytds>=0.3
-tabulate
-GitPython>2
-click>=7.1.2
-click-spinner
-acryl-pyhive[hive]>=0.6.13
-pydeequ>=1.0.1
-deltalake!=0.6.4,>=0.6.3
-parse>=1.19.0
-tableschema>=1.20.2
-grpcio-tools<2,>=1.44.0
+flask-openid>=1.3.0
+typeguard<3
+sqllineage==1.3.6
+smart-open[s3]>=5.2.1
+pandas
+snowflake-sqlalchemy!=1.2.5,>=1.2.4
+progressbar2
+entrypoints
 msal
+parse>=1.19.0
+deltalake!=0.6.4,>=0.6.3
+gql[requests]>=3.3.0
+greenlet
+google-cloud-bigquery
+avro<1.11,>=1.10.2
+click>=7.1.2
+sql-metadata==2.2.2
+jsonref
+acryl-datahub-classify==0.0.6
+tabulate
+aiohttp<4
+lkml>=1.3.0b5
+click-default-group
+grpcio<2,>=1.44.0
+google-cloud-logging<=3.5.0
 pymongo[srv]>=3.11
-redash-toolbelt
+sqlparse
+tableauserverclient>=0.17.0
+pyspark==3.0.3
+databricks-dbapi
+tenacity>=8.0.1
+python-dateutil>=2.8.0
+snowflake-connector-python!=2.8.2,<3.0.0
+psycopg2-binary
+python-ldap>=2.4
+great-expectations!=0.15.23,!=0.15.24,!=0.15.25,!=0.15.26
 botocore!=1.23.0
-flask-openid>=1.3.0
-pymysql>=1.0.2
-great-expectations<=0.15.41,>=0.15.12
-gql[requests]>=3.3.0
+ijson
+pydeequ>=1.0.1
+termcolor>=1.0.0
+okta~=1.7.0
+elasticsearch==7.13.4
 packaging
-PyAthena[sqlalchemy]==2.4.1
-progressbar2
-msal==1.16.0
-PyYAML
-confluent_kafka>=1.5.0
-lark[regex]==1.1.4
-acryl-pyhive[hive]>=0.6.12
-apache-airflow>=2.0.2
-acryl-datahub-classify>=0.0.4
+pydruid>=0.6.2
 cx_Oracle
-tableauserverclient>=0.17.0
-sqlalchemy-redshift
-pandas
-wcmatch
-smart-open[s3]>=5.2.1
-great_expectations
-Deprecated
+docker
+tableschema>=1.20.2
+trino[sqlalchemy]!=0.317,>=0.308
+redash-toolbelt
+spacy==3.4.3
+acryl-iceberg-legacy==0.0.4
 ratelimiter
-click-default-group
-more-itertools>=8.12.0
-GeoAlchemy2
-sqlalchemy-bigquery>=1.4.1
-lkml>=1.3.0b5
-sql-metadata==2.2.2
-tenacity>=8.0.1
-entrypoints
-termcolor>=1.0.0
-greenlet
-avro<1.11,>=1.10.2
-gql>=3.3.0
+humanfriendly
 feast~=0.29.0
-okta~=1.7.0
+msal==1.16.0
+ujson>=5.2.0
+pyarrow>=6.0.1
+redshift-connector
+fastavro>=1.2.0
+boto3
+GitPython>2
+apache-airflow>=2.0.2
 vertica-sqlalchemy-dialect[vertica-python]==0.0.1
-elasticsearch==7.13.4
+sql-metadata
+sqlalchemy-redshift
+databricks-cli==0.17.3
 cached_property
-snowflake-connector-python!=2.8.2
-docker
+looker-sdk==23.0.0
+expandvars>=0.6.5
+confluent_kafka>=1.5.0
+jsonschema
+acryl-pyhive[hive]>=0.6.12
 SQLAlchemy<1.4.42
-google-cloud-bigquery
+sqlalchemy-pytds>=0.3
 cryptography
-google-api-python-client
+great-expectations<=0.15.50,>=0.15.12
+lark[regex]==1.1.4
+sqlalchemy
+grpcio-tools<2,>=1.44.0
+networkx>=2.6.2
+traitlets<5.2.2
+sqlalchemy-bigquery>=1.4.1
+sqlalchemy<2,>=1.3.24
+Deprecated
+gql>=3.3.0
+simple-salesforce
 
 [all:platform_machine != "aarch64" and platform_machine != "arm64"]
-hdbcli>=2.11.20
 sqlalchemy-hana>=0.5.0
+hdbcli>=2.11.20
 
 [all:platform_system != "Darwin" and (platform_machine == "aarch64" or platform_machine == "arm64")]
 confluent_kafka<1.9.0
 
 [athena]
-Deprecated
-python-dateutil>=2.8.0
-traitlets<5.2.2
-ratelimiter
-click-default-group
-scipy>=1.7.2
-toml>=0.10.0
-aiohttp<4
-great-expectations<=0.15.41,>=0.15.12
-entrypoints
-termcolor>=1.0.0
-sqlalchemy<2,>=1.3.24
-stackprinter>=0.2.6
-greenlet
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyAthena[sqlalchemy]==2.4.1
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-
-[azure-ad]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+PyAthena[sqlalchemy]==2.4.1
+ratelimiter
+humanfriendly
+great-expectations<=0.15.50,>=0.15.12
 toml>=0.10.0
-aiohttp<4
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
+traitlets<5.2.2
+progressbar2
 entrypoints
+sqlalchemy<2,>=1.3.24
+Deprecated
+ijson
+greenlet
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[azure-ad]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+ratelimiter
+humanfriendly
+toml>=0.10.0
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
+entrypoints
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [base]
-python-dateutil>=2.8.0
-requests_file
 packaging
-progressbar2
+docker
+avro-gen3==0.7.10
 PyYAML
-humanfriendly
-ijson
-expandvars>=0.6.5
-Deprecated
+click-spinner
+requests_file
 ratelimiter
-click-default-group
+humanfriendly
 toml>=0.10.0
-aiohttp<4
+psutil>=5.8.0
+progressbar2
 entrypoints
-termcolor>=1.0.0
-stackprinter>=0.2.6
 avro<1.11,>=1.10.2
-avro-gen3==0.7.8
-psutil>=5.8.0
-tabulate
 cached_property
-docker
 click>=7.1.2
-click-spinner
+jsonref
+tabulate
+aiohttp<4
+expandvars>=0.6.5
+jsonschema
+click-default-group
+python-dateutil>=2.8.0
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [bigquery]
-Deprecated
-python-dateutil>=2.8.0
-traitlets<5.2.2
-ratelimiter
-click-default-group
-scipy>=1.7.2
+avro<1.11,>=1.10.2
+cached_property
+click>=7.1.2
 more-itertools>=8.12.0
-google-cloud-logging<3.1.2
-toml>=0.10.0
+jsonref
 sql_metadata
-aiohttp<4
-sqlalchemy-bigquery>=1.4.1
-great-expectations<=0.15.41,>=0.15.12
-sqllineage==1.3.6
-entrypoints
-termcolor>=1.0.0
-sqlalchemy<2,>=1.3.24
-stackprinter>=0.2.6
-greenlet
-avro<1.11,>=1.10.2
-requests_file
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 google-cloud-datacatalog-lineage==0.2.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
-google-cloud-bigquery
+aiohttp<4
 expandvars>=0.6.5
-google-api-python-client
-
-[bigquery-beta]
-Deprecated
-python-dateutil>=2.8.0
-traitlets<5.2.2
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
-scipy>=1.7.2
-more-itertools>=8.12.0
-google-cloud-logging<3.1.2
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
+humanfriendly
+google-cloud-logging<=3.5.0
 toml>=0.10.0
-sql_metadata
-aiohttp<4
-sqlalchemy-bigquery>=1.4.1
-great-expectations<=0.15.41,>=0.15.12
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
+traitlets<5.2.2
 sqllineage==1.3.6
+progressbar2
+sqlalchemy-bigquery>=1.4.1
 entrypoints
-termcolor>=1.0.0
 sqlalchemy<2,>=1.3.24
-stackprinter>=0.2.6
+Deprecated
+ijson
 greenlet
+google-cloud-bigquery
+termcolor>=1.0.0
+
+[bigquery-beta]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+more-itertools>=8.12.0
+jsonref
+sql_metadata
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
-google-cloud-bigquery
+aiohttp<4
 expandvars>=0.6.5
-google-api-python-client
-
-[circuit-breaker]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
+humanfriendly
+google-cloud-logging<=3.5.0
 toml>=0.10.0
-aiohttp<4
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
+traitlets<5.2.2
+sqllineage==1.3.6
+progressbar2
+sqlalchemy-bigquery>=1.4.1
 entrypoints
+sqlalchemy<2,>=1.3.24
+Deprecated
+ijson
+greenlet
+google-cloud-bigquery
 termcolor>=1.0.0
-gql[requests]>=3.3.0
-stackprinter>=0.2.6
+
+[circuit-breaker]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
-gql>=3.3.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-
-[clickhouse]
-Deprecated
-python-dateutil>=2.8.0
-traitlets<5.2.2
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
-scipy>=1.7.2
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
-great-expectations<=0.15.41,>=0.15.12
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
 entrypoints
+Deprecated
+ijson
+gql[requests]>=3.3.0
+gql>=3.3.0
 termcolor>=1.0.0
-sqlalchemy<2,>=1.3.24
-stackprinter>=0.2.6
-greenlet
+
+[clickhouse]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
-clickhouse-sqlalchemy>=0.1.8
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-
-[clickhouse-usage]
-Deprecated
-python-dateutil>=2.8.0
-traitlets<5.2.2
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
-scipy>=1.7.2
+clickhouse-sqlalchemy>=0.1.8
+ratelimiter
+humanfriendly
+great-expectations<=0.15.50,>=0.15.12
 toml>=0.10.0
-aiohttp<4
-great-expectations<=0.15.41,>=0.15.12
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
+traitlets<5.2.2
+progressbar2
 entrypoints
-termcolor>=1.0.0
 sqlalchemy<2,>=1.3.24
-stackprinter>=0.2.6
+Deprecated
+ijson
 greenlet
+termcolor>=1.0.0
+
+[clickhouse-usage]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
-clickhouse-sqlalchemy>=0.1.8
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
-sqlparse
+aiohttp<4
 expandvars>=0.6.5
-
-[datahub-business-glossary]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+clickhouse-sqlalchemy>=0.1.8
+ratelimiter
+humanfriendly
+great-expectations<=0.15.50,>=0.15.12
 toml>=0.10.0
-aiohttp<4
+scipy>=1.7.2
+psutil>=5.8.0
+sqlparse
+python-dateutil>=2.8.0
+traitlets<5.2.2
+progressbar2
 entrypoints
+sqlalchemy<2,>=1.3.24
+Deprecated
+ijson
+greenlet
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[datahub-business-glossary]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-
-[datahub-kafka]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
 entrypoints
+Deprecated
+ijson
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[datahub-kafka]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
-confluent_kafka>=1.5.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
+confluent_kafka>=1.5.0
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+ratelimiter
+humanfriendly
+toml>=0.10.0
+psutil>=5.8.0
+python-dateutil>=2.8.0
 fastavro>=1.2.0
+progressbar2
+entrypoints
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [datahub-kafka:platform_system != "Darwin" and (platform_machine == "aarch64" or platform_machine == "arm64")]
 confluent_kafka<1.9.0
 
 [datahub-lineage-file]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
+avro<1.11,>=1.10.2
+cached_property
+click>=7.1.2
+jsonref
+packaging
+tabulate
+docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
 entrypoints
+Deprecated
+ijson
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[datahub-lite]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-
-[datahub-lite]
-Deprecated
-python-dateutil>=2.8.0
 fastapi
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
+psutil>=5.8.0
 duckdb
+python-dateutil>=2.8.0
+uvicorn
+progressbar2
 entrypoints
+Deprecated
+ijson
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[datahub-rest]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-uvicorn
-
-[datahub-rest]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
-requests
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
+requests
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
 entrypoints
+Deprecated
+ijson
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[dbt]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-
-[dbt]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
-requests
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
+requests
+psutil>=5.8.0
+python-dateutil>=2.8.0
 botocore!=1.23.0
-aiohttp<4
+progressbar2
 boto3
 entrypoints
+Deprecated
+ijson
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[dbt-cloud]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-
-[dbt-cloud]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
-requests
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
+requests
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
 entrypoints
+Deprecated
+ijson
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[delta-lake]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
+wcmatch
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
-expandvars>=0.6.5
-
-[delta-lake]
-Deprecated
-python-dateutil>=2.8.0
-parse>=1.19.0
-ratelimiter
+aiohttp<4
 tableschema>=1.20.2
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
+toml>=0.10.0
+moto[s3]
+ujson>=5.2.0
+psutil>=5.8.0
 pyspark==3.0.3
 pyarrow>=6.0.1
-toml>=0.10.0
+python-dateutil>=2.8.0
+smart-open[s3]>=5.2.1
+deltalake!=0.6.4,>=0.6.3
 botocore!=1.23.0
-aiohttp<4
+progressbar2
 boto3
 entrypoints
-termcolor>=1.0.0
-stackprinter>=0.2.6
-avro<1.11,>=1.10.2
-requests_file
-packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
-ujson>=5.2.0
-moto[s3]
-tabulate
-cached_property
-humanfriendly
+Deprecated
 ijson
-docker
-click>=7.1.2
-click-spinner
 pydeequ>=1.0.1
-expandvars>=0.6.5
-wcmatch
-deltalake!=0.6.4,>=0.6.3
-smart-open[s3]>=5.2.1
+parse>=1.19.0
+termcolor>=1.0.0
 
 [dev]
-scipy>=1.7.2
-boto3
-looker-sdk==22.2.1
-mypy_extensions>=0.4.3
-black==22.12.0
-types-tabulate
-moto[s3]
+types-dataclasses
+more-itertools>=8.12.0
+types-PyMySQL
+acryl-pyhive[hive]>=0.6.13
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
 clickhouse-sqlalchemy>=0.1.8
-databricks-dbapi
-databricks-cli==0.17.3
-ijson
-psycopg2-binary
+mixpanel>=4.9.0
+types-Deprecated
 types-protobuf>=4.21.0.1
-sqlparse
-fastavro>=1.2.0
-flake8-tidy-imports>=4.3.0
-fastapi
 toml>=0.10.0
-sql_metadata
-stackprinter>=0.2.6
-pytest>=6.2.2
-avro-gen3==0.7.8
-grpcio<2,>=1.44.0
+requests
+scipy>=1.7.2
 psutil>=5.8.0
-mixpanel>=4.9.0
-ujson>=5.2.0
-GitPython>2
+isort>=5.7.0
+smart-open[s3]>=5.2.1
+types-pkg_resources
+progressbar2
+types-pytz
+msal
+coverage>=5.1
+greenlet
 click>=7.1.2
-acryl-pyhive[hive]>=0.6.13
+sql-metadata==2.2.2
+jsonref
+acryl-datahub-classify==0.0.6
+pytest-cov>=2.8.1
+requests-mock
+apache-airflow[snowflake]>=2.0.2
+types-termcolor>=1.0.0
 boto3-stubs[glue,s3,sagemaker,sts]
-types-ujson>=5.2.0
-parse>=1.19.0
+sqlparse
+tableauserverclient>=0.17.0
+pyspark==3.0.3
+snowflake-connector-python!=2.8.2,<3.0.0
+freezegun
+python-ldap>=2.4
+types-requests>=2.28.11.6
+types-python-dateutil
+botocore!=1.23.0
+types-click==0.1.12
+ijson
+pydeequ>=1.0.1
+okta~=1.7.0
+packaging
+cx_Oracle
 tableschema>=1.20.2
-redash-toolbelt
-great-expectations<=0.15.41,>=0.15.12
-apache-airflow[snowflake]>=2.0.2
+trino[sqlalchemy]!=0.317,>=0.308
+spacy==3.4.3
+ratelimiter
+msal==1.16.0
+black==22.12.0
+pyarrow>=6.0.1
+redshift-connector
+fastavro>=1.2.0
+flake8>=3.8.3
+types-click-spinner>=0.1.13.1
+sqlalchemy-redshift
+cached_property
 pydantic>=1.9.0
-types-pytz
-progressbar2
-PyYAML
+looker-sdk==23.0.0
+expandvars>=0.6.5
 confluent_kafka>=1.5.0
+fastapi
+flake8-bugbear==23.3.12
 acryl-pyhive[hive]>=0.6.12
-acryl-datahub-classify>=0.0.4
-types-dataclasses
-sqlalchemy-redshift
-pandas
-types-click-spinner>=0.1.13.1
-types-toml
-ratelimiter
-types-termcolor>=1.0.0
-GeoAlchemy2
-lkml>=1.3.0b5
-entrypoints
-termcolor>=1.0.0
-avro<1.11,>=1.10.2
-types-pyOpenSSL
-okta~=1.7.0
-types-pkg_resources
-elasticsearch==7.13.4
-cached_property
-google-cloud-bigquery
-types-click==0.1.12
-google-api-python-client
-python-dateutil>=2.8.0
-mypy==0.991
-pydruid>=0.6.2
-azure-identity==1.10.0
-requests
-pyarrow>=6.0.1
-google-cloud-logging<3.1.2
+mypy_extensions>=0.4.3
+SQLAlchemy<1.4.42
+great-expectations<=0.15.50,>=0.15.12
+cryptography
+lark[regex]==1.1.4
 networkx>=2.6.2
-types-python-dateutil
-sqllineage==1.3.6
-typing-inspect
-spacy==3.4.3
+duckdb
 sqlalchemy<2,>=1.3.24
-requests_file
-trino[sqlalchemy]!=0.317,>=0.308
+requests_ntlm
+types-PyYAML
+deepdiff
+pymysql>=1.0.2
+wcmatch
+types-six
+sql_metadata
 google-cloud-datacatalog-lineage==0.2.0
+GeoAlchemy2
+azure-identity==1.10.0
+moto[s3]
+types-pyOpenSSL
+pytest-asyncio>=0.16.0
+sqllineage==1.3.6
+pandas
 snowflake-sqlalchemy!=1.2.5,>=1.2.4
-deepdiff
-humanfriendly
-expandvars>=0.6.5
-types-PyYAML
-types-Deprecated
-acryl-iceberg-legacy==0.0.4
-traitlets<5.2.2
-pytest-docker>=1.0.1
-requests_ntlm
-pyspark==3.0.3
-great-expectations!=0.15.23,!=0.15.24,!=0.15.25,!=0.15.26
+entrypoints
+parse>=1.19.0
+deltalake!=0.6.4,>=0.6.3
+google-cloud-bigquery
+avro<1.11,>=1.10.2
+tabulate
 aiohttp<4
-types-requests>=2.28.11.6
-python-ldap>=2.4
-duckdb
-types-PyMySQL
-sql-metadata
+pytest>=6.2.2
+lkml>=1.3.0b5
+types-ujson>=5.2.0
+click-default-group
+twine
+grpcio<2,>=1.44.0
+google-cloud-logging<=3.5.0
+typing-inspect
+databricks-dbapi
+python-dateutil>=2.8.0
 types-cachetools
-simple-salesforce
+psycopg2-binary
+uvicorn
+great-expectations!=0.15.23,!=0.15.24,!=0.15.25,!=0.15.26
 types-freezegun
-tabulate
-click-spinner
-flake8>=3.8.3
-pydeequ>=1.0.1
-deltalake!=0.6.4,>=0.6.3
+types-toml
+mypy==1.0.0
+pytest-docker>=1.0.1
+termcolor>=1.0.0
+elasticsearch==7.13.4
+pydruid>=0.6.2
+docker
+build
+flake8-tidy-imports>=4.3.0
+redash-toolbelt
+acryl-iceberg-legacy==0.0.4
+pydantic!=1.10.3,>=1.5.1
+types-tabulate
+humanfriendly
+ujson>=5.2.0
+boto3
+GitPython>2
+sql-metadata
+databricks-cli==0.17.3
+jsonschema
 grpcio-tools<2,>=1.44.0
-msal
-botocore!=1.23.0
 jsonpickle
-pytest-cov>=2.8.1
-pymysql>=1.0.2
-coverage>=5.1
-packaging
-msal==1.16.0
-build
-lark[regex]==1.1.4
-freezegun
-cx_Oracle
-tableauserverclient>=0.17.0
+traitlets<5.2.2
 virtualenv
-wcmatch
-twine
-smart-open[s3]>=5.2.1
-uvicorn
-Deprecated
-click-default-group
-more-itertools>=8.12.0
 sqlalchemy-bigquery>=1.4.1
-pytest-asyncio>=0.16.0
-sql-metadata==2.2.2
-pydantic!=1.10.3,>=1.5.1
-isort>=5.7.0
-greenlet
-snowflake-connector-python!=2.8.2
-docker
-types-six
-SQLAlchemy<1.4.42
-cryptography
-requests-mock
+Deprecated
+simple-salesforce
 
 [dev:platform_system != "Darwin" and (platform_machine == "aarch64" or platform_machine == "arm64")]
 confluent_kafka<1.9.0
 
 [dev:python_version < "3.8"]
 typing_extensions>=3.7.4.3
 
 [dev:python_version >= "3.8"]
 typing_extensions>=3.10.0.2
 
 [druid]
-Deprecated
-python-dateutil>=2.8.0
-traitlets<5.2.2
-ratelimiter
+avro<1.11,>=1.10.2
+cached_property
+click>=7.1.2
+jsonref
+packaging
 pydruid>=0.6.2
+tabulate
+docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
-scipy>=1.7.2
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
-great-expectations<=0.15.41,>=0.15.12
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
+traitlets<5.2.2
+progressbar2
 entrypoints
-termcolor>=1.0.0
 sqlalchemy<2,>=1.3.24
-stackprinter>=0.2.6
+Deprecated
+ijson
 greenlet
+termcolor>=1.0.0
+
+[elasticsearch]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+elasticsearch==7.13.4
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-
-[elasticsearch]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
 entrypoints
+Deprecated
+ijson
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[feast]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
-elasticsearch==7.13.4
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-
-[feast]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
+feast~=0.29.0
 toml>=0.10.0
-aiohttp<4
 flask-openid>=1.3.0
+typeguard<3
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
 entrypoints
+Deprecated
+ijson
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[glue]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
-feast~=0.29.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-
-[glue]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
+psutil>=5.8.0
+python-dateutil>=2.8.0
 botocore!=1.23.0
-aiohttp<4
+progressbar2
 boto3
 entrypoints
+Deprecated
+ijson
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[great-expectations]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-
-[great-expectations]
-Deprecated
-python-dateutil>=2.8.0
-traitlets<5.2.2
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
-scipy>=1.7.2
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
-great-expectations<=0.15.41,>=0.15.12
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
+traitlets<5.2.2
 sqllineage==1.3.6
+progressbar2
 entrypoints
-termcolor>=1.0.0
 sqlalchemy<2,>=1.3.24
-stackprinter>=0.2.6
+Deprecated
+ijson
 greenlet
+termcolor>=1.0.0
+
+[hana]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-
-[hana]
-Deprecated
-python-dateutil>=2.8.0
-traitlets<5.2.2
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
-scipy>=1.7.2
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
-great-expectations<=0.15.41,>=0.15.12
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
+traitlets<5.2.2
+progressbar2
 entrypoints
-termcolor>=1.0.0
 sqlalchemy<2,>=1.3.24
-stackprinter>=0.2.6
-greenlet
-avro<1.11,>=1.10.2
-requests_file
-packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
-tabulate
-cached_property
-humanfriendly
+Deprecated
 ijson
-docker
-click>=7.1.2
-click-spinner
-expandvars>=0.6.5
+greenlet
+termcolor>=1.0.0
 
 [hana:platform_machine != "aarch64" and platform_machine != "arm64"]
-hdbcli>=2.11.20
 sqlalchemy-hana>=0.5.0
+hdbcli>=2.11.20
 
 [hive]
-Deprecated
-python-dateutil>=2.8.0
-traitlets<5.2.2
-ratelimiter
+avro<1.11,>=1.10.2
+cached_property
+click>=7.1.2
+jsonref
+packaging
+tabulate
+docker
+aiohttp<4
+expandvars>=0.6.5
+acryl-pyhive[hive]>=0.6.13
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
+humanfriendly
+toml>=0.10.0
 scipy>=1.7.2
+psutil>=5.8.0
+databricks-dbapi
+python-dateutil>=2.8.0
+traitlets<5.2.2
 great-expectations!=0.15.23,!=0.15.24,!=0.15.25,!=0.15.26
-toml>=0.10.0
-aiohttp<4
-great-expectations<=0.15.41,>=0.15.12
+progressbar2
 entrypoints
-termcolor>=1.0.0
 sqlalchemy<2,>=1.3.24
-stackprinter>=0.2.6
+Deprecated
+ijson
 greenlet
+termcolor>=1.0.0
+
+[iceberg]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
-databricks-dbapi
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
-acryl-pyhive[hive]>=0.6.13
+aiohttp<4
 expandvars>=0.6.5
-
-[iceberg]
-Deprecated
-python-dateutil>=2.8.0
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
 azure-identity==1.10.0
 ratelimiter
-click-default-group
+humanfriendly
+acryl-iceberg-legacy==0.0.4
 toml>=0.10.0
-aiohttp<4
-entrypoints
-termcolor>=1.0.0
-stackprinter>=0.2.6
-avro<1.11,>=1.10.2
-requests_file
-packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
 psutil>=5.8.0
-tabulate
-cached_property
-humanfriendly
+python-dateutil>=2.8.0
+progressbar2
+entrypoints
+Deprecated
 ijson
-docker
-click>=7.1.2
-click-spinner
-expandvars>=0.6.5
-acryl-iceberg-legacy==0.0.4
+termcolor>=1.0.0
 
 [integration-tests]
-parse>=1.19.0
-tableschema>=1.20.2
+pymysql>=1.0.2
+wcmatch
+packaging
 pydruid>=0.6.2
+tableschema>=1.20.2
+acryl-pyhive[hive]>=0.6.13
 azure-identity==1.10.0
+clickhouse-sqlalchemy>=0.1.8
+redash-toolbelt
+PyAthena[sqlalchemy]==2.4.1
+acryl-iceberg-legacy==0.0.4
+JPype1
+requests
+moto[s3]
 scipy>=1.7.2
+ujson>=5.2.0
 pyarrow>=6.0.1
-requests
-pymongo[srv]>=3.11
-botocore!=1.23.0
-redash-toolbelt
-boto3
-great-expectations<=0.15.41,>=0.15.12
-pymysql>=1.0.2
+smart-open[s3]>=5.2.1
 sqllineage==1.3.6
+boto3
+parse>=1.19.0
+deltalake!=0.6.4,>=0.6.3
+greenlet
 gql[requests]>=3.3.0
-sqlalchemy<2,>=1.3.24
-packaging
-PyAthena[sqlalchemy]==2.4.1
-moto[s3]
-clickhouse-sqlalchemy>=0.1.8
+sql-metadata
+sqlalchemy-pytds>=0.3
+great-expectations<=0.15.50,>=0.15.12
+pymongo[srv]>=3.11
+pyspark==3.0.3
 databricks-dbapi
-wcmatch
-smart-open[s3]>=5.2.1
-acryl-iceberg-legacy==0.0.4
 traitlets<5.2.2
-JPype1
-pyspark==3.0.3
-great-expectations!=0.15.23,!=0.15.24,!=0.15.25,!=0.15.26
 python-ldap>=2.4
-sql-metadata
-greenlet
-ujson>=5.2.0
+great-expectations!=0.15.23,!=0.15.24,!=0.15.25,!=0.15.26
+botocore!=1.23.0
+sqlalchemy<2,>=1.3.24
 gql>=3.3.0
-acryl-pyhive[hive]>=0.6.13
 pydeequ>=1.0.1
-deltalake!=0.6.4,>=0.6.3
-sqlalchemy-pytds>=0.3
 
 [integration-tests:platform_machine != "aarch64" and platform_machine != "arm64"]
-hdbcli>=2.11.20
 sqlalchemy-hana>=0.5.0
+hdbcli>=2.11.20
 
-[kafka]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
+[json-schema]
+avro<1.11,>=1.10.2
+cached_property
+click>=7.1.2
+jsonref
+packaging
+tabulate
+docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
-grpcio-tools<2,>=1.44.0
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
-networkx>=2.6.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
 entrypoints
+Deprecated
+ijson
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[kafka]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-grpcio<2,>=1.44.0
-PyYAML
-psutil>=5.8.0
-confluent_kafka>=1.5.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
+confluent_kafka>=1.5.0
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+grpcio<2,>=1.44.0
+ratelimiter
+humanfriendly
+toml>=0.10.0
+psutil>=5.8.0
+grpcio-tools<2,>=1.44.0
+networkx>=2.6.2
+python-dateutil>=2.8.0
 fastavro>=1.2.0
+progressbar2
+entrypoints
+Deprecated
+ijson
+termcolor>=1.0.0
 
 [kafka-connect]
-Deprecated
-python-dateutil>=2.8.0
-traitlets<5.2.2
+avro<1.11,>=1.10.2
+cached_property
+click>=7.1.2
+jsonref
+packaging
+tabulate
+docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+great-expectations<=0.15.50,>=0.15.12
 ratelimiter
+humanfriendly
 JPype1
-click-default-group
+toml>=0.10.0
 requests
 scipy>=1.7.2
-toml>=0.10.0
-aiohttp<4
-great-expectations<=0.15.41,>=0.15.12
+psutil>=5.8.0
+python-dateutil>=2.8.0
+traitlets<5.2.2
+progressbar2
 entrypoints
-termcolor>=1.0.0
 sqlalchemy<2,>=1.3.24
-stackprinter>=0.2.6
-greenlet
-avro<1.11,>=1.10.2
-requests_file
-packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
-tabulate
-cached_property
-humanfriendly
+Deprecated
 ijson
-docker
-click>=7.1.2
-click-spinner
-expandvars>=0.6.5
+greenlet
+termcolor>=1.0.0
 
 [kafka:platform_system != "Darwin" and (platform_machine == "aarch64" or platform_machine == "arm64")]
 confluent_kafka<1.9.0
 
 [ldap]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
+avro<1.11,>=1.10.2
+cached_property
+click>=7.1.2
+jsonref
+packaging
+tabulate
+docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
+psutil>=5.8.0
+python-dateutil>=2.8.0
 python-ldap>=2.4
+progressbar2
 entrypoints
+Deprecated
+ijson
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[looker]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+sql-metadata==2.2.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
+lkml>=1.3.0b5
 expandvars>=0.6.5
-
-[looker]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
+looker-sdk==23.0.0
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
-lkml>=1.3.0b5
-aiohttp<4
-sql-metadata==2.2.2
-looker-sdk==22.2.1
+psutil>=5.8.0
+python-dateutil>=2.8.0
 sqllineage==1.3.6
+progressbar2
 entrypoints
+GitPython>2
+Deprecated
+ijson
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[lookml]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+sql-metadata==2.2.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
-GitPython>2
+aiohttp<4
+lkml>=1.3.0b5
 expandvars>=0.6.5
-
-[lookml]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
+looker-sdk==23.0.0
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
-lkml>=1.3.0b5
-aiohttp<4
-sql-metadata==2.2.2
-looker-sdk==22.2.1
+psutil>=5.8.0
+python-dateutil>=2.8.0
 sqllineage==1.3.6
+progressbar2
 entrypoints
+GitPython>2
+Deprecated
+ijson
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[mariadb]
+pymysql>=1.0.2
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
-GitPython>2
+aiohttp<4
 expandvars>=0.6.5
-
-[mariadb]
-Deprecated
-python-dateutil>=2.8.0
-traitlets<5.2.2
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
-scipy>=1.7.2
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
-pymysql>=1.0.2
-great-expectations<=0.15.41,>=0.15.12
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
+traitlets<5.2.2
+progressbar2
 entrypoints
-termcolor>=1.0.0
 sqlalchemy<2,>=1.3.24
-stackprinter>=0.2.6
+Deprecated
+ijson
 greenlet
+termcolor>=1.0.0
+
+[metabase]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-
-[metabase]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
-requests
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
+requests
+psutil>=5.8.0
+python-dateutil>=2.8.0
 sqllineage==1.3.6
+progressbar2
 entrypoints
+Deprecated
+ijson
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[mode]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-
-[mode]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
-requests
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
+requests
+psutil>=5.8.0
 tenacity>=8.0.1
+python-dateutil>=2.8.0
 sqllineage==1.3.6
+progressbar2
 entrypoints
+Deprecated
+ijson
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[mongodb]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-
-[mongodb]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
-pymongo[srv]>=3.11
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
+pymongo[srv]>=3.11
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
 entrypoints
+Deprecated
+ijson
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[mssql]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-
-[mssql]
-Deprecated
-python-dateutil>=2.8.0
-traitlets<5.2.2
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
-scipy>=1.7.2
+sqlalchemy-pytds>=0.3
+ratelimiter
+humanfriendly
+great-expectations<=0.15.50,>=0.15.12
 toml>=0.10.0
-aiohttp<4
-great-expectations<=0.15.41,>=0.15.12
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
+traitlets<5.2.2
+progressbar2
 entrypoints
-termcolor>=1.0.0
 sqlalchemy<2,>=1.3.24
-stackprinter>=0.2.6
+Deprecated
+ijson
 greenlet
+termcolor>=1.0.0
+
+[mssql-odbc]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-sqlalchemy-pytds>=0.3
-
-[mssql-odbc]
-Deprecated
-python-dateutil>=2.8.0
-traitlets<5.2.2
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
-scipy>=1.7.2
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
-great-expectations<=0.15.41,>=0.15.12
+scipy>=1.7.2
+psutil>=5.8.0
+pyodbc
+python-dateutil>=2.8.0
+traitlets<5.2.2
+progressbar2
 entrypoints
-termcolor>=1.0.0
 sqlalchemy<2,>=1.3.24
-stackprinter>=0.2.6
+Deprecated
+ijson
 greenlet
+termcolor>=1.0.0
+
+[mysql]
+pymysql>=1.0.2
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-pyodbc
-
-[mysql]
-Deprecated
-python-dateutil>=2.8.0
-traitlets<5.2.2
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
-scipy>=1.7.2
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
-pymysql>=1.0.2
-great-expectations<=0.15.41,>=0.15.12
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
+traitlets<5.2.2
+progressbar2
 entrypoints
-termcolor>=1.0.0
 sqlalchemy<2,>=1.3.24
-stackprinter>=0.2.6
+Deprecated
+ijson
 greenlet
+termcolor>=1.0.0
+
+[nifi]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-
-[nifi]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
-requests
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
+requests
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
 entrypoints
+Deprecated
+ijson
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[okta]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+okta~=1.7.0
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-
-[okta]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
 entrypoints
+Deprecated
+ijson
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[oracle]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
-okta~=1.7.0
+cx_Oracle
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-
-[oracle]
-Deprecated
-python-dateutil>=2.8.0
-traitlets<5.2.2
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
-scipy>=1.7.2
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
-great-expectations<=0.15.41,>=0.15.12
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
+traitlets<5.2.2
+progressbar2
 entrypoints
-termcolor>=1.0.0
 sqlalchemy<2,>=1.3.24
-stackprinter>=0.2.6
+Deprecated
+ijson
 greenlet
+termcolor>=1.0.0
+
+[postgres]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
-cx_Oracle
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
+GeoAlchemy2
 expandvars>=0.6.5
-
-[postgres]
-Deprecated
-python-dateutil>=2.8.0
-traitlets<5.2.2
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
-scipy>=1.7.2
-GeoAlchemy2
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
-great-expectations<=0.15.41,>=0.15.12
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
+psycopg2-binary
+traitlets<5.2.2
+progressbar2
 entrypoints
-termcolor>=1.0.0
 sqlalchemy<2,>=1.3.24
-stackprinter>=0.2.6
+Deprecated
+ijson
 greenlet
+termcolor>=1.0.0
+
+[powerbi]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
-psycopg2-binary
+aiohttp<4
 expandvars>=0.6.5
-
-[powerbi]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+lark[regex]==1.1.4
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
+msal==1.16.0
+sqlparse
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
 entrypoints
+Deprecated
+ijson
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[powerbi-report-server]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-msal==1.16.0
-PyYAML
-psutil>=5.8.0
-lark[regex]==1.1.4
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
-sqlparse
+aiohttp<4
 expandvars>=0.6.5
-
-[powerbi-report-server]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
-requests
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
+requests
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
 entrypoints
+Deprecated
+ijson
+requests_ntlm
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[presto]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
+trino[sqlalchemy]!=0.317,>=0.308
 expandvars>=0.6.5
-requests_ntlm
-
-[presto]
-Deprecated
-python-dateutil>=2.8.0
-traitlets<5.2.2
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
-scipy>=1.7.2
+acryl-pyhive[hive]>=0.6.12
+ratelimiter
+humanfriendly
+great-expectations<=0.15.50,>=0.15.12
 toml>=0.10.0
-aiohttp<4
-great-expectations<=0.15.41,>=0.15.12
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
+traitlets<5.2.2
+progressbar2
 entrypoints
-termcolor>=1.0.0
 sqlalchemy<2,>=1.3.24
-stackprinter>=0.2.6
+Deprecated
+ijson
 greenlet
+termcolor>=1.0.0
+
+[presto-on-hive]
+pymysql>=1.0.2
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-trino[sqlalchemy]!=0.317,>=0.308
-PyYAML
-psutil>=5.8.0
-acryl-pyhive[hive]>=0.6.12
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-
-[presto-on-hive]
-Deprecated
-python-dateutil>=2.8.0
-traitlets<5.2.2
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
-scipy>=1.7.2
+acryl-pyhive[hive]>=0.6.12
+ratelimiter
+humanfriendly
+great-expectations<=0.15.50,>=0.15.12
 toml>=0.10.0
-aiohttp<4
-pymysql>=1.0.2
-great-expectations<=0.15.41,>=0.15.12
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
+psycopg2-binary
+traitlets<5.2.2
+progressbar2
 entrypoints
-termcolor>=1.0.0
 sqlalchemy<2,>=1.3.24
-stackprinter>=0.2.6
+Deprecated
+ijson
 greenlet
+termcolor>=1.0.0
+
+[pulsar]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
-acryl-pyhive[hive]>=0.6.12
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
-psycopg2-binary
+aiohttp<4
 expandvars>=0.6.5
-
-[pulsar]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
-requests
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
+requests
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
 entrypoints
+Deprecated
+ijson
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[redash]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-
-[redash]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
 redash-toolbelt
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
+psutil>=5.8.0
+python-dateutil>=2.8.0
 sqllineage==1.3.6
+progressbar2
 entrypoints
-termcolor>=1.0.0
+Deprecated
+ijson
 sql-metadata
-stackprinter>=0.2.6
-avro<1.11,>=1.10.2
-requests_file
+termcolor>=1.0.0
+
+[redshift]
+wcmatch
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
-tabulate
-cached_property
-humanfriendly
-ijson
+GeoAlchemy2
 docker
-click>=7.1.2
+avro-gen3==0.7.10
+PyYAML
 click-spinner
-expandvars>=0.6.5
-
-[redshift]
-Deprecated
-python-dateutil>=2.8.0
-traitlets<5.2.2
+requests_file
 ratelimiter
-parse>=1.19.0
-click-default-group
-scipy>=1.7.2
-GeoAlchemy2
+humanfriendly
 toml>=0.10.0
-aiohttp<4
-great-expectations<=0.15.41,>=0.15.12
+scipy>=1.7.2
+psutil>=5.8.0
+redshift-connector
 sqllineage==1.3.6
+progressbar2
 entrypoints
-termcolor>=1.0.0
-sqlalchemy<2,>=1.3.24
-stackprinter>=0.2.6
+parse>=1.19.0
 greenlet
+sqlalchemy-redshift
 avro<1.11,>=1.10.2
-requests_file
-packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
-tabulate
 cached_property
-humanfriendly
-ijson
-docker
 click>=7.1.2
-click-spinner
-sqlalchemy-redshift
+jsonref
+tabulate
+aiohttp<4
 expandvars>=0.6.5
-psycopg2-binary
-wcmatch
-
-[redshift-usage]
-Deprecated
+jsonschema
+click-default-group
+great-expectations<=0.15.50,>=0.15.12
+sqlparse
 python-dateutil>=2.8.0
 traitlets<5.2.2
-ratelimiter
-parse>=1.19.0
-click-default-group
-scipy>=1.7.2
+psycopg2-binary
+sqlalchemy<2,>=1.3.24
+Deprecated
+ijson
+termcolor>=1.0.0
+
+[redshift-legacy]
+avro<1.11,>=1.10.2
+cached_property
+click>=7.1.2
+jsonref
+wcmatch
+packaging
+tabulate
+docker
+aiohttp<4
 GeoAlchemy2
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
-great-expectations<=0.15.41,>=0.15.12
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
+traitlets<5.2.2
 sqllineage==1.3.6
+psycopg2-binary
+progressbar2
 entrypoints
-termcolor>=1.0.0
 sqlalchemy<2,>=1.3.24
-stackprinter>=0.2.6
+Deprecated
+ijson
+parse>=1.19.0
 greenlet
+sqlalchemy-redshift
+termcolor>=1.0.0
+
+[redshift-usage-legacy]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
+wcmatch
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
-sqlalchemy-redshift
+aiohttp<4
+GeoAlchemy2
 expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
+click-default-group
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
+humanfriendly
+toml>=0.10.0
+scipy>=1.7.2
+psutil>=5.8.0
 sqlparse
+python-dateutil>=2.8.0
+traitlets<5.2.2
+sqllineage==1.3.6
 psycopg2-binary
-wcmatch
-
-[s3]
+progressbar2
+entrypoints
+sqlalchemy<2,>=1.3.24
 Deprecated
-python-dateutil>=2.8.0
+ijson
 parse>=1.19.0
-ratelimiter
+greenlet
+sqlalchemy-redshift
+termcolor>=1.0.0
+
+[s3]
+avro<1.11,>=1.10.2
+cached_property
+click>=7.1.2
+jsonref
+wcmatch
+packaging
+tabulate
+docker
+aiohttp<4
 tableschema>=1.20.2
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
+toml>=0.10.0
+moto[s3]
+ujson>=5.2.0
+psutil>=5.8.0
 pyarrow>=6.0.1
 pyspark==3.0.3
-toml>=0.10.0
+python-dateutil>=2.8.0
+smart-open[s3]>=5.2.1
 botocore!=1.23.0
-aiohttp<4
+progressbar2
 boto3
 entrypoints
+Deprecated
+ijson
+pydeequ>=1.0.1
+parse>=1.19.0
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[sagemaker]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
-ujson>=5.2.0
-moto[s3]
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
-wcmatch
+aiohttp<4
 expandvars>=0.6.5
-pydeequ>=1.0.1
-smart-open[s3]>=5.2.1
-
-[sagemaker]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
+psutil>=5.8.0
+python-dateutil>=2.8.0
 botocore!=1.23.0
-aiohttp<4
+progressbar2
 boto3
 entrypoints
+Deprecated
+ijson
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[salesforce]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-
-[salesforce]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
-entrypoints
-termcolor>=1.0.0
-stackprinter>=0.2.6
-avro<1.11,>=1.10.2
-requests_file
-packaging
-avro-gen3==0.7.8
-progressbar2
-simple-salesforce
-PyYAML
 psutil>=5.8.0
-tabulate
-cached_property
-humanfriendly
+python-dateutil>=2.8.0
+progressbar2
+entrypoints
+Deprecated
 ijson
-docker
-click>=7.1.2
-click-spinner
-expandvars>=0.6.5
+simple-salesforce
+termcolor>=1.0.0
 
 [snowflake]
-python-dateutil>=2.8.0
-msal
-scipy>=1.7.2
-great-expectations<=0.15.41,>=0.15.12
-spacy==3.4.3
-sqlalchemy<2,>=1.3.24
-requests_file
 packaging
-progressbar2
+docker
+avro-gen3==0.7.10
 PyYAML
-snowflake-sqlalchemy!=1.2.5,>=1.2.4
-acryl-datahub-classify>=0.0.4
-humanfriendly
-ijson
-pandas
-sqlparse
-expandvars>=0.6.5
-Deprecated
-traitlets<5.2.2
+click-spinner
+requests_file
+spacy==3.4.3
 ratelimiter
-click-default-group
+humanfriendly
 toml>=0.10.0
-aiohttp<4
+scipy>=1.7.2
+psutil>=5.8.0
+pandas
+snowflake-sqlalchemy!=1.2.5,>=1.2.4
+progressbar2
 entrypoints
-termcolor>=1.0.0
-stackprinter>=0.2.6
+msal
 greenlet
 avro<1.11,>=1.10.2
-avro-gen3==0.7.8
-psutil>=5.8.0
-tabulate
 cached_property
-docker
 click>=7.1.2
-click-spinner
-snowflake-connector-python!=2.8.2
+acryl-datahub-classify==0.0.6
+jsonref
+tabulate
+aiohttp<4
+expandvars>=0.6.5
 SQLAlchemy<1.4.42
+jsonschema
 cryptography
-
-[snowflake-beta]
+click-default-group
+great-expectations<=0.15.50,>=0.15.12
+sqlparse
 python-dateutil>=2.8.0
-msal
-scipy>=1.7.2
-great-expectations<=0.15.41,>=0.15.12
-spacy==3.4.3
+snowflake-connector-python!=2.8.2,<3.0.0
+traitlets<5.2.2
 sqlalchemy<2,>=1.3.24
-requests_file
+Deprecated
+ijson
+termcolor>=1.0.0
+
+[snowflake-beta]
 packaging
-progressbar2
+docker
+avro-gen3==0.7.10
 PyYAML
-snowflake-sqlalchemy!=1.2.5,>=1.2.4
-acryl-datahub-classify>=0.0.4
-humanfriendly
-ijson
-pandas
-sqlparse
-expandvars>=0.6.5
-Deprecated
-traitlets<5.2.2
+click-spinner
+requests_file
+spacy==3.4.3
 ratelimiter
-click-default-group
+humanfriendly
 toml>=0.10.0
-aiohttp<4
+scipy>=1.7.2
+psutil>=5.8.0
+pandas
+snowflake-sqlalchemy!=1.2.5,>=1.2.4
+progressbar2
 entrypoints
-termcolor>=1.0.0
-stackprinter>=0.2.6
+msal
 greenlet
 avro<1.11,>=1.10.2
-avro-gen3==0.7.8
-psutil>=5.8.0
-tabulate
 cached_property
-docker
 click>=7.1.2
-click-spinner
-snowflake-connector-python!=2.8.2
+acryl-datahub-classify==0.0.6
+jsonref
+tabulate
+aiohttp<4
+expandvars>=0.6.5
 SQLAlchemy<1.4.42
+jsonschema
 cryptography
-
-[sqlalchemy]
-Deprecated
+click-default-group
+great-expectations<=0.15.50,>=0.15.12
+sqlparse
 python-dateutil>=2.8.0
+snowflake-connector-python!=2.8.2,<3.0.0
 traitlets<5.2.2
-ratelimiter
+sqlalchemy<2,>=1.3.24
+Deprecated
+ijson
+termcolor>=1.0.0
+
+[sqlalchemy]
+avro<1.11,>=1.10.2
+cached_property
+click>=7.1.2
+jsonref
+packaging
+tabulate
+docker
+aiohttp<4
+expandvars>=0.6.5
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
-scipy>=1.7.2
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
-great-expectations<=0.15.41,>=0.15.12
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
+traitlets<5.2.2
+progressbar2
 entrypoints
-termcolor>=1.0.0
 sqlalchemy<2,>=1.3.24
-stackprinter>=0.2.6
+Deprecated
+ijson
 greenlet
+termcolor>=1.0.0
+
+[starburst-trino-usage]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
+trino[sqlalchemy]!=0.317,>=0.308
 expandvars>=0.6.5
-
-[starburst-trino-usage]
-Deprecated
-python-dateutil>=2.8.0
-traitlets<5.2.2
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
-scipy>=1.7.2
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
-great-expectations<=0.15.41,>=0.15.12
+scipy>=1.7.2
+psutil>=5.8.0
+sqlparse
+python-dateutil>=2.8.0
+traitlets<5.2.2
+progressbar2
 entrypoints
-termcolor>=1.0.0
 sqlalchemy<2,>=1.3.24
-stackprinter>=0.2.6
+Deprecated
+ijson
 greenlet
+termcolor>=1.0.0
+
+[superset]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-trino[sqlalchemy]!=0.317,>=0.308
-PyYAML
-psutil>=5.8.0
+great_expectations
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
-sqlparse
+aiohttp<4
 expandvars>=0.6.5
-
-[superset]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
-requests
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
+requests
+sqlalchemy
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
 entrypoints
-termcolor>=1.0.0
-stackprinter>=0.2.6
+Deprecated
+ijson
 greenlet
-sqlalchemy
+termcolor>=1.0.0
+
+[tableau]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-great_expectations
-
-[tableau]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
+psutil>=5.8.0
+tableauserverclient>=0.17.0
+python-dateutil>=2.8.0
+sqllineage==1.3.6
+progressbar2
 entrypoints
+Deprecated
+ijson
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[trino]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
-tableauserverclient>=0.17.0
+aiohttp<4
+trino[sqlalchemy]!=0.317,>=0.308
 expandvars>=0.6.5
-
-[trino]
-Deprecated
-python-dateutil>=2.8.0
-traitlets<5.2.2
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
-scipy>=1.7.2
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
-great-expectations<=0.15.41,>=0.15.12
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
+traitlets<5.2.2
+progressbar2
 entrypoints
-termcolor>=1.0.0
 sqlalchemy<2,>=1.3.24
-stackprinter>=0.2.6
+Deprecated
+ijson
 greenlet
+termcolor>=1.0.0
+
+[unity-catalog]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+databricks-cli==0.17.3
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-trino[sqlalchemy]!=0.317,>=0.308
-PyYAML
-psutil>=5.8.0
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-
-[unity-catalog]
-Deprecated
-python-dateutil>=2.8.0
-ratelimiter
-requests
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
+requests
+psutil>=5.8.0
+python-dateutil>=2.8.0
+progressbar2
 entrypoints
+Deprecated
+ijson
 termcolor>=1.0.0
-stackprinter>=0.2.6
+
+[vertica]
 avro<1.11,>=1.10.2
-requests_file
+cached_property
+click>=7.1.2
+jsonref
 packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
-databricks-cli==0.17.3
 tabulate
-cached_property
-humanfriendly
-ijson
 docker
-click>=7.1.2
-click-spinner
+aiohttp<4
 expandvars>=0.6.5
-
-[vertica]
-Deprecated
-python-dateutil>=2.8.0
-traitlets<5.2.2
-ratelimiter
+avro-gen3==0.7.10
+PyYAML
+click-spinner
+requests_file
+jsonschema
 click-default-group
-scipy>=1.7.2
+great-expectations<=0.15.50,>=0.15.12
+ratelimiter
+humanfriendly
 toml>=0.10.0
-aiohttp<4
-great-expectations<=0.15.41,>=0.15.12
+scipy>=1.7.2
+psutil>=5.8.0
+python-dateutil>=2.8.0
+traitlets<5.2.2
+progressbar2
 entrypoints
-termcolor>=1.0.0
 sqlalchemy<2,>=1.3.24
-stackprinter>=0.2.6
-greenlet
-avro<1.11,>=1.10.2
-requests_file
-packaging
-avro-gen3==0.7.8
-progressbar2
-PyYAML
-psutil>=5.8.0
-vertica-sqlalchemy-dialect[vertica-python]==0.0.1
-tabulate
-cached_property
-humanfriendly
+Deprecated
 ijson
-docker
-click>=7.1.2
-click-spinner
-expandvars>=0.6.5
+vertica-sqlalchemy-dialect[vertica-python]==0.0.1
+greenlet
+termcolor>=1.0.0
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/api/circuit_breaker/assertion_circuit_breaker.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/api/circuit_breaker/assertion_circuit_breaker.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/api/circuit_breaker/circuit_breaker.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/api/circuit_breaker/circuit_breaker.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/api/circuit_breaker/operation_circuit_breaker.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/api/circuit_breaker/operation_circuit_breaker.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/api/entities/corpgroup/corpgroup.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/lite/lite_local.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,84 +1,114 @@
-from __future__ import annotations
+from abc import ABCMeta, abstractmethod
+from enum import Enum
+from typing import Dict, Generic, Iterable, List, Optional, Type, TypeVar, Union, cast
 
-from dataclasses import dataclass, field
-from typing import Callable, Iterable, Optional, Union
-
-import datahub.emitter.mce_builder as builder
-from datahub.emitter.kafka_emitter import DatahubKafkaEmitter
+from datahub.configuration.common import ConfigModel
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
-from datahub.emitter.rest_emitter import DatahubRestEmitter
-from datahub.metadata.schema_classes import (
-    CorpGroupEditableInfoClass,
-    CorpGroupInfoClass,
-)
-
-
-@dataclass
-class CorpGroup:
-    """This is a CorpGroup class which represents a CorpGroup
-
-    Args:
-        id (str): The id of the group
-        display_name (Optional[str]): The name of the group
-        email (Optional[str]): email of this group
-        description (Optional[str]): A description of the group
-        overrideEditable (bool): If True, group information that is editable in the UI will be overridden
-        picture_link (Optional[str]): A URL which points to a picture which user wants to set as the photo for the group
-        slack (Optional[str]): Slack channel for the group
-    """
+from datahub.ingestion.api.closeable import Closeable
+from datahub.metadata.schema_classes import MetadataChangeEventClass, _Aspect
+from datahub.utilities.type_annotations import get_class_from_annotation
+
+LiteConfig = TypeVar("LiteConfig", bound=ConfigModel)
+
 
+class AutoComplete(ConfigModel):
+    success_path: str
+    failed_token: str
+    suggested_path: str
+
+
+class Browseable(ConfigModel):
     id: str
-    urn: str = field(init=False)
+    name: str
+    leaf: bool = False
+    parents: Optional[List[str]] = None
+    auto_complete: Optional[AutoComplete] = None
+
+
+class Searchable(ConfigModel):
+    id: str
+    aspect: Optional[str]
+    snippet: Optional[str]
+
+
+class SearchFlavor(Enum):
+    FREE_TEXT = "free-text"
+    EXACT = "exact"
+
 
-    # These are for CorpGroupInfo
-    display_name: Optional[str] = None
-    email: Optional[str] = None
-    description: Optional[str] = None
-
-    # These are for CorpGroupEditableInfo
-    overrideEditable: bool = False
-    picture_link: Optional[str] = None
-    slack: Optional[str] = None
-
-    def __post_init__(self):
-        self.urn = builder.make_group_urn(self.id)
-
-    def generate_mcp(self) -> Iterable[MetadataChangeProposalWrapper]:
-        if self.overrideEditable:
-            mcp = MetadataChangeProposalWrapper(
-                entityUrn=str(self.urn),
-                aspect=CorpGroupEditableInfoClass(
-                    description=self.description,
-                    pictureLink=self.picture_link,
-                    slack=self.slack,
-                    email=self.email,
-                ),
-            )
-            yield mcp
-
-        mcp = MetadataChangeProposalWrapper(
-            entityUrn=str(self.urn),
-            aspect=CorpGroupInfoClass(
-                admins=[],  # Deprecated, replaced by Ownership aspect
-                members=[],  # Deprecated, replaced by GroupMembership aspect
-                groups=[],  # Deprecated, this field is unused
-                displayName=self.display_name,
-                email=self.email,
-                description=self.description,
-            ),
-        )
-        yield mcp
+class PathNotFoundException(Exception):
+    pass
 
-    def emit(
+
+class DataHubLiteLocal(Generic[LiteConfig], Closeable, metaclass=ABCMeta):
+    """The embedded version of DataHub Lite. All DataHub Lite implementations need to extend this class"""
+
+    def __init__(self, config: LiteConfig):
+        pass
+
+    @classmethod
+    def get_config_class(cls) -> Type[LiteConfig]:
+        config_class = get_class_from_annotation(cls, DataHubLiteLocal, ConfigModel)
+        assert config_class, "DataHubLiteLocal subclasses must define a config class"
+        return cast(Type[LiteConfig], config_class)
+
+    @abstractmethod
+    def location(self) -> str:
+        pass
+
+    @abstractmethod
+    def destroy(self) -> None:
+        pass
+
+    @abstractmethod
+    def write(
         self,
-        emitter: Union[DatahubRestEmitter, DatahubKafkaEmitter],
-        callback: Optional[Callable[[Exception, str], None]] = None,
+        record: Union[
+            MetadataChangeEventClass,
+            MetadataChangeProposalWrapper,
+        ],
     ) -> None:
-        """
-        Emit the CorpGroup entity to Datahub
+        pass
+
+    @abstractmethod
+    def list_ids(self) -> Iterable[str]:
+        pass
 
-        :param emitter: Datahub Emitter to emit the proccess event
-        :param callback: The callback method for KafkaEmitter if it is used
-        """
-        for mcp in self.generate_mcp():
-            emitter.emit(mcp, callback)
+    @abstractmethod
+    def get(
+        self,
+        id: str,
+        aspects: Optional[List[str]],
+        typed: bool = False,
+        as_of: Optional[int] = None,
+        details: Optional[bool] = False,
+    ) -> Optional[Dict[str, Union[str, dict, _Aspect]]]:
+        pass
+
+    @abstractmethod
+    def search(
+        self,
+        query: str,
+        flavor: SearchFlavor,
+        aspects: List[str] = [],
+        snippet: bool = True,
+    ) -> Iterable[Searchable]:
+        pass
+
+    @abstractmethod
+    def ls(self, path: str) -> List[Browseable]:
+        pass
+
+    @abstractmethod
+    def get_all_entities(
+        self, typed: bool = False
+    ) -> Iterable[Dict[str, Union[dict, _Aspect]]]:
+        pass
+
+    @abstractmethod
+    def get_all_aspects(self) -> Iterable[MetadataChangeProposalWrapper]:
+        pass
+
+    @abstractmethod
+    def reindex(self) -> None:
+        pass
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/api/entities/datajob/dataflow.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/datajob/dataflow.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/api/entities/datajob/datajob.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/datajob/datajob.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/api/entities/dataprocess/dataprocess_instance.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/api/entities/dataprocess/dataprocess_instance.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/api/graphql/assertion.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/api/graphql/assertion.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/api/graphql/base.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/api/graphql/base.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/api/graphql/operation.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/api/graphql/operation.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/cli/check_cli.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/check_cli.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/cli/cli_utils.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/cli_utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 import requests
 import yaml
 from pydantic import BaseModel, ValidationError
 from requests.models import Response
 from requests.sessions import Session
 
 from datahub.emitter.aspect import ASPECT_MAP, TIMESERIES_ASPECT_MAP
-from datahub.emitter.request_helper import _make_curl_command
+from datahub.emitter.request_helper import make_curl_command
 from datahub.emitter.serialization_helper import post_json_transform
 from datahub.metadata.schema_classes import _Aspect
 from datahub.utilities.urns.urn import Urn, guess_entity_type
 
 log = logging.getLogger(__name__)
 
 DEFAULT_GMS_HOST = "http://localhost:8080"
@@ -381,15 +381,15 @@
         and entity_type_lower == "dataset"
         or entity_type_lower == "dataflow"
         or entity_type_lower == "datajob"
         or entity_type_lower == "container"
     ):
         filter_criteria.append(
             {
-                "field": "platform",
+                "field": "platform.keyword",
                 "value": f"urn:li:dataPlatform:{platform}",
                 "condition": "EQUAL",
             }
         )
     if platform is not None and entity_type_lower in {"chart", "dashboard"}:
         filter_criteria.append(
             {
@@ -570,14 +570,16 @@
         encoded_urn: str = urn
     elif urn.startswith("urn:"):
         encoded_urn = Urn.url_encode(urn)
     else:
         raise Exception(
             f"urn {urn} does not seem to be a valid raw (starts with urn:) or encoded urn (starts with urn%3A)"
         )
+
+    # TODO: Replace with DataHubGraph.get_entity_raw.
     endpoint: str = f"/entitiesV2/{encoded_urn}"
 
     if aspect and len(aspect):
         endpoint = f"{endpoint}?aspects=List(" + ",".join(aspect) + ")"
 
     response = session.get(gms_host + endpoint)
     response.raise_for_status()
@@ -606,15 +608,15 @@
                 "value": json.dumps(aspect_value),
             },
         },
         "async": is_async,
     }
     payload = json.dumps(proposal)
     url = gms_host + endpoint
-    curl_command = _make_curl_command(session, "POST", url, payload)
+    curl_command = make_curl_command(session, "POST", url, payload)
     log.debug(
         "Attempting to emit to DataHub GMS; using curl equivalent to:\n%s",
         curl_command,
     )
     response = session.post(url, payload)
     if not response.ok:
         try:
@@ -649,15 +651,15 @@
     except Exception:
         # Ignore exceptions
         return {}
 
 
 def get_aspects_for_entity(
     entity_urn: str,
-    aspects: List[str] = [],
+    aspects: List[str],
     typed: bool = False,
     cached_session_host: Optional[Tuple[Session, str]] = None,
 ) -> Dict[str, Union[dict, _Aspect]]:
     # Process non-timeseries aspects
     non_timeseries_aspects = [a for a in aspects if a not in TIMESERIES_ASPECT_MAP]
     entity_response = get_entity(
         entity_urn, non_timeseries_aspects, cached_session_host
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/cli/delete_cli.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/delete_cli.py`

 * *Files 2% similar despite different names*

```diff
@@ -428,15 +428,20 @@
             payload_obj,
             "/entities?action=delete",
             cached_session_host=cached_session_host,
         )
         deletion_result.num_records = rows_affected
         deletion_result.num_timeseries_records = ts_rows_affected
     else:
-        logger.info(f"[Dry-run] Would hard-delete {urn} {soft_delete_msg}")
+        if aspect_name:
+            logger.info(
+                f"[Dry-run] Would hard-delete aspect {aspect_name} of {urn} {soft_delete_msg}"
+            )
+        else:
+            logger.info(f"[Dry-run] Would hard-delete {urn} {soft_delete_msg}")
         deletion_result.num_records = (
             UNKNOWN_NUM_RECORDS  # since we don't know how many rows will be affected
         )
 
     deletion_result.end()
     return deletion_result
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/cli/docker_check.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/docker_check.py`

 * *Files 13% similar despite different names*

```diff
@@ -3,56 +3,32 @@
 from contextlib import contextmanager
 from dataclasses import dataclass
 from typing import Any, Dict, Iterator, List, Optional
 
 import docker
 import docker.errors
 import docker.models.containers
+import yaml
 
 from datahub.configuration.common import ExceptionWithProps
 
-REQUIRED_CONTAINERS = [
-    "elasticsearch",
-    "datahub-gms",
-    "datahub-frontend-react",
-    "broker",
-]
-
-# We expect these containers to exit 0, while all other containers
-# are expected to be running and healthy.
-ENSURE_EXIT_SUCCESS = [
-    "kafka-setup",
-    "elasticsearch-setup",
-    "mysql-setup",
-    "datahub-upgrade",
-]
-
-# If present, we check that the container is ok. If it exists
-# in ENSURE_EXIT_SUCCESS, we check that it exited 0. Otherwise,
-# we check that it is running and healthy.
-CONTAINERS_TO_CHECK_IF_PRESENT = [
-    "mysql",
-    "mysql-setup",
-    "cassandra",
-    "cassandra-setup",
-    "neo4j",
-    "elasticsearch-setup",
-    "schema-registry",
-    "zookeeper",
-    "datahub-upgrade",
-    "kafka-setup",
-    # "datahub-mce-consumer",
-    # "datahub-mae-consumer",
-]
-
 # Docker seems to under-report memory allocated, so we also need a bit of buffer to account for it.
 MIN_MEMORY_NEEDED = 3.8  # GB
 
 DATAHUB_COMPOSE_PROJECT_FILTER = {"label": "com.docker.compose.project=datahub"}
 
+DATAHUB_COMPOSE_LEGACY_VOLUME_FILTERS = [
+    {"name": "datahub_neo4jdata"},
+    {"name": "datahub_mysqldata"},
+    {"name": "datahub_zkdata"},
+    {"name": "datahub_esdata"},
+    {"name": "datahub_cassandradata"},
+    {"name": "datahub_broker"},
+]
+
 
 class DockerNotRunningError(Exception):
     SHOW_STACK_TRACE = False
 
 
 class DockerLowMemoryError(Exception):
     SHOW_STACK_TRACE = False
@@ -185,36 +161,46 @@
                 },
             },
         )
 
 
 def check_docker_quickstart() -> QuickstartStatus:
     container_statuses: List[DockerContainerStatus] = []
-
     with get_docker_client() as client:
         containers = client.containers.list(
             all=True,
             filters=DATAHUB_COMPOSE_PROJECT_FILTER,
         )
+        if len(containers) == 0:
+            return QuickstartStatus([])
+
+        # load the expected containers from the docker-compose file
+        config_files = (
+            containers[0]
+            .labels.get("com.docker.compose.project.config_files")
+            .split(",")
+        )
+        all_containers = set()
+        for config_file in config_files:
+            with open(config_file, "r") as config_file:
+                all_containers.update(
+                    yaml.safe_load(config_file).get("services", {}).keys()
+                )
 
+        existing_containers = set()
         # Check that the containers are running and healthy.
         container: docker.models.containers.Container
         for container in containers:
-            name = container.name
+            name = container.labels.get("com.docker.compose.service", container.name)
+            existing_containers.add(name)
             status = ContainerStatus.OK
-
-            if container.name not in (
-                REQUIRED_CONTAINERS + CONTAINERS_TO_CHECK_IF_PRESENT
-            ):
-                # Ignores things like "datahub-frontend" which are no longer used.
-                # This way, we only check required containers like "datahub-frontend-react"
-                # even if there are some old containers lying around.
+            if name not in all_containers:
+                # Ignores containers that are not part of the datahub docker-compose
                 continue
-
-            if container.name in ENSURE_EXIT_SUCCESS:
+            if container.labels.get("datahub_setup_job", False):
                 if container.status != "exited":
                     status = ContainerStatus.STILL_RUNNING
                 elif container.attrs["State"]["ExitCode"] != 0:
                     status = ContainerStatus.EXITED_WITH_FAILURE
 
             elif container.status != "running":
                 status = ContainerStatus.DIED
@@ -223,15 +209,14 @@
                     status = ContainerStatus.STARTING
                 elif container.attrs["State"]["Health"]["Status"] != "healthy":
                     status = ContainerStatus.UNHEALTHY
 
             container_statuses.append(DockerContainerStatus(name, status))
 
         # Check for missing containers.
-        existing_containers = {container.name for container in containers}
-        missing_containers = set(REQUIRED_CONTAINERS) - existing_containers
+        missing_containers = set(all_containers) - existing_containers
         for missing in missing_containers:
             container_statuses.append(
                 DockerContainerStatus(missing, ContainerStatus.MISSING)
             )
 
     return QuickstartStatus(container_statuses)
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/cli/docker_cli.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/docker_cli.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,39 +7,37 @@
 import platform
 import subprocess
 import sys
 import tempfile
 import time
 from enum import Enum
 from pathlib import Path
-from typing import Dict, List, NoReturn, Optional
+from typing import Dict, List, Optional
 
 import click
 import click_spinner
 import pydantic
 import requests
 from expandvars import expandvars
 from requests_file import FileAdapter
 
 from datahub.cli.cli_utils import DATAHUB_ROOT_FOLDER
 from datahub.cli.docker_check import (
+    DATAHUB_COMPOSE_LEGACY_VOLUME_FILTERS,
     DATAHUB_COMPOSE_PROJECT_FILTER,
     DockerComposeVersionError,
     check_docker_quickstart,
     get_docker_client,
     run_quickstart_preflight_checks,
 )
+from datahub.cli.quickstart_versioning import QuickstartVersionMappingConfig
 from datahub.ingestion.run.pipeline import Pipeline
 from datahub.telemetry import telemetry
 from datahub.upgrade import upgrade
-from datahub.utilities.sample_data import (
-    BOOTSTRAP_MCES_FILE,
-    DOCKER_COMPOSE_BASE,
-    download_sample_data,
-)
+from datahub.utilities.sample_data import BOOTSTRAP_MCES_FILE, download_sample_data
 
 logger = logging.getLogger(__name__)
 
 NEO4J_AND_ELASTIC_QUICKSTART_COMPOSE_FILE = (
     "docker/quickstart/docker-compose.quickstart.yml"
 )
 ELASTIC_QUICKSTART_COMPOSE_FILE = (
@@ -56,26 +54,14 @@
 )
 ELASTIC_CONSUMERS_QUICKSTART_COMPOSE_FILE = (
     "docker/quickstart/docker-compose.consumers-without-neo4j.quickstart.yml"
 )
 KAFKA_SETUP_QUICKSTART_COMPOSE_FILE = (
     "docker/quickstart/docker-compose.kafka-setup.quickstart.yml"
 )
-NEO4J_AND_ELASTIC_QUICKSTART_COMPOSE_URL = (
-    f"{DOCKER_COMPOSE_BASE}/{NEO4J_AND_ELASTIC_QUICKSTART_COMPOSE_FILE}"
-)
-ELASTIC_QUICKSTART_COMPOSE_URL = (
-    f"{DOCKER_COMPOSE_BASE}/{ELASTIC_QUICKSTART_COMPOSE_FILE}"
-)
-NEO4J_AND_ELASTIC_M1_QUICKSTART_COMPOSE_URL = (
-    f"{DOCKER_COMPOSE_BASE}/{NEO4J_AND_ELASTIC_M1_QUICKSTART_COMPOSE_FILE}"
-)
-ELASTIC_M1_QUICKSTART_COMPOSE_URL = (
-    f"{DOCKER_COMPOSE_BASE}/{ELASTIC_M1_QUICKSTART_COMPOSE_FILE}"
-)
 
 
 class Architectures(Enum):
     x86 = "x86"
     arm64 = "arm64"
     m1 = "m1"
     m2 = "m2"
@@ -98,32 +84,19 @@
 @click.group()
 def docker() -> None:
     """Helper commands for setting up and interacting with a local
     DataHub instance using Docker."""
     pass
 
 
-def _print_issue_list_and_exit(
-    issues: List[str], header: str, footer: Optional[str] = None
-) -> NoReturn:
-    click.secho(header, fg="bright_red")
-    for issue in issues:
-        click.echo(f"- {issue}")
-    if footer:
-        click.echo()
-        click.echo(footer)
-    sys.exit(1)
-
-
 @docker.command()
 @upgrade.check_upgrade
 @telemetry.with_telemetry()
 def check() -> None:
     """Check that the Docker containers are healthy"""
-
     status = check_docker_quickstart()
     if status.is_ok():
         click.secho(" No issues detected", fg="green")
     else:
         raise status.to_exception("The following issues were detected:")
 
 
@@ -162,15 +135,15 @@
         if len(client.volumes.list(filters={"name": "datahub_neo4jdata"})) > 0:
             click.echo(
                 "Datahub Neo4j volume found, starting with neo4j as graph service.\n"
                 "If you want to run using elastic, run `datahub docker nuke` and re-ingest your data.\n"
             )
             return True
 
-        click.echo(
+        logger.debug(
             "No Datahub Neo4j volume found, starting with elasticsearch as graph service.\n"
             "To use neo4j as a graph backend, run \n"
             "`datahub docker quickstart --graph-service-impl neo4j`"
             "\nfrom the root of the datahub repo\n"
         )
         return False
 
@@ -446,16 +419,16 @@
     return quickstart_arch
 
 
 @docker.command()
 @click.option(
     "--version",
     type=str,
-    default=None,
-    help="Datahub version to be deployed. If not set, deploy using the defaults from the quickstart compose",
+    default="default",
+    help="Datahub version to be deployed. If not set, deploy using the defaults from the quickstart compose. Use 'stable' to start the latest stable version.",
 )
 @click.option(
     "--build-locally",
     type=bool,
     is_flag=True,
     default=False,
     help="Attempt to build the containers locally before starting",
@@ -654,14 +627,19 @@
             restore_primary=restore,
             primary_restore_file=restore_file,
             restore_indices=restore_indices_flag,
         )
         return
 
     quickstart_arch = detect_quickstart_arch(arch)
+    quickstart_versioning = QuickstartVersionMappingConfig.fetch_quickstart_config()
+    quickstart_execution_plan = quickstart_versioning.get_quickstart_execution_plan(
+        version
+    )
+    logger.info(f"Using quickstart plan: {quickstart_execution_plan}")
 
     # Run pre-flight checks.
     with get_docker_client() as client:
         run_quickstart_preflight_checks(client)
 
     quickstart_compose_file = list(
         quickstart_compose_file
@@ -679,19 +657,20 @@
         download_compose_files(
             quickstart_compose_file_name,
             quickstart_compose_file,
             graph_service_impl,
             kafka_setup,
             quickstart_arch,
             standalone_consumers,
+            quickstart_execution_plan.composefile_git_ref,
         )
 
     # set version
     _set_environment_variables(
-        version=version,
+        version=quickstart_execution_plan.docker_tag,
         mysql_port=mysql_port,
         zk_port=zk_port,
         kafka_broker_port=kafka_broker_port,
         schema_registry_port=schema_registry_port,
         elastic_port=elastic_port,
         kafka_setup=kafka_setup,
     )
@@ -705,20 +684,26 @@
         "-p",
         "datahub",
     ]
 
     # Pull and possibly build the latest containers.
     try:
         if pull_images:
-            click.echo(
-                "Pulling docker images...This may take a while depending on your network bandwidth."
+            click.echo("\nPulling docker images... ")
+            click.secho(
+                "This may take a while depending on your network bandwidth.", dim=True
             )
-            with click_spinner.spinner():
+
+            # docker compose v2 seems to spam the stderr when used in a non-interactive environment.
+            # As such, we'll only use the quiet flag if we're in an interactive environment.
+            # If we're in quiet mode, then we'll show a spinner instead.
+            quiet = not sys.stderr.isatty()
+            with click_spinner.spinner(disable=not quiet):
                 subprocess.run(
-                    [*base_command, "pull", "-q"],
+                    [*base_command, "pull", *(("-q",) if quiet else ())],
                     check=True,
                     env=_docker_subprocess_env(),
                 )
             click.secho("Finished pulling docker images!")
     except subprocess.CalledProcessError:
         click.secho(
             "Error while pulling images. Going to attempt to move on to docker compose up assuming the images have "
@@ -737,23 +722,25 @@
             ],
             check=True,
             env=_docker_subprocess_env(),
         )
         logger.info("Finished building docker images!")
 
     # Start it up! (with retries)
+    click.echo("\nStarting up DataHub...")
     max_wait_time = datetime.timedelta(minutes=8)
     start_time = datetime.datetime.now()
     sleep_interval = datetime.timedelta(seconds=2)
     up_interval = datetime.timedelta(seconds=30)
     up_attempts = 0
     while (datetime.datetime.now() - start_time) < max_wait_time:
         # Attempt to run docker compose up every `up_interval`.
         if (datetime.datetime.now() - start_time) > up_attempts * up_interval:
-            click.echo()
+            if up_attempts > 0:
+                click.echo()
             subprocess.run(
                 base_command + ["up", "-d", "--remove-orphans"],
                 env=_docker_subprocess_env(),
             )
             up_attempts += 1
 
         # Check docker health every few seconds.
@@ -799,57 +786,73 @@
     )
     click.secho(
         "Need support? Get in touch on Slack: https://slack.datahubproject.io/",
         fg="magenta",
     )
 
 
+def get_docker_compose_base_url(version_tag: str) -> str:
+    if os.environ.get("DOCKER_COMPOSE_BASE"):
+        return os.environ["DOCKER_COMPOSE_BASE"]
+
+    return f"https://raw.githubusercontent.com/datahub-project/datahub/{version_tag}"
+
+
+def get_github_file_url(neo4j: bool, is_m1: bool, release_version_tag: str) -> str:
+    base_url = get_docker_compose_base_url(release_version_tag)
+    if neo4j:
+        github_file = (
+            f"{base_url}/{NEO4J_AND_ELASTIC_QUICKSTART_COMPOSE_FILE}"
+            if not is_m1
+            else f"{base_url}/{NEO4J_AND_ELASTIC_M1_QUICKSTART_COMPOSE_FILE}"
+        )
+    else:
+        github_file = (
+            f"{base_url}/{ELASTIC_QUICKSTART_COMPOSE_FILE}"
+            if not is_m1
+            else f"{base_url}/{ELASTIC_M1_QUICKSTART_COMPOSE_FILE}"
+        )
+    return github_file
+
+
 def download_compose_files(
     quickstart_compose_file_name,
     quickstart_compose_file_list,
     graph_service_impl,
     kafka_setup,
     quickstart_arch,
     standalone_consumers,
+    compose_git_ref,
 ):
     # download appropriate quickstart file
     should_use_neo4j = should_use_neo4j_for_graph_service(graph_service_impl)
-    if should_use_neo4j:
-        github_file = (
-            NEO4J_AND_ELASTIC_QUICKSTART_COMPOSE_URL
-            if not is_arch_m1(quickstart_arch)
-            else NEO4J_AND_ELASTIC_M1_QUICKSTART_COMPOSE_URL
-        )
-    else:
-        github_file = (
-            ELASTIC_QUICKSTART_COMPOSE_URL
-            if not is_arch_m1(quickstart_arch)
-            else ELASTIC_M1_QUICKSTART_COMPOSE_URL
-        )
+    is_m1 = is_arch_m1(quickstart_arch)
+    github_file = get_github_file_url(should_use_neo4j, is_m1, compose_git_ref)
     # also allow local files
     request_session = requests.Session()
     request_session.mount("file://", FileAdapter())
     with open(
         quickstart_compose_file_name, "wb"
     ) if quickstart_compose_file_name else tempfile.NamedTemporaryFile(
         suffix=".yml", delete=False
     ) as tmp_file:
         path = pathlib.Path(tmp_file.name)
         quickstart_compose_file_list.append(path)
-        click.echo(f"Fetching docker-compose file {github_file} from GitHub")
+        logger.info(f"Fetching docker-compose file {github_file} from GitHub")
         # Download the quickstart docker-compose file from GitHub.
         quickstart_download_response = request_session.get(github_file)
         quickstart_download_response.raise_for_status()
         tmp_file.write(quickstart_download_response.content)
         logger.debug(f"Copied to {path}")
     if standalone_consumers:
+        base_url = get_docker_compose_base_url(compose_git_ref)
         consumer_github_file = (
-            f"{DOCKER_COMPOSE_BASE}/{CONSUMERS_QUICKSTART_COMPOSE_FILE}"
+            f"{base_url}/{CONSUMERS_QUICKSTART_COMPOSE_FILE}"
             if should_use_neo4j
-            else f"{DOCKER_COMPOSE_BASE}/{ELASTIC_CONSUMERS_QUICKSTART_COMPOSE_FILE}"
+            else f"{base_url}/{ELASTIC_CONSUMERS_QUICKSTART_COMPOSE_FILE}"
         )
 
         default_consumer_compose_file = (
             Path(DATAHUB_ROOT_FOLDER) / "quickstart/docker-compose.consumers.yml"
         )
         with open(
             default_consumer_compose_file, "wb"
@@ -963,13 +966,16 @@
         ):
             container.remove(v=True, force=True)
 
         if keep_data:
             click.echo("Skipping deleting data volumes in the datahub project")
         else:
             click.echo("Removing volumes in the datahub project")
-            for volume in client.volumes.list(filters=DATAHUB_COMPOSE_PROJECT_FILTER):
-                volume.remove(force=True)
+            for filter in DATAHUB_COMPOSE_LEGACY_VOLUME_FILTERS + [
+                DATAHUB_COMPOSE_PROJECT_FILTER
+            ]:
+                for volume in client.volumes.list(filters=filter):
+                    volume.remove(force=True)
 
         click.echo("Removing networks in the datahub project")
         for network in client.networks.list(filters=DATAHUB_COMPOSE_PROJECT_FILTER):
             network.remove()
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/cli/get_cli.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/get_cli.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/cli/ingest_cli.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/ingest_cli.py`

 * *Files 2% similar despite different names*

```diff
@@ -94,15 +94,24 @@
     default=False,
     help="Turn off default reporting of ingestion results to DataHub",
 )
 @click.option(
     "--no-spinner", type=bool, is_flag=True, default=False, help="Turn off spinner"
 )
 @click.pass_context
-@telemetry.with_telemetry()
+@telemetry.with_telemetry(
+    capture_kwargs=[
+        "dry_run",
+        "preview",
+        "strict_warnings",
+        "test_source_connection",
+        "no_default_report",
+        "no_spinner",
+    ]
+)
 @memory_leak_detector.with_leak_detection
 def run(
     ctx: click.Context,
     config: str,
     dry_run: bool,
     preview: bool,
     strict_warnings: bool,
@@ -154,15 +163,14 @@
                 # we check the other futures quickly on success
                 version_stats = await asyncio.wait_for(version_stats_future, 0.5)
                 upgrade.maybe_print_upgrade_message(version_stats=version_stats)
             except Exception as e:
                 logger.debug(
                     f"timed out with {e} waiting for version stats to be computed... skipping ahead."
                 )
-
         sys.exit(ret)
 
     # main function begins
     logger.info("DataHub CLI version: %s", datahub_package.nice_version_name())
 
     pipeline_config = load_config_file(
         config,
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/cli/json_file.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/json_file.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/cli/lite_cli.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/lite_cli.py`

 * *Files 1% similar despite different names*

```diff
@@ -15,19 +15,20 @@
     get_client_config,
     persist_datahub_config,
 )
 from datahub.ingestion.api.common import PipelineContext, RecordEnvelope
 from datahub.ingestion.api.sink import NoopWriteCallback
 from datahub.ingestion.run.pipeline import Pipeline
 from datahub.ingestion.sink.file import FileSink, FileSinkConfig
-from datahub.lite.duckdb_lite import DuckDBLiteConfig, SearchFlavor
+from datahub.lite.duckdb_lite_config import DuckDBLiteConfig
 from datahub.lite.lite_local import (
     AutoComplete,
     DataHubLiteLocal,
     PathNotFoundException,
+    SearchFlavor,
 )
 from datahub.lite.lite_util import LiteLocalConfig, get_datahub_lite
 from datahub.telemetry import telemetry
 
 logger = logging.getLogger(__name__)
 
 DEFAULT_LITE_IMPL = "duckdb"
@@ -316,15 +317,14 @@
 
 @lite.command(context_settings=dict(allow_extra_args=True))
 @click.option("--type", required=False, default=DEFAULT_LITE_IMPL)
 @click.option("--file", required=False)
 @click.pass_context
 @telemetry.with_telemetry()
 def init(ctx: click.Context, type: Optional[str], file: Optional[str]) -> None:
-
     lite_config = get_lite_config()
     new_lite_config_dict = lite_config.dict()
     # Update the type and config sections only
     new_lite_config_dict["type"] = type
     if file:
         new_lite_config_dict["config"]["file"] = file
     new_lite_config = LiteLocalConfig.parse_obj(new_lite_config_dict)
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/cli/migrate.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/migrate.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/cli/migration_utils.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/migration_utils.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/cli/put_cli.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/put_cli.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/cli/timeline_cli.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/cli/timeline_cli.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/configuration/_config_enum.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/_config_enum.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/configuration/common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/common.py`

 * *Files 0% similar despite different names*

```diff
@@ -11,15 +11,14 @@
 from typing_extensions import Protocol, runtime_checkable
 
 from datahub.configuration._config_enum import ConfigEnum
 from datahub.utilities.dedup_list import deduplicate_list
 
 _ConfigSelf = TypeVar("_ConfigSelf", bound="ConfigModel")
 
-
 REDACT_KEYS = {
     "password",
     "token",
     "secret",
     "options",
     "sqlalchemy_uri",
 }
@@ -74,19 +73,19 @@
         underscore_attrs_are_private = True
         keep_untouched = (
             cached_property,
         )  # needed to allow cached_property to work. See https://github.com/samuelcolvin/pydantic/issues/1241 for more info.
 
         @staticmethod
         def schema_extra(schema: Dict[str, Any], model: Type["ConfigModel"]) -> None:
-            # We use the custom "hidden_from_schema" attribute to hide fields from the
+            # We use the custom "hidden_from_docs" attribute to hide fields from the
             # autogenerated docs.
             remove_fields = []
             for key, prop in schema.get("properties", {}).items():
-                if prop.get("hidden_from_schema"):
+                if prop.get("hidden_from_docs"):
                     remove_fields.append(key)
 
             for key in remove_fields:
                 del schema["properties"][key]
 
     @classmethod
     def parse_obj_allow_extras(cls: Type[_ConfigSelf], obj: Any) -> _ConfigSelf:
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/configuration/config_loader.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/config_loader.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 import io
 import pathlib
 import re
 import sys
 import unittest.mock
 from typing import Any, Dict, Set, Union
+from urllib import parse
 
+import requests
 from expandvars import UnboundVariable, expandvars
 
 from datahub.configuration.common import ConfigurationError, ConfigurationMechanism
 from datahub.configuration.toml import TomlConfigurationMechanism
 from datahub.configuration.yaml import YamlConfigurationMechanism
 
 
@@ -62,39 +64,47 @@
         resolve_env_variables(config)
 
     calls = mock_getenv.mock_calls
     return set([call[1][0] for call in calls])
 
 
 def load_config_file(
-    config_file: Union[pathlib.Path, str],
+    config_file: Union[str, pathlib.Path],
     squirrel_original_config: bool = False,
     squirrel_field: str = "__orig_config",
     allow_stdin: bool = False,
 ) -> dict:
     config_mech: ConfigurationMechanism
     if allow_stdin and config_file == "-":
         # If we're reading from stdin, we assume that the input is a YAML file.
         config_mech = YamlConfigurationMechanism()
         raw_config_file = sys.stdin.read()
     else:
-        config_file = pathlib.Path(config_file)
-        if not config_file.is_file():
-            raise ConfigurationError(f"Cannot open config file {config_file}")
-
-        if config_file.suffix in {".yaml", ".yml"}:
+        config_file_path = pathlib.Path(config_file)
+        if config_file_path.suffix in {".yaml", ".yml"}:
             config_mech = YamlConfigurationMechanism()
-        elif config_file.suffix == ".toml":
+        elif config_file_path.suffix == ".toml":
             config_mech = TomlConfigurationMechanism()
         else:
             raise ConfigurationError(
-                f"Only .toml and .yml are supported. Cannot process file type {config_file.suffix}"
+                f"Only .toml and .yml are supported. Cannot process file type {config_file_path.suffix}"
             )
-
-        raw_config_file = config_file.read_text()
+        url_parsed = parse.urlparse(str(config_file))
+        if url_parsed.scheme in ("file", ""):  # Possibly a local file
+            if not config_file_path.is_file():
+                raise ConfigurationError(f"Cannot open config file {config_file_path}")
+            raw_config_file = config_file_path.read_text()
+        else:
+            try:
+                response = requests.get(str(config_file))
+                raw_config_file = response.text
+            except Exception as e:
+                raise ConfigurationError(
+                    f"Cannot read remote file {config_file_path}, error:{e}"
+                )
 
     config_fp = io.StringIO(raw_config_file)
     raw_config = config_mech.load_config(config_fp)
     config = resolve_env_variables(raw_config)
     if squirrel_original_config:
         config[squirrel_field] = raw_config
     return config
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/configuration/github.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/git.py`

 * *Files 1% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 _GITHUB_PREFIX = "https://github.com/"
 _GITLAB_PREFIX = "https://gitlab.com/"
 
 _GITHUB_URL_TEMPLATE = "{repo_url}/blob/{branch}/{file_path}"
 _GITLAB_URL_TEMPLATE = "{repo_url}/-/blob/{branch}/{file_path}"
 
 
-class GitHubReference(ConfigModel):
+class GitReference(ConfigModel):
     """Reference to a hosted Git repository. Used to generate "view source" links."""
 
     repo: str = Field(
         description="Name of your Git repo e.g. https://github.com/datahub-project/datahub or https://gitlab.com/gitlab-org/gitlab. If organization/repo is provided, we assume it is a GitHub repo."
     )
     branch: str = Field(
         "main",
@@ -68,15 +68,15 @@
     def get_url_for_file_path(self, file_path: str) -> str:
         assert self.url_template
         return self.url_template.format(
             repo_url=self.repo, branch=self.branch, file_path=file_path
         )
 
 
-class GitHubInfo(GitHubReference):
+class GitInfo(GitReference):
     """A reference to a Git repository, including a deploy key that can be used to clone it."""
 
     deploy_key_file: Optional[FilePath] = Field(
         None,
         description="A private key file that contains an ssh key that has been configured as a deploy key for this repository. Use a file where possible, else see deploy_key for a config field that accepts a raw string.",
     )
     deploy_key: Optional[SecretStr] = Field(
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/configuration/kafka.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/kafka.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/configuration/pydantic_field_deprecation.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/pydantic_field_deprecation.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,20 +1,22 @@
 import warnings
 from typing import Optional, Type
 
 import pydantic
 
 from datahub.configuration.common import ConfigurationWarning
+from datahub.utilities.global_warning_util import add_global_warning
 
 
 def pydantic_field_deprecated(field: str, message: Optional[str] = None) -> classmethod:
     if message:
         output = message
     else:
         output = f"{field} is deprecated and will be removed in a future release. Please remove it from your config."
 
     def _validate_deprecated(cls: Type, values: dict) -> dict:
         if field in values:
+            add_global_warning(output)
             warnings.warn(output, ConfigurationWarning, stacklevel=2)
         return values
 
     return pydantic.root_validator(pre=True, allow_reuse=True)(_validate_deprecated)
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/configuration/source_common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/source_common.py`

 * *Files 6% similar despite different names*

```diff
@@ -11,58 +11,54 @@
 
 # Get all the constants from the FabricTypeClass. It's not an enum, so this is a bit hacky but works.
 ALL_ENV_TYPES: Set[str] = set(
     [value for name, value in vars(FabricTypeClass).items() if not name.startswith("_")]
 )
 
 
-class PlatformSourceConfigBase(ConfigModel):
+class PlatformInstanceConfigMixin(ConfigModel):
     """
     Any source that connects to a platform should inherit this class
     """
 
-    platform: Optional[str] = Field(
-        default=None, description="The platform that this source connects to"
-    )
-
     platform_instance: Optional[str] = Field(
         default=None,
         description="The instance of the platform that all assets produced by this recipe belong to",
     )
 
 
-class EnvBasedSourceConfigBase(ConfigModel):
+class EnvConfigMixin(ConfigModel):
     """
     Any source that produces dataset urns in a single environment should inherit this class
     """
 
     env: str = Field(
         default=DEFAULT_ENV,
         description="The environment that all assets produced by this connector belong to",
     )
 
     _env_deprecation = pydantic_field_deprecated(
         "env",
-        "env is deprecated and will be removed in a future release. Please use platform_instance instead.",
+        message="env is deprecated and will be removed in a future release. Please use platform_instance instead.",
     )
 
     @validator("env")
     def env_must_be_one_of(cls, v: str) -> str:
         if v.upper() not in ALL_ENV_TYPES:
             raise ConfigurationError(f"env must be one of {ALL_ENV_TYPES}, found {v}")
         return v.upper()
 
 
-class DatasetSourceConfigBase(PlatformSourceConfigBase, EnvBasedSourceConfigBase):
+class DatasetSourceConfigMixin(PlatformInstanceConfigMixin, EnvConfigMixin):
     """
     Any source that is a primary producer of Dataset metadata should inherit this class
     """
 
 
-class DatasetLineageProviderConfigBase(EnvBasedSourceConfigBase):
+class DatasetLineageProviderConfigBase(EnvConfigMixin):
     """
     Any non-Dataset source that produces lineage to Datasets should inherit this class.
     e.g. Orchestrators, Pipelines, BI Tools etc.
     """
 
     platform_instance_map: Optional[Dict[str, str]] = Field(
         default=None,
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/configuration/time_window_config.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/time_window_config.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/configuration/validate_field_removal.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/validate_field_removal.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/configuration/validate_field_rename.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/validate_field_rename.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 import warnings
 from typing import Callable, Type, TypeVar
 
 import pydantic
 
 from datahub.configuration.common import ConfigurationWarning
+from datahub.utilities.global_warning_util import add_global_warning
 
 _T = TypeVar("_T")
 
 
 def _default_rename_transform(value: _T) -> _T:
     return value
 
@@ -22,16 +23,18 @@
         if old_name in values:
             if new_name in values:
                 raise ValueError(
                     f"Cannot specify both {old_name} and {new_name} in the same config. Note that {old_name} has been deprecated in favor of {new_name}."
                 )
             else:
                 if print_warning:
+                    msg = f"{old_name} is deprecated, please use {new_name} instead."
+                    add_global_warning(msg)
                     warnings.warn(
-                        f"{old_name} is deprecated, please use {new_name} instead.",
+                        msg,
                         ConfigurationWarning,
                         stacklevel=2,
                     )
                 values[new_name] = transform(values.pop(old_name))
         return values
 
     # Why aren't we using pydantic.validator here?
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/configuration/validate_host_port.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/configuration/validate_host_port.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/emitter/kafka_emitter.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/kafka_emitter.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,14 +7,15 @@
 from confluent_kafka.schema_registry.avro import AvroSerializer
 from confluent_kafka.serialization import SerializationContext, StringSerializer
 
 from datahub.configuration.common import ConfigModel
 from datahub.configuration.kafka import KafkaProducerConnectionConfig
 from datahub.configuration.validate_field_rename import pydantic_renamed_field
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
+from datahub.ingestion.api.closeable import Closeable
 from datahub.metadata.schema_classes import (
     MetadataChangeEventClass as MetadataChangeEvent,
     MetadataChangeProposalClass as MetadataChangeProposal,
 )
 from datahub.metadata.schemas import (
     getMetadataChangeEventSchema,
     getMetadataChangeProposalSchema,
@@ -50,15 +51,15 @@
     @pydantic.validator("topic_routes")
     def validate_topic_routes(cls, v: Dict[str, str]) -> Dict[str, str]:
         assert MCE_KEY in v, f"topic_routes must contain a route for {MCE_KEY}"
         assert MCP_KEY in v, f"topic_routes must contain a route for {MCP_KEY}"
         return v
 
 
-class DatahubKafkaEmitter:
+class DatahubKafkaEmitter(Closeable):
     def __init__(self, config: KafkaEmitterConfig):
         self.config = config
         schema_registry_conf = {
             "url": self.config.connection.schema_registry_url,
             **self.config.connection.schema_registry_config,
         }
         schema_registry_client = SchemaRegistryClient(schema_registry_conf)
@@ -150,11 +151,14 @@
             on_delivery=callback,
         )
 
     def flush(self) -> None:
         for producer in self.producers.values():
             producer.flush()
 
+    def close(self) -> None:
+        self.flush()
+
 
 def _error_reporting_callback(err: Exception, msg: str) -> None:
     if err:
         logger.error(f"Failed to emit to kafka: {err} {msg}")
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/emitter/mce_builder.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/mce_builder.py`

 * *Files 2% similar despite different names*

```diff
@@ -33,14 +33,15 @@
     SchemaFieldKeyClass,
     TagAssociationClass,
     UpstreamClass,
     UpstreamLineageClass,
     _Aspect as AspectAbstract,
 )
 from datahub.utilities.urn_encoder import UrnEncoder
+from datahub.utilities.urns.data_flow_urn import DataFlowUrn
 from datahub.utilities.urns.dataset_urn import DatasetUrn
 
 logger = logging.getLogger(__name__)
 Aspect = TypeVar("Aspect", bound=AspectAbstract)
 
 DEFAULT_ENV = DEFAULT_ENV_CONFIGURATION
 DEFAULT_FLOW_CLUSTER = "prod"
@@ -159,52 +160,91 @@
     results = re.search(pattern, assertion_urn)
     if results is not None:
         return AssertionKeyClass(assertionId=results[1])
     return None
 
 
 def make_user_urn(username: str) -> str:
-    return f"urn:li:corpuser:{username}"
+    """
+    Makes a user urn if the input is not a user urn already
+    """
+    return (
+        f"urn:li:corpuser:{username}"
+        if not username.startswith("urn:li:corpuser:")
+        else username
+    )
 
 
 def make_group_urn(groupname: str) -> str:
-    return f"urn:li:corpGroup:{groupname}"
+    """
+    Makes a group urn if the input is not a group urn already
+    """
+    if groupname and groupname.startswith("urn:li:corpGroup:"):
+        return groupname
+    else:
+        return f"urn:li:corpGroup:{groupname}"
 
 
 def make_tag_urn(tag: str) -> str:
-    return f"urn:li:tag:{tag}"
+    """
+    Makes a tag urn if the input is not a tag urn already
+    """
+    if tag and tag.startswith("urn:li:tag:"):
+        return tag
+    else:
+        return f"urn:li:tag:{tag}"
 
 
 def make_owner_urn(owner: str, owner_type: OwnerType) -> str:
     return f"urn:li:{owner_type.value}:{owner}"
 
 
 def make_term_urn(term: str) -> str:
-    return f"urn:li:glossaryTerm:{term}"
+    """
+    Makes a term urn if the input is not a term urn already
+    """
+    if term and term.startswith("urn:li:glossaryTerm:"):
+        return term
+    else:
+        return f"urn:li:glossaryTerm:{term}"
 
 
 def make_data_flow_urn(
-    orchestrator: str, flow_id: str, cluster: str = DEFAULT_FLOW_CLUSTER
+    orchestrator: str,
+    flow_id: str,
+    cluster: str = DEFAULT_FLOW_CLUSTER,
+    platform_instance: Optional[str] = None,
 ) -> str:
-    return f"urn:li:dataFlow:({orchestrator},{flow_id},{cluster})"
+    return str(
+        DataFlowUrn.create_from_ids(
+            orchestrator=orchestrator,
+            flow_id=flow_id,
+            env=cluster,
+            platform_instance=platform_instance,
+        )
+    )
 
 
 def make_data_job_urn_with_flow(flow_urn: str, job_id: str) -> str:
     return f"urn:li:dataJob:({flow_urn},{job_id})"
 
 
 def make_data_process_instance_urn(dataProcessInstanceId: str) -> str:
     return f"urn:li:dataProcessInstance:{dataProcessInstanceId}"
 
 
 def make_data_job_urn(
-    orchestrator: str, flow_id: str, job_id: str, cluster: str = DEFAULT_FLOW_CLUSTER
+    orchestrator: str,
+    flow_id: str,
+    job_id: str,
+    cluster: str = DEFAULT_FLOW_CLUSTER,
+    platform_instance: Optional[str] = None,
 ) -> str:
     return make_data_job_urn_with_flow(
-        make_data_flow_urn(orchestrator, flow_id, cluster), job_id
+        make_data_flow_urn(orchestrator, flow_id, cluster, platform_instance), job_id
     )
 
 
 def make_dashboard_urn(
     platform: str, name: str, platform_instance: Optional[str] = None
 ) -> str:
     # FIXME: dashboards don't currently include data platform urn prefixes.
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/emitter/mcp.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/mcp.py`

 * *Files 4% similar despite different names*

```diff
@@ -101,44 +101,51 @@
 
     @classmethod
     def construct_many(
         cls, entityUrn: str, aspects: List[Optional[_Aspect]]
     ) -> List["MetadataChangeProposalWrapper"]:
         return [cls(entityUrn=entityUrn, aspect=aspect) for aspect in aspects if aspect]
 
+    def _make_mcp_without_aspects(self) -> MetadataChangeProposalClass:
+        return MetadataChangeProposalClass(
+            entityType=self.entityType,
+            entityUrn=self.entityUrn,
+            changeType=self.changeType,
+            auditHeader=self.auditHeader,
+            aspectName=self.aspectName,
+            systemMetadata=self.systemMetadata,
+        )
+
     def make_mcp(self) -> MetadataChangeProposalClass:
         serializedEntityKeyAspect: Union[None, GenericAspectClass] = None
         if isinstance(self.entityKeyAspect, DictWrapper):
             serializedEntityKeyAspect = _make_generic_aspect(self.entityKeyAspect)
 
         serializedAspect = None
         if self.aspect is not None:
             serializedAspect = _make_generic_aspect(self.aspect)
 
-        return MetadataChangeProposalClass(
-            entityType=self.entityType,
-            entityUrn=self.entityUrn,
-            entityKeyAspect=serializedEntityKeyAspect,
-            changeType=self.changeType,
-            auditHeader=self.auditHeader,
-            aspectName=self.aspectName,
-            aspect=serializedAspect,
-            systemMetadata=self.systemMetadata,
-        )
+        mcp = self._make_mcp_without_aspects()
+        mcp.entityKeyAspect = serializedEntityKeyAspect
+        mcp.aspect = serializedAspect
+        return mcp
 
     def validate(self) -> bool:
         if self.entityUrn is None and self.entityKeyAspect is None:
             return False
         if self.entityUrn is not None and self.entityKeyAspect is not None:
             return False
         if self.entityKeyAspect is not None and not self.entityKeyAspect.validate():
             return False
         if self.aspect and not self.aspect.validate():
             return False
-        if not self.make_mcp().validate():
+        if not self._make_mcp_without_aspects().validate():
+            # PERF: Because we've already validated the aspects above, we can skip
+            # re-validating them by using _make_mcp_without_aspects() instead of
+            # make_mcp(). This way, we avoid doing unnecessary JSON serialization.
             return False
         return True
 
     def to_obj(self, tuples: bool = False, simplified_structure: bool = False) -> dict:
         # The simplified_structure parameter is used to make the output
         # not contain nested JSON strings. Instead, it unpacks the JSON
         # string into an object.
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/emitter/mcp_builder.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/mcp_builder.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/emitter/mcp_patch_builder.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/mcp_patch_builder.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/emitter/request_helper.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/request_helper.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,22 +1,28 @@
 import itertools
 import shlex
-from typing import List
+from typing import List, Union
 
 import requests
 
 
-def _make_curl_command(
+def _format_header(name: str, value: Union[str, bytes]) -> str:
+    if name == "Authorization":
+        return f"{name!s}: <redacted>"
+    return f"{name!s}: {value!s}"
+
+
+def make_curl_command(
     session: requests.Session, method: str, url: str, payload: str
 ) -> str:
     fragments: List[str] = [
         "curl",
         *itertools.chain(
             *[
                 ("-X", method),
-                *[("-H", f"{k!s}: {v!s}") for (k, v) in session.headers.items()],
+                *[("-H", _format_header(k, v)) for (k, v) in session.headers.items()],
                 ("--data", payload),
             ]
         ),
         url,
     ]
     return " ".join(shlex.quote(fragment) for fragment in fragments)
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/emitter/rest_emitter.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/rest_emitter.py`

 * *Files 1% similar despite different names*

```diff
@@ -9,50 +9,50 @@
 import requests
 from requests.adapters import HTTPAdapter, Retry
 from requests.exceptions import HTTPError, RequestException
 
 from datahub.cli.cli_utils import get_system_auth
 from datahub.configuration.common import ConfigurationError, OperationalError
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
-from datahub.emitter.request_helper import _make_curl_command
+from datahub.emitter.request_helper import make_curl_command
 from datahub.emitter.serialization_helper import pre_json_transform
 from datahub.ingestion.api.closeable import Closeable
 from datahub.metadata.com.linkedin.pegasus2avro.mxe import (
     MetadataChangeEvent,
     MetadataChangeProposal,
 )
 from datahub.metadata.com.linkedin.pegasus2avro.usage import UsageAggregation
 
 logger = logging.getLogger(__name__)
 
+_DEFAULT_CONNECT_TIMEOUT_SEC = 30  # 30 seconds should be plenty to connect
+_DEFAULT_READ_TIMEOUT_SEC = (
+    30  # Any ingest call taking longer than 30 seconds should be abandoned
+)
+_DEFAULT_RETRY_STATUS_CODES = [  # Additional status codes to retry on
+    429,
+    502,
+    503,
+    504,
+]
+_DEFAULT_RETRY_METHODS = ["HEAD", "GET", "POST", "PUT", "DELETE", "OPTIONS", "TRACE"]
+_DEFAULT_RETRY_MAX_TIMES = int(
+    os.getenv("DATAHUB_REST_EMITTER_DEFAULT_RETRY_MAX_TIMES", "3")
+)
 
-class DataHubRestEmitter(Closeable):
-    DEFAULT_CONNECT_TIMEOUT_SEC = 30  # 30 seconds should be plenty to connect
-    DEFAULT_READ_TIMEOUT_SEC = (
-        30  # Any ingest call taking longer than 30 seconds should be abandoned
-    )
-    DEFAULT_RETRY_STATUS_CODES = [  # Additional status codes to retry on
-        429,
-        502,
-        503,
-        504,
-    ]
-    DEFAULT_RETRY_METHODS = ["HEAD", "GET", "POST", "PUT", "DELETE", "OPTIONS", "TRACE"]
-    DEFAULT_RETRY_MAX_TIMES = int(
-        os.getenv("DATAHUB_REST_EMITTER_DEFAULT_RETRY_MAX_TIMES", "3")
-    )
 
+class DataHubRestEmitter(Closeable):
     _gms_server: str
     _token: Optional[str]
     _session: requests.Session
-    _connect_timeout_sec: float = DEFAULT_CONNECT_TIMEOUT_SEC
-    _read_timeout_sec: float = DEFAULT_READ_TIMEOUT_SEC
-    _retry_status_codes: List[int] = DEFAULT_RETRY_STATUS_CODES
-    _retry_methods: List[str] = DEFAULT_RETRY_METHODS
-    _retry_max_times: int = DEFAULT_RETRY_MAX_TIMES
+    _connect_timeout_sec: float = _DEFAULT_CONNECT_TIMEOUT_SEC
+    _read_timeout_sec: float = _DEFAULT_READ_TIMEOUT_SEC
+    _retry_status_codes: List[int] = _DEFAULT_RETRY_STATUS_CODES
+    _retry_methods: List[str] = _DEFAULT_RETRY_METHODS
+    _retry_max_times: int = _DEFAULT_RETRY_MAX_TIMES
 
     def __init__(
         self,
         gms_server: str,
         token: Optional[str] = None,
         connect_timeout_sec: Optional[float] = None,
         read_timeout_sec: Optional[float] = None,
@@ -242,15 +242,15 @@
                 usage_obj,
             ]
         }
         payload = json.dumps(snapshot)
         self._emit_generic(url, payload)
 
     def _emit_generic(self, url: str, payload: str) -> None:
-        curl_command = _make_curl_command(self._session, "POST", url, payload)
+        curl_command = make_curl_command(self._session, "POST", url, payload)
         logger.debug(
             "Attempting to emit to DataHub GMS; using curl equivalent to:\n%s",
             curl_command,
         )
         try:
             response = self._session.post(url, data=payload)
             response.raise_for_status()
@@ -280,11 +280,9 @@
             f"DataHubRestEmitter: configured to talk to {self._gms_server}{token_str}"
         )
 
     def close(self) -> None:
         self._session.close()
 
 
-class DatahubRestEmitter(DataHubRestEmitter):
-    """This class exists as a pass-through for backwards compatibility"""
-
-    pass
+"""This class exists as a pass-through for backwards compatibility"""
+DatahubRestEmitter = DataHubRestEmitter
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/emitter/serialization_helper.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/emitter/serialization_helper.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/entrypoints.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/entrypoints.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 import logging
 import os
 import platform
 import sys
-from typing import Optional
+from typing import ContextManager, Optional
 
 import click
-import stackprinter
 
 import datahub as datahub_package
 from datahub.cli.check_cli import check
 from datahub.cli.cli_utils import (
     DATAHUB_CONFIG_PATH,
     get_boolean_env_variable,
     make_shim_command,
@@ -17,24 +16,26 @@
 )
 from datahub.cli.delete_cli import delete
 from datahub.cli.docker_cli import docker
 from datahub.cli.get_cli import get
 from datahub.cli.ingest_cli import ingest
 from datahub.cli.migrate import migrate
 from datahub.cli.put_cli import put
+from datahub.cli.specific.group_cli import group
+from datahub.cli.specific.user_cli import user
 from datahub.cli.state_cli import state
 from datahub.cli.telemetry import telemetry as telemetry_cli
 from datahub.cli.timeline_cli import timeline
 from datahub.configuration.common import should_show_stack_trace
 from datahub.telemetry import telemetry
 from datahub.utilities.logging_manager import configure_logging
 from datahub.utilities.server_config_util import get_gms_config
 
 logger = logging.getLogger(__name__)
-_logging_configured = None
+_logging_configured: Optional[ContextManager] = None
 
 MAX_CONTENT_WIDTH = 120
 
 
 @click.group(
     context_settings=dict(
         # Avoid truncation of help text.
@@ -92,16 +93,19 @@
     # Note that we're purposely leaking the context manager here.
     # Technically we should wrap this with ctx.with_resource(). However, we have
     # some important error logging in the main() wrapper function that we don't
     # want to miss. If we wrap this with ctx.with_resource(), then click would
     # clean it up before those error handlers are processed.
     # So why is this ok? Because we're leaking a context manager, this will
     # still get cleaned up automatically when the memory is reclaimed, which is
-    # worse-case at program exit.
+    # worse-case at program exit. In a slightly better case, the context manager's
+    # exit call will be triggered by the finally clause of the main() function.
     global _logging_configured
+    if _logging_configured is not None:
+        _logging_configured.__exit__(None, None, None)
     _logging_configured = None  # see if we can force python to GC this
     _logging_configured = configure_logging(debug=debug, log_file=log_file)
     _logging_configured.__enter__()
 
     # Setup the context for the memory_leak_detector decorator.
     ctx.ensure_object(dict)
     ctx.obj["detect_memory_leaks"] = detect_memory_leaks
@@ -144,14 +148,16 @@
 datahub.add_command(delete)
 datahub.add_command(get)
 datahub.add_command(put)
 datahub.add_command(state)
 datahub.add_command(telemetry_cli)
 datahub.add_command(migrate)
 datahub.add_command(timeline)
+datahub.add_command(user)
+datahub.add_command(group)
 
 try:
     from datahub.cli.lite_cli import lite
 
     datahub.add_command(lite)
 except ImportError as e:
     logger.debug(f"Failed to load datahub lite command: {e}")
@@ -177,52 +183,29 @@
     except click.Abort:
         # Click already automatically prints an abort message, so we can just exit.
         sys.exit(1)
     except click.ClickException as error:
         error.show()
         sys.exit(1)
     except Exception as exc:
-        if "--debug-vars" in sys.argv:
-            show_vals = "like_source"
-        else:
-            # Unless --debug-vars is passed, we don't want to print the values of variables.
-            show_vals = None
-
         if not should_show_stack_trace(exc):
             # Don't print the full stack trace for simple config errors.
             logger.debug("Error: %s", exc, exc_info=exc)
             click.secho(f"{exc}", fg="red")
-        elif logger.isEnabledFor(logging.DEBUG):
-            # We only print rich stacktraces during debug.
-            logger.error(
-                stackprinter.format(
-                    exc,
-                    line_wrap=MAX_CONTENT_WIDTH,
-                    truncate_vals=10 * MAX_CONTENT_WIDTH,
-                    suppressed_vars=[
-                        r".*password.*",
-                        r".*secret.*",
-                        r".*key.*",
-                        r".*access.*",
-                        # needed because sometimes secrets are in url
-                        r".*url.*",
-                        # needed because sqlalchemy uses it underneath
-                        # and passes all params
-                        r".*cparams.*",
-                    ],
-                    suppressed_paths=[r"lib/python.*/site-packages/click/"],
-                    show_vals=show_vals,
-                )
-            )
         else:
             logger.exception(f"Command failed: {exc}")
 
         logger.debug(
             f"DataHub CLI version: {datahub_package.__version__} at {datahub_package.__file__}"
         )
         logger.debug(
             f"Python version: {sys.version} at {sys.executable} on {platform.platform()}"
         )
         gms_config = get_gms_config()
         if gms_config:
             logger.debug(f"GMS config {gms_config}")
         sys.exit(1)
+    finally:
+        global _logging_configured
+        if _logging_configured:
+            _logging_configured.__exit__(None, None, None)
+            _logging_configured = None
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/committable.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/committable.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/common.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 from abc import ABCMeta, abstractmethod
 from dataclasses import dataclass
 from typing import TYPE_CHECKING, Dict, Generic, Iterable, Optional, Tuple, TypeVar
 
 import requests
 
+from datahub.configuration.common import ConfigurationError
 from datahub.emitter.mce_builder import set_dataset_urn_to_lower
 from datahub.ingestion.api.committable import Committable
 from datahub.ingestion.graph.client import DatahubClientConfig, DataHubGraph
 
 if TYPE_CHECKING:
     from datahub.ingestion.run.pipeline import PipelineConfig
 
@@ -55,15 +56,15 @@
         self.run_id = run_id
         self.pipeline_name = pipeline_name
         self.dry_run_mode = dry_run
         self.preview_mode = preview_mode
         self.checkpointers: Dict[str, Committable] = {}
         try:
             self.graph = DataHubGraph(datahub_api) if datahub_api is not None else None
-        except requests.exceptions.ConnectionError as e:
+        except (requests.exceptions.ConnectionError, ConfigurationError) as e:
             raise Exception("Failed to connect to DataHub") from e
         except Exception as e:
             raise Exception(
                 "Failed to instantiate a valid DataHub Graph instance"
             ) from e
 
         self._set_dataset_urn_to_lower_if_needed()
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/decorators.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/decorators.py`

 * *Files 8% similar despite different names*

```diff
@@ -11,39 +11,35 @@
 
     def default_create(cls: Type, config_dict: Dict, ctx: PipelineContext) -> Type:
         config = config_cls.parse_obj(config_dict)
         return cls(config=config, ctx=ctx)
 
     def wrapper(cls: Type) -> Type:
         # add a get_config_class method
-        setattr(cls, "get_config_class", lambda: config_cls)
+        cls.get_config_class = lambda: config_cls
         if not hasattr(cls, "create") or (
-            getattr(cls, "create").__func__ == getattr(Source, "create").__func__
+            cls.create.__func__ == Source.create.__func__  # type: ignore
         ):
             # add the create method only if it has not been overridden from the base Source.create method
-            setattr(cls, "create", classmethod(default_create))
+            cls.create = classmethod(default_create)
 
         return cls
 
     return wrapper
 
 
 def platform_name(
     platform_name: str, id: Optional[str] = None, doc_order: Optional[int] = None
 ) -> Callable[[Type], Type]:
     """Adds a get_platform_name method to the decorated class"""
 
     def wrapper(cls: Type) -> Type:
-        setattr(cls, "get_platform_name", lambda: platform_name)
-        setattr(
-            cls,
-            "get_platform_id",
-            lambda: id or platform_name.lower().replace(" ", "-"),
-        )
-        setattr(cls, "get_platform_doc_order", lambda: doc_order or None)
+        cls.get_platform_name = lambda: platform_name
+        cls.get_platform_id = lambda: id or platform_name.lower().replace(" ", "-")
+        cls.get_platform_doc_order = lambda: doc_order or None
 
         return cls
 
     if id and " " in id:
         raise Exception(
             f'Platform id "{id}" contains white-space, please use a platform id without spaces.'
         )
@@ -72,15 +68,15 @@
 
 def support_status(
     support_status: SupportStatus,
 ) -> Callable[[Type], Type]:
     """Adds a get_support_status method to the decorated class"""
 
     def wrapper(cls: Type) -> Type:
-        setattr(cls, "get_support_status", lambda: support_status)
+        cls.get_support_status = lambda: support_status
         return cls
 
     return wrapper
 
 
 @dataclass
 class CapabilitySetting:
@@ -94,16 +90,16 @@
 ) -> Callable[[Type], Type]:
     """
     A decorator to mark a source as having a certain capability
     """
 
     def wrapper(cls: Type) -> Type:
         if not hasattr(cls, "__capabilities"):
-            setattr(cls, "__capabilities", {})
-            setattr(cls, "get_capabilities", lambda: cls.__capabilities.values())
+            cls.__capabilities = {}
+            cls.get_capabilities = lambda: cls.__capabilities.values()
 
         cls.__capabilities[capability_name] = CapabilitySetting(
             capability=capability_name, description=description, supported=supported
         )
         return cls
 
     return wrapper
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/ingestion_job_checkpointing_provider_base.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/ingestion_job_checkpointing_provider_base.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 from abc import abstractmethod
 from dataclasses import dataclass
-from typing import Any, Dict, NewType, Type, TypeVar
+from typing import Any, Dict, NewType, Optional, Type, TypeVar
 
 import datahub.emitter.mce_builder as builder
 from datahub.configuration.common import ConfigModel
 from datahub.ingestion.api.committable import CommitPolicy, StatefulCommittable
 from datahub.ingestion.api.common import PipelineContext
 from datahub.metadata.schema_classes import DatahubIngestionCheckpointClass
 
@@ -39,28 +39,25 @@
     ) -> "_Self":
         pass
 
     @abstractmethod
     def commit(self) -> None:
         pass
 
+    @abstractmethod
+    def get_latest_checkpoint(
+        self,
+        pipeline_name: str,
+        job_name: JobId,
+    ) -> Optional[DatahubIngestionCheckpointClass]:
+        pass
+
     @staticmethod
     def get_data_job_urn(
         orchestrator: str,
         pipeline_name: str,
         job_name: JobId,
     ) -> str:
         """
         Standardizes datajob urn minting for all ingestion job state providers.
         """
         return builder.make_data_job_urn(orchestrator, pipeline_name, job_name)
-
-    @staticmethod
-    def get_data_job_legacy_urn(
-        orchestrator: str,
-        pipeline_name: str,
-        job_name: JobId,
-        platform_instance_id: str,
-    ) -> str:
-        return IngestionCheckpointingProviderBase.get_data_job_urn(
-            orchestrator, f"{pipeline_name}_{platform_instance_id}", job_name
-        )
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/pipeline_run_listener.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/pipeline_run_listener.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/registry.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/registry.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,19 +1,35 @@
 import importlib
 import inspect
-from typing import Any, Callable, Dict, Generic, Optional, Tuple, Type, TypeVar, Union
+from typing import (
+    Any,
+    Callable,
+    Dict,
+    Generic,
+    List,
+    Optional,
+    Tuple,
+    Type,
+    TypeVar,
+    Union,
+)
 
 import entrypoints
 import typing_inspect
 
 from datahub import __package_name__
 from datahub.configuration.common import ConfigurationError
 
 T = TypeVar("T")
 
+# TODO: The `entrypoints` library is in maintenance mode and is not actively developed.
+# We should switch to importlib.metadata once we drop support for Python 3.7.
+# See https://entrypoints.readthedocs.io/en/latest/ and
+# https://docs.python.org/3/library/importlib.metadata.html.
+
 
 def _is_importable(path: str) -> bool:
     return "." in path or ":" in path
 
 
 def import_path(path: str) -> Any:
     """
@@ -32,20 +48,22 @@
     item = importlib.import_module(module_name)
     for attr in object_name.split("."):
         item = getattr(item, attr)
     return item
 
 
 class PluginRegistry(Generic[T]):
+    _entrypoints: List[str]
     _mapping: Dict[str, Union[str, Type[T], Exception]]
     _aliases: Dict[str, Tuple[str, Callable[[], None]]]
 
     def __init__(
         self, extra_cls_check: Optional[Callable[[Type[T]], None]] = None
     ) -> None:
+        self._entrypoints = []
         self._mapping = {}
         self._aliases = {}
         self._extra_cls_check = extra_cls_check
 
     def _get_registered_type(self) -> Type[T]:
         cls = typing_inspect.get_generic_type(self)
         tp = typing_inspect.get_args(cls)[0]
@@ -86,48 +104,61 @@
 
     def register_alias(
         self, alias: str, real_key: str, fn: Callable[[], None] = lambda: None
     ) -> None:
         self._aliases[alias] = (real_key, fn)
 
     def _ensure_not_lazy(self, key: str) -> Union[Type[T], Exception]:
+        self._materialize_entrypoints()
+
         path = self._mapping[key]
         if not isinstance(path, str):
             return path
         try:
             plugin_class = import_path(path)
             self.register(key, plugin_class, override=True)
             return plugin_class
         except (AssertionError, ImportError) as e:
             self.register_disabled(key, e, override=True)
             return e
 
     def is_enabled(self, key: str) -> bool:
+        self._materialize_entrypoints()
+
         tp = self._mapping[key]
         return not isinstance(tp, Exception)
 
-    def register_from_entrypoint(self, entry_point_key: str, lazy: bool = True) -> None:
+    def register_from_entrypoint(self, entry_point_key: str) -> None:
+        self._entrypoints.append(entry_point_key)
+
+    def _load_entrypoint(self, entry_point_key: str) -> None:
         entry_point: entrypoints.EntryPoint
         for entry_point in entrypoints.get_group_all(entry_point_key):
             name = entry_point.name
 
             if entry_point.object_name is None:
                 path = entry_point.module_name
             else:
                 path = f"{entry_point.module_name}:{entry_point.object_name}"
 
             self.register_lazy(name, path)
-            if not lazy:
-                self._ensure_not_lazy(name)
+
+    def _materialize_entrypoints(self) -> None:
+        for entry_point_key in self._entrypoints:
+            self._load_entrypoint(entry_point_key)
+        self._entrypoints = []
 
     @property
     def mapping(self) -> Dict[str, Union[str, Type[T], Exception]]:
+        self._materialize_entrypoints()
         return self._mapping
 
     def get(self, key: str) -> Type[T]:
+        self._materialize_entrypoints()
+
         if _is_importable(key):
             # If the key contains a dot or colon, we treat it as a import path and attempt
             # to load it dynamically.
             MyClass = import_path(key)
             self._check_cls(MyClass)
             return MyClass
 
@@ -151,14 +182,16 @@
         else:
             # If it's not an exception, then it's a registered type.
             return tp
 
     def summary(
         self, verbose: bool = True, col_width: int = 15, verbose_col_width: int = 20
     ) -> str:
+        self._materialize_entrypoints()
+
         lines = []
         for key in sorted(self._mapping.keys()):
             # We want to attempt to load all plugins before printing a summary.
             self._ensure_not_lazy(key)
 
             line = f"{key}"
             if not self.is_enabled(key):
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/report.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/report.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/report_helpers.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/report_helpers.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/sink.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/sink.py`

 * *Files 0% similar despite different names*

```diff
@@ -86,15 +86,15 @@
     """All Sinks must inherit this base class."""
 
     ctx: PipelineContext
     config: SinkConfig
     report: SinkReportType
 
     @classmethod
-    def get_config_class(cls) -> Type[SinkConfig]:
+    def get_config_class(cls: Type[Self]) -> Type[SinkConfig]:
         config_class = get_class_from_annotation(cls, Sink, ConfigModel)
         assert config_class, "Sink subclasses must define a config class"
         return cast(Type[SinkConfig], config_class)
 
     @classmethod
     def get_report_class(cls) -> Type[SinkReportType]:
         report_class = get_class_from_annotation(cls, Sink, SinkReport)
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/source.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/source.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/transform.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/transform.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/api/workunit.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/api/workunit.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/extractor/mce_extractor.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/extractor/mce_extractor.py`

 * *Files 4% similar despite different names*

```diff
@@ -9,18 +9,22 @@
 from datahub.metadata.com.linkedin.pegasus2avro.mxe import (
     MetadataChangeEvent,
     MetadataChangeProposal,
     SystemMetadata,
 )
 from datahub.metadata.schema_classes import UsageAggregationClass
 
-try:
-    import black
-except ImportError:
-    black = None  # type: ignore
+
+def _try_reformat_with_black(code: str) -> str:
+    try:
+        import black
+
+        return black.format_str(code, mode=black.FileMode())
+    except ImportError:
+        return code
 
 
 class WorkUnitRecordExtractorConfig(ConfigModel):
     set_system_metadata = True
     unpack_mces_into_mcps = False
 
 
@@ -64,36 +68,30 @@
                 if (
                     isinstance(workunit.metadata, MetadataChangeEvent)
                     and len(workunit.metadata.proposedSnapshot.aspects) == 0
                 ):
                     raise AttributeError("every mce must have at least one aspect")
             if not workunit.metadata.validate():
                 invalid_mce = str(workunit.metadata)
-
-                if black is not None:
-                    invalid_mce = black.format_str(invalid_mce, mode=black.FileMode())
+                invalid_mce = _try_reformat_with_black(invalid_mce)
 
                 raise ValueError(
                     f"source produced an invalid metadata work unit: {invalid_mce}"
                 )
 
             yield RecordEnvelope(
                 workunit.metadata,
                 {
                     "workunit_id": workunit.id,
                 },
             )
         elif isinstance(workunit, UsageStatsWorkUnit):
             if not workunit.usageStats.validate():
                 invalid_usage_stats = str(workunit.usageStats)
-
-                if black is not None:
-                    invalid_usage_stats = black.format_str(
-                        invalid_usage_stats, mode=black.FileMode()
-                    )
+                invalid_usage_stats = _try_reformat_with_black(invalid_usage_stats)
 
                 raise ValueError(
                     f"source produced an invalid usage stat: {invalid_usage_stats}"
                 )
             yield RecordEnvelope(
                 workunit.usageStats,
                 {
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/extractor/protobuf_util.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/extractor/protobuf_util.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/extractor/schema_util.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/extractor/schema_util.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/glossary/classification_mixin.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/glossary/classification_mixin.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/glossary/classifier.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/glossary/classifier.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/glossary/datahub_classifier.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/glossary/datahub_classifier.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/graph/client.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/graph/client.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,20 +1,22 @@
 import json
 import logging
+from dataclasses import dataclass
+from enum import Enum
 from json.decoder import JSONDecodeError
-from typing import Any, Dict, Iterable, List, Optional, Type, Union
+from typing import Any, Dict, Iterable, List, Optional, Type
 
-import pydantic
 from avro.schema import RecordSchema
 from deprecated import deprecated
 from requests.adapters import Response
 from requests.models import HTTPError
 
 from datahub.cli.cli_utils import get_boolean_env_variable, get_url_and_token
 from datahub.configuration.common import ConfigModel, GraphError, OperationalError
+from datahub.emitter.aspect import TIMESERIES_ASPECT_MAP
 from datahub.emitter.mce_builder import Aspect
 from datahub.emitter.rest_emitter import DatahubRestEmitter
 from datahub.emitter.serialization_helper import post_json_transform
 from datahub.metadata.schema_classes import (
     BrowsePathsClass,
     DatasetPropertiesClass,
     DatasetUsageStatisticsClass,
@@ -40,27 +42,25 @@
     server: str = "http://localhost:8080"
     token: Optional[str]
     timeout_sec: Optional[int]
     retry_status_codes: Optional[List[int]]
     retry_max_times: Optional[int]
     extra_headers: Optional[Dict[str, str]]
     ca_certificate_path: Optional[str]
-    max_threads: int = 1
+    max_threads: int = 15
     disable_ssl_verification: bool = False
 
 
-class DataHubGraphConfig(DatahubClientConfig):
-    class Config:
-        extra = (
-            pydantic.Extra.allow
-        )  # lossy to allow interop with DataHubRestSinkConfig
+# Alias for backwards compatibility.
+# DEPRECATION: Remove in v0.10.2.
+DataHubGraphConfig = DatahubClientConfig
 
 
 class DataHubGraph(DatahubRestEmitter):
-    def __init__(self, config: Union[DatahubClientConfig, DataHubGraphConfig]) -> None:
+    def __init__(self, config: DatahubClientConfig) -> None:
         self.config = config
         super().__init__(
             gms_server=self.config.server,
             token=self.config.token,
             connect_timeout_sec=self.config.timeout_sec,  # reuse timeout_sec for connect timeout
             read_timeout_sec=self.config.timeout_sec,
             retry_status_codes=self.config.retry_status_codes,
@@ -78,17 +78,17 @@
                 "urn:li:telemetry:clientId", TelemetryClientIdClass
             )
             self.server_id = client_id.clientId if client_id else "missing"
         except Exception as e:
             self.server_id = "missing"
             logger.debug(f"Failed to get server id due to {e}")
 
-    def _get_generic(self, url: str) -> Dict:
+    def _get_generic(self, url: str, params: Optional[Dict] = None) -> Dict:
         try:
-            response = self._session.get(url)
+            response = self._session.get(url, params=params)
             response.raise_for_status()
             return response.json()
         except HTTPError as e:
             try:
                 info = response.json()
                 raise OperationalError(
                     "Unable to get metadata from DataHub", info
@@ -123,35 +123,39 @@
         entity_urn: str,
         aspect_type: Type[Aspect],
         version: int = 0,
     ) -> Optional[Aspect]:
         """
         Get an aspect for an entity.
 
-        :param str entity_urn: The urn of the entity
-        :param Type[Aspect] aspect_type: The type class of the aspect being requested (e.g. datahub.metadata.schema_classes.DatasetProperties)
+        :param entity_urn: The urn of the entity
+        :param aspect_type: The type class of the aspect being requested (e.g. datahub.metadata.schema_classes.DatasetProperties)
         :param version: The version of the aspect to retrieve. The default of 0 means latest. Versions > 0 go from oldest to newest, so 1 is the oldest.
         :return: the Aspect as a dictionary if present, None if no aspect was found (HTTP status 404)
 
+        :raises TypeError: if the aspect type is a timeseries aspect
         :raises HttpError: if the HTTP response is not a 200 or a 404
         """
 
         aspect = aspect_type.ASPECT_NAME
+        if aspect in TIMESERIES_ASPECT_MAP:
+            raise TypeError(
+                'Cannot get a timeseries aspect using "get_aspect". Use "get_latest_timeseries_value" instead.'
+            )
+
         url: str = f"{self._gms_server}/aspects/{Urn.url_encode(entity_urn)}?aspect={aspect}&version={version}"
         response = self._session.get(url)
         if response.status_code == 404:
             # not found
             return None
         response.raise_for_status()
         response_json = response.json()
 
         # Figure out what field to look in.
-        record_schema: RecordSchema = aspect_type.__getattribute__(
-            aspect_type, "RECORD_SCHEMA"
-        )
+        record_schema: RecordSchema = aspect_type.RECORD_SCHEMA
         aspect_type_name = record_schema.fullname.replace(".pegasus2avro", "")
 
         # Deserialize the aspect json into the aspect type.
         aspect_json = response_json.get("aspect", {}).get(aspect_type_name)
         if aspect_json is not None:
             # need to apply a transform to the response to match rest.li and avro serialization
             post_json_obj = post_json_transform(aspect_json)
@@ -298,70 +302,72 @@
                 return aspect_type.from_obj(json.loads(aspect_json), tuples=False)
             else:
                 raise GraphError(
                     f"Failed to find {aspect_type} in response {aspect_json}"
                 )
         return None
 
+    def get_entity_raw(
+        self, entity_urn: str, aspects: Optional[List[str]] = None
+    ) -> Dict:
+        endpoint: str = f"{self.config.server}/entitiesV2/{Urn.url_encode(entity_urn)}"
+        if aspects is not None:
+            assert aspects, "if provided, aspects must be a non-empty list"
+            endpoint = f"{endpoint}?aspects=List(" + ",".join(aspects) + ")"
+
+        response = self._session.get(endpoint)
+        response.raise_for_status()
+        return response.json()
+
     def get_aspects_for_entity(
         self,
         entity_urn: str,
         aspects: List[str],
         aspect_types: List[Type[Aspect]],
-    ) -> Optional[Dict[str, Optional[Aspect]]]:
+    ) -> Dict[str, Optional[Aspect]]:
         """
         Get multiple aspects for an entity. To get a single aspect for an entity, use the `get_aspect_v2` method.
         Warning: Do not use this method to determine if an entity exists!
         This method will always return an entity, even if it doesn't exist. This is an issue with how DataHub server
         responds to these calls, and will be fixed automatically when the server-side issue is fixed.
 
         :param str entity_urn: The urn of the entity
         :param List[Type[Aspect]] aspect_type_list: List of aspect type classes being requested (e.g. [datahub.metadata.schema_classes.DatasetProperties])
         :param List[str] aspects_list: List of aspect names being requested (e.g. [schemaMetadata, datasetProperties])
         :return: Optionally, a map of aspect_name to aspect_value as a dictionary if present, aspect_value will be set to None if that aspect was not found. Returns None on HTTP status 404.
-        :rtype: Optional[Dict[str, Optional[Aspect]]]
-        :raises HttpError: if the HTTP response is not a 200 or a 404
+        :raises HttpError: if the HTTP response is not a 200
         """
         assert len(aspects) == len(
             aspect_types
         ), f"number of aspects requested ({len(aspects)}) should be the same as number of aspect types provided ({len(aspect_types)})"
-        aspects_list = ",".join(aspects)
-        url: str = f"{self._gms_server}/entitiesV2/{Urn.url_encode(entity_urn)}?aspects=List({aspects_list})"
 
-        response = self._session.get(url)
-        if response.status_code == 404:
-            # not found
-            return None
-        response.raise_for_status()
-        response_json = response.json()
+        # TODO: generate aspects list from type classes
+        response_json = self.get_entity_raw(entity_urn, aspects)
 
         result: Dict[str, Optional[Aspect]] = {}
         for aspect_type in aspect_types:
-            record_schema: RecordSchema = aspect_type.__getattribute__(
-                aspect_type, "RECORD_SCHEMA"
-            )
-            if not record_schema:
-                logger.warning(
-                    f"Failed to infer type name of the aspect from the aspect type class {aspect_type}. Continuing, but this will fail."
-                )
-            else:
-                aspect_type_name = record_schema.props["Aspect"]["name"]
+            record_schema = aspect_type.RECORD_SCHEMA
+            aspect_type_name = record_schema.props["Aspect"]["name"]
+
             aspect_json = response_json.get("aspects", {}).get(aspect_type_name)
             if aspect_json:
                 # need to apply a transform to the response to match rest.li and avro serialization
                 post_json_obj = post_json_transform(aspect_json)
                 result[aspect_type_name] = aspect_type.from_obj(post_json_obj["value"])
             else:
                 result[aspect_type_name] = None
 
         return result
 
     def _get_search_endpoint(self):
         return f"{self.config.server}/entities?action=search"
 
+    def _get_relationships_endpoint(self):
+        return f"{self.config.server}/openapi/relationships/v1/"
+
     def _get_aspect_count_endpoint(self):
         return f"{self.config.server}/aspects?action=getCount"
 
     def get_domain_urn_by_name(self, domain_name: str) -> Optional[str]:
         """Retrieve a domain urn based on its name. Returns None if there is no match found"""
 
         filters = []
@@ -449,11 +455,63 @@
     def get_aspect_counts(self, aspect: str, urn_like: Optional[str] = None) -> int:
         args = {"aspect": aspect}
         if urn_like is not None:
             args["urnLike"] = urn_like
         results = self._post_generic(self._get_aspect_count_endpoint(), args)
         return results["value"]
 
+    def execute_graphql(self, query: str, variables: Optional[Dict] = None) -> Dict:
+        url = f"{self.config.server}/api/graphql"
+        body: Dict = {
+            "query": query,
+        }
+        if variables:
+            body["variables"] = variables
+
+        result = self._post_generic(url, body)
+        if result.get("errors"):
+            raise GraphError(f"Error executing graphql query: {result['errors']}")
+
+        return result["data"]
+
+    class RelationshipDirection(str, Enum):
+        INCOMING = "INCOMING"
+        OUTGOING = "OUTGOING"
+
+    @dataclass
+    class RelatedEntity:
+        urn: str
+        relationship_type: str
+
+    def get_related_entities(
+        self,
+        entity_urn: str,
+        relationship_types: List[str],
+        direction: RelationshipDirection,
+    ) -> Iterable[RelatedEntity]:
+        relationship_endpoint = self._get_relationships_endpoint()
+        done = False
+        start = 0
+        while not done:
+            response = self._get_generic(
+                url=relationship_endpoint,
+                params={
+                    "urn": entity_urn,
+                    "direction": direction,
+                    "relationshipTypes": relationship_types,
+                    "start": start,
+                },
+            )
+            for related_entity in response.get("entities", []):
+                yield DataHubGraph.RelatedEntity(
+                    urn=related_entity["urn"],
+                    relationship_type=related_entity["relationshipType"],
+                )
+            done = response.get("count", 0) == 0 or response.get("count", 0) < len(
+                response.get("entities", [])
+            )
+            start = start + response.get("count", 0)
+
 
 def get_default_graph() -> DataHubGraph:
     (url, token) = get_url_and_token()
-    return DataHubGraph(DataHubGraphConfig(server=url, token=token))
+    return DataHubGraph(DatahubClientConfig(server=url, token=token))
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/reporting/datahub_ingestion_run_summary_provider.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/reporting/datahub_ingestion_run_summary_provider.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/reporting/file_reporter.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/reporting/file_reporter.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/run/connection.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/run/connection.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/run/pipeline.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/run/pipeline.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 import contextlib
 import itertools
 import logging
 import os
 import platform
+import shutil
 import sys
 import time
 from dataclasses import dataclass
 from typing import Any, Dict, Iterable, Iterator, List, Optional, cast
 
 import click
 import humanfriendly
@@ -32,14 +33,18 @@
 from datahub.ingestion.run.pipeline_config import PipelineConfig, ReporterConfig
 from datahub.ingestion.sink.file import FileSink, FileSinkConfig
 from datahub.ingestion.sink.sink_registry import sink_registry
 from datahub.ingestion.source.source_registry import source_registry
 from datahub.ingestion.transformer.transform_registry import transform_registry
 from datahub.metadata.schema_classes import MetadataChangeProposalClass
 from datahub.telemetry import stats, telemetry
+from datahub.utilities.global_warning_util import (
+    clear_global_warnings,
+    get_global_warnings,
+)
 from datahub.utilities.lossy_collections import LossyDict, LossyList
 
 logger = logging.getLogger(__name__)
 
 
 class LoggingCallback(WriteCallback):
     def __init__(self, name: str = "") -> None:
@@ -119,19 +124,39 @@
 @dataclass
 class CliReport(Report):
     cli_version: str = datahub.nice_version_name()
     cli_entry_location: str = datahub.__file__
     py_version: str = sys.version
     py_exec_path: str = sys.executable
     os_details: str = platform.platform()
+    _peak_memory_usage: int = 0
+    _peak_disk_usage: int = 0
 
     def compute_stats(self) -> None:
-        self.mem_info = humanfriendly.format_size(
-            psutil.Process(os.getpid()).memory_info().rss
-        )
+        try:
+            mem_usage = psutil.Process(os.getpid()).memory_info().rss
+            if self._peak_memory_usage < mem_usage:
+                self._peak_memory_usage = mem_usage
+                self.peak_memory_usage = humanfriendly.format_size(
+                    self._peak_memory_usage
+                )
+            self.mem_info = humanfriendly.format_size(mem_usage)
+
+            disk_usage = shutil.disk_usage("/")
+            if self._peak_disk_usage < disk_usage.used:
+                self._peak_disk_usage = disk_usage.used
+                self.peak_disk_usage = humanfriendly.format_size(self._peak_disk_usage)
+            self.disk_info = {
+                "total": humanfriendly.format_size(disk_usage.total),
+                "used": humanfriendly.format_size(disk_usage.used),
+                "free": humanfriendly.format_size(disk_usage.free),
+            }
+        except Exception as e:
+            logger.warning(f"Failed to compute report memory usage: {e}")
+
         return super().compute_stats()
 
 
 class Pipeline:
     config: PipelineConfig
     ctx: PipelineContext
     source: Source
@@ -380,14 +405,16 @@
             self.process_commits()
             self.final_status = "completed"
         except (SystemExit, RuntimeError) as e:
             self.final_status = "cancelled"
             logger.error("Caught error", exc_info=e)
             raise
         finally:
+            clear_global_warnings()
+
             if callback and hasattr(callback, "close"):
                 callback.close()  # type: ignore
 
             self._notify_reporters_on_ingestion_completion()
 
     def transform(self, records: Iterable[RecordEnvelope]) -> Iterable[RecordEnvelope]:
         """
@@ -442,41 +469,48 @@
     def raise_from_status(self, raise_warnings: bool = False) -> None:
         if self.source.get_report().failures:
             raise PipelineExecutionError(
                 "Source reported errors", self.source.get_report()
             )
         if self.sink.get_report().failures:
             raise PipelineExecutionError("Sink reported errors", self.sink.get_report())
-        if raise_warnings and (
-            self.source.get_report().warnings or self.sink.get_report().warnings
-        ):
-            raise PipelineExecutionError(
-                "Source reported warnings", self.source.get_report()
-            )
+        if raise_warnings:
+            if self.source.get_report().warnings:
+                raise PipelineExecutionError(
+                    "Source reported warnings", self.source.get_report()
+                )
+            if self.sink.get_report().warnings:
+                raise PipelineExecutionError(
+                    "Sink reported warnings", self.sink.get_report()
+                )
 
     def log_ingestion_stats(self) -> None:
         source_failures = self._approx_all_vals(self.source.get_report().failures)
         source_warnings = self._approx_all_vals(self.source.get_report().warnings)
         sink_failures = len(self.sink.get_report().failures)
         sink_warnings = len(self.sink.get_report().warnings)
+        global_warnings = len(get_global_warnings())
 
         telemetry.telemetry_instance.ping(
             "ingest_stats",
             {
                 "source_type": self.config.source.type,
                 "sink_type": self.config.sink.type,
                 "records_written": stats.discretize(
                     self.sink.get_report().total_records_written
                 ),
                 "source_failures": stats.discretize(source_failures),
                 "source_warnings": stats.discretize(source_warnings),
                 "sink_failures": stats.discretize(sink_failures),
                 "sink_warnings": stats.discretize(sink_warnings),
+                "global_warnings": global_warnings,
                 "failures": stats.discretize(source_failures + sink_failures),
-                "warnings": stats.discretize(source_warnings + sink_warnings),
+                "warnings": stats.discretize(
+                    source_warnings + sink_warnings + global_warnings
+                ),
             },
             self.ctx.graph,
         )
 
     def _approx_all_vals(self, d: LossyDict[str, LossyList]) -> int:
         result = d.dropped_keys_count()
         for k in d:
@@ -500,14 +534,18 @@
         click.echo()
         click.secho("Cli report:", bold=True)
         click.secho(self.cli_report.as_string())
         click.secho(f"Source ({self.config.source.type}) report:", bold=True)
         click.echo(self.source.get_report().as_string())
         click.secho(f"Sink ({self.config.sink.type}) report:", bold=True)
         click.echo(self.sink.get_report().as_string())
+        global_warnings = get_global_warnings()
+        if len(global_warnings) > 0:
+            click.secho("Global Warnings:", bold=True)
+            click.echo(global_warnings)
         click.echo()
         workunits_produced = self.source.get_report().events_produced
         duration_message = f"in {humanfriendly.format_timespan(self.source.get_report().running_time)}."
 
         if self.source.get_report().failures or self.sink.get_report().failures:
             num_failures_source = self._approx_all_vals(
                 self.source.get_report().failures
@@ -519,19 +557,24 @@
                     running=currently_running,
                     failures=True,
                     warnings=False,
                 ),
                 bold=True,
             )
             return 1
-        elif self.source.get_report().warnings or self.sink.get_report().warnings:
+        elif (
+            self.source.get_report().warnings
+            or self.sink.get_report().warnings
+            or len(global_warnings) > 0
+        ):
             num_warn_source = self._approx_all_vals(self.source.get_report().warnings)
             num_warn_sink = len(self.sink.get_report().warnings)
+            num_warn_global = len(global_warnings)
             click.secho(
-                f"{'' if currently_running else ''} Pipeline {'running' if currently_running else 'finished'} with at least {num_warn_source+num_warn_sink} warnings{' so far' if currently_running else ''}; produced {workunits_produced} events {duration_message}",
+                f"{'' if currently_running else ''} Pipeline {'running' if currently_running else 'finished'} with at least {num_warn_source+num_warn_sink+num_warn_global} warnings{' so far' if currently_running else ''}; produced {workunits_produced} events {duration_message}",
                 fg=self._get_text_color(
                     running=currently_running, failures=False, warnings=True
                 ),
                 bold=True,
             )
             return 1 if warnings_as_failure else 0
         else:
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/run/pipeline_config.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/run/pipeline_config.py`

 * *Files 5% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 from typing import Any, Dict, List, Optional
 
 from pydantic import Field, root_validator, validator
 
 from datahub.cli.cli_utils import get_url_and_token
 from datahub.configuration import config_loader
 from datahub.configuration.common import ConfigModel, DynamicTypedConfig
-from datahub.ingestion.graph.client import DataHubGraphConfig
+from datahub.ingestion.graph.client import DatahubClientConfig
 from datahub.ingestion.sink.file import FileSinkConfig
 
 logger = logging.getLogger(__name__)
 
 # Sentinel value used to check if the run ID is the default value.
 DEFAULT_RUN_ID = "__DEFAULT_RUN_ID"
 
@@ -43,15 +43,15 @@
     # See https://github.com/samuelcolvin/pydantic/pull/2336.
 
     source: SourceConfig
     sink: DynamicTypedConfig
     transformers: Optional[List[DynamicTypedConfig]]
     reporting: List[ReporterConfig] = []
     run_id: str = DEFAULT_RUN_ID
-    datahub_api: Optional[DataHubGraphConfig] = None
+    datahub_api: Optional[DatahubClientConfig] = None
     pipeline_name: Optional[str] = None
     failure_log: FailureLoggingConfig = FailureLoggingConfig()
 
     _raw_dict: Optional[
         dict
     ] = None  # the raw dict that was parsed to construct this config
 
@@ -87,21 +87,21 @@
             )
             values["sink"] = default_sink_config
 
         return values
 
     @validator("datahub_api", always=True)
     def datahub_api_should_use_rest_sink_as_default(
-        cls, v: Optional[DataHubGraphConfig], values: Dict[str, Any], **kwargs: Any
-    ) -> Optional[DataHubGraphConfig]:
+        cls, v: Optional[DatahubClientConfig], values: Dict[str, Any], **kwargs: Any
+    ) -> Optional[DatahubClientConfig]:
         if v is None and "sink" in values and hasattr(values["sink"], "type"):
             sink_type = values["sink"].type
             if sink_type == "datahub-rest":
                 sink_config = values["sink"].config
-                v = DataHubGraphConfig.parse_obj(sink_config)
+                v = DatahubClientConfig.parse_obj_allow_extras(sink_config)
         return v
 
     @classmethod
     def from_dict(
         cls, resolved_dict: dict, raw_dict: Optional[dict] = None
     ) -> "PipelineConfig":
         config = cls.parse_obj(resolved_dict)
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/sink/console.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/console.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/sink/datahub_kafka.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/datahub_kafka.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/sink/datahub_lite.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/datahub_lite.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/sink/datahub_rest.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/datahub_rest.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/sink/file.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/sink/file.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/aws_common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/aws_common.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 from pydantic.fields import Field
 
 from datahub.configuration.common import (
     AllowDenyPattern,
     ConfigModel,
     PermissiveConfigModel,
 )
-from datahub.configuration.source_common import EnvBasedSourceConfigBase
+from datahub.configuration.source_common import EnvConfigMixin
 
 if TYPE_CHECKING:
     from mypy_boto3_glue import GlueClient
     from mypy_boto3_s3 import S3Client, S3ServiceResource
     from mypy_boto3_sagemaker import SageMakerClient
     from mypy_boto3_sts import STSClient
 
@@ -197,15 +197,15 @@
     def get_glue_client(self) -> "GlueClient":
         return self.get_session().client("glue")
 
     def get_sagemaker_client(self) -> "SageMakerClient":
         return self.get_session().client("sagemaker")
 
 
-class AwsSourceConfig(EnvBasedSourceConfigBase, AwsConnectionConfig):
+class AwsSourceConfig(EnvConfigMixin, AwsConnectionConfig):
     """
     Common AWS credentials config.
 
     Currently used by:
         - Glue source
         - SageMaker source
     """
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/glue.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/glue.py`

 * *Files 2% similar despite different names*

```diff
@@ -18,14 +18,15 @@
 
 import botocore.exceptions
 import yaml
 from pydantic import validator
 from pydantic.fields import Field
 
 from datahub.configuration.common import AllowDenyPattern, ConfigurationError
+from datahub.configuration.source_common import DatasetSourceConfigMixin
 from datahub.emitter import mce_builder
 from datahub.emitter.mce_builder import (
     get_sys_time,
     make_data_platform_urn,
     make_dataplatform_instance_urn,
     make_dataset_urn_with_platform_instance,
     make_domain_urn,
@@ -48,14 +49,18 @@
     support_status,
 )
 from datahub.ingestion.api.ingestion_job_checkpointing_provider_base import JobId
 from datahub.ingestion.api.workunit import MetadataWorkUnit
 from datahub.ingestion.source.aws import s3_util
 from datahub.ingestion.source.aws.aws_common import AwsSourceConfig
 from datahub.ingestion.source.aws.s3_util import is_s3_uri, make_s3_urn
+from datahub.ingestion.source.common.subtypes import (
+    DatasetContainerSubTypes,
+    DatasetSubTypes,
+)
 from datahub.ingestion.source.glue_profiling_config import GlueProfilingConfig
 from datahub.ingestion.source.state.checkpoint import Checkpoint
 from datahub.ingestion.source.state.sql_common_state import (
     BaseSQLAlchemyCheckpointState,
 )
 from datahub.ingestion.source.state.stale_entity_removal_handler import (
     StaleEntityRemovalHandler,
@@ -105,26 +110,29 @@
 logger = logging.getLogger(__name__)
 
 
 DEFAULT_PLATFORM = "glue"
 VALID_PLATFORMS = [DEFAULT_PLATFORM, "athena"]
 
 
-class GlueSourceConfig(AwsSourceConfig, StatefulIngestionConfigBase):
+class GlueSourceConfig(
+    StatefulIngestionConfigBase, DatasetSourceConfigMixin, AwsSourceConfig
+):
+    platform: str = Field(
+        default=DEFAULT_PLATFORM,
+        description=f"The platform to use for the dataset URNs. Must be one of {VALID_PLATFORMS}.",
+    )
+
     extract_owners: Optional[bool] = Field(
         default=True,
         description="When enabled, extracts ownership from Glue directly and overwrites existing owners. When disabled, ownership is left empty for datasets.",
     )
     extract_transforms: Optional[bool] = Field(
         default=True, description="Whether to extract Glue transform jobs."
     )
-    underlying_platform: Optional[str] = Field(
-        default=None,
-        description="@deprecated(Use `platform`) Override for platform name. Allowed values - `glue`, `athena`",
-    )
     ignore_unsupported_connectors: Optional[bool] = Field(
         default=True,
         description="Whether to ignore unsupported connectors. If disabled, an error will be raised.",
     )
     emit_s3_lineage: bool = Field(
         default=False, description=" Whether to emit S3-to-Glue lineage."
     )
@@ -164,34 +172,25 @@
     @property
     def s3_client(self):
         return self.get_s3_client()
 
     @validator("glue_s3_lineage_direction")
     def check_direction(cls, v: str) -> str:
         if v.lower() not in ["upstream", "downstream"]:
-            raise ConfigurationError(
+            raise ValueError(
                 "glue_s3_lineage_direction must be either upstream or downstream"
             )
         return v.lower()
 
-    @validator("underlying_platform")
-    def underlying_platform_validator(cls, v: str) -> str:
-        if not v or v in VALID_PLATFORMS:
-            return v
-        else:
-            raise ConfigurationError(
-                f"'underlying_platform' can only take following values: {VALID_PLATFORMS}"
-            )
-
     @validator("platform")
     def platform_validator(cls, v: str) -> str:
         if not v or v in VALID_PLATFORMS:
             return v
         else:
-            raise ConfigurationError(
+            raise ValueError(
                 f"'platform' can only take following values: {VALID_PLATFORMS}"
             )
 
 
 @dataclass
 class GlueSourceReport(StaleEntityRemovalSourceReport):
     tables_scanned = 0
@@ -291,24 +290,15 @@
     @classmethod
     def create(cls, config_dict, ctx):
         config = GlueSourceConfig.parse_obj(config_dict)
         return cls(config, ctx)
 
     @property
     def platform(self) -> str:
-        """
-        This deprecates "underlying_platform" field in favour of the standard "platform" one, which has
-        more priority when both are defined.
-        :return: platform, otherwise underlying_platform, otherwise "glue"
-        """
-        return (
-            self.source_config.platform
-            or self.source_config.underlying_platform
-            or DEFAULT_PLATFORM
-        )
+        return self.source_config.platform
 
     def get_all_jobs(self):
         """
         List all jobs in Glue. boto3 get_jobs api call doesn't support cross account access.
         """
 
         jobs = []
@@ -896,15 +886,15 @@
         self, database: Mapping[str, Any]
     ) -> Iterable[MetadataWorkUnit]:
         domain_urn = self._gen_domain_urn(database["Name"])
         database_container_key = self.gen_database_key(database["Name"])
         container_workunits = gen_containers(
             container_key=database_container_key,
             name=database["Name"],
-            sub_types=["Database"],
+            sub_types=[DatasetContainerSubTypes.DATABASE],
             domain_urn=domain_urn,
             description=database.get("Description"),
             qualified_name=self.get_glue_arn(
                 account_id=database["CatalogId"], database=database["Name"]
             ),
         )
 
@@ -980,15 +970,15 @@
             self.report.report_workunit(workunit)
             yield workunit
 
             # We also want to assign "table" subType to the dataset representing glue table - unfortunately it is not
             # possible via Dataset snapshot embedded in a mce, so we have to generate a mcp.
             workunit = MetadataChangeProposalWrapper(
                 entityUrn=dataset_urn,
-                aspect=SubTypes(typeNames=["table"]),
+                aspect=SubTypes(typeNames=[DatasetSubTypes.TABLE]),
             ).as_workunit()
             self.report.report_workunit(workunit)
             yield workunit
 
             yield from self._get_domain_wu(
                 dataset_name=full_table_name,
                 entity_urn=dataset_urn,
@@ -1162,17 +1152,16 @@
                     tags_to_add.extend(
                         [current_tag.tag for current_tag in current_tags.tags]
                     )
             else:
                 logger.warning(
                     "Could not connect to DatahubApi. No current tags to maintain"
                 )
-
             # Remove duplicate tags
-            tags_to_add = list(set(tags_to_add))
+            tags_to_add = sorted(list(set(tags_to_add)))
             new_tags = GlobalTagsClass(
                 tags=[TagAssociationClass(tag_to_add) for tag_to_add in tags_to_add]
             )
             return new_tags
 
         def get_schema_metadata() -> Optional[SchemaMetadata]:
             if not table.get("StorageDescriptor"):
@@ -1247,10 +1236,7 @@
                 dataset_snapshot.aspects.append(s3_tags)
 
         metadata_record = MetadataChangeEvent(proposedSnapshot=dataset_snapshot)
         return metadata_record
 
     def get_report(self):
         return self.report
-
-    def get_platform_instance_id(self) -> str:
-        return self.source_config.platform_instance or self.platform
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/path_spec.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/path_spec.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/s3_boto_utils.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/s3_boto_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -63,15 +63,15 @@
             aspect_type=GlobalTagsClass,
         )
         if current_tags:
             tags_to_add.extend([current_tag.tag for current_tag in current_tags.tags])
     else:
         logger.warn("Could not connect to DatahubApi. No current tags to maintain")
     # Remove duplicate tags
-    tags_to_add = list(set(tags_to_add))
+    tags_to_add = sorted(list(set(tags_to_add)))
     new_tags = GlobalTagsClass(
         tags=[TagAssociationClass(tag_to_add) for tag_to_add in tags_to_add]
     )
     return new_tags
 
 
 def list_folders_path(
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/s3_util.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/s3_util.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/sagemaker.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/sagemaker_processors/common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/common.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/sagemaker_processors/feature_groups.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/feature_groups.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/sagemaker_processors/job_classes.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/job_classes.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/sagemaker_processors/jobs.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/jobs.py`

 * *Files 1% similar despite different names*

```diff
@@ -195,33 +195,39 @@
 
         return jobs
 
     def update_model_image_jobs(
         self,
         model_data_url: str,
         job_key: JobKey,
-        metrics: Dict[str, Any] = {},
-        hyperparameters: Dict[str, Any] = {},
+        metrics: Optional[Dict[str, Any]] = None,
+        hyperparameters: Optional[Dict[str, Any]] = None,
     ) -> None:
+        metrics = metrics or {}
+        hyperparameters = hyperparameters or {}
+
         model_jobs = self.model_image_to_jobs[model_data_url]
 
         # if model doesn't have job yet, init
         if job_key in model_jobs:
             model_jobs[job_key].update(hyperparameters, metrics)
 
         else:
             model_jobs[job_key] = ModelJob(hyperparameters, metrics)
 
     def update_model_name_jobs(
         self,
         model_name: str,
         job_key: JobKey,
-        metrics: Dict[str, Any] = {},
-        hyperparameters: Dict[str, Any] = {},
+        metrics: Optional[Dict[str, Any]] = None,
+        hyperparameters: Optional[Dict[str, Any]] = None,
     ) -> None:
+        metrics = metrics or {}
+        hyperparameters = hyperparameters or {}
+
         model_jobs = self.model_name_to_jobs[model_name]
 
         # if model doesn't have job yet, init
         if job_key in model_jobs:
             model_jobs[job_key].update(hyperparameters, metrics)
 
         else:
@@ -841,15 +847,15 @@
                     "uri": s3_uri,
                     "datatype": s3_source.get("S3Datatype"),
                     "distribution_type": s3_source.get("S3DataDistributionType"),
                     "attribute_names": s3_source.get("AttributeNames"),
                     "channel_name": config.get("ChannelName"),
                 }
 
-        output_s3_uri = job.get("OutputDataConfig", {}).get("S3OutputPath")
+        output_data_s3_uri = job.get("OutputDataConfig", {}).get("S3OutputPath")
         checkpoint_s3_uri = job.get("CheckpointConfig", {}).get("S3Uri")
         debug_s3_path = job.get("DebugHookConfig", {}).get("S3OutputPath")
         tensorboard_output_path = job.get("TensorBoardOutputConfig", {}).get(
             "S3OutputPath"
         )
         profiler_output_path = job.get("ProfilerConfig", {}).get("S3OutputPath")
 
@@ -862,15 +868,15 @@
             config.get("S3OutputPath") for config in profiler_rule_configs
         ]
 
         output_datasets = {}
 
         # process all output datasets at once
         for output_s3_uri in [
-            output_s3_uri,
+            output_data_s3_uri,
             checkpoint_s3_uri,
             debug_s3_path,
             tensorboard_output_path,
             profiler_output_path,
             *processed_debug_configs,
             *processed_profiler_configs,
         ]:
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/sagemaker_processors/lineage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/lineage.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/aws/sagemaker_processors/models.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/aws/sagemaker_processors/models.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/azure/azure_common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/azure/azure_common.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/bigquery_v2/bigquery.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/bigquery.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 import atexit
 import logging
 import os
 import re
 import traceback
 from collections import defaultdict
-from datetime import datetime, timedelta
-from typing import Dict, Iterable, List, Optional, Tuple, Type, Union, cast
+from datetime import datetime, timedelta, timezone
+from typing import Dict, Iterable, List, Optional, Set, Tuple, Type, Union, cast
 
 from google.cloud import bigquery
 from google.cloud.bigquery.table import TableListItem
 
 from datahub.configuration.pattern_utils import is_schema_allowed
 from datahub.emitter.mce_builder import (
     make_data_platform_urn,
@@ -31,15 +31,18 @@
 from datahub.ingestion.api.source import (
     CapabilityReport,
     SourceCapability,
     TestableSource,
     TestConnectionReport,
 )
 from datahub.ingestion.api.workunit import MetadataWorkUnit
-from datahub.ingestion.source.bigquery_v2.bigquery_audit import BigqueryTableIdentifier
+from datahub.ingestion.source.bigquery_v2.bigquery_audit import (
+    BigqueryTableIdentifier,
+    BigQueryTableRef,
+)
 from datahub.ingestion.source.bigquery_v2.bigquery_config import BigQueryV2Config
 from datahub.ingestion.source.bigquery_v2.bigquery_report import BigQueryV2Report
 from datahub.ingestion.source.bigquery_v2.bigquery_schema import (
     BigqueryColumn,
     BigQueryDataDictionary,
     BigqueryDataset,
     BigqueryProject,
@@ -47,17 +50,24 @@
     BigqueryView,
 )
 from datahub.ingestion.source.bigquery_v2.common import (
     BQ_EXTERNAL_DATASET_URL_TEMPLATE,
     BQ_EXTERNAL_TABLE_URL_TEMPLATE,
     get_bigquery_client,
 )
-from datahub.ingestion.source.bigquery_v2.lineage import BigqueryLineageExtractor
+from datahub.ingestion.source.bigquery_v2.lineage import (
+    BigqueryLineageExtractor,
+    LineageEdge,
+)
 from datahub.ingestion.source.bigquery_v2.profiler import BigqueryProfiler
 from datahub.ingestion.source.bigquery_v2.usage import BigQueryUsageExtractor
+from datahub.ingestion.source.common.subtypes import (
+    DatasetContainerSubTypes,
+    DatasetSubTypes,
+)
 from datahub.ingestion.source.sql.sql_utils import (
     add_table_to_schema_container,
     gen_database_container,
     gen_schema_container,
     get_domain_wu,
 )
 from datahub.ingestion.source.state.profiling_state_handler import ProfilingHandler
@@ -95,26 +105,28 @@
     SchemaFieldDataType,
     SchemaMetadata,
     StringType,
     TimeType,
 )
 from datahub.metadata.schema_classes import (
     DataPlatformInstanceClass,
+    DatasetLineageTypeClass,
     GlobalTagsClass,
     TagAssociationClass,
 )
 from datahub.specific.dataset import DatasetPatchBuilder
 from datahub.utilities.hive_schema_to_avro import (
     HiveColumnToAvroConverter,
     get_schema_fields_for_hive_column,
 )
 from datahub.utilities.mapping import Constants
 from datahub.utilities.perf_timer import PerfTimer
 from datahub.utilities.registries.domain_registry import DomainRegistry
 from datahub.utilities.source_helpers import (
+    auto_materialize_referenced_tags,
     auto_stale_entity_removal,
     auto_status_aspect,
     auto_workunit_reporter,
 )
 from datahub.utilities.time import datetime_to_ts_millis
 
 logger: logging.Logger = logging.getLogger(__name__)
@@ -132,15 +144,19 @@
         )
         os.unlink(config._credentials_path)
 
 
 @platform_name("BigQuery", doc_order=1)
 @config_class(BigQueryV2Config)
 @support_status(SupportStatus.CERTIFIED)
-@capability(SourceCapability.PLATFORM_INSTANCE, "Enabled by default")
+@capability(
+    SourceCapability.PLATFORM_INSTANCE,
+    "Not supported since BigQuery project ids are globally unique",
+    supported=False,
+)
 @capability(SourceCapability.DOMAINS, "Supported via the `domain` config field")
 @capability(SourceCapability.CONTAINERS, "Enabled by default")
 @capability(SourceCapability.SCHEMA_METADATA, "Enabled by default")
 @capability(
     SourceCapability.DATA_PROFILING,
     "Optionally enabled via configuration",
 )
@@ -208,23 +224,14 @@
 
         set_dataset_urn_to_lower(self.config.convert_urns_to_lowercase)
 
         # For database, schema, tables, views, etc
         self.lineage_extractor = BigqueryLineageExtractor(config, self.report)
         self.usage_extractor = BigQueryUsageExtractor(config, self.report)
 
-        # Currently caching using instance variables
-        # TODO - rewrite cache for readability or use out of the box solution
-        self.db_tables: Dict[str, Dict[str, List[BigqueryTable]]] = {}
-        self.db_views: Dict[str, Dict[str, List[BigqueryView]]] = {}
-
-        self.schema_columns: Dict[
-            Tuple[str, str], Optional[Dict[str, List[BigqueryColumn]]]
-        ] = {}
-
         # Create and register the stateful ingestion use-case handler.
         self.stale_entity_removal_handler = StaleEntityRemovalHandler(
             source=self,
             config=self.config,
             state_type_class=BaseSQLAlchemyCheckpointState,
             pipeline_name=self.ctx.pipeline_name,
             run_id=self.ctx.run_id,
@@ -251,14 +258,19 @@
                 pipeline_name=self.ctx.pipeline_name,
                 run_id=self.ctx.run_id,
             )
         self.profiler = BigqueryProfiler(
             config, self.report, self.profiling_state_handler
         )
 
+        # Global store of table identifiers for lineage filtering
+        self.table_refs: Set[str] = set()
+        # Maps project -> view_ref -> [upstream_table_ref], for view lineage
+        self.view_upstream_tables: Dict[str, Dict[str, List[str]]] = defaultdict(dict)
+
         atexit.register(cleanup, config)
 
     @classmethod
     def create(cls, config_dict: dict, ctx: PipelineContext) -> "BigqueryV2Source":
         config = BigQueryV2Config.parse_obj(config_dict)
         return cls(ctx, config)
 
@@ -292,15 +304,15 @@
                 tables = BigQueryDataDictionary.get_tables_for_dataset(
                     conn=client,
                     project_id=project_id,
                     dataset_name=result[0].name,
                     tables={},
                     with_data_read_permission=config.profiling.enabled,
                 )
-                if len(tables) == 0:
+                if len(list(tables)) == 0:
                     return CapabilityReport(
                         capable=False,
                         failure_reason=f"Tables query did not return any table. It is either empty or no tables in project {project_id}.{result[0].name}",
                     )
 
             except Exception as e:
                 return CapabilityReport(
@@ -315,15 +327,15 @@
         connection_conf: BigQueryV2Config,
         project_ids: List[str],
         report: BigQueryV2Report,
     ) -> CapabilityReport:
         lineage_extractor = BigqueryLineageExtractor(connection_conf, report)
         for project_id in project_ids:
             try:
-                logger.info((f"Lineage capability test for project {project_id}"))
+                logger.info(f"Lineage capability test for project {project_id}")
                 lineage_extractor.test_capability(project_id)
             except Exception as e:
                 return CapabilityReport(
                     capable=False,
                     failure_reason=f"Lineage capability test failed with: {e}",
                 )
 
@@ -334,15 +346,15 @@
         connection_conf: BigQueryV2Config,
         project_ids: List[str],
         report: BigQueryV2Report,
     ) -> CapabilityReport:
         usage_extractor = BigQueryUsageExtractor(connection_conf, report)
         for project_id in project_ids:
             try:
-                logger.info((f"Usage capability test for project {project_id}"))
+                logger.info(f"Usage capability test for project {project_id}")
                 failures_before_test = len(report.failures)
                 usage_extractor.test_capability(project_id)
                 if failures_before_test != len(report.failures):
                     return CapabilityReport(
                         capable=False,
                         failure_reason="Usage capability test failed. Check the logs for further info",
                     )
@@ -418,21 +430,14 @@
             )
             return MetadataChangeProposalWrapper(
                 entityUrn=dataset_urn, aspect=aspect
             ).as_workunit()
         else:
             return None
 
-    def get_platform_instance_id(self) -> str:
-        """
-        The source identifier such as the specific source host address required for stateful ingestion.
-        Individual subclasses need to override this method appropriately.
-        """
-        return f"{self.platform}"
-
     def gen_dataset_key(self, db_name: str, schema: str) -> PlatformKey:
         return BigQueryDatasetKey(
             project_id=db_name,
             dataset_id=schema,
             platform=self.platform,
             instance=self.config.platform_instance,
             backcompat_instance_for_guid=self.config.env,
@@ -448,104 +453,170 @@
 
     def gen_project_id_containers(self, database: str) -> Iterable[MetadataWorkUnit]:
         database_container_key = self.gen_project_id_key(database)
 
         yield from gen_database_container(
             database=database,
             name=database,
-            sub_types=["Project"],
+            sub_types=[DatasetContainerSubTypes.BIGQUERY_PROJECT],
             domain_registry=self.domain_registry,
             domain_config=self.config.domain,
             report=self.report,
             database_container_key=database_container_key,
         )
 
     def gen_dataset_containers(
-        self, dataset: str, project_id: str
+        self, dataset: str, project_id: str, tags: Optional[Dict[str, str]] = None
     ) -> Iterable[MetadataWorkUnit]:
         schema_container_key = self.gen_dataset_key(project_id, dataset)
 
+        tags_joined: Optional[List[str]] = None
+        if tags and self.config.capture_dataset_label_as_tag:
+            tags_joined = [f"{k}:{v}" for k, v in tags.items()]
         database_container_key = self.gen_project_id_key(database=project_id)
 
         yield from gen_schema_container(
             database=project_id,
             schema=dataset,
-            sub_types=["Dataset"],
+            sub_types=[DatasetContainerSubTypes.BIGQUERY_DATASET],
             domain_registry=self.domain_registry,
             domain_config=self.config.domain,
             report=self.report,
             schema_container_key=schema_container_key,
             database_container_key=database_container_key,
             external_url=BQ_EXTERNAL_DATASET_URL_TEMPLATE.format(
                 project=project_id, dataset=dataset
             )
             if self.config.include_external_url
             else None,
+            tags=tags_joined,
         )
 
     def get_workunits_internal(self) -> Iterable[MetadataWorkUnit]:
         logger.info("Getting projects")
         conn: bigquery.Client = get_bigquery_client(self.config)
         self.add_config_to_report()
 
-        projects: List[BigqueryProject]
-        if self.config.project_id:
-            project = BigqueryProject(
-                id=self.config.project_id, name=self.config.project_id
+        projects = self._get_projects(conn)
+        if len(projects) == 0:
+            logger.error(
+                "Get projects didn't return any project. "
+                "Maybe resourcemanager.projects.get permission is missing for the service account. "
+                "You can assign predefined roles/bigquery.metadataViewer role to your service account."
             )
-            projects = [project]
-        else:
-            try:
-                projects = BigQueryDataDictionary.get_projects(conn)
-                if len(projects) == 0:
-                    logger.error(
-                        "Get projects didn't return any project. Maybe resourcemanager.projects.get permission is missing for the service account. You can assign predefined roles/bigquery.metadataViewer role to your service account."
-                    )
-                    self.report.report_failure(
-                        "metadata-extraction",
-                        "Get projects didn't return any project. Maybe resourcemanager.projects.get permission is missing for the service account. You can assign predefined roles/bigquery.metadataViewer role to your service account.",
-                    )
-                    return
-            except Exception as e:
-                logger.error(
-                    f"Get projects didn't return any project. Maybe resourcemanager.projects.get permission is missing for the service account. You can assign predefined roles/bigquery.metadataViewer role to your service account. The error was: {e}"
-                )
-                self.report.report_failure(
-                    "metadata-extraction",
-                    f"Get projects didn't return any project. Maybe resourcemanager.projects.get permission is missing for the service account. You can assign predefined roles/bigquery.metadataViewer role to your service account. The error was: {e}",
-                )
-                return None
+            self.report.report_failure(
+                "metadata-extraction",
+                "Get projects didn't return any project. "
+                "Maybe resourcemanager.projects.get permission is missing for the service account. "
+                "You can assign predefined roles/bigquery.metadataViewer role to your service account.",
+            )
+            return
 
         for project_id in projects:
             if not self.config.project_id_pattern.allowed(project_id.id):
                 self.report.report_dropped(project_id.id)
                 continue
             logger.info(f"Processing project: {project_id.id}")
+            self.report.set_project_state(project_id.id, "Metadata Extraction")
             yield from self._process_project(conn, project_id)
 
-        if self.config.profiling.enabled:
-            logger.info("Starting profiling...")
-            yield from self.profiler.get_workunits(self.db_tables)
+        if self._should_ingest_usage():
+            yield from self.usage_extractor.run(
+                [p.id for p in projects], self.table_refs
+            )
+
+        if self._should_ingest_lineage():
+            for project in projects:
+                self.report.set_project_state(project.id, "Lineage Extraction")
+                yield from self.generate_lineage(project.id)
 
     def get_workunits(self) -> Iterable[MetadataWorkUnit]:
-        return auto_stale_entity_removal(
-            self.stale_entity_removal_handler,
-            auto_workunit_reporter(
-                self.report,
-                auto_status_aspect(self.get_workunits_internal()),
-            ),
+        return auto_materialize_referenced_tags(
+            auto_stale_entity_removal(
+                self.stale_entity_removal_handler,
+                auto_workunit_reporter(
+                    self.report,
+                    auto_status_aspect(self.get_workunits_internal()),
+                ),
+            )
         )
 
+    def _should_ingest_usage(self) -> bool:
+        if not self.config.include_usage_statistics:
+            return False
+
+        if self.config.store_last_usage_extraction_timestamp:
+            if self.redundant_run_skip_handler.should_skip_this_run(
+                cur_start_time_millis=datetime_to_ts_millis(self.config.start_time)
+            ):
+                self.report.report_warning(
+                    "usage-extraction",
+                    f"Skip this run as there was a run later than the current start time: {self.config.start_time}",
+                )
+                return False
+            else:
+                # Update the checkpoint state for this run.
+                self.redundant_run_skip_handler.update_state(
+                    start_time_millis=datetime_to_ts_millis(self.config.start_time),
+                    end_time_millis=datetime_to_ts_millis(self.config.end_time),
+                )
+        return True
+
+    def _should_ingest_lineage(self) -> bool:
+        if not self.config.include_table_lineage:
+            return False
+
+        if self.config.store_last_lineage_extraction_timestamp:
+            if self.redundant_run_skip_handler.should_skip_this_run(
+                cur_start_time_millis=datetime_to_ts_millis(self.config.start_time)
+            ):
+                # Skip this run
+                self.report.report_warning(
+                    "lineage-extraction",
+                    f"Skip this run as there was a run later than the current start time: {self.config.start_time}",
+                )
+                return False
+            else:
+                # Update the checkpoint state for this run.
+                self.redundant_run_skip_handler.update_state(
+                    start_time_millis=datetime_to_ts_millis(self.config.start_time),
+                    end_time_millis=datetime_to_ts_millis(self.config.end_time),
+                )
+        return True
+
+    def _get_projects(self, conn: bigquery.Client) -> List[BigqueryProject]:
+        if self.config.project_ids or self.config.project_id:
+            project_ids = self.config.project_ids or [self.config.project_id]  # type: ignore
+            return [
+                BigqueryProject(id=project_id, name=project_id)
+                for project_id in project_ids
+            ]
+        else:
+            try:
+                return BigQueryDataDictionary.get_projects(conn)
+            except Exception as e:
+                # TODO: Merge with error logging in `get_workunits_internal`
+                trace = traceback.format_exc()
+                logger.error(
+                    f"Get projects didn't return any project. Maybe resourcemanager.projects.get permission is missing for the service account. You can assign predefined roles/bigquery.metadataViewer role to your service account. The error was: {e}"
+                )
+                logger.error(trace)
+                self.report.report_failure(
+                    "metadata-extraction",
+                    f"Get projects didn't return any project. Maybe resourcemanager.projects.get permission is missing for the service account. You can assign predefined roles/bigquery.metadataViewer role to your service account. The error was: {e} Stacktrace: {trace}",
+                )
+                return []
+
     def _process_project(
         self, conn: bigquery.Client, bigquery_project: BigqueryProject
     ) -> Iterable[MetadataWorkUnit]:
-        project_id = bigquery_project.id
+        db_tables: Dict[str, List[BigqueryTable]] = {}
+        db_views: Dict[str, List[BigqueryView]] = {}
 
-        self.db_tables[project_id] = {}
-        self.db_views[project_id] = {}
+        project_id = bigquery_project.id
 
         yield from self.gen_project_id_containers(project_id)
 
         try:
             bigquery_project.datasets = (
                 BigQueryDataDictionary.get_datasets_for_project_id(conn, project_id)
             )
@@ -575,202 +646,246 @@
                 bigquery_dataset.name,
                 project_id,
                 self.config.match_fully_qualified_names,
             ):
                 self.report.report_dropped(f"{bigquery_dataset.name}.*")
                 continue
             try:
-                yield from self._process_schema(conn, project_id, bigquery_dataset)
+                # db_tables and db_views are populated in the this method
+                yield from self._process_schema(
+                    conn, project_id, bigquery_dataset, db_tables, db_views
+                )
+
             except Exception as e:
                 error_message = f"Unable to get tables for dataset {bigquery_dataset.name} in project {project_id}, skipping. Does your service account has bigquery.tables.list, bigquery.routines.get, bigquery.routines.list permission? The error was: {e}"
                 if self.config.profiling.enabled:
                     error_message = f"Unable to get tables for dataset {bigquery_dataset.name} in project {project_id}, skipping. Does your service account has bigquery.tables.list, bigquery.routines.get, bigquery.routines.list permission, bigquery.tables.getData permission? The error was: {e}"
 
                 trace = traceback.format_exc()
                 logger.error(trace)
                 logger.error(error_message)
                 self.report.report_failure(
                     "metadata-extraction",
-                    f"{project_id}.{bigquery_dataset.name} - {error_message}",
+                    f"{project_id}.{bigquery_dataset.name} - {error_message} - {trace}",
                 )
                 continue
 
-        if self.config.include_table_lineage:
-            if (
-                self.config.store_last_lineage_extraction_timestamp
-                and self.redundant_run_skip_handler.should_skip_this_run(
-                    cur_start_time_millis=datetime_to_ts_millis(self.config.start_time)
-                )
-            ):
-                # Skip this run
-                self.report.report_warning(
-                    "lineage-extraction",
-                    f"Skip this run as there was a run later than the current start time: {self.config.start_time}",
-                )
-                return
+        if self.config.profiling.enabled:
+            logger.info(f"Starting profiling project {project_id}")
+            self.report.set_project_state(project_id, "Profiling")
+            yield from self.profiler.get_workunits(
+                project_id=project_id,
+                tables=db_tables,
+            )
 
-            if self.config.store_last_lineage_extraction_timestamp:
-                # Update the checkpoint state for this run.
-                self.redundant_run_skip_handler.update_state(
-                    start_time_millis=datetime_to_ts_millis(self.config.start_time),
-                    end_time_millis=datetime_to_ts_millis(self.config.end_time),
-                )
+    def generate_lineage(self, project_id: str) -> Iterable[MetadataWorkUnit]:
+        logger.info(f"Generate lineage for {project_id}")
+        lineage = self.lineage_extractor.calculate_lineage_for_project(project_id)
 
-            yield from self.generate_lineage(project_id)
+        if self.config.lineage_parse_view_ddl:
+            for view, upstream_tables in self.view_upstream_tables[project_id].items():
+                # Override upstreams obtained by parsing audit logs as they may contain indirectly referenced tables
+                lineage[view] = {
+                    LineageEdge(
+                        table=table,
+                        auditStamp=datetime.now(timezone.utc),
+                        type=DatasetLineageTypeClass.VIEW,
+                    )
+                    for table in upstream_tables
+                }
 
-        if self.config.include_usage_statistics:
-            if (
-                self.config.store_last_usage_extraction_timestamp
-                and self.redundant_run_skip_handler.should_skip_this_run(
-                    cur_start_time_millis=datetime_to_ts_millis(self.config.start_time)
-                )
-            ):
-                self.report.report_warning(
-                    "usage-extraction",
-                    f"Skip this run as there was a run later than the current start time: {self.config.start_time}",
-                )
-                return
+        for lineage_key in lineage.keys():
+            if lineage_key not in self.table_refs:
+                continue
 
-            if self.config.store_last_usage_extraction_timestamp:
-                # Update the checkpoint state for this run.
-                self.redundant_run_skip_handler.update_state(
-                    start_time_millis=datetime_to_ts_millis(self.config.start_time),
-                    end_time_millis=datetime_to_ts_millis(self.config.end_time),
-                )
+            table_ref = BigQueryTableRef.from_string_name(lineage_key)
+            dataset_urn = self.gen_dataset_urn(
+                project_id=table_ref.table_identifier.project_id,
+                dataset_name=table_ref.table_identifier.dataset,
+                table=table_ref.table_identifier.get_table_display_name(),
+            )
 
-            yield from self.generate_usage_statistics(project_id)
+            lineage_info = self.lineage_extractor.get_lineage_for_table(
+                bq_table=table_ref,
+                platform=self.platform,
+                lineage_metadata=lineage,
+            )
 
-    def generate_lineage(self, project_id: str) -> Iterable[MetadataWorkUnit]:
-        logger.info(f"Generate lineage for {project_id}")
-        for dataset in self.db_tables[project_id]:
-            for table in self.db_tables[project_id][dataset]:
-                dataset_urn = self.gen_dataset_urn(dataset, project_id, table.name)
-                lineage_info = self.lineage_extractor.get_upstream_lineage_info(
-                    project_id=project_id,
-                    dataset_name=dataset,
-                    table=table,
-                    platform=self.platform,
-                )
-                if lineage_info:
-                    yield from self.gen_lineage(dataset_urn, lineage_info)
-        for dataset in self.db_views[project_id]:
-            for view in self.db_views[project_id][dataset]:
-                dataset_urn = self.gen_dataset_urn(dataset, project_id, view.name)
-                lineage_info = self.lineage_extractor.get_upstream_lineage_info(
-                    project_id=project_id,
-                    dataset_name=dataset,
-                    table=view,
-                    platform=self.platform,
-                )
+            if lineage_info:
                 yield from self.gen_lineage(dataset_urn, lineage_info)
 
-    def generate_usage_statistics(self, project_id: str) -> Iterable[MetadataWorkUnit]:
-        logger.info(f"Generate usage for {project_id}")
-        tables: Dict[str, List[str]] = defaultdict()
-        for dataset in self.db_tables[project_id]:
-            tables[dataset] = [
-                BigqueryTableIdentifier(
-                    project_id, dataset, table.name
-                ).get_table_name()
-                for table in self.db_tables[project_id][dataset]
-            ]
-        for dataset in self.db_views[project_id]:
-            tables[dataset].extend(
-                [table.name for table in self.db_views[project_id][dataset]]
-            )
-        yield from self.usage_extractor.generate_usage_for_project(project_id, tables)
-
     def _process_schema(
-        self, conn: bigquery.Client, project_id: str, bigquery_dataset: BigqueryDataset
+        self,
+        conn: bigquery.Client,
+        project_id: str,
+        bigquery_dataset: BigqueryDataset,
+        db_tables: Dict[str, List[BigqueryTable]],
+        db_views: Dict[str, List[BigqueryView]],
     ) -> Iterable[MetadataWorkUnit]:
         dataset_name = bigquery_dataset.name
 
         yield from self.gen_dataset_containers(
-            dataset_name,
-            project_id,
+            dataset_name, project_id, bigquery_dataset.labels
+        )
+
+        columns = BigQueryDataDictionary.get_columns_for_dataset(
+            conn,
+            project_id=project_id,
+            dataset_name=dataset_name,
+            column_limit=self.config.column_limit,
+            run_optimized_column_query=self.config.run_optimized_column_query,
         )
 
         if self.config.include_tables:
-            bigquery_dataset.tables = self.get_tables_for_dataset(
-                conn, project_id, dataset_name
+            db_tables[dataset_name] = list(
+                self.get_tables_for_dataset(conn, project_id, dataset_name)
             )
-            for table in bigquery_dataset.tables:
-                yield from self._process_table(conn, table, project_id, dataset_name)
+
+            for table in db_tables[dataset_name]:
+                table_columns = columns.get(table.name, []) if columns else []
+                yield from self._process_table(
+                    table=table,
+                    columns=table_columns,
+                    project_id=project_id,
+                    dataset_name=dataset_name,
+                )
 
         if self.config.include_views:
-            bigquery_dataset.views = self.get_views_for_dataset(
-                conn, project_id, dataset_name
+            db_views[dataset_name] = list(
+                BigQueryDataDictionary.get_views_for_dataset(
+                    conn, project_id, dataset_name, self.config.profiling.enabled
+                )
             )
 
-            for view in bigquery_dataset.views:
-                yield from self._process_view(conn, view, project_id, dataset_name)
+            for view in db_views[dataset_name]:
+                view_columns = columns.get(view.name, []) if columns else []
+                yield from self._process_view(
+                    view=view,
+                    columns=view_columns,
+                    project_id=project_id,
+                    dataset_name=dataset_name,
+                )
+
+    # This method is used to generate the ignore list for datatypes the profiler doesn't support we have to do it here
+    # because the profiler doesn't have access to columns
+    def generate_profile_ignore_list(self, columns: List[BigqueryColumn]) -> List[str]:
+        ignore_list: List[str] = []
+        for column in columns:
+            if not column.data_type or any(
+                word in column.data_type.lower()
+                for word in ["array", "struct", "geography", "json"]
+            ):
+                ignore_list.append(column.field_path)
+        return ignore_list
 
     def _process_table(
         self,
-        conn: bigquery.Client,
         table: BigqueryTable,
+        columns: List[BigqueryColumn],
         project_id: str,
-        schema_name: str,
+        dataset_name: str,
     ) -> Iterable[MetadataWorkUnit]:
-        table_identifier = BigqueryTableIdentifier(project_id, schema_name, table.name)
+        table_identifier = BigqueryTableIdentifier(project_id, dataset_name, table.name)
 
         self.report.report_entity_scanned(table_identifier.raw_table_name())
 
         if not self.config.table_pattern.allowed(table_identifier.raw_table_name()):
             self.report.report_dropped(table_identifier.raw_table_name())
             return
 
-        table.columns = self.get_columns_for_table(
-            conn, table_identifier, self.config.column_limit
-        )
-        if not table.columns:
+        if self.config.include_table_lineage or self.config.include_usage_statistics:
+            self.table_refs.add(
+                str(BigQueryTableRef(table_identifier).get_sanitized_table_ref())
+            )
+        table.column_count = len(columns)
+
+        # We only collect profile ignore list if profiling is enabled and profile_table_level_only is false
+        if (
+            self.config.profiling.enabled
+            and not self.config.profiling.profile_table_level_only
+        ):
+            table.columns_ignore_from_profiling = self.generate_profile_ignore_list(
+                columns
+            )
+
+        if not table.column_count:
             logger.warning(
                 f"Table doesn't have any column or unable to get columns for table: {table_identifier}"
             )
 
-        yield from self.gen_table_dataset_workunits(table, project_id, schema_name)
+        # If table has time partitioning, set the data type of the partitioning field
+        if table.partition_info:
+            table.partition_info.column = next(
+                (
+                    column
+                    for column in columns
+                    if column.name == table.partition_info.field
+                ),
+                None,
+            )
+        yield from self.gen_table_dataset_workunits(
+            table, columns, project_id, dataset_name
+        )
 
     def _process_view(
         self,
-        conn: bigquery.Client,
         view: BigqueryView,
+        columns: List[BigqueryColumn],
         project_id: str,
         dataset_name: str,
     ) -> Iterable[MetadataWorkUnit]:
         table_identifier = BigqueryTableIdentifier(project_id, dataset_name, view.name)
 
         self.report.report_entity_scanned(table_identifier.raw_table_name(), "view")
 
         if not self.config.view_pattern.allowed(table_identifier.raw_table_name()):
             self.report.report_dropped(table_identifier.raw_table_name())
             return
 
-        view.columns = self.get_columns_for_table(
-            conn, table_identifier, column_limit=self.config.column_limit
-        )
-
-        if dataset_name not in self.db_views[project_id]:
-            self.db_views[project_id][dataset_name] = []
+        if self.config.include_table_lineage or self.config.include_usage_statistics:
+            table_ref = str(
+                BigQueryTableRef(table_identifier).get_sanitized_table_ref()
+            )
+            self.table_refs.add(table_ref)
+            if self.config.lineage_parse_view_ddl:
+                upstream_tables = self.lineage_extractor.parse_view_lineage(
+                    project_id, dataset_name, view
+                )
+                if upstream_tables is not None:
+                    self.view_upstream_tables[project_id][table_ref] = [
+                        str(BigQueryTableRef(table_id).get_sanitized_table_ref())
+                        for table_id in upstream_tables
+                    ]
 
-        self.db_views[project_id][dataset_name].append(view)
+        view.column_count = len(columns)
+        if not view.column_count:
+            logger.warning(
+                f"View doesn't have any column or unable to get columns for table: {table_identifier}"
+            )
 
-        yield from self.gen_view_dataset_workunits(view, project_id, dataset_name)
+        yield from self.gen_view_dataset_workunits(
+            table=view,
+            columns=columns,
+            project_id=project_id,
+            dataset_name=dataset_name,
+        )
 
     def gen_table_dataset_workunits(
         self,
         table: BigqueryTable,
+        columns: List[BigqueryColumn],
         project_id: str,
         dataset_name: str,
     ) -> Iterable[MetadataWorkUnit]:
         custom_properties: Dict[str, str] = {}
         if table.expires:
             custom_properties["expiration_date"] = str(table.expires)
 
-        if table.time_partitioning:
-            custom_properties["time_partitioning"] = str(table.time_partitioning)
+        if table.partition_info:
+            custom_properties["partition_info"] = str(table.partition_info)
 
         if table.size_in_bytes:
             custom_properties["size_in_bytes"] = str(table.size_in_bytes)
 
         if table.active_billable_bytes:
             custom_properties["billable_bytes_active"] = str(
                 table.active_billable_bytes
@@ -782,80 +897,92 @@
             )
 
         if table.max_partition_id:
             custom_properties["number_of_partitions"] = str(table.num_partitions)
             custom_properties["max_partition_id"] = str(table.max_partition_id)
             custom_properties["is_partitioned"] = str(True)
 
-        sub_types: List[str] = ["table"]
+        sub_types: List[str] = [DatasetSubTypes.TABLE]
         if table.max_shard_id:
             custom_properties["max_shard_id"] = str(table.max_shard_id)
             custom_properties["is_sharded"] = str(True)
-            sub_types = ["sharded table", "table"]
+            sub_types = ["sharded table"] + sub_types
 
         tags_to_add = None
         if table.labels and self.config.capture_table_label_as_tag:
             tags_to_add = []
             tags_to_add.extend(
                 [make_tag_urn(f"""{k}:{v}""") for k, v in table.labels.items()]
             )
 
         yield from self.gen_dataset_workunits(
             table=table,
+            columns=columns,
             project_id=project_id,
             dataset_name=dataset_name,
             sub_types=sub_types,
             tags_to_add=tags_to_add,
             custom_properties=custom_properties,
         )
 
     def gen_view_dataset_workunits(
         self,
         table: BigqueryView,
+        columns: List[BigqueryColumn],
         project_id: str,
         dataset_name: str,
     ) -> Iterable[MetadataWorkUnit]:
         yield from self.gen_dataset_workunits(
             table=table,
+            columns=columns,
             project_id=project_id,
             dataset_name=dataset_name,
-            sub_types=["view"],
+            sub_types=[DatasetSubTypes.VIEW],
         )
 
         view = cast(BigqueryView, table)
         view_definition_string = view.view_definition
         view_properties_aspect = ViewProperties(
-            materialized=False, viewLanguage="SQL", viewLogic=view_definition_string
+            materialized=view.materialized,
+            viewLanguage="SQL",
+            viewLogic=view_definition_string,
         )
         yield MetadataChangeProposalWrapper(
-            entityUrn=self.gen_dataset_urn(dataset_name, project_id, table.name),
+            entityUrn=self.gen_dataset_urn(
+                project_id=project_id, dataset_name=dataset_name, table=table.name
+            ),
             aspect=view_properties_aspect,
         ).as_workunit()
 
     def gen_dataset_workunits(
         self,
         table: Union[BigqueryTable, BigqueryView],
+        columns: List[BigqueryColumn],
         project_id: str,
         dataset_name: str,
         sub_types: List[str],
         tags_to_add: Optional[List[str]] = None,
         custom_properties: Optional[Dict[str, str]] = None,
     ) -> Iterable[MetadataWorkUnit]:
-        dataset_urn = self.gen_dataset_urn(dataset_name, project_id, table.name)
+        dataset_urn = self.gen_dataset_urn(
+            project_id=project_id, dataset_name=dataset_name, table=table.name
+        )
 
         status = Status(removed=False)
         yield MetadataChangeProposalWrapper(
             entityUrn=dataset_urn, aspect=status
         ).as_workunit()
 
         datahub_dataset_name = BigqueryTableIdentifier(
             project_id, dataset_name, table.name
         )
 
-        yield self.gen_schema_metadata(dataset_urn, table, str(datahub_dataset_name))
+        yield self.gen_schema_metadata(
+            dataset_urn, table, columns, str(datahub_dataset_name)
+        )
 
         dataset_properties = DatasetProperties(
             name=datahub_dataset_name.get_table_display_name(),
             description=table.comment,
             qualifiedName=str(datahub_dataset_name),
             created=TimeStamp(time=int(table.created.timestamp() * 1000))
             if table.created is not None
@@ -941,15 +1068,15 @@
         tags = GlobalTagsClass(
             tags=[TagAssociationClass(tag_to_add) for tag_to_add in tags_to_add]
         )
         return MetadataChangeProposalWrapper(
             entityUrn=dataset_urn, aspect=tags
         ).as_workunit()
 
-    def gen_dataset_urn(self, dataset_name: str, project_id: str, table: str) -> str:
+    def gen_dataset_urn(self, project_id: str, dataset_name: str, table: str) -> str:
         datahub_dataset_name = BigqueryTableIdentifier(project_id, dataset_name, table)
         dataset_urn = make_dataset_urn_with_platform_instance(
             self.platform,
             str(datahub_dataset_name),
             self.config.platform_instance,
             self.config.env,
         )
@@ -1006,221 +1133,141 @@
             last_id = col.ordinal_position
         return schema_fields
 
     def gen_schema_metadata(
         self,
         dataset_urn: str,
         table: Union[BigqueryTable, BigqueryView],
+        columns: List[BigqueryColumn],
         dataset_name: str,
     ) -> MetadataWorkUnit:
         schema_metadata = SchemaMetadata(
             schemaName=dataset_name,
             platform=make_data_platform_urn(self.platform),
             version=0,
             hash="",
             platformSchema=MySqlDDL(tableSchema=""),
-            fields=self.gen_schema_fields(table.columns),
+            # fields=[],
+            fields=self.gen_schema_fields(columns),
         )
         return MetadataChangeProposalWrapper(
             entityUrn=dataset_urn, aspect=schema_metadata
         ).as_workunit()
 
     def get_report(self) -> BigQueryV2Report:
         return self.report
 
     def get_tables_for_dataset(
         self,
         conn: bigquery.Client,
         project_id: str,
         dataset_name: str,
-    ) -> List[BigqueryTable]:
-        bigquery_tables: Optional[List[BigqueryTable]] = (
-            self.db_tables[project_id].get(dataset_name)
-            if project_id in self.db_tables
-            else []
-        )
-
+    ) -> Iterable[BigqueryTable]:
         # In bigquery there is no way to query all tables in a Project id
-        if not bigquery_tables:
-            with PerfTimer() as timer:
-                bigquery_tables = []
-                table_count: int = 0
-                table_items: Dict[str, TableListItem] = {}
-                # Dict to store sharded table and the last seen max shard id
-                sharded_tables: Dict[str, TableListItem] = defaultdict()
-                # Partitions view throw exception if we try to query partition info for too many tables
-                # so we have to limit the number of tables we query partition info.
-                # The conn.list_tables returns table infos that information_schema doesn't contain and this
-                # way we can merge that info with the queried one.
-                # https://cloud.google.com/bigquery/docs/information-schema-partitions
-                for table in conn.list_tables(f"{project_id}.{dataset_name}"):
-                    table_identifier = BigqueryTableIdentifier(
-                        project_id=project_id,
-                        dataset=dataset_name,
-                        table=table.table_id,
-                    )
-
-                    _, shard = BigqueryTableIdentifier.get_table_and_shard(
-                        table_identifier.raw_table_name()
-                    )
-                    table_name = table_identifier.get_table_name().split(".")[-1]
-
-                    # For sharded tables we only process the latest shard
-                    # which has the highest date in the table name.
-                    # Sharded tables look like: table_20220120
-                    # We only has one special case where the table name is a date
-                    # in this case we merge all these tables under dataset name as table name.
-                    # For example some_dataset.20220110 will be turned to some_dataset.some_dataset
-                    # It seems like there are some bigquery user who uses this way the tables.
-                    if shard:
-                        if not sharded_tables.get(table_identifier.get_table_name()):
-                            # When table is only a shard we use dataset_name as table_name
-                            sharded_tables[table_name] = table
-                            continue
-                        else:
-                            stored_table_identifier = BigqueryTableIdentifier(
-                                project_id=project_id,
-                                dataset=dataset_name,
-                                table=sharded_tables[table_name].table_id,
-                            )
-                            (
-                                _,
-                                stored_shard,
-                            ) = BigqueryTableIdentifier.get_table_and_shard(
-                                stored_table_identifier.raw_table_name()
-                            )
-                            # When table is none, we use dataset_name as table_name
-                            table_name = table_identifier.get_table_name().split(".")[
-                                -1
-                            ]
-                            assert stored_shard
-                            if stored_shard < shard:
-                                sharded_tables[table_name] = table
-                            continue
-                    else:
-                        table_count = table_count + 1
-                        table_items[table.table_id] = table
-
-                    if str(table_identifier).startswith(
-                        self.config.temp_table_dataset_prefix
-                    ):
-                        logger.debug(
-                            f"Dropping temporary table {table_identifier.table}"
-                        )
-                        self.report.report_dropped(table_identifier.raw_table_name())
-                        continue
-
-                    if (
-                        table_count % self.config.number_of_datasets_process_in_batch
-                        == 0
-                    ):
-                        bigquery_tables.extend(
-                            BigQueryDataDictionary.get_tables_for_dataset(
-                                conn,
-                                project_id,
-                                dataset_name,
-                                table_items,
-                                with_data_read_permission=self.config.profiling.enabled,
-                            )
-                        )
-                        table_items.clear()
-
-                # Sharded tables don't have partition keys, so it is safe to add to the list as
-                # it should not affect the number of tables will be touched in the partitions system view.
-                # Because we have the batched query of get_tables_for_dataset to makes sure
-                # we won't hit too many tables queried with partitions system view.
-                # The key in the map is the actual underlying table name and not the friendly name and
-                # that's why we need to get the actual table names and not the normalized ones.
-                table_items.update(
-                    {value.table_id: value for value in sharded_tables.values()}
-                )
-
-                if table_items:
-                    bigquery_tables.extend(
-                        BigQueryDataDictionary.get_tables_for_dataset(
-                            conn,
-                            project_id,
-                            dataset_name,
-                            table_items,
-                            with_data_read_permission=self.config.profiling.enabled,
-                        )
+        with PerfTimer() as timer:
+            # Partitions view throw exception if we try to query partition info for too many tables
+            # so we have to limit the number of tables we query partition info.
+            # The conn.list_tables returns table infos that information_schema doesn't contain and this
+            # way we can merge that info with the queried one.
+            # https://cloud.google.com/bigquery/docs/information-schema-partitions
+            max_batch_size: int = (
+                self.config.number_of_datasets_process_in_batch
+                if not self.config.profiling.enabled
+                else self.config.number_of_datasets_process_in_batch_if_profiling_enabled
+            )
+
+            # We get the list of tables in the dataset to get core table properties and to be able to process the tables in batches
+            # We collect only the latest shards from sharded tables (tables with _YYYYMMDD suffix) and ignore temporary tables
+            table_items = self.get_core_table_details(conn, dataset_name, project_id)
+
+            items_to_get: Dict[str, TableListItem] = {}
+            for table_item in table_items.keys():
+                items_to_get[table_item] = table_items[table_item]
+                if len(items_to_get) % max_batch_size == 0:
+                    yield from BigQueryDataDictionary.get_tables_for_dataset(
+                        conn,
+                        project_id,
+                        dataset_name,
+                        items_to_get,
+                        with_data_read_permission=self.config.profiling.enabled,
                     )
+                    items_to_get.clear()
 
-                self.db_tables[project_id][dataset_name] = bigquery_tables
+            if items_to_get:
+                yield from BigQueryDataDictionary.get_tables_for_dataset(
+                    conn,
+                    project_id,
+                    dataset_name,
+                    items_to_get,
+                    with_data_read_permission=self.config.profiling.enabled,
+                )
+
+        self.report.metadata_extraction_sec[f"{project_id}.{dataset_name}"] = round(
+            timer.elapsed_seconds(), 2
+        )
+
+    def get_core_table_details(
+        self, conn: bigquery.Client, dataset_name: str, project_id: str
+    ) -> Dict[str, TableListItem]:
+        table_items: Dict[str, TableListItem] = {}
+        # Dict to store sharded table and the last seen max shard id
+        sharded_tables: Dict[str, TableListItem] = {}
+
+        for table in conn.list_tables(f"{project_id}.{dataset_name}"):
+            table_identifier = BigqueryTableIdentifier(
+                project_id=project_id,
+                dataset=dataset_name,
+                table=table.table_id,
+            )
+
+            _, shard = BigqueryTableIdentifier.get_table_and_shard(
+                table_identifier.table
+            )
+            table_name = table_identifier.get_table_name().split(".")[-1]
+
+            # Sharded tables look like: table_20220120
+            # For sharded tables we only process the latest shard and ignore the rest
+            # to find the latest shard we iterate over the list of tables and store the maximum shard id
+            # We only have one special case where the table name is a date `20220110`
+            # in this case we merge all these tables under dataset name as table name.
+            # For example some_dataset.20220110 will be turned to some_dataset.some_dataset
+            # It seems like there are some bigquery user who uses this non-standard way of sharding the tables.
+            if shard:
+                if table_name not in sharded_tables:
+                    sharded_tables[table_name] = table
+                    continue
 
-                self.report.metadata_extraction_sec[
-                    f"{project_id}.{dataset_name}"
-                ] = round(timer.elapsed_seconds(), 2)
-
-                return bigquery_tables
-
-        # Some schema may not have any table
-        return (
-            self.db_tables[project_id].get(dataset_name, [])
-            if project_id in self.db_tables
-            else []
-        )
-
-    def get_views_for_dataset(
-        self,
-        conn: bigquery.Client,
-        project_id: str,
-        dataset_name: str,
-    ) -> List[BigqueryView]:
-        views = self.db_views.get(project_id, {}).get(dataset_name, [])
-
-        if not views:
-            return BigQueryDataDictionary.get_views_for_dataset(
-                conn, project_id, dataset_name, self.config.profiling.enabled
-            )
-        return views
-
-    def get_columns_for_table(
-        self,
-        conn: bigquery.Client,
-        table_identifier: BigqueryTableIdentifier,
-        column_limit: Optional[int] = None,
-    ) -> List[BigqueryColumn]:
-        if (
-            table_identifier.project_id,
-            table_identifier.dataset,
-        ) not in self.schema_columns.keys():
-            columns = BigQueryDataDictionary.get_columns_for_dataset(
-                conn,
-                project_id=table_identifier.project_id,
-                dataset_name=table_identifier.dataset,
-                column_limit=column_limit,
-            )
-            self.schema_columns[
-                (table_identifier.project_id, table_identifier.dataset)
-            ] = columns
-        else:
-            columns = self.schema_columns[
-                (table_identifier.project_id, table_identifier.dataset)
-            ]
+                stored_table_identifier = BigqueryTableIdentifier(
+                    project_id=project_id,
+                    dataset=dataset_name,
+                    table=sharded_tables[table_name].table_id,
+                )
+                _, stored_shard = BigqueryTableIdentifier.get_table_and_shard(
+                    stored_table_identifier.table
+                )
+                # When table is none, we use dataset_name as table_name
+                assert stored_shard
+                if stored_shard < shard:
+                    sharded_tables[table_name] = table
+                continue
+            elif str(table_identifier).startswith(
+                self.config.temp_table_dataset_prefix
+            ):
+                logger.debug(f"Dropping temporary table {table_identifier.table}")
+                self.report.report_dropped(table_identifier.raw_table_name())
+                continue
 
-        # get all columns for schema failed,
-        # falling back to get columns for table
-        if not columns:
-            logger.warning(
-                f"Couldn't get columns on the dataset level for {table_identifier}. Trying to get on table level..."
-            )
-            return BigQueryDataDictionary.get_columns_for_table(
-                conn, table_identifier, self.config.column_limit
-            )
+            table_items[table.table_id] = table
+        # Adding maximum shards to the list of tables
+        table_items.update({value.table_id: value for value in sharded_tables.values()})
 
-        # Access to table but none of its columns - is this possible ?
-        return columns.get(table_identifier.table, [])
+        return table_items
 
     def add_config_to_report(self):
         self.report.include_table_lineage = self.config.include_table_lineage
         self.report.use_date_sharded_audit_log_tables = (
             self.config.use_date_sharded_audit_log_tables
         )
         self.report.log_page_size = self.config.log_page_size
         self.report.use_exported_bigquery_audit_metadata = (
             self.config.use_exported_bigquery_audit_metadata
         )
-
-    def warn(self, log: logging.Logger, key: str, reason: str) -> None:
-        self.report.report_warning(key, reason)
-        log.warning(f"{key} => {reason}")
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/bigquery_v2/bigquery_audit.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/bigquery_audit.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 import json
 import logging
 import re
 from dataclasses import dataclass, field
 from datetime import datetime
-from typing import Any, ClassVar, Dict, List, Optional, Pattern, Set, Tuple
+from typing import Any, ClassVar, Dict, List, Optional, Pattern, Set, Tuple, Union
 
 from dateutil import parser
 
 from datahub.emitter.mce_builder import make_dataset_urn
 from datahub.utilities.parsing_util import (
     get_first_missing_key,
     get_first_missing_key_any,
@@ -80,15 +80,27 @@
     invalid_chars: ClassVar[Set[str]] = {"$", "@"}
     _BIGQUERY_DEFAULT_SHARDED_TABLE_REGEX: ClassVar[str] = "((.+)[_$])?(\\d{8})$"
     _BIGQUERY_WILDCARD_REGEX: ClassVar[str] = "((_(\\d+)?)\\*$)|\\*$"
     _BQ_SHARDED_TABLE_SUFFIX: str = "_yyyymmdd"
 
     @staticmethod
     def get_table_and_shard(table_name: str) -> Tuple[str, Optional[str]]:
-        match = re.search(
+        """
+        Args:
+            table_name:
+                table name (in form <table-id> or <table-prefix>_<shard>`, optionally prefixed with <project-id>.<dataset-id>)
+                See https://cloud.google.com/bigquery/docs/reference/standard-sql/wildcard-table-reference
+                If table_name is fully qualified, i.e. prefixed with <project-id>.<dataset-id> then the special case of
+                dataset as a sharded table, i.e. table-id itself as shard can not be detected.
+        Returns:
+            (table_name_without_shard, shard):
+                In case of non-sharded tables, returns (<table-id>, None)
+                In case of sharded tables, returns (<table-prefix>, shard)
+        """
+        match = re.match(
             BigqueryTableIdentifier._BIGQUERY_DEFAULT_SHARDED_TABLE_REGEX,
             table_name,
             re.IGNORECASE,
         )
         if match:
             table_name = match.group(2)
             shard = match.group(3)
@@ -100,51 +112,60 @@
         parts = table.split(".")
         return cls(parts[0], parts[1], parts[2])
 
     def raw_table_name(self):
         return f"{self.project_id}.{self.dataset}.{self.table}"
 
     def get_table_display_name(self) -> str:
-        shortened_table_name = self.table
-        # if table name ends in _* or * then we strip it as that represents a query on a sharded table
-        shortened_table_name = re.sub(
-            self._BIGQUERY_WILDCARD_REGEX, "", shortened_table_name
-        )
+        """
+        Returns table display name to be used for DataHub
+            - removes shard suffix (table_yyyymmdd-> table)
+            - removes wildcard part (table_yyyy* -> table)
+            - remove time decorator (table@1624046611000 -> table)
+        """
+        # if table name ends in _* or * or _yyyy* or _yyyymm* then we strip it as that represents a query on a sharded table
+        shortened_table_name = re.sub(self._BIGQUERY_WILDCARD_REGEX, "", self.table)
+
+        matches = BigQueryTableRef.SNAPSHOT_TABLE_REGEX.match(shortened_table_name)
+        if matches:
+            shortened_table_name = matches.group(1)
+            logger.debug(
+                f"Found table snapshot. Using {shortened_table_name} as the table name."
+            )
 
         table_name, _ = self.get_table_and_shard(shortened_table_name)
         if not table_name:
             table_name = self.dataset
 
-        matches = BigQueryTableRef.SNAPSHOT_TABLE_REGEX.match(table_name)
-        if matches:
-            table_name = matches.group(1)
-            logger.debug(f"Found table snapshot. Using {table_name} as the table name.")
-
         # Handle exceptions
         invalid_chars_in_table_name: List[str] = [
             c for c in self.invalid_chars if c in table_name
         ]
         if invalid_chars_in_table_name:
             raise ValueError(
                 f"Cannot handle {self.raw_table_name()} - poorly formatted table name, contains {invalid_chars_in_table_name}"
             )
         return table_name
 
     def get_table_name(self) -> str:
+        """
+        Returns qualified table name to be used for DataHub
+            - removes shard suffix (table_yyyymmdd-> table)
+            - removes wildcard part (table_yyyy* -> table)
+            - remove time decorator (table@1624046611000 -> table)
+        """
         table_name: str = (
             f"{self.project_id}.{self.dataset}.{self.get_table_display_name()}"
         )
         if self.is_sharded_table():
-            table_name = (
-                f"{table_name}{BigqueryTableIdentifier._BQ_SHARDED_TABLE_SUFFIX}"
-            )
+            table_name += BigqueryTableIdentifier._BQ_SHARDED_TABLE_SUFFIX
         return table_name
 
     def is_sharded_table(self) -> bool:
-        _, shard = self.get_table_and_shard(self.raw_table_name())
+        _, shard = self.get_table_and_shard(self.table)
         if shard:
             return True
 
         if re.match(
             f".*({BigqueryTableIdentifier._BIGQUERY_WILDCARD_REGEX})",
             self.raw_table_name(),
             re.IGNORECASE,
@@ -155,16 +176,17 @@
 
     def __str__(self) -> str:
         return self.get_table_name()
 
 
 @dataclass(frozen=True, order=True)
 class BigQueryTableRef:
-    # Handle table snapshots
-    # See https://cloud.google.com/bigquery/docs/table-snapshots-intro.
+    # Handle table time travel. See https://cloud.google.com/bigquery/docs/time-travel
+    # See https://cloud.google.com/bigquery/docs/table-decorators#time_decorators
+    # Handling for @0 and @-TIME_OFFSET is apparently missing
     SNAPSHOT_TABLE_REGEX: ClassVar[Pattern[str]] = re.compile(r"^(.+)@(\d{13})$")
 
     table_identifier: BigqueryTableIdentifier
 
     @classmethod
     def from_bigquery_table(cls, table: BigqueryTableIdentifier) -> "BigQueryTableRef":
         return cls(
@@ -174,15 +196,15 @@
     @classmethod
     def from_spec_obj(cls, spec: dict) -> "BigQueryTableRef":
         for key in ["projectId", "datasetId", "tableId"]:
             if key not in spec.keys():
                 raise ValueError(f"invalid BigQuery table reference dict: {spec}")
 
         return cls(
-            # spec dict always has to have projectId, datasetId, tableId otherwise it is an ivalid spec
+            # spec dict always has to have projectId, datasetId, tableId otherwise it is an invalid spec
             BigqueryTableIdentifier(
                 spec["projectId"], spec["datasetId"], spec["tableId"]
             )
         )
 
     @classmethod
     def from_string_name(cls, ref: str) -> "BigQueryTableRef":
@@ -305,31 +327,33 @@
             statementType=job_query_conf.get("statementType", "UNKNOWN"),
         )
         # destinationTable
         raw_dest_table = job_query_conf.get("destinationTable")
         if raw_dest_table:
             query_event.destinationTable = BigQueryTableRef.from_spec_obj(
                 raw_dest_table
-            )
+            ).get_sanitized_table_ref()
         # statementType
         # referencedTables
         job_stats: Dict = job["jobStatistics"]
         if job_stats.get("totalBilledBytes"):
             query_event.billed_bytes = job_stats["totalBilledBytes"]
 
         raw_ref_tables = job_stats.get("referencedTables")
         if raw_ref_tables:
             query_event.referencedTables = [
-                BigQueryTableRef.from_spec_obj(spec) for spec in raw_ref_tables
+                BigQueryTableRef.from_spec_obj(spec).get_sanitized_table_ref()
+                for spec in raw_ref_tables
             ]
         # referencedViews
         raw_ref_views = job_stats.get("referencedViews")
         if raw_ref_views:
             query_event.referencedViews = [
-                BigQueryTableRef.from_spec_obj(spec) for spec in raw_ref_views
+                BigQueryTableRef.from_spec_obj(spec).get_sanitized_table_ref()
+                for spec in raw_ref_views
             ]
         # payload
         query_event.payload = entry.payload if debug_include_full_payloads else None
         if not query_event.job_name:
             logger.debug(
                 "jobName from query events is absent. "
                 "Auditlog entry - {logEntry}".format(logEntry=entry)
@@ -384,26 +408,28 @@
         # jobName
         query_event.job_name = job.get("jobName")
         # destinationTable
         raw_dest_table = query_config.get("destinationTable")
         if raw_dest_table:
             query_event.destinationTable = BigQueryTableRef.from_string_name(
                 raw_dest_table
-            )
+            ).get_sanitized_table_ref()
         # referencedTables
         raw_ref_tables = query_stats.get("referencedTables")
         if raw_ref_tables:
             query_event.referencedTables = [
-                BigQueryTableRef.from_string_name(spec) for spec in raw_ref_tables
+                BigQueryTableRef.from_string_name(spec).get_sanitized_table_ref()
+                for spec in raw_ref_tables
             ]
         # referencedViews
         raw_ref_views = query_stats.get("referencedViews")
         if raw_ref_views:
             query_event.referencedViews = [
-                BigQueryTableRef.from_string_name(spec) for spec in raw_ref_views
+                BigQueryTableRef.from_string_name(spec).get_sanitized_table_ref()
+                for spec in raw_ref_views
             ]
         # payload
         query_event.payload = payload if debug_include_full_payloads else None
 
         if not query_event.job_name:
             logger.debug(
                 "jobName from query events is absent. "
@@ -448,27 +474,29 @@
         )
         query_event.job_name = job.get("jobName")
         # destinationTable
         raw_dest_table = query_config.get("destinationTable")
         if raw_dest_table:
             query_event.destinationTable = BigQueryTableRef.from_string_name(
                 raw_dest_table
-            )
+            ).get_sanitized_table_ref()
         # statementType
         # referencedTables
         raw_ref_tables = query_stats.get("referencedTables")
         if raw_ref_tables:
             query_event.referencedTables = [
-                BigQueryTableRef.from_string_name(spec) for spec in raw_ref_tables
+                BigQueryTableRef.from_string_name(spec).get_sanitized_table_ref()
+                for spec in raw_ref_tables
             ]
         # referencedViews
         raw_ref_views = query_stats.get("referencedViews")
         if raw_ref_views:
             query_event.referencedViews = [
-                BigQueryTableRef.from_string_name(spec) for spec in raw_ref_views
+                BigQueryTableRef.from_string_name(spec).get_sanitized_table_ref()
+                for spec in raw_ref_views
             ]
         # payload
         query_event.payload = payload if debug_include_full_payloads else None
 
         if not query_event.job_name:
             logger.debug(
                 "jobName from query events is absent. "
@@ -537,18 +565,22 @@
 
         # https://cloud.google.com/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.TableDataRead.Reason
         readReason = readInfo.get("reason")
         jobName = None
         if readReason == "JOB":
             jobName = readInfo.get("jobName")
 
+        resource = BigQueryTableRef.from_string_name(
+            resourceName
+        ).get_sanitized_table_ref()
+
         readEvent = ReadEvent(
             actor_email=user,
             timestamp=entry.timestamp,
-            resource=BigQueryTableRef.from_string_name(resourceName),
+            resource=resource,
             fieldsRead=fields,
             readReason=readReason,
             jobName=jobName,
             payload=entry.payload if debug_include_full_payloads else None,
         )
         if readReason == "JOB" and not jobName:
             logger.debug(
@@ -571,18 +603,22 @@
 
         # https://cloud.google.com/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.TableDataRead.Reason
         readReason = readInfo.get("reason")
         jobName = None
         if readReason == "JOB":
             jobName = readInfo.get("jobName")
 
+        resource = BigQueryTableRef.from_string_name(
+            resourceName
+        ).get_sanitized_table_ref()
+
         readEvent = ReadEvent(
             actor_email=user,
             timestamp=row["timestamp"],
-            resource=BigQueryTableRef.from_string_name(resourceName),
+            resource=resource,
             fieldsRead=fields,
             readReason=readReason,
             jobName=jobName,
             payload=payload if debug_include_full_payloads else None,
         )
         if readReason == "JOB" and not jobName:
             logger.debug(
@@ -592,7 +628,16 @@
         return readEvent
 
 
 @dataclass()
 class AuditEvent:
     read_event: Optional[ReadEvent] = None
     query_event: Optional[QueryEvent] = None
+
+    @classmethod
+    def create(cls, event: Union[ReadEvent, QueryEvent]) -> "AuditEvent":
+        if isinstance(event, QueryEvent):
+            return AuditEvent(query_event=event)
+        elif isinstance(event, ReadEvent):
+            return AuditEvent(read_event=event)
+        else:
+            raise TypeError(f"Cannot create AuditEvent: {event}")
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/bigquery_v2/bigquery_config.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/bigquery_config.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,46 +1,44 @@
 import logging
 import os
 from datetime import timedelta
 from typing import Any, Dict, List, Optional
 
-from pydantic import Field, PositiveInt, PrivateAttr, root_validator, validator
+from pydantic import Field, PositiveInt, PrivateAttr, root_validator
 
-from datahub.configuration.common import AllowDenyPattern, ConfigurationError
+from datahub.configuration.common import AllowDenyPattern
+from datahub.configuration.validate_field_removal import pydantic_removed_field
 from datahub.ingestion.source.sql.sql_config import SQLAlchemyConfig
 from datahub.ingestion.source.state.stateful_ingestion_base import (
-    LineageStatefulIngestionConfig,
-    ProfilingStatefulIngestionConfig,
-    UsageStatefulIngestionConfig,
+    StatefulLineageConfigMixin,
+    StatefulProfilingConfigMixin,
+    StatefulUsageConfigMixin,
 )
 from datahub.ingestion.source.usage.usage_common import BaseUsageConfig
 from datahub.ingestion.source_config.bigquery import BigQueryBaseConfig
 from datahub.ingestion.source_config.usage.bigquery_usage import BigQueryCredential
 
 logger = logging.getLogger(__name__)
 
 
 class BigQueryUsageConfig(BaseUsageConfig):
-    query_log_delay: Optional[PositiveInt] = Field(
-        default=None,
-        description="To account for the possibility that the query event arrives after the read event in the audit logs, we wait for at least query_log_delay additional events to be processed before attempting to resolve BigQuery job information from the logs. If query_log_delay is None, it gets treated as an unlimited delay, which prioritizes correctness at the expense of memory usage.",
-    )
+    _query_log_delay_removed = pydantic_removed_field("query_log_delay")
 
     max_query_duration: timedelta = Field(
         default=timedelta(minutes=15),
         description="Correction to pad start_time and end_time with. For handling the case where the read happens within our time range but the query completion event is delayed and happens after the configured end time.",
     )
 
 
 class BigQueryV2Config(
     BigQueryBaseConfig,
     SQLAlchemyConfig,
-    LineageStatefulIngestionConfig,
-    UsageStatefulIngestionConfig,
-    ProfilingStatefulIngestionConfig,
+    StatefulUsageConfigMixin,
+    StatefulLineageConfigMixin,
+    StatefulProfilingConfigMixin,
 ):
     project_id_pattern: AllowDenyPattern = Field(
         default=AllowDenyPattern.allow_all(),
         description="Regex patterns for project_id to filter in ingestion.",
     )
 
     usage: BigQueryUsageConfig = Field(
@@ -50,15 +48,20 @@
     include_usage_statistics: bool = Field(
         default=True,
         description="Generate usage statistic",
     )
 
     capture_table_label_as_tag: bool = Field(
         default=False,
-        description="Capture BigQuery table labels as tag",
+        description="Capture BigQuery table labels as DataHub tag",
+    )
+
+    capture_dataset_label_as_tag: bool = Field(
+        default=False,
+        description="Capture BigQuery dataset labels as DataHub tag",
     )
 
     dataset_pattern: AllowDenyPattern = Field(
         default=AllowDenyPattern.allow_all(),
         description="Regex patterns for dataset to filter in ingestion. Specify regex to only match the schema name. e.g. to match all tables in schema analytics, use the regex 'analytics'",
     )
 
@@ -74,51 +77,62 @@
 
     debug_include_full_payloads: bool = Field(
         default=False,
         description="Include full payload into events. It is only for debugging and internal use.",
     )
 
     number_of_datasets_process_in_batch: int = Field(
-        default=80,
-        description="Number of table queried in batch when getting metadata. This is a low level config property which should be touched with care. This restriction is needed because we query partitions system view which throws error if we try to touch too many tables.",
+        hidden_from_docs=True,
+        default=500,
+        description="Number of table queried in batch when getting metadata. This is a low level config property which should be touched with care.",
     )
+
+    number_of_datasets_process_in_batch_if_profiling_enabled: int = Field(
+        default=200,
+        description="Number of partitioned table queried in batch when getting metadata. This is a low level config property which should be touched with care. This restriction is needed because we query partitions system view which throws error if we try to touch too many tables.",
+    )
+
     column_limit: int = Field(
         default=300,
         description="Maximum number of columns to process in a table. This is a low level config property which should be touched with care. This restriction is needed because excessively wide tables can result in failure to ingest the schema.",
     )
     # The inheritance hierarchy is wonky here, but these options need modifications.
     project_id: Optional[str] = Field(
         default=None,
-        description="[deprecated] Use project_id_pattern instead. You can use this property if you only want to ingest one project and don't want to give project resourcemanager.projects.list to your service account",
+        description="[deprecated] Use project_id_pattern or project_ids instead.",
+    )
+    project_ids: List[str] = Field(
+        default_factory=list,
+        description="Ingests specified project_ids. Use this property if you only want to ingest one project and don't want to give project resourcemanager.projects.list to your service account.",
     )
 
     project_on_behalf: Optional[str] = Field(
         default=None,
-        description="[Advanced] The BigQuery project in which queries are executed. Will be passed when creating a job. If not passed, falls back to the project associated with the service account..",
+        description="[Advanced] The BigQuery project in which queries are executed. Will be passed when creating a job. If not passed, falls back to the project associated with the service account.",
     )
 
-    storage_project_id: None = Field(default=None, hidden_from_schema=True)
+    storage_project_id: None = Field(default=None, hidden_from_docs=True)
 
     lineage_use_sql_parser: bool = Field(
-        default=False,
-        description="Experimental. Use sql parser to resolve view/table lineage. If there is a view being referenced then bigquery sends both the view as well as underlying tablein the references. There is no distinction between direct/base objects accessed. So doing sql parsing to ensure we only use direct objects accessed for lineage.",
+        default=True,
+        description="Use sql parser to resolve view/table lineage. Only invoked on tables with both upstream tables and views. Used to distinguish between direct/base objects accessed, to only emit upstream lineage for directly accessed objects.",
     )
     lineage_parse_view_ddl: bool = Field(
         default=True,
         description="Sql parse view ddl to get lineage.",
     )
 
     lineage_sql_parser_use_raw_names: bool = Field(
         default=False,
         description="This parameter ignores the lowercase pattern stipulated in the SQLParser. NOTE: Ignored if lineage_use_sql_parser is False.",
     )
 
     extract_lineage_from_catalog: bool = Field(
         default=False,
-        description="This flag enables the data lineage extraction from Data Lineage API exposed by Google Data Catalog. NOTE: This extractor can't build views lineage. It's recommended to enable the view's DDL parsing. Read the docs to have more information about: https://cloud.google.com/data-catalog/docs/reference/data-lineage/rest",
+        description="This flag enables the data lineage extraction from Data Lineage API exposed by Google Data Catalog. NOTE: This extractor can't build views lineage. It's recommended to enable the view's DDL parsing. Read the docs to have more information about: https://cloud.google.com/data-catalog/docs/concepts/about-data-lineage",
     )
 
     convert_urns_to_lowercase: bool = Field(
         default=False,
         description="Convert urns to lowercase.",
     )
 
@@ -160,19 +174,33 @@
     )
     use_date_sharded_audit_log_tables: bool = Field(
         default=False,
         description="Whether to read date sharded tables or time partitioned tables when extracting usage from exported audit logs.",
     )
     _credentials_path: Optional[str] = PrivateAttr(None)
 
+    _cache_path: Optional[str] = PrivateAttr(None)
+
     upstream_lineage_in_report: bool = Field(
         default=False,
         description="Useful for debugging lineage information. Set to True to see the raw lineage created internally.",
     )
 
+    run_optimized_column_query: bool = Field(
+        hidden_from_docs=True,
+        default=False,
+        description="Run optimized column query to get column information. This is an experimental feature and may not work for all cases.",
+    )
+
+    file_backed_cache_size: int = Field(
+        hidden_from_docs=True,
+        default=200,
+        description="Maximum number of entries for the in-memory caches of FileBacked data structures.",
+    )
+
     def __init__(self, **data: Any):
         super().__init__(**data)
 
         if self.credential:
             self._credentials_path = self.credential.create_credential_temp_file()
             logger.debug(
                 f"Creating temporary credential file at {self._credentials_path}"
@@ -243,17 +271,10 @@
         if self.project_on_behalf:
             return f"bigquery://{self.project_on_behalf}"
         # When project_id is not set, we will attempt to detect the project ID
         # based on the credentials or environment variables.
         # See https://github.com/mxmzdlv/pybigquery#authentication.
         return "bigquery://"
 
-    @validator("platform")
-    def platform_is_always_bigquery(cls, v):
-        return "bigquery"
-
-    @validator("platform_instance")
-    def bigquery_doesnt_need_platform_instance(cls, v):
-        if v is not None:
-            raise ConfigurationError(
-                "BigQuery project ids are globally unique. You do not need to specify a platform instance."
-            )
+    platform_instance_not_supported_for_bigquery = pydantic_removed_field(
+        "platform_instance"
+    )
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/bigquery_v2/bigquery_report.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/bigquery_report.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,48 +1,54 @@
 import collections
 import dataclasses
+import logging
 from dataclasses import dataclass, field
 from datetime import datetime
 from typing import Counter, Dict, List, Optional
 
 import pydantic
 
 from datahub.ingestion.source.sql.sql_generic_profiler import ProfilingSqlReport
 from datahub.utilities.lossy_collections import LossyDict, LossyList
-from datahub.utilities.stats_collections import TopKDict
+from datahub.utilities.perf_timer import PerfTimer
+from datahub.utilities.stats_collections import TopKDict, int_top_k_dict
+
+logger: logging.Logger = logging.getLogger(__name__)
 
 
 @dataclass
 class BigQueryV2Report(ProfilingSqlReport):
     num_total_lineage_entries: TopKDict[str, int] = field(default_factory=TopKDict)
     num_skipped_lineage_entries_missing_data: TopKDict[str, int] = field(
-        default_factory=TopKDict
+        default_factory=int_top_k_dict
     )
     num_skipped_lineage_entries_not_allowed: TopKDict[str, int] = field(
-        default_factory=TopKDict
+        default_factory=int_top_k_dict
     )
     num_lineage_entries_sql_parser_failure: TopKDict[str, int] = field(
-        default_factory=TopKDict
-    )
-    num_lineage_entries_sql_parser_success: TopKDict[str, int] = field(
-        default_factory=TopKDict
+        default_factory=int_top_k_dict
     )
     num_skipped_lineage_entries_other: TopKDict[str, int] = field(
-        default_factory=TopKDict
+        default_factory=int_top_k_dict
+    )
+    num_total_log_entries: TopKDict[str, int] = field(default_factory=int_top_k_dict)
+    num_parsed_log_entries: TopKDict[str, int] = field(default_factory=int_top_k_dict)
+    num_lineage_log_parse_failures: TopKDict[str, int] = field(
+        default_factory=int_top_k_dict
     )
-    num_total_log_entries: TopKDict[str, int] = field(default_factory=TopKDict)
-    num_parsed_log_entries: TopKDict[str, int] = field(default_factory=TopKDict)
-    num_total_audit_entries: TopKDict[str, int] = field(default_factory=TopKDict)
-    num_parsed_audit_entries: TopKDict[str, int] = field(default_factory=TopKDict)
     bigquery_audit_metadata_datasets_missing: Optional[bool] = None
     lineage_failed_extraction: LossyList[str] = field(default_factory=LossyList)
     lineage_metadata_entries: TopKDict[str, int] = field(default_factory=TopKDict)
     lineage_mem_size: Dict[str, str] = field(default_factory=TopKDict)
     lineage_extraction_sec: Dict[str, float] = field(default_factory=TopKDict)
     usage_extraction_sec: Dict[str, float] = field(default_factory=TopKDict)
+    usage_error_count: Dict[str, int] = field(default_factory=int_top_k_dict)
+    num_usage_resources_dropped: int = 0
+    num_usage_operations_dropped: int = 0
+    operation_dropped: LossyList[str] = field(default_factory=LossyList)
     usage_failed_extraction: LossyList[str] = field(default_factory=LossyList)
     num_project_datasets_to_scan: Dict[str, int] = field(default_factory=TopKDict)
     metadata_extraction_sec: Dict[str, float] = field(default_factory=TopKDict)
     include_table_lineage: Optional[bool] = None
     use_date_sharded_audit_log_tables: Optional[bool] = None
     log_page_size: Optional[pydantic.PositiveInt] = None
     use_exported_bigquery_audit_metadata: Optional[bool] = None
@@ -54,21 +60,37 @@
     upstream_lineage: LossyDict = field(default_factory=LossyDict)
     partition_info: Dict[str, str] = field(default_factory=TopKDict)
     profile_table_selection_criteria: Dict[str, str] = field(default_factory=TopKDict)
     selected_profile_tables: Dict[str, List[str]] = field(default_factory=TopKDict)
     invalid_partition_ids: Dict[str, str] = field(default_factory=TopKDict)
     allow_pattern: Optional[str] = None
     deny_pattern: Optional[str] = None
-    num_usage_workunits_emitted: Optional[int] = None
-    query_log_delay: Optional[int] = None
-    total_query_log_entries: Optional[int] = None
-    num_read_events: Optional[int] = None
-    num_query_events: Optional[int] = None
-    num_filtered_read_events: Optional[int] = None
-    num_filtered_query_events: Optional[int] = None
-    num_operational_stats_workunits_emitted: Optional[int] = None
+    num_usage_workunits_emitted: int = 0
+    total_query_log_entries: int = 0
+    num_read_events: int = 0
+    num_query_events: int = 0
+    num_filtered_read_events: int = 0
+    num_filtered_query_events: int = 0
+    num_operational_stats_workunits_emitted: int = 0
     read_reasons_stat: Counter[str] = dataclasses.field(
         default_factory=collections.Counter
     )
     operation_types_stat: Counter[str] = dataclasses.field(
         default_factory=collections.Counter
     )
+    current_project_status: Optional[str] = None
+
+    timer: Optional[PerfTimer] = field(
+        default=None, init=False, repr=False, compare=False
+    )
+
+    def set_project_state(self, project: str, stage: str) -> None:
+        if self.timer:
+            logger.info(
+                f"Time spent in stage <{self.current_project_status}>: "
+                f"{self.timer.elapsed_seconds():.2f} seconds"
+            )
+        else:
+            self.timer = PerfTimer()
+
+        self.current_project_status = f"{project}: {stage} at {datetime.now()}"
+        self.timer.start()
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/bigquery_v2/bigquery_schema.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/bigquery_schema.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,56 +1,128 @@
 import logging
 from collections import defaultdict
 from dataclasses import dataclass, field
 from datetime import datetime, timezone
-from typing import Dict, List, Optional, cast
+from typing import Any, Dict, Iterator, List, Optional
 
 from google.cloud import bigquery
-from google.cloud.bigquery.table import RowIterator, TableListItem, TimePartitioning
+from google.cloud.bigquery.table import (
+    RowIterator,
+    TableListItem,
+    TimePartitioning,
+    TimePartitioningType,
+)
 
 from datahub.ingestion.source.bigquery_v2.bigquery_audit import BigqueryTableIdentifier
+from datahub.ingestion.source.bigquery_v2.bigquery_report import BigQueryV2Report
 from datahub.ingestion.source.sql.sql_generic import BaseColumn, BaseTable, BaseView
 
 logger: logging.Logger = logging.getLogger(__name__)
 
 
+class BigqueryTableType:
+    # See https://cloud.google.com/bigquery/docs/information-schema-tables#schema
+    BASE_TABLE = "BASE TABLE"
+    EXTERNAL = "EXTERNAL"
+    VIEW = "VIEW"
+    MATERIALIZED_VIEW = "MATERIALIZED VIEW"
+    CLONE = "CLONE"
+    SNAPSHOT = "SNAPSHOT"
+
+
 @dataclass
 class BigqueryColumn(BaseColumn):
     field_path: str
     is_partition_column: bool
 
 
+RANGE_PARTITION_NAME: str = "RANGE"
+
+
+@dataclass
+class PartitionInfo:
+    field: str
+    # Data type is optional as we not have it when we set it from TimePartitioning
+    column: Optional[BigqueryColumn] = None
+    type: str = TimePartitioningType.DAY
+    expiration_ms: Optional[int] = None
+    require_partition_filter: bool = False
+
+    # TimePartitioning field doesn't provide data_type so we have to add it afterwards
+    @classmethod
+    def from_time_partitioning(
+        cls, time_partitioning: TimePartitioning
+    ) -> "PartitionInfo":
+        return cls(
+            field=time_partitioning.field
+            if time_partitioning.field
+            else "_PARTITIONTIME",
+            type=time_partitioning.type_,
+            expiration_ms=time_partitioning.expiration_ms,
+            require_partition_filter=time_partitioning.require_partition_filter,
+        )
+
+    @classmethod
+    def from_range_partitioning(
+        cls, range_partitioning: Dict[str, Any]
+    ) -> Optional["PartitionInfo"]:
+        field: Optional[str] = range_partitioning.get("field")
+        if not field:
+            return None
+
+        return cls(
+            field=field,
+            type="RANGE",
+        )
+
+    @classmethod
+    def from_table_info(cls, table_info: TableListItem) -> Optional["PartitionInfo"]:
+        RANGE_PARTITIONING_KEY: str = "rangePartitioning"
+
+        if table_info.time_partitioning:
+            return PartitionInfo.from_time_partitioning(table_info.time_partitioning)
+        elif RANGE_PARTITIONING_KEY in table_info._properties:
+            return PartitionInfo.from_range_partitioning(
+                table_info._properties[RANGE_PARTITIONING_KEY]
+            )
+        else:
+            return None
+
+
 @dataclass
 class BigqueryTable(BaseTable):
     expires: Optional[datetime] = None
     clustering_fields: Optional[List[str]] = None
     labels: Optional[Dict[str, str]] = None
     num_partitions: Optional[int] = None
     max_partition_id: Optional[str] = None
     max_shard_id: Optional[str] = None
     active_billable_bytes: Optional[int] = None
     long_term_billable_bytes: Optional[int] = None
-    time_partitioning: Optional[TimePartitioning] = None
-    columns: List[BigqueryColumn] = field(default_factory=list)
+    partition_info: Optional[PartitionInfo] = None
+    columns_ignore_from_profiling: List[str] = field(default_factory=list)
 
 
 @dataclass
 class BigqueryView(BaseView):
     columns: List[BigqueryColumn] = field(default_factory=list)
+    materialized: bool = False
 
 
 @dataclass
 class BigqueryDataset:
     name: str
+    labels: Optional[Dict[str, str]] = None
     created: Optional[datetime] = None
     last_altered: Optional[datetime] = None
     location: Optional[str] = None
     comment: Optional[str] = None
     tables: List[BigqueryTable] = field(default_factory=list)
     views: List[BigqueryView] = field(default_factory=list)
+    columns: List[BigqueryColumn] = field(default_factory=list)
 
 
 @dataclass
 class BigqueryProject:
     id: str
     name: str
     datasets: List[BigqueryDataset] = field(default_factory=list)
@@ -74,15 +146,15 @@
   left join `{project_id}`.INFORMATION_SCHEMA.SCHEMATA_OPTIONS as o on o.schema_name = s.schema_name
   and o.option_name = "description"
 order by
   s.schema_name
 """
 
     # https://cloud.google.com/bigquery/docs/information-schema-table-storage?hl=en
-    tables_for_dataset = """
+    tables_for_dataset = f"""
 SELECT
   t.table_catalog as table_catalog,
   t.table_schema as table_schema,
   t.table_name as table_name,
   t.table_type as table_type,
   t.creation_time as created,
   ts.last_modified_time as last_altered,
@@ -95,111 +167,111 @@
   max_partition_id,
   active_billable_bytes,
   long_term_billable_bytes,
   REGEXP_EXTRACT(t.table_name, r".*_(\\d+)$") as table_suffix,
   REGEXP_REPLACE(t.table_name, r"_(\\d+)$", "") as table_base
 
 FROM
-  `{project_id}`.`{dataset_name}`.INFORMATION_SCHEMA.TABLES t
-  join `{project_id}`.`{dataset_name}`.__TABLES__ as ts on ts.table_id = t.TABLE_NAME
-  left join `{project_id}`.`{dataset_name}`.INFORMATION_SCHEMA.TABLE_OPTIONS as tos on t.table_schema = tos.table_schema
+  `{{project_id}}`.`{{dataset_name}}`.INFORMATION_SCHEMA.TABLES t
+  join `{{project_id}}`.`{{dataset_name}}`.__TABLES__ as ts on ts.table_id = t.TABLE_NAME
+  left join `{{project_id}}`.`{{dataset_name}}`.INFORMATION_SCHEMA.TABLE_OPTIONS as tos on t.table_schema = tos.table_schema
   and t.TABLE_NAME = tos.TABLE_NAME
   and tos.OPTION_NAME = "description"
   left join (
     select
         table_name,
         sum(case when partition_id not in ('__NULL__', '__UNPARTITIONED__', '__STREAMING_UNPARTITIONED__') then 1 else 0 END) as num_partitions,
         max(case when partition_id not in ('__NULL__', '__UNPARTITIONED__', '__STREAMING_UNPARTITIONED__') then partition_id else NULL END) as max_partition_id,
         sum(total_rows) as total_rows,
         sum(case when storage_tier = 'LONG_TERM' then total_billable_bytes else 0 end) as long_term_billable_bytes,
         sum(case when storage_tier = 'ACTIVE' then total_billable_bytes else 0 end) as active_billable_bytes,
     from
-        `{project_id}`.`{dataset_name}`.INFORMATION_SCHEMA.PARTITIONS
+        `{{project_id}}`.`{{dataset_name}}`.INFORMATION_SCHEMA.PARTITIONS
     group by
         table_name) as p on
     t.table_name = p.table_name
 WHERE
-  table_type in ('BASE TABLE', 'EXTERNAL')
-{table_filter}
+  table_type in ('{BigqueryTableType.BASE_TABLE}', '{BigqueryTableType.EXTERNAL}')
+{{table_filter}}
 order by
   table_schema ASC,
   table_base ASC,
   table_suffix DESC
 """
 
-    tables_for_dataset_without_partition_data = """
+    tables_for_dataset_without_partition_data = f"""
 SELECT
   t.table_catalog as table_catalog,
   t.table_schema as table_schema,
   t.table_name as table_name,
   t.table_type as table_type,
   t.creation_time as created,
   tos.OPTION_VALUE as comment,
   is_insertable_into,
   ddl,
   REGEXP_EXTRACT(t.table_name, r".*_(\\d+)$") as table_suffix,
   REGEXP_REPLACE(t.table_name, r"_(\\d+)$", "") as table_base
 
 FROM
-  `{project_id}`.`{dataset_name}`.INFORMATION_SCHEMA.TABLES t
-  left join `{project_id}`.`{dataset_name}`.INFORMATION_SCHEMA.TABLE_OPTIONS as tos on t.table_schema = tos.table_schema
+  `{{project_id}}`.`{{dataset_name}}`.INFORMATION_SCHEMA.TABLES t
+  left join `{{project_id}}`.`{{dataset_name}}`.INFORMATION_SCHEMA.TABLE_OPTIONS as tos on t.table_schema = tos.table_schema
   and t.TABLE_NAME = tos.TABLE_NAME
   and tos.OPTION_NAME = "description"
 WHERE
-  table_type in ('BASE TABLE', 'EXTERNAL')
-{table_filter}
+  table_type in ('{BigqueryTableType.BASE_TABLE}', '{BigqueryTableType.EXTERNAL}')
+{{table_filter}}
 order by
   table_schema ASC,
   table_base ASC,
   table_suffix DESC
 """
 
-    views_for_dataset: str = """
+    views_for_dataset: str = f"""
 SELECT
   t.table_catalog as table_catalog,
   t.table_schema as table_schema,
   t.table_name as table_name,
   t.table_type as table_type,
   t.creation_time as created,
   ts.last_modified_time as last_altered,
   tos.OPTION_VALUE as comment,
   is_insertable_into,
   ddl as view_definition,
   row_count,
   size_bytes
 FROM
-  `{project_id}`.`{dataset_name}`.INFORMATION_SCHEMA.TABLES t
-  join `{project_id}`.`{dataset_name}`.__TABLES__ as ts on ts.table_id = t.TABLE_NAME
-  left join `{project_id}`.`{dataset_name}`.INFORMATION_SCHEMA.TABLE_OPTIONS as tos on t.table_schema = tos.table_schema
+  `{{project_id}}`.`{{dataset_name}}`.INFORMATION_SCHEMA.TABLES t
+  join `{{project_id}}`.`{{dataset_name}}`.__TABLES__ as ts on ts.table_id = t.TABLE_NAME
+  left join `{{project_id}}`.`{{dataset_name}}`.INFORMATION_SCHEMA.TABLE_OPTIONS as tos on t.table_schema = tos.table_schema
   and t.TABLE_NAME = tos.TABLE_NAME
   and tos.OPTION_NAME = "description"
 WHERE
-  table_type in ('VIEW MATERIALIZED', 'VIEW')
+  table_type in ('{BigqueryTableType.VIEW}', '{BigqueryTableType.MATERIALIZED_VIEW}')
 order by
   table_schema ASC,
   table_name ASC
 """
 
-    views_for_dataset_without_data_read: str = """
+    views_for_dataset_without_data_read: str = f"""
 SELECT
   t.table_catalog as table_catalog,
   t.table_schema as table_schema,
   t.table_name as table_name,
   t.table_type as table_type,
   t.creation_time as created,
   tos.OPTION_VALUE as comment,
   is_insertable_into,
   ddl as view_definition
 FROM
-  `{project_id}`.`{dataset_name}`.INFORMATION_SCHEMA.TABLES t
-  left join `{project_id}`.`{dataset_name}`.INFORMATION_SCHEMA.TABLE_OPTIONS as tos on t.table_schema = tos.table_schema
+  `{{project_id}}`.`{{dataset_name}}`.INFORMATION_SCHEMA.TABLES t
+  left join `{{project_id}}`.`{{dataset_name}}`.INFORMATION_SCHEMA.TABLE_OPTIONS as tos on t.table_schema = tos.table_schema
   and t.TABLE_NAME = tos.TABLE_NAME
   and tos.OPTION_NAME = "description"
 WHERE
-  table_type in ('VIEW MATERIALIZED', 'VIEW')
+  table_type in ('{BigqueryTableType.VIEW}', '{BigqueryTableType.MATERIALIZED_VIEW}')
 order by
   table_schema ASC,
   table_name ASC
 """
 
     columns_for_dataset: str = """
 select
@@ -217,14 +289,42 @@
 from
   `{project_id}`.`{dataset_name}`.INFORMATION_SCHEMA.COLUMNS c
   join `{project_id}`.`{dataset_name}`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS as cfp on cfp.table_name = c.table_name
   and cfp.column_name = c.column_name
 ORDER BY
   table_catalog, table_schema, table_name, ordinal_position ASC, data_type DESC"""
 
+    optimized_columns_for_dataset: str = """
+select * from
+(select
+  c.table_catalog as table_catalog,
+  c.table_schema as table_schema,
+  c.table_name as table_name,
+  c.column_name as column_name,
+  c.ordinal_position as ordinal_position,
+  cfp.field_path as field_path,
+  c.is_nullable as is_nullable,
+  CASE WHEN CONTAINS_SUBSTR(field_path, ".") THEN NULL ELSE c.data_type END as data_type,
+  description as comment,
+  c.is_hidden as is_hidden,
+  c.is_partitioning_column as is_partitioning_column,
+  -- We count the columns to be able limit it later
+  row_number() over (partition by c.table_catalog, c.table_schema, c.table_name order by c.ordinal_position asc, c.data_type DESC) as column_num,
+  -- Getting the maximum shard for each table
+  row_number() over (partition by c.table_catalog, c.table_schema, ifnull(REGEXP_EXTRACT(c.table_name, r'(.*)_\\d{{8}}$'), c.table_name), cfp.field_path order by c.table_catalog, c.table_schema asc, c.table_name desc) as shard_num
+from
+  `{project_id}`.`{dataset_name}`.INFORMATION_SCHEMA.COLUMNS c
+  join `{project_id}`.`{dataset_name}`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS as cfp on cfp.table_name = c.table_name
+  and cfp.column_name = c.column_name
+  )
+-- We filter column limit + 1 to make sure we warn about the limit being reached but not reading too much data
+where column_num <= {column_limit} and shard_num = 1
+ORDER BY
+  table_catalog, table_schema, table_name, ordinal_position, column_num ASC, table_name, data_type DESC"""
+
     columns_for_table: str = """
 select
   c.table_catalog as table_catalog,
   c.table_schema as table_schema,
   c.table_name as table_name,
   c.column_name as column_name,
   c.ordinal_position as ordinal_position,
@@ -259,24 +359,27 @@
             BigqueryProject(id=p.project_id, name=p.friendly_name) for p in projects
         ]
 
     @staticmethod
     def get_datasets_for_project_id(
         conn: bigquery.Client, project_id: str, maxResults: Optional[int] = None
     ) -> List[BigqueryDataset]:
-        # FIXME: Due to a bug in BigQuery's type annotations, we need to cast here.
-        maxResults = cast(int, maxResults)
         datasets = conn.list_datasets(project_id, max_results=maxResults)
-
-        return [BigqueryDataset(name=d.dataset_id) for d in datasets]
+        return [BigqueryDataset(name=d.dataset_id, labels=d.labels) for d in datasets]
 
     @staticmethod
     def get_datasets_for_project_id_with_information_schema(
         conn: bigquery.Client, project_id: str
     ) -> List[BigqueryDataset]:
+        """
+        This method is not used as of now, due to below limitation.
+        Current query only fetches datasets in US region
+        We'll need Region wise separate queries to fetch all datasets
+        https://cloud.google.com/bigquery/docs/information-schema-datasets-schemata
+        """
         schemas = BigQueryDataDictionary.get_query_result(
             conn,
             BigqueryQuery.datasets_for_project_id.format(project_id=project_id),
         )
         return [
             BigqueryDataset(
                 name=s.table_schema,
@@ -291,15 +394,16 @@
     @staticmethod
     def get_tables_for_dataset(
         conn: bigquery.Client,
         project_id: str,
         dataset_name: str,
         tables: Dict[str, TableListItem],
         with_data_read_permission: bool = False,
-    ) -> List[BigqueryTable]:
+        report: Optional[BigQueryV2Report] = None,
+    ) -> Iterator[BigqueryTable]:
         filter: str = ", ".join(f"'{table}'" for table in tables.keys())
 
         if with_data_read_permission:
             # Tables are ordered by name and table suffix to make sure we always process the latest sharded table
             # and skip the others. Sharded tables are tables with suffix _20220102
             cur = BigQueryDataDictionary.get_query_result(
                 conn,
@@ -317,58 +421,77 @@
                 BigqueryQuery.tables_for_dataset_without_partition_data.format(
                     project_id=project_id,
                     dataset_name=dataset_name,
                     table_filter=f" and t.table_name in ({filter})" if filter else "",
                 ),
             )
 
-        # Some property we want to capture only available from the TableListItem we get from an earlier query of
-        # the list of tables.
-        return [
-            BigqueryTable(
-                name=table.table_name,
-                created=table.created,
-                last_altered=datetime.fromtimestamp(
-                    table.get("last_altered") / 1000, tz=timezone.utc
+        for table in cur:
+            try:
+                yield BigQueryDataDictionary._make_bigquery_table(
+                    table, tables.get(table.table_name)
+                )
+            except Exception as e:
+                table_name = f"{project_id}.{dataset_name}.{table.table_name}"
+                logger.warning(
+                    f"Error while processing table {table_name}",
+                    exc_info=True,
                 )
-                if table.get("last_altered") is not None
-                else table.created,
-                size_in_bytes=table.get("bytes"),
-                rows_count=table.get("row_count"),
-                comment=table.comment,
-                ddl=table.ddl,
-                expires=tables[table.table_name].expires if tables else None,
-                labels=tables[table.table_name].labels if tables else None,
-                time_partitioning=tables[table.table_name].time_partitioning
-                if tables
-                else None,
-                clustering_fields=tables[table.table_name].clustering_fields
-                if tables
-                else None,
-                max_partition_id=table.get("max_partition_id"),
-                max_shard_id=BigqueryTableIdentifier.get_table_and_shard(
-                    table.table_name
-                )[1]
-                if len(BigqueryTableIdentifier.get_table_and_shard(table.table_name))
-                == 2
-                else None,
-                num_partitions=table.get("num_partitions"),
-                active_billable_bytes=table.get("active_billable_bytes"),
-                long_term_billable_bytes=table.get("long_term_billable_bytes"),
+                if report:
+                    report.report_warning(
+                        "metadata-extraction",
+                        f"Failed to get table {table_name}: {e}",
+                    )
+
+    @staticmethod
+    def _make_bigquery_table(
+        table: bigquery.Row, table_basic: Optional[TableListItem]
+    ) -> BigqueryTable:
+        # Some properties we want to capture are only available from the TableListItem
+        # we get from an earlier query of the list of tables.
+        try:
+            expiration = table_basic.expires if table_basic else None
+        except OverflowError:
+            logger.info(f"Invalid expiration time for table {table.table_name}.")
+            expiration = None
+
+        _, shard = BigqueryTableIdentifier.get_table_and_shard(table.table_name)
+        return BigqueryTable(
+            name=table.table_name,
+            created=table.created,
+            last_altered=datetime.fromtimestamp(
+                table.get("last_altered") / 1000, tz=timezone.utc
             )
-            for table in cur
-        ]
+            if table.get("last_altered") is not None
+            else table.created,
+            size_in_bytes=table.get("bytes"),
+            rows_count=table.get("row_count"),
+            comment=table.comment,
+            ddl=table.ddl,
+            expires=expiration,
+            labels=table_basic.labels if table_basic else None,
+            partition_info=PartitionInfo.from_table_info(table_basic)
+            if table_basic
+            else None,
+            clustering_fields=table_basic.clustering_fields if table_basic else None,
+            max_partition_id=table.get("max_partition_id"),
+            max_shard_id=shard,
+            num_partitions=table.get("num_partitions"),
+            active_billable_bytes=table.get("active_billable_bytes"),
+            long_term_billable_bytes=table.get("long_term_billable_bytes"),
+        )
 
     @staticmethod
     def get_views_for_dataset(
         conn: bigquery.Client,
         project_id: str,
         dataset_name: str,
         has_data_read: bool,
-    ) -> List[BigqueryView]:
+        report: Optional[BigQueryV2Report] = None,
+    ) -> Iterator[BigqueryView]:
         if has_data_read:
             cur = BigQueryDataDictionary.get_query_result(
                 conn,
                 BigqueryQuery.views_for_dataset.format(
                     project_id=project_id, dataset_name=dataset_name
                 ),
             )
@@ -376,40 +499,64 @@
             cur = BigQueryDataDictionary.get_query_result(
                 conn,
                 BigqueryQuery.views_for_dataset_without_data_read.format(
                     project_id=project_id, dataset_name=dataset_name
                 ),
             )
 
-        return [
-            BigqueryView(
-                name=table.table_name,
-                created=table.created,
-                last_altered=table.last_altered
-                if "last_altered" in table
-                else table.created,
-                comment=table.comment,
-                view_definition=table.view_definition,
+        for table in cur:
+            try:
+                yield BigQueryDataDictionary._make_bigquery_view(table)
+            except Exception as e:
+                view_name = f"{project_id}.{dataset_name}.{table.table_name}"
+                logger.warning(
+                    f"Error while processing view {view_name}",
+                    exc_info=True,
+                )
+                if report:
+                    report.report_warning(
+                        "metadata-extraction",
+                        f"Failed to get view {view_name}: {e}",
+                    )
+
+    @staticmethod
+    def _make_bigquery_view(view: bigquery.Row) -> BigqueryView:
+        return BigqueryView(
+            name=view.table_name,
+            created=view.created,
+            last_altered=datetime.fromtimestamp(
+                view.get("last_altered") / 1000, tz=timezone.utc
             )
-            for table in cur
-        ]
+            if view.get("last_altered") is not None
+            else view.created,
+            comment=view.comment,
+            view_definition=view.view_definition,
+            materialized=view.table_type == BigqueryTableType.MATERIALIZED_VIEW,
+        )
 
     @staticmethod
     def get_columns_for_dataset(
         conn: bigquery.Client,
         project_id: str,
         dataset_name: str,
-        column_limit: Optional[int] = None,
+        column_limit: int,
+        run_optimized_column_query: bool = False,
     ) -> Optional[Dict[str, List[BigqueryColumn]]]:
         columns: Dict[str, List[BigqueryColumn]] = defaultdict(list)
         try:
             cur = BigQueryDataDictionary.get_query_result(
                 conn,
                 BigqueryQuery.columns_for_dataset.format(
                     project_id=project_id, dataset_name=dataset_name
+                )
+                if not run_optimized_column_query
+                else BigqueryQuery.optimized_columns_for_dataset.format(
+                    project_id=project_id,
+                    dataset_name=dataset_name,
+                    column_limit=column_limit,
                 ),
             )
         except Exception as e:
             logger.warning(f"Columns for dataset query failed with exception: {e}")
             # Error - Information schema query returned too much data.
             # Please repeat query with more selective predicates.
             return None
@@ -460,15 +607,14 @@
                 and column.table_name in columns
                 and len(columns[column.table_name]) >= column_limit
             ):
                 if last_seen_table != column.table_name:
                     logger.warning(
                         f"{table_identifier.project_id}.{table_identifier.dataset}.{column.table_name} contains more than {column_limit} columns, only processing {column_limit} columns"
                     )
-                    last_seen_table = column.table_name
             else:
                 columns.append(
                     BigqueryColumn(
                         name=column.column_name,
                         ordinal_position=column.ordinal_position,
                         is_nullable=column.is_nullable == "YES",
                         field_path=column.field_path,
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/bigquery_v2/common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/common.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/bigquery_v2/lineage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/lineage.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,50 +1,58 @@
 import collections
 import logging
 import textwrap
-from collections import defaultdict
-from typing import Dict, Iterable, List, Optional, Set, Tuple, Union
+from dataclasses import dataclass
+from datetime import datetime, timezone
+from typing import Any, Callable, Dict, Iterable, List, Optional, Set, Tuple, Union
 
 import humanfriendly
 from google.cloud.bigquery import Client as BigQueryClient
 from google.cloud.datacatalog import lineage_v1
 from google.cloud.logging_v2.client import Client as GCPLoggingClient
 from ratelimiter import RateLimiter
 
 from datahub.emitter import mce_builder
 from datahub.ingestion.source.bigquery_v2.bigquery_audit import (
     AuditLogEntry,
     BigQueryAuditMetadata,
     BigqueryTableIdentifier,
     BigQueryTableRef,
     QueryEvent,
+    ReadEvent,
 )
 from datahub.ingestion.source.bigquery_v2.bigquery_config import BigQueryV2Config
 from datahub.ingestion.source.bigquery_v2.bigquery_report import BigQueryV2Report
-from datahub.ingestion.source.bigquery_v2.bigquery_schema import (
-    BigqueryTable,
-    BigqueryView,
-)
+from datahub.ingestion.source.bigquery_v2.bigquery_schema import BigqueryView
 from datahub.ingestion.source.bigquery_v2.common import (
     BQ_DATE_SHARD_FORMAT,
     BQ_DATETIME_FORMAT,
     _make_gcp_logging_client,
+    get_bigquery_client,
 )
 from datahub.metadata.schema_classes import (
+    AuditStampClass,
     DatasetLineageTypeClass,
     UpstreamClass,
     UpstreamLineageClass,
 )
 from datahub.utilities import memory_footprint
 from datahub.utilities.bigquery_sql_parser import BigQuerySQLParser
 from datahub.utilities.perf_timer import PerfTimer
 
 logger: logging.Logger = logging.getLogger(__name__)
 
 
+@dataclass(order=True, eq=True, frozen=True)
+class LineageEdge:
+    table: str
+    auditStamp: datetime
+    type: str = DatasetLineageTypeClass.TRANSFORMED
+
+
 class BigqueryLineageExtractor:
     BQ_FILTER_RULE_TEMPLATE_V2 = """
 resource.type=("bigquery_project")
 AND
 (
     protoPayload.methodName=
         (
@@ -64,32 +72,29 @@
         protoPayload.metadata.jobChange.job.jobStats.queryStats.referencedTables !~ "projects/.*/datasets/_.*/tables/anon.*"
         AND
         protoPayload.metadata.jobChange.job.jobStats.queryStats.referencedTables !~ "projects/.*/datasets/.*/tables/INFORMATION_SCHEMA.*"
         AND
         protoPayload.metadata.jobChange.job.jobStats.queryStats.referencedTables !~ "projects/.*/datasets/.*/tables/__TABLES__"
         AND
         protoPayload.metadata.jobChange.job.jobConfig.queryConfig.destinationTable !~ "projects/.*/datasets/_.*/tables/anon.*"
-
     )
 
 )
 AND
 timestamp >= "{start_time}"
 AND
 timestamp < "{end_time}"
 """.strip()
 
     def __init__(self, config: BigQueryV2Config, report: BigQueryV2Report):
         self.config = config
         self.report = report
-        self.lineage_metadata: Dict[str, Set[str]] = defaultdict(set)
-        self.loaded_project_ids: List[str] = []
 
     def error(self, log: logging.Logger, key: str, reason: str) -> None:
-        self.report.report_failure(key, reason)
+        self.report.report_warning(key, reason)
         log.error(f"{key} => {reason}")
 
     @staticmethod
     def bigquery_audit_metadata_query_template(
         dataset: str, use_date_sharded_tables: bool, limit: Optional[int] = None
     ) -> str:
         """
@@ -100,109 +105,51 @@
         - do not contain errors (jobStatus.errorResults is none)
         :param dataset: the dataset to query against in the form of $PROJECT.$DATASET
         :param use_date_sharded_tables: whether to read from date sharded audit log tables or time partitioned audit log
                tables
         :param limit: set a limit for the maximum event to return. It is used for connection testing currently
         :return: a query template, when supplied start_time and end_time, can be used to query audit logs from BigQuery
         """
-        query: str
+        limit_text = f"limit {limit}" if limit else ""
+
+        shard_condition = ""
         if use_date_sharded_tables:
-            query = (
-                f"""
-            SELECT
-                timestamp,
-                logName,
-                insertId,
-                protopayload_auditlog AS protoPayload,
-                protopayload_auditlog.metadataJson AS metadata
-            FROM
-                `{dataset}.cloudaudit_googleapis_com_data_access_*`
-            """
-                + """
-            WHERE
-                _TABLE_SUFFIX BETWEEN "{start_time}" AND "{end_time}" AND
-            """
+            from_table = f"`{dataset}.cloudaudit_googleapis_com_data_access_*`"
+            shard_condition = (
+                """ AND _TABLE_SUFFIX BETWEEN "{start_date}" AND "{end_date}" """
             )
         else:
-            query = f"""
+            from_table = f"`{dataset}.cloudaudit_googleapis_com_data_access`"
+
+        query = f"""
             SELECT
                 timestamp,
                 logName,
                 insertId,
                 protopayload_auditlog AS protoPayload,
                 protopayload_auditlog.metadataJson AS metadata
             FROM
-                `{dataset}.cloudaudit_googleapis_com_data_access`
-            WHERE
-            """
-
-        audit_log_filter = """    timestamp >= "{start_time}"
-        AND timestamp < "{end_time}"
-        AND protopayload_auditlog.serviceName="bigquery.googleapis.com"
-        AND JSON_EXTRACT_SCALAR(protopayload_auditlog.metadataJson, "$.jobChange.job.jobStatus.jobState") = "DONE"
-        AND JSON_EXTRACT(protopayload_auditlog.metadataJson, "$.jobChange.job.jobStatus.errorResults") IS NULL
-        AND JSON_EXTRACT(protopayload_auditlog.metadataJson, "$.jobChange.job.jobConfig.queryConfig") IS NOT NULL
+                {from_table}
+            WHERE (
+                timestamp >= "{{start_time}}"
+                AND timestamp < "{{end_time}}"
+            )
+            {shard_condition}
+            AND protopayload_auditlog.serviceName="bigquery.googleapis.com"
+            AND JSON_EXTRACT_SCALAR(protopayload_auditlog.metadataJson, "$.jobChange.job.jobStatus.jobState") = "DONE"
+            AND JSON_EXTRACT(protopayload_auditlog.metadataJson, "$.jobChange.job.jobStatus.errorResults") IS NULL
+            AND JSON_EXTRACT(protopayload_auditlog.metadataJson, "$.jobChange.job.jobConfig.queryConfig") IS NOT NULL
+            {limit_text};
         """
 
-        if limit is not None:
-            audit_log_filter = audit_log_filter + f" LIMIT {limit}"
-        query = textwrap.dedent(query) + audit_log_filter + ";"
-
         return textwrap.dedent(query)
 
-    def compute_bigquery_lineage_via_gcp_logging(
-        self, project_id: str
-    ) -> Dict[str, Set[str]]:
-        logger.info(f"Populating lineage info via GCP audit logs for {project_id}")
-        try:
-            clients: GCPLoggingClient = _make_gcp_logging_client(project_id)
-
-            log_entries: Iterable[AuditLogEntry] = self._get_bigquery_log_entries(
-                clients
-            )
-            logger.info("Log Entries loaded")
-            parsed_entries: Iterable[QueryEvent] = self._parse_bigquery_log_entries(
-                log_entries
-            )
-            return self._create_lineage_map(parsed_entries)
-        except Exception as e:
-            self.error(
-                logger,
-                "lineage-gcp-logs",
-                f"Failed to get lineage gcp logging for {project_id}. The error message was {e}",
-            )
-            raise e
-
-    def compute_bigquery_lineage_via_exported_bigquery_audit_metadata(
-        self,
-    ) -> Dict[str, Set[str]]:
-        logger.info("Populating lineage info via exported GCP audit logs")
-        try:
-            # For exported logs we want to submit queries with the credentials project_id.
-            _client: BigQueryClient = BigQueryClient()
-            exported_bigquery_audit_metadata: Iterable[
-                BigQueryAuditMetadata
-            ] = self._get_exported_bigquery_audit_metadata(_client)
-            parsed_entries: Iterable[
-                QueryEvent
-            ] = self._parse_exported_bigquery_audit_metadata(
-                exported_bigquery_audit_metadata
-            )
-            return self._create_lineage_map(parsed_entries)
-        except Exception as e:
-            self.error(
-                logger,
-                "lineage-exported-gcp-audit-logs",
-                f"Error: {e}",
-            )
-            raise e
-
-    def compute_bigquery_lineage_via_catalog_lineage_api(
+    def lineage_via_catalog_lineage_api(
         self, project_id: str
-    ) -> Dict[str, Set[str]]:
+    ) -> Dict[str, Set[LineageEdge]]:
         """
         Uses Data Catalog API to request lineage metadata. Please take a look at the API documentation for more details.
 
         NOTE: It's necessary for you to enable the service API in your Google Cloud Project.
 
         Args:
             project_id(str): Google project id. Used to search for tables and datasets.
@@ -220,15 +167,15 @@
             )
 
         # Regions to search for BigQuery tables: projects/{project_id}/locations/{region}
         enabled_regions: List[str] = ["US", "EU"]
 
         try:
             lineage_client: lineage_v1.LineageClient = lineage_v1.LineageClient()
-            bigquery_client: BigQueryClient = BigQueryClient()
+            bigquery_client: BigQueryClient = get_bigquery_client(self.config)
             # Filtering datasets
             datasets = list(bigquery_client.list_datasets(project_id))
             project_tables = []
             for dataset in datasets:
                 # Enables only tables where type is TABLE (removes VIEWS)
                 project_tables.extend(
                     [
@@ -244,15 +191,16 @@
                     lambda table: "{}.{}.{}".format(
                         table.project, table.dataset_id, table.table_id
                     ),
                     project_tables,
                 )
             )
 
-            lineage_map: Dict[str, Set[str]] = {}
+            lineage_map: Dict[str, Set[LineageEdge]] = {}
+            curr_date = datetime.now()
             for table in project_tables:
                 logger.info("Creating lineage map for table %s", table)
                 upstreams = []
                 downstream_table = lineage_v1.EntityReference()
                 # fully_qualified_name in format: "bigquery:<project_id>.<dataset_id>.<table_id>"
                 downstream_table.fully_qualified_name = f"bigquery:{table}"
                 # Searches in different regions
@@ -278,36 +226,63 @@
                     )
                 )
 
                 # Only builds lineage map when the table has upstreams
                 if upstreams:
                     lineage_map[destination_table_str] = set(
                         [
-                            str(
-                                BigQueryTableRef(
-                                    table_identifier=BigqueryTableIdentifier.from_string_name(
-                                        source_table
+                            LineageEdge(
+                                table=str(
+                                    BigQueryTableRef(
+                                        table_identifier=BigqueryTableIdentifier.from_string_name(
+                                            source_table
+                                        )
                                     )
-                                )
+                                ),
+                                auditStamp=curr_date,
                             )
                             for source_table in upstreams
                         ]
                     )
             return lineage_map
         except Exception as e:
             self.error(
                 logger,
                 "lineage-exported-catalog-lineage-api",
                 f"Error: {e}",
             )
             raise e
 
+    def _get_parsed_audit_log_events(self, project_id: str) -> Iterable[QueryEvent]:
+        parse_fn: Callable[[Any], Optional[Union[ReadEvent, QueryEvent]]]
+        if self.config.use_exported_bigquery_audit_metadata:
+            logger.info("Populating lineage info via exported GCP audit logs")
+            bq_client = get_bigquery_client(self.config)
+            entries = self._get_exported_bigquery_audit_metadata(bq_client)
+            parse_fn = self._parse_exported_bigquery_audit_metadata
+        else:
+            logger.info("Populating lineage info via exported GCP audit logs")
+            logging_client = _make_gcp_logging_client(project_id)
+            entries = self._get_bigquery_log_entries(logging_client)
+            parse_fn = self._parse_bigquery_log_entries
+
+        for entry in entries:
+            self.report.num_total_log_entries[project_id] += 1
+            try:
+                event = parse_fn(entry)
+                if event:
+                    self.report.num_parsed_log_entries[project_id] += 1
+                    yield event
+            except Exception as e:
+                logger.warning(f"Unable to parse log entry `{entry}`: {e}")
+                self.report.num_lineage_log_parse_failures[project_id] += 1
+
     def _get_bigquery_log_entries(
         self, client: GCPLoggingClient, limit: Optional[int] = None
-    ) -> Union[Iterable[AuditLogEntry], Iterable[BigQueryAuditMetadata]]:
+    ) -> Union[Iterable[AuditLogEntry]]:
         self.report.num_total_log_entries[client.project] = 0
         # Add a buffer to start and end time to account for delays in logging events.
         start_time = (self.config.start_time - self.config.max_query_duration).strftime(
             BQ_DATETIME_FORMAT
         )
         self.report.log_entry_start_time = start_time
 
@@ -358,47 +333,40 @@
         if self.config.bigquery_audit_metadata_datasets is None:
             self.error(
                 logger, "audit-metadata", "bigquery_audit_metadata_datasets not set"
             )
             self.report.bigquery_audit_metadata_datasets_missing = True
             return
 
-        start_time: str = (
-            self.config.start_time - self.config.max_query_duration
-        ).strftime(
-            BQ_DATE_SHARD_FORMAT
-            if self.config.use_date_sharded_audit_log_tables
-            else BQ_DATETIME_FORMAT
-        )
+        corrected_start_time = self.config.start_time - self.config.max_query_duration
+        start_time = corrected_start_time.strftime(BQ_DATETIME_FORMAT)
+        start_date = corrected_start_time.strftime(BQ_DATE_SHARD_FORMAT)
         self.report.audit_start_time = start_time
 
-        end_time: str = (
-            self.config.end_time + self.config.max_query_duration
-        ).strftime(
-            BQ_DATE_SHARD_FORMAT
-            if self.config.use_date_sharded_audit_log_tables
-            else BQ_DATETIME_FORMAT
-        )
+        corrected_end_time = self.config.end_time + self.config.max_query_duration
+        end_time = corrected_end_time.strftime(BQ_DATETIME_FORMAT)
+        end_date = corrected_end_time.strftime(BQ_DATE_SHARD_FORMAT)
         self.report.audit_end_time = end_time
 
         for dataset in self.config.bigquery_audit_metadata_datasets:
             logger.info(
                 f"Start loading log entries from BigQueryAuditMetadata in {dataset}"
             )
 
             query: str = self.bigquery_audit_metadata_query_template(
                 dataset=dataset,
                 use_date_sharded_tables=self.config.use_date_sharded_audit_log_tables,
+                limit=limit,
             ).format(
                 start_time=start_time,
                 end_time=end_time,
+                start_date=start_date,
+                end_date=end_date,
             )
 
-            if limit is not None:
-                query = query + f" LIMIT {limit}"
             query_job = bigquery_client.query(query)
 
             logger.info(
                 f"Finished loading log entries from BigQueryAuditMetadata in {dataset}"
             )
 
             if self.config.rate_limit:
@@ -407,133 +375,109 @@
             else:
                 yield from query_job
 
     # Currently we only parse JobCompleted events but in future we would want to parse other
     # events to also create field level lineage.
     def _parse_bigquery_log_entries(
         self,
-        entries: Union[Iterable[AuditLogEntry], Iterable[BigQueryAuditMetadata]],
-    ) -> Iterable[QueryEvent]:
-        for entry in entries:
-            event: Optional[QueryEvent] = None
+        entry: AuditLogEntry,
+    ) -> Optional[QueryEvent]:
+        event: Optional[QueryEvent] = None
 
-            missing_entry = QueryEvent.get_missing_key_entry(entry=entry)
-            if missing_entry is None:
-                event = QueryEvent.from_entry(
-                    entry,
-                    debug_include_full_payloads=self.config.debug_include_full_payloads,
-                )
+        missing_entry = QueryEvent.get_missing_key_entry(entry=entry)
+        if missing_entry is None:
+            event = QueryEvent.from_entry(
+                entry,
+                debug_include_full_payloads=self.config.debug_include_full_payloads,
+            )
 
-            missing_entry_v2 = QueryEvent.get_missing_key_entry_v2(entry=entry)
-            if event is None and missing_entry_v2 is None:
-                event = QueryEvent.from_entry_v2(
-                    entry, self.config.debug_include_full_payloads
-                )
+        missing_entry_v2 = QueryEvent.get_missing_key_entry_v2(entry=entry)
+        if event is None and missing_entry_v2 is None:
+            event = QueryEvent.from_entry_v2(
+                entry, self.config.debug_include_full_payloads
+            )
 
-            if event is None:
-                self.error(
-                    logger,
-                    f"{entry.log_name}-{entry.insert_id}",
-                    f"Unable to parse log missing {missing_entry}, missing v2 {missing_entry_v2} for {entry}",
-                )
-            else:
-                self.report.num_parsed_log_entries[event.project_id] = (
-                    self.report.num_parsed_log_entries.get(event.project_id, 0) + 1
-                )
-                yield event
+        if event is None:
+            logger.warning(
+                f"Unable to parse log missing {missing_entry}, missing v2 {missing_entry_v2} for {entry}",
+            )
+            return None
+        else:
+            return event
 
     def _parse_exported_bigquery_audit_metadata(
-        self, audit_metadata_rows: Iterable[BigQueryAuditMetadata]
-    ) -> Iterable[QueryEvent]:
-        for audit_metadata in audit_metadata_rows:
-            event: Optional[QueryEvent] = None
-
-            missing_exported_audit = (
-                QueryEvent.get_missing_key_exported_bigquery_audit_metadata(
-                    audit_metadata
-                )
-            )
+        self, audit_metadata: BigQueryAuditMetadata
+    ) -> Optional[QueryEvent]:
+        event: Optional[QueryEvent] = None
 
-            if missing_exported_audit is None:
-                event = QueryEvent.from_exported_bigquery_audit_metadata(
-                    audit_metadata, self.config.debug_include_full_payloads
-                )
+        missing_exported_audit = (
+            QueryEvent.get_missing_key_exported_bigquery_audit_metadata(audit_metadata)
+        )
 
-            if event is None:
-                self.error(
-                    logger,
-                    f"{audit_metadata['logName']}-{audit_metadata['insertId']}",
-                    f"Unable to parse audit metadata missing {missing_exported_audit} for {audit_metadata}",
-                )
-            else:
-                self.report.num_parsed_audit_entries[event.project_id] = (
-                    self.report.num_parsed_audit_entries.get(event.project_id, 0) + 1
-                )
-                self.report.num_total_audit_entries[event.project_id] = (
-                    self.report.num_total_audit_entries.get(event.project_id, 0) + 1
-                )
-                yield event
+        if missing_exported_audit is None:
+            event = QueryEvent.from_exported_bigquery_audit_metadata(
+                audit_metadata, self.config.debug_include_full_payloads
+            )
+
+        if event is None:
+            logger.warning(
+                f"Unable to parse audit metadata missing {missing_exported_audit} for {audit_metadata}",
+            )
+            return None
+        else:
+            return event
 
-    def _create_lineage_map(self, entries: Iterable[QueryEvent]) -> Dict[str, Set[str]]:
+    def _create_lineage_map(
+        self, entries: Iterable[QueryEvent]
+    ) -> Dict[str, Set[LineageEdge]]:
         logger.info("Entering create lineage map function")
-        lineage_map: Dict[str, Set[str]] = collections.defaultdict(set)
+        lineage_map: Dict[str, Set[LineageEdge]] = collections.defaultdict(set)
         for e in entries:
             self.report.num_total_lineage_entries[e.project_id] = (
                 self.report.num_total_lineage_entries.get(e.project_id, 0) + 1
             )
 
             if e.destinationTable is None or not (
                 e.referencedTables or e.referencedViews
             ):
-                self.report.num_skipped_lineage_entries_missing_data[e.project_id] = (
-                    self.report.num_skipped_lineage_entries_missing_data.get(
-                        e.project_id, 0
-                    )
-                    + 1
-                )
-                continue
-            # Skip if schema/table pattern don't allow the destination table
-            try:
-                destination_table = e.destinationTable.get_sanitized_table_ref()
-            except Exception:
-                self.report.num_skipped_lineage_entries_missing_data[e.project_id] = (
-                    self.report.num_skipped_lineage_entries_missing_data.get(
-                        e.project_id, 0
-                    )
-                    + 1
-                )
+                self.report.num_skipped_lineage_entries_missing_data[e.project_id] += 1
                 continue
 
-            destination_table_str = str(
-                BigQueryTableRef(table_identifier=destination_table.table_identifier)
-            )
-
             if not self.config.dataset_pattern.allowed(
-                destination_table.table_identifier.dataset
+                e.destinationTable.table_identifier.dataset
             ) or not self.config.table_pattern.allowed(
-                destination_table.table_identifier.get_table_name()
+                e.destinationTable.table_identifier.get_table_name()
             ):
-                self.report.num_skipped_lineage_entries_not_allowed[e.project_id] = (
-                    self.report.num_skipped_lineage_entries_not_allowed.get(
-                        e.project_id, 0
-                    )
-                    + 1
-                )
+                self.report.num_skipped_lineage_entries_not_allowed[e.project_id] += 1
                 continue
+
+            destination_table_str = str(e.destinationTable)
             has_table = False
             for ref_table in e.referencedTables:
-                ref_table_str = str(ref_table.get_sanitized_table_ref())
-                if ref_table_str != destination_table_str:
-                    lineage_map[destination_table_str].add(ref_table_str)
+                if str(ref_table) != destination_table_str:
+                    lineage_map[destination_table_str].add(
+                        LineageEdge(
+                            table=str(ref_table),
+                            auditStamp=e.end_time
+                            if e.end_time
+                            else datetime.now(tz=timezone.utc),
+                        )
+                    )
                     has_table = True
             has_view = False
             for ref_view in e.referencedViews:
-                ref_view_str = str(ref_view.get_sanitized_table_ref())
-                if ref_view_str != destination_table_str:
-                    lineage_map[destination_table_str].add(ref_view_str)
+                if str(ref_view) != destination_table_str:
+                    lineage_map[destination_table_str].add(
+                        LineageEdge(
+                            table=str(ref_view),
+                            auditStamp=e.end_time
+                            if e.end_time
+                            else datetime.now(tz=timezone.utc),
+                        )
+                    )
                     has_view = True
             if self.config.lineage_use_sql_parser and has_table and has_view:
                 # If there is a view being referenced then bigquery sends both the view as well as underlying table
                 # in the references. There is no distinction between direct/base objects accessed. So doing sql parsing
                 # to ensure we only use direct objects accessed for lineage
                 try:
                     parser = BigQuerySQLParser(
@@ -544,214 +488,188 @@
                     referenced_objs = set(
                         map(lambda x: x.split(".")[-1], parser.get_tables())
                     )
                 except Exception as ex:
                     logger.debug(
                         f"Sql Parser failed on query: {e.query}. It won't cause any issue except table/view lineage can't be detected reliably. The error was {ex}."
                     )
-                    self.report.num_lineage_entries_sql_parser_failure[e.project_id] = (
-                        self.report.num_lineage_entries_sql_parser_failure.get(
-                            e.project_id, 0
-                        )
-                        + 1
-                    )
+                    self.report.num_lineage_entries_sql_parser_failure[
+                        e.project_id
+                    ] += 1
                     continue
-                curr_lineage_str = lineage_map[destination_table_str]
-                new_lineage_str = set()
-                for lineage_str in curr_lineage_str:
-                    name = lineage_str.split("/")[-1]
+                curr_lineage = lineage_map[destination_table_str]
+                new_lineage = set()
+                for lineage in curr_lineage:
+                    name = lineage.table.split("/")[-1]
                     if name in referenced_objs:
-                        new_lineage_str.add(lineage_str)
-                lineage_map[destination_table_str] = new_lineage_str
+                        new_lineage.add(lineage)
+                lineage_map[destination_table_str] = new_lineage
             if not (has_table or has_view):
-                self.report.num_skipped_lineage_entries_other[e.project_id] = (
-                    self.report.num_skipped_lineage_entries_other.get(e.project_id, 0)
-                    + 1
-                )
+                self.report.num_skipped_lineage_entries_other[e.project_id] += 1
 
         logger.info("Exiting create lineage map function")
         return lineage_map
 
     def parse_view_lineage(
         self, project: str, dataset: str, view: BigqueryView
-    ) -> List[BigqueryTableIdentifier]:
+    ) -> Optional[List[BigqueryTableIdentifier]]:
+        if not view.view_definition:
+            return None
+
         parsed_tables = set()
-        if view.view_definition:
-            try:
-                parser = BigQuerySQLParser(
-                    view.view_definition,
-                    self.config.sql_parser_use_external_process,
-                    use_raw_names=self.config.lineage_sql_parser_use_raw_names,
+        try:
+            parser = BigQuerySQLParser(
+                view.view_definition,
+                self.config.sql_parser_use_external_process,
+                use_raw_names=self.config.lineage_sql_parser_use_raw_names,
+            )
+            tables = parser.get_tables()
+        except Exception as ex:
+            logger.debug(
+                f"View {view.name} definination sql parsing failed on query: {view.view_definition}. "
+                f"Edge from physical table to view won't be added. The error was {ex}."
+            )
+            return None
+
+        for table in tables:
+            parts = table.split(".")
+            if len(parts) == 1:
+                parsed_tables.add(
+                    BigqueryTableIdentifier(
+                        project_id=project, dataset=dataset, table=table
+                    )
+                )
+            elif len(parts) == 2:
+                parsed_tables.add(
+                    BigqueryTableIdentifier(
+                        project_id=project, dataset=parts[0], table=parts[1]
+                    )
+                )
+            elif len(parts) == 3:
+                parsed_tables.add(
+                    BigqueryTableIdentifier(
+                        project_id=parts[0], dataset=parts[1], table=parts[2]
+                    )
                 )
-                tables = parser.get_tables()
-            except Exception as ex:
-                logger.debug(
-                    f"View {view.name} definination sql parsing failed on query: {view.view_definition}. Edge from physical table to view won't be added. The error was {ex}."
+            else:
+                logger.warning(
+                    f"Invalid table identifier {table} when parsing view lineage for view {view.name}"
                 )
-                return []
-
-            for table in tables:
-                parts = table.split(".")
-                if len(parts) == 1:
-                    parsed_tables.add(
-                        BigqueryTableIdentifier(
-                            project_id=project, dataset=dataset, table=table
-                        )
-                    )
-                elif len(parts) == 2:
-                    parsed_tables.add(
-                        BigqueryTableIdentifier(
-                            project_id=project, dataset=parts[0], table=parts[1]
-                        )
-                    )
-                elif len(parts) == 3:
-                    parsed_tables.add(
-                        BigqueryTableIdentifier(
-                            project_id=parts[0], dataset=parts[1], table=parts[2]
-                        )
-                    )
-                else:
-                    continue
 
-            return list(parsed_tables)
-        else:
-            return []
+        return list(parsed_tables)
 
-    def _compute_bigquery_lineage(self, project_id: str) -> Dict[str, Set[str]]:
-        lineage_extractor: BigqueryLineageExtractor = BigqueryLineageExtractor(
-            config=self.config, report=self.report
-        )
-        lineage_metadata: Dict[str, Set[str]]
+    def _compute_bigquery_lineage(self, project_id: str) -> Dict[str, Set[LineageEdge]]:
+        lineage_metadata: Dict[str, Set[LineageEdge]]
         try:
             if self.config.extract_lineage_from_catalog and self.config.include_tables:
-                lineage_metadata = (
-                    lineage_extractor.compute_bigquery_lineage_via_catalog_lineage_api(
-                        project_id
-                    )
-                )
+                lineage_metadata = self.lineage_via_catalog_lineage_api(project_id)
             else:
-                if self.config.use_exported_bigquery_audit_metadata:
-                    # Exported bigquery_audit_metadata should contain every projects' audit metada
-                    if self.loaded_project_ids:
-                        return {}
-                    lineage_metadata = (
-                        lineage_extractor.compute_bigquery_lineage_via_exported_bigquery_audit_metadata()
-                    )
-                else:
-                    lineage_metadata = (
-                        lineage_extractor.compute_bigquery_lineage_via_gcp_logging(
-                            project_id
-                        )
-                    )
+                events = self._get_parsed_audit_log_events(project_id)
+                lineage_metadata = self._create_lineage_map(events)
         except Exception as e:
             if project_id:
                 self.report.lineage_failed_extraction.append(project_id)
-            logger.error(
-                f"Unable to extract lineage for project {project_id} due to error {e}"
+            self.error(
+                logger,
+                "lineage",
+                f"{project_id}: {e}",
             )
             lineage_metadata = {}
 
-        if lineage_metadata is None:
-            lineage_metadata = {}
-
         self.report.lineage_mem_size[project_id] = humanfriendly.format_size(
             memory_footprint.total_size(lineage_metadata)
         )
         self.report.lineage_metadata_entries[project_id] = len(lineage_metadata)
         logger.info(f"Built lineage map containing {len(lineage_metadata)} entries.")
         logger.debug(f"lineage metadata is {lineage_metadata}")
         return lineage_metadata
 
     def get_upstream_tables(
-        self, bq_table: str, tables_seen: List[str] = []
-    ) -> Set[BigQueryTableRef]:
-        upstreams: Set[BigQueryTableRef] = set()
-        for ref_table in self.lineage_metadata[str(bq_table)]:
+        self,
+        bq_table: BigQueryTableRef,
+        lineage_metadata: Dict[str, Set[LineageEdge]],
+        tables_seen: List[str],
+    ) -> Set[LineageEdge]:
+        upstreams: Set[LineageEdge] = set()
+        for ref_lineage in lineage_metadata[str(bq_table)]:
+            ref_table = ref_lineage.table
             upstream_table = BigQueryTableRef.from_string_name(ref_table)
             if upstream_table.is_temporary_table(
                 [self.config.temp_table_dataset_prefix]
             ):
                 # making sure we don't process a table twice and not get into a recursive loop
                 if ref_table in tables_seen:
                     logger.debug(
                         f"Skipping table {ref_table} because it was seen already"
                     )
                     continue
                 tables_seen.append(ref_table)
-                if ref_table in self.lineage_metadata:
+                if ref_table in lineage_metadata:
                     upstreams = upstreams.union(
-                        self.get_upstream_tables(ref_table, tables_seen=tables_seen)
+                        self.get_upstream_tables(
+                            upstream_table,
+                            lineage_metadata=lineage_metadata,
+                            tables_seen=tables_seen,
+                        )
                     )
             else:
-                upstreams.add(upstream_table)
+                upstreams.add(ref_lineage)
 
         return upstreams
 
-    def get_upstream_lineage_info(
+    def calculate_lineage_for_project(
+        self, project_id: str
+    ) -> Dict[str, Set[LineageEdge]]:
+        with PerfTimer() as timer:
+            lineage = self._compute_bigquery_lineage(project_id)
+
+            self.report.lineage_extraction_sec[project_id] = round(
+                timer.elapsed_seconds(), 2
+            )
+
+        return lineage
+
+    def get_lineage_for_table(
         self,
-        project_id: str,
-        dataset_name: str,
-        table: Union[BigqueryTable, BigqueryView],
+        bq_table: BigQueryTableRef,
+        lineage_metadata: Dict[str, Set[LineageEdge]],
         platform: str,
     ) -> Optional[Tuple[UpstreamLineageClass, Dict[str, str]]]:
-        table_identifier = BigqueryTableIdentifier(project_id, dataset_name, table.name)
+        upstream_list: List[UpstreamClass] = []
+        # Sorting the list of upstream lineage events in order to avoid creating multiple aspects in backend
+        # even if the lineage is same but the order is different.
+        for upstream in sorted(
+            self.get_upstream_tables(bq_table, lineage_metadata, tables_seen=[])
+        ):
+            upstream_table = BigQueryTableRef.from_string_name(upstream.table)
+            upstream_table_class = UpstreamClass(
+                dataset=mce_builder.make_dataset_urn_with_platform_instance(
+                    platform,
+                    upstream_table.table_identifier.get_table_name(),
+                    self.config.platform_instance,
+                    self.config.env,
+                ),
+                type=upstream.type,
+                auditStamp=AuditStampClass(
+                    actor="urn:li:corpuser:datahub",
+                    time=int(upstream.auditStamp.timestamp() * 1000),
+                ),
+            )
+            if self.config.upstream_lineage_in_report:
+                current_lineage_map: Set = self.report.upstream_lineage.get(
+                    str(bq_table), set()
+                )
+                current_lineage_map.add(str(upstream_table))
+                self.report.upstream_lineage[str(bq_table)] = current_lineage_map
+            upstream_list.append(upstream_table_class)
+
+        if upstream_list:
+            upstream_lineage = UpstreamLineageClass(upstreams=upstream_list)
+            return upstream_lineage, {}
 
-        if table_identifier.project_id not in self.loaded_project_ids:
-            with PerfTimer() as timer:
-                self.lineage_metadata.update(
-                    self._compute_bigquery_lineage(table_identifier.project_id)
-                )
-                self.report.lineage_extraction_sec[table_identifier.project_id] = round(
-                    timer.elapsed_seconds(), 2
-                )
-                self.loaded_project_ids.append(table_identifier.project_id)
-
-        if self.config.lineage_parse_view_ddl and isinstance(table, BigqueryView):
-            for table_id in self.parse_view_lineage(project_id, dataset_name, table):
-                if table_identifier.get_table_name() in self.lineage_metadata:
-                    self.lineage_metadata[
-                        str(
-                            BigQueryTableRef(table_identifier).get_sanitized_table_ref()
-                        )
-                    ].add(str(BigQueryTableRef(table_id).get_sanitized_table_ref()))
-                else:
-                    self.lineage_metadata[
-                        str(
-                            BigQueryTableRef(table_identifier).get_sanitized_table_ref()
-                        )
-                    ] = {str(BigQueryTableRef(table_id).get_sanitized_table_ref())}
-
-        bq_table = BigQueryTableRef.from_bigquery_table(table_identifier)
-        if str(bq_table) in self.lineage_metadata:
-            upstream_list: List[UpstreamClass] = []
-            # Sorting the list of upstream lineage events in order to avoid creating multiple aspects in backend
-            # even if the lineage is same but the order is different.
-            for upstream_table in sorted(
-                self.get_upstream_tables(str(bq_table), tables_seen=[])
-            ):
-                upstream_table_class = UpstreamClass(
-                    mce_builder.make_dataset_urn_with_platform_instance(
-                        platform,
-                        f"{upstream_table.table_identifier.project_id}.{upstream_table.table_identifier.dataset}.{upstream_table.table_identifier.table}",
-                        self.config.platform_instance,
-                        self.config.env,
-                    ),
-                    DatasetLineageTypeClass.TRANSFORMED,
-                )
-                if self.config.upstream_lineage_in_report:
-                    current_lineage_map: Set = self.report.upstream_lineage.get(
-                        str(bq_table), set()
-                    )
-                    current_lineage_map.add(str(upstream_table))
-                    self.report.upstream_lineage[str(bq_table)] = current_lineage_map
-                upstream_list.append(upstream_table_class)
-
-            if upstream_list:
-                upstream_lineage = UpstreamLineageClass(upstreams=upstream_list)
-                return upstream_lineage, {}
         return None
 
     def test_capability(self, project_id: str) -> None:
         if self.config.use_exported_bigquery_audit_metadata:
             bigquery_client: BigQueryClient = BigQueryClient(project=project_id)
             entries = self._get_exported_bigquery_audit_metadata(
                 bigquery_client=bigquery_client, limit=1
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/bigquery_v2/profiler.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/profiler.py`

 * *Files 4% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 from datahub.emitter.mce_builder import make_dataset_urn_with_platform_instance
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.ingestion.api.workunit import MetadataWorkUnit
 from datahub.ingestion.source.bigquery_v2.bigquery_audit import BigqueryTableIdentifier
 from datahub.ingestion.source.bigquery_v2.bigquery_config import BigQueryV2Config
 from datahub.ingestion.source.bigquery_v2.bigquery_report import BigQueryV2Report
 from datahub.ingestion.source.bigquery_v2.bigquery_schema import (
-    BigqueryColumn,
+    RANGE_PARTITION_NAME,
     BigqueryTable,
 )
 from datahub.ingestion.source.ge_data_profiler import GEProfilerRequest
 from datahub.ingestion.source.sql.sql_generic_profiler import (
     GenericProfiler,
     TableProfilerRequest,
 )
@@ -75,36 +75,33 @@
         return partition_datetime, upper_bound_partition_datetime
 
     def generate_partition_profiler_query(
         self,
         project: str,
         schema: str,
         table: BigqueryTable,
-        partition_datetime: Optional[datetime],
+        partition_datetime: Optional[datetime] = None,
     ) -> Tuple[Optional[str], Optional[str]]:
         """
         Method returns partition id if table is partitioned or sharded and generate custom partition query for
         partitioned table.
         See more about partitioned tables at https://cloud.google.com/bigquery/docs/partitioned-tables
         """
         logger.debug(
             f"generate partition profiler query for project: {project} schema: {schema} and table {table.name}, partition_datetime: {partition_datetime}"
         )
         partition = table.max_partition_id
-        if partition:
+        if table.partition_info and partition:
             partition_where_clause: str
 
-            if not table.time_partitioning:
-                partition_column: Optional[BigqueryColumn] = None
-                for column in table.columns:
-                    if column.is_partition_column:
-                        partition_column = column
-                        break
-                if partition_column:
-                    partition_where_clause = f"{partition_column.name} >= {partition}"
+            if table.partition_info.type == RANGE_PARTITION_NAME:
+                if table.partition_info and table.partition_info.column:
+                    partition_where_clause = (
+                        f"{table.partition_info.column.name} >= {partition}"
+                    )
                 else:
                     logger.warning(
                         f"Partitioned table {table.name} without partiton column"
                     )
                     return None, None
             else:
                 logger.debug(
@@ -122,25 +119,27 @@
                         f"Unable to get partition range for partition id: {partition} it failed with exception {e}"
                     )
                     self.report.invalid_partition_ids[
                         f"{schema}.{table.name}"
                     ] = partition
                     return None, None
 
-                # ingestion time partitoned tables partition column is not in the schema, so we default to TIMESTAMP type
-                partition_column_type: str = "TIMESTAMP"
-                for c in table.columns:
-                    if c.is_partition_column:
-                        partition_column_type = c.data_type
-
-                if table.time_partitioning.type_ in ("HOUR", "DAY", "MONTH", "YEAR"):
-                    partition_where_clause = f"{partition_column_type}(`{table.time_partitioning.field}`) BETWEEN {partition_column_type}('{partition_datetime}') AND {partition_column_type}('{upper_bound_partition_datetime}')"
+                partition_data_type: str = "TIMESTAMP"
+                # Ingestion time partitioned tables has a pseudo column called _PARTITIONTIME
+                # See more about this at
+                # https://cloud.google.com/bigquery/docs/partitioned-tables#ingestion_time
+                partition_column_name = "_PARTITIONTIME"
+                if table.partition_info.column:
+                    partition_column_name = table.partition_info.column.name
+                    partition_data_type = table.partition_info.column.data_type
+                if table.partition_info.type in ("HOUR", "DAY", "MONTH", "YEAR"):
+                    partition_where_clause = f"{partition_data_type}(`{partition_column_name}`) BETWEEN {partition_data_type}('{partition_datetime}') AND {partition_data_type}('{upper_bound_partition_datetime}')"
                 else:
                     logger.warning(
-                        f"Not supported partition type {table.time_partitioning.type_}"
+                        f"Not supported partition type {table.partition_info.type}"
                     )
                     return None, None
             custom_sql = """
 SELECT
     *
 FROM
     `{table_catalog}.{table_schema}.{table_name}`
@@ -157,51 +156,46 @@
         if table.max_shard_id:
             # For sharded table we want to get the partition id but not needed to generate custom query
             return table.max_shard_id, None
 
         return None, None
 
     def get_workunits(
-        self, tables: Dict[str, Dict[str, List[BigqueryTable]]]
+        self, project_id: str, tables: Dict[str, List[BigqueryTable]]
     ) -> Iterable[MetadataWorkUnit]:
         # Otherwise, if column level profiling is enabled, use  GE profiler.
-        for project in tables.keys():
-            if not self.config.project_id_pattern.allowed(project):
+        if not self.config.project_id_pattern.allowed(project_id):
+            return
+        profile_requests = []
+
+        for dataset in tables:
+            if not self.config.schema_pattern.allowed(dataset):
                 continue
-            profile_requests = []
 
-            for dataset in tables[project]:
-                if not self.config.schema_pattern.allowed(dataset):
-                    continue
-
-                for table in tables[project][dataset]:
-                    normalized_table_name = BigqueryTableIdentifier(
-                        project_id=project, dataset=dataset, table=table.name
-                    ).get_table_name()
-                    for column in table.columns:
-                        # Profiler has issues with complex types (array, struct, geography, json), so we deny those types from profiling
-                        # We also filter columns without data type as it means that column is part of a complex type.
-                        if not column.data_type or any(
-                            word in column.data_type.lower()
-                            for word in ["array", "struct", "geography", "json"]
-                        ):
-                            self.config.profile_pattern.deny.append(
-                                f"^{normalized_table_name}.{column.field_path}$"
-                            )
-
-                    # Emit the profile work unit
-                    profile_request = self.get_bigquery_profile_request(
-                        project=project, dataset=dataset, table=table
+            for table in tables[dataset]:
+                normalized_table_name = BigqueryTableIdentifier(
+                    project_id=project_id, dataset=dataset, table=table.name
+                ).get_table_name()
+                for column in table.columns_ignore_from_profiling:
+                    # Profiler has issues with complex types (array, struct, geography, json), so we deny those types from profiling
+                    # We also filter columns without data type as it means that column is part of a complex type.
+                    self.config.profile_pattern.deny.append(
+                        f"^{normalized_table_name}.{column}$"
                     )
-                    if profile_request is not None:
-                        profile_requests.append(profile_request)
 
-            if len(profile_requests) == 0:
-                continue
-            yield from self.generate_wu_from_profile_requests(profile_requests)
+                # Emit the profile work unit
+                profile_request = self.get_bigquery_profile_request(
+                    project=project_id, dataset=dataset, table=table
+                )
+                if profile_request is not None:
+                    profile_requests.append(profile_request)
+
+        if len(profile_requests) == 0:
+            return
+        yield from self.generate_wu_from_profile_requests(profile_requests)
 
     def generate_wu_from_profile_requests(
         self, profile_requests: List[BigqueryProfilerRequest]
     ) -> Iterable[MetadataWorkUnit]:
         table_profile_requests = cast(List[TableProfilerRequest], profile_requests)
         for request, profile in self.generate_profiles(
             table_profile_requests,
@@ -246,33 +240,28 @@
         dataset_name = BigqueryTableIdentifier(
             project_id=project, dataset=dataset, table=table.name
         ).get_table_name()
         if not self.is_dataset_eligible_for_profiling(
             dataset_name, table.last_altered, table.size_in_bytes, table.rows_count
         ):
             profile_table_level_only = True
-            self.report.num_tables_not_eligible_profiling[f"{project}.{dataset}"] = (
-                self.report.num_tables_not_eligible_profiling.get(
-                    f"{project}.{dataset}", 0
-                )
-                + 1
-            )
+            self.report.num_tables_not_eligible_profiling[f"{project}.{dataset}"] += 1
 
-        if not table.columns:
+        if not table.column_count:
             skip_profiling = True
 
         if skip_profiling:
             if self.config.profiling.report_dropped_profiles:
                 self.report.report_dropped(f"profile of {dataset_name}")
             return None
         (partition, custom_sql) = self.generate_partition_profiler_query(
             project, dataset, table, self.config.profiling.partition_datetime
         )
 
-        if partition is None and table.time_partitioning:
+        if partition is None and table.partition_info:
             self.report.report_warning(
                 "profile skipped as partitioned table is empty or partition id was invalid",
                 dataset_name,
             )
             return None
 
         if (
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/bigquery_v2/usage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/bigquery_v2/usage.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,24 +1,35 @@
-import collections
+import json
 import logging
 import textwrap
 import time
+import uuid
 from dataclasses import dataclass
 from datetime import datetime
-from typing import Any, Dict, Iterable, List, MutableMapping, Optional, Union, cast
+from typing import (
+    Any,
+    Callable,
+    Collection,
+    Dict,
+    Iterable,
+    Iterator,
+    List,
+    Optional,
+    Tuple,
+    Union,
+)
 
-import cachetools
 from google.cloud.bigquery import Client as BigQueryClient
 from google.cloud.logging_v2.client import Client as GCPLoggingClient
-from more_itertools import partition
 from ratelimiter import RateLimiter
 
 from datahub.configuration.time_window_config import get_time_bucket
 from datahub.emitter.mce_builder import make_user_urn
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
+from datahub.ingestion.api.closeable import Closeable
 from datahub.ingestion.api.workunit import MetadataWorkUnit
 from datahub.ingestion.source.bigquery_v2.bigquery_audit import (
     BQ_AUDIT_V2,
     AuditEvent,
     AuditLogEntry,
     BigQueryAuditMetadata,
     BigQueryTableRef,
@@ -27,33 +38,48 @@
 )
 from datahub.ingestion.source.bigquery_v2.bigquery_config import BigQueryV2Config
 from datahub.ingestion.source.bigquery_v2.bigquery_report import BigQueryV2Report
 from datahub.ingestion.source.bigquery_v2.common import (
     BQ_DATE_SHARD_FORMAT,
     BQ_DATETIME_FORMAT,
     _make_gcp_logging_client,
+    get_bigquery_client,
 )
-from datahub.ingestion.source.usage.usage_common import GenericAggregatedDataset
+from datahub.ingestion.source.usage.usage_common import make_usage_workunit
 from datahub.metadata.schema_classes import OperationClass, OperationTypeClass
-from datahub.utilities.delayed_iter import delayed_iter
+from datahub.utilities.file_backed_collections import ConnectionWrapper, FileBackedDict
 from datahub.utilities.perf_timer import PerfTimer
 
 logger: logging.Logger = logging.getLogger(__name__)
 
-AggregatedDataset = GenericAggregatedDataset[BigQueryTableRef]
 
+# See https://cloud.google.com/java/docs/reference/google-cloud-bigquery/latest/com.google.cloud.bigquery.JobStatistics.QueryStatistics.StatementType
+# https://pkg.go.dev/google.golang.org/genproto/googleapis/cloud/audit may be more complete
 OPERATION_STATEMENT_TYPES = {
     "INSERT": OperationTypeClass.INSERT,
     "UPDATE": OperationTypeClass.UPDATE,
     "DELETE": OperationTypeClass.DELETE,
     "MERGE": OperationTypeClass.UPDATE,
     "CREATE": OperationTypeClass.CREATE,
     "CREATE_TABLE_AS_SELECT": OperationTypeClass.CREATE,
+    "CREATE_EXTERNAL_TABLE": OperationTypeClass.CREATE,
+    "CREATE_SNAPSHOT_TABLE": OperationTypeClass.CREATE,
+    "CREATE_VIEW": OperationTypeClass.CREATE,
+    "CREATE_MATERIALIZED_VIEW": OperationTypeClass.CREATE,
     "CREATE_SCHEMA": OperationTypeClass.CREATE,
     "DROP_TABLE": OperationTypeClass.DROP,
+    "DROP_EXTERNAL_TABLE": OperationTypeClass.DROP,
+    "DROP_SNAPSHOT_TABLE": OperationTypeClass.DROP,
+    "DROP_VIEW": OperationTypeClass.DROP,
+    "DROP_MATERIALIZED_VIEW": OperationTypeClass.DROP,
+    "DROP_SCHEMA": OperationTypeClass.DROP,
+    "ALTER_TABLE": OperationTypeClass.ALTER,
+    "ALTER_VIEW": OperationTypeClass.ALTER,
+    "ALTER_MATERIALIZED_VIEW": OperationTypeClass.ALTER,
+    "ALTER_SCHEMA": OperationTypeClass.ALTER,
 }
 
 READ_STATEMENT_TYPES: List[str] = ["SELECT"]
 
 
 @dataclass(frozen=True, order=True)
 class OperationalDataMeta:
@@ -72,81 +98,206 @@
     """
     Receives a dataset (with project specified) and returns a query template that is used to query exported
     v2 AuditLogs containing protoPayloads of type BigQueryAuditMetadata.
     :param dataset: the dataset to query against in the form of $PROJECT.$DATASET
     :param use_date_sharded_tables: whether to read from date sharded audit log tables or time partitioned audit log
            tables
     :param table_allow_filter: regex used to filter on log events that contain the wanted datasets
+    :param limit: maximum number of events to query for
     :return: a query template, when supplied start_time and end_time, can be used to query audit logs from BigQuery
     """
     allow_filter = f"""
       AND EXISTS (SELECT *
               from UNNEST(JSON_EXTRACT_ARRAY(protopayload_auditlog.metadataJson,
                                              "$.jobChange.job.jobStats.queryStats.referencedTables")) AS x
               where REGEXP_CONTAINS(x, r'(projects/.*/datasets/.*/tables/{table_allow_filter if table_allow_filter else ".*"})'))
     """
 
-    query: str
+    limit_text = f"limit {limit}" if limit else ""
+
+    shard_condition = ""
     if use_date_sharded_tables:
-        query = (
-            f"""
-        SELECT
-            timestamp,
-            logName,
-            insertId,
-            protopayload_auditlog AS protoPayload,
-            protopayload_auditlog.metadataJson AS metadata
-        FROM
-            `{dataset}.cloudaudit_googleapis_com_data_access_*`
-        """
-            + """
-        WHERE
-            _TABLE_SUFFIX BETWEEN "{start_time}" AND "{end_time}"
-        """
+        from_table = f"`{dataset}.cloudaudit_googleapis_com_data_access_*`"
+        shard_condition = (
+            """ AND _TABLE_SUFFIX BETWEEN "{start_date}" AND "{end_date}" """
         )
     else:
-        query = f"""
+        from_table = f"`{dataset}.cloudaudit_googleapis_com_data_access`"
+
+    query = f"""
         SELECT
             timestamp,
             logName,
             insertId,
             protopayload_auditlog AS protoPayload,
             protopayload_auditlog.metadataJson AS metadata
         FROM
-            `{dataset}.cloudaudit_googleapis_com_data_access`
-        WHERE 1=1
-        """
-    audit_log_filter_timestamps = """AND (timestamp >= "{start_time}"
-        AND timestamp < "{end_time}"
-    );
-    """
-    audit_log_filter_query_complete = f"""
-    AND (
-            (
-                protopayload_auditlog.serviceName="bigquery.googleapis.com"
-                AND JSON_EXTRACT_SCALAR(protopayload_auditlog.metadataJson, "$.jobChange.job.jobStatus.jobState") = "DONE"
-                AND JSON_EXTRACT(protopayload_auditlog.metadataJson, "$.jobChange.job.jobConfig.queryConfig") IS NOT NULL
-                {allow_filter}
-            )
+            {from_table}
+        WHERE (
+            timestamp >= "{{start_time}}"
+            AND timestamp < "{{end_time}}"
+        )
+        {shard_condition}
+        AND (
+                (
+                    protopayload_auditlog.serviceName="bigquery.googleapis.com"
+                    AND JSON_EXTRACT_SCALAR(protopayload_auditlog.metadataJson, "$.jobChange.job.jobStatus.jobState") = "DONE"
+                    AND JSON_EXTRACT(protopayload_auditlog.metadataJson, "$.jobChange.job.jobConfig.queryConfig") IS NOT NULL
+                    {allow_filter}
+                )
             OR
             JSON_EXTRACT_SCALAR(protopayload_auditlog.metadataJson, "$.tableDataRead.reason") = "JOB"
-    )
+        )
+        {limit_text};
     """
 
-    limit_text = f"limit {limit}" if limit else ""
-    query = (
-        textwrap.dedent(query)
-        + audit_log_filter_query_complete
-        + audit_log_filter_timestamps
-        + limit_text
-    )
-
     return textwrap.dedent(query)
 
 
+class BigQueryUsageState(Closeable):
+    read_events: FileBackedDict[ReadEvent]
+    query_events: FileBackedDict[QueryEvent]
+    column_accesses: FileBackedDict[Tuple[str, str]]
+
+    def __init__(self, config: BigQueryV2Config):
+        self.conn = ConnectionWrapper()
+        self.read_events = FileBackedDict[ReadEvent](
+            shared_connection=self.conn,
+            tablename="read_events",
+            extra_columns={
+                "resource": lambda e: str(e.resource),
+                "name": lambda e: e.jobName,
+                "timestamp": lambda e: get_time_bucket(
+                    e.timestamp, config.bucket_duration
+                ),
+                "user": lambda e: e.actor_email,
+            },
+            cache_max_size=config.file_backed_cache_size,
+        )
+        # Keyed by job_name
+        self.query_events = FileBackedDict[QueryEvent](
+            shared_connection=self.conn,
+            tablename="query_events",
+            extra_columns={
+                "query": lambda e: e.query,
+                "is_read": lambda e: int(e.statementType in READ_STATEMENT_TYPES),
+            },
+            cache_max_size=config.file_backed_cache_size,
+        )
+        # Created just to store column accesses in sqlite for JOIN
+        self.column_accesses = FileBackedDict[Tuple[str, str]](
+            shared_connection=self.conn,
+            tablename="column_accesses",
+            extra_columns={"read_event": lambda p: p[0], "field": lambda p: p[1]},
+            cache_max_size=config.file_backed_cache_size,
+        )
+
+    def close(self) -> None:
+        self.read_events.close()
+        self.query_events.close()
+        self.column_accesses.close()
+        self.conn.close()
+
+    def standalone_events(self) -> Iterable[AuditEvent]:
+        for read_event in self.read_events.values():
+            query_event = (
+                self.query_events.get(read_event.jobName)
+                if read_event.jobName
+                else None
+            )
+            yield AuditEvent(read_event=read_event, query_event=query_event)
+        for _, query_event in self.query_events.items_snapshot("NOT is_read"):
+            yield AuditEvent(query_event=query_event)
+
+    @staticmethod
+    def usage_statistics_query(top_n: int) -> str:
+        return f"""
+        SELECT a.timestamp, a.resource, a.query_count, b.query_freq, c.user_freq, d.column_freq FROM (
+            SELECT
+                r.timestamp,
+                r.resource,
+                COUNT(q.query) query_count
+            FROM
+                read_events r
+                LEFT JOIN query_events q ON r.name = q.key
+            GROUP BY r.timestamp, r.resource
+        ) a
+        LEFT JOIN (
+            SELECT timestamp, resource, json_group_array(json_array(query, query_count)) as query_freq FROM (
+                SELECT
+                    r.timestamp,
+                    r.resource,
+                    q.query,
+                    COUNT(r.key) as query_count,
+                    ROW_NUMBER() over (PARTITION BY r.timestamp, r.resource, q.query ORDER BY COUNT(r.key) DESC, q.query) as rank
+                FROM
+                    read_events r
+                    LEFT JOIN query_events q ON r.name = q.key
+                GROUP BY r.timestamp, r.resource, q.query
+                ORDER BY r.timestamp, r.resource, query_count DESC, q.query
+            ) WHERE rank <= {top_n}
+            GROUP BY timestamp, resource
+        ) b ON a.timestamp = b.timestamp AND a.resource = b.resource
+        LEFT JOIN (
+            SELECT timestamp, resource, json_group_array(json_array(user, user_count)) as user_freq FROM (
+                SELECT
+                    r.timestamp,
+                    r.resource,
+                    r.user,
+                    COUNT(r.key) user_count
+                FROM
+                    read_events r
+                GROUP BY r.timestamp, r.resource, r.user
+                ORDER BY r.timestamp, r.resource, user_count DESC, r.user
+            )
+            GROUP BY timestamp, resource
+        ) c ON a.timestamp = c.timestamp AND a.resource = c.resource
+        LEFT JOIN (
+            SELECT timestamp, resource, json_group_array(json_array(column, column_count)) as column_freq FROM (
+                SELECT
+                    r.timestamp,
+                    r.resource,
+                    c.field column,
+                    COUNT(r.key) column_count
+                FROM
+                    read_events r
+                    INNER JOIN column_accesses c ON r.key = c.read_event
+                GROUP BY r.timestamp, r.resource, c.field
+                ORDER BY r.timestamp, r.resource, column_count DESC, c.field
+            )
+            GROUP BY timestamp, resource
+        ) d ON a.timestamp = d.timestamp AND a.resource = d.resource
+        ORDER BY a.timestamp, a.resource
+        """
+
+    @dataclass
+    class UsageStatistic:
+        timestamp: str
+        resource: str
+        query_count: int
+        query_freq: List[Tuple[str, int]]
+        user_freq: List[Tuple[str, int]]
+        column_freq: List[Tuple[str, int]]
+
+    def usage_statistics(self, top_n: int) -> Iterator[UsageStatistic]:
+        query = self.usage_statistics_query(top_n)
+        rows = self.read_events.sql_query_iterator(
+            query, refs=[self.query_events, self.column_accesses]
+        )
+        for row in rows:
+            yield self.UsageStatistic(
+                timestamp=row["timestamp"],
+                resource=row["resource"],
+                query_count=row["query_count"],
+                query_freq=json.loads(row["query_freq"]),
+                user_freq=json.loads(row["user_freq"]),
+                column_freq=json.loads(row["column_freq"]),
+            )
+
+
 class BigQueryUsageExtractor:
     """
     This plugin extracts the following:
     * Statistics on queries issued and tables and columns accessed (excludes views)
     * Aggregation of these statistics into buckets, by day or hour granularity
 
     :::note
@@ -154,226 +305,255 @@
     :::
     """
 
     def __init__(self, config: BigQueryV2Config, report: BigQueryV2Report):
         self.config: BigQueryV2Config = config
         self.report: BigQueryV2Report = report
 
-    def add_config_to_report(self):
-        self.report.query_log_delay = self.config.usage.query_log_delay
-
     def _is_table_allowed(self, table_ref: Optional[BigQueryTableRef]) -> bool:
         return (
             table_ref is not None
             and self.config.dataset_pattern.allowed(table_ref.table_identifier.dataset)
             and self.config.table_pattern.allowed(table_ref.table_identifier.table)
         )
 
-    def generate_usage_for_project(
-        self, project_id: str, tables: Dict[str, List[str]]
+    def run(
+        self, projects: Iterable[str], table_refs: Collection[str]
     ) -> Iterable[MetadataWorkUnit]:
-        aggregated_info: Dict[
-            datetime, Dict[BigQueryTableRef, AggregatedDataset]
-        ] = collections.defaultdict(dict)
-
-        parsed_bigquery_log_events: Iterable[
-            Union[ReadEvent, QueryEvent, MetadataWorkUnit]
-        ]
-        with PerfTimer() as timer:
-            try:
-                bigquery_log_entries = self._get_parsed_bigquery_log_events(project_id)
-                if self.config.use_exported_bigquery_audit_metadata:
-                    parsed_bigquery_log_events = (
-                        self._parse_exported_bigquery_audit_metadata(
-                            bigquery_log_entries
-                        )
-                    )
-                else:
-                    parsed_bigquery_log_events = self._parse_bigquery_log_entries(
-                        bigquery_log_entries
+        events = self._get_usage_events(projects)
+        yield from self._run(events, table_refs)
+
+    def _run(
+        self, events: Iterable[AuditEvent], table_refs: Collection[str]
+    ) -> Iterable[MetadataWorkUnit]:
+        try:
+            with BigQueryUsageState(self.config) as usage_state:
+                self._ingest_events(events, table_refs, usage_state)
+
+                if self.config.usage.include_operational_stats:
+                    yield from self._generate_operational_workunits(
+                        usage_state, table_refs
                     )
 
-                parsed_events_uncasted: Iterable[
-                    Union[ReadEvent, QueryEvent, MetadataWorkUnit]
-                ]
-                last_updated_work_units_uncasted: Iterable[
-                    Union[ReadEvent, QueryEvent, MetadataWorkUnit]
-                ]
-                parsed_events_uncasted, last_updated_work_units_uncasted = partition(
-                    lambda x: isinstance(x, MetadataWorkUnit),
-                    parsed_bigquery_log_events,
-                )
-                parsed_events: Iterable[Union[ReadEvent, QueryEvent]] = cast(
-                    Iterable[Union[ReadEvent, QueryEvent]], parsed_events_uncasted
-                )
-
-                hydrated_read_events = self._join_events_by_job_id(parsed_events)
-                # storing it all in one big object.
-
-                # TODO: handle partitioned tables
-
-                # TODO: perhaps we need to continuously prune this, rather than
-                num_aggregated: int = 0
-                self.report.num_operational_stats_workunits_emitted = 0
-                for event in hydrated_read_events:
-                    if self.config.usage.include_operational_stats:
-                        operational_wu = self._create_operation_aspect_work_unit(event)
-                        if operational_wu:
-                            yield operational_wu
-                            self.report.num_operational_stats_workunits_emitted += 1
-                    if event.read_event:
-                        aggregated_info = self._aggregate_enriched_read_events(
-                            aggregated_info, event, tables
-                        )
-                        num_aggregated += 1
-                logger.info(f"Total number of events aggregated = {num_aggregated}.")
-                bucket_level_stats: str = "\n\t" + "\n\t".join(
-                    [
-                        f'bucket:{db.strftime("%m-%d-%Y:%H:%M:%S")}, size={len(ads)}'
-                        for db, ads in aggregated_info.items()
-                    ]
+                yield from self._generate_usage_workunits(usage_state)
+        except Exception as e:
+            logger.error("Error processing usage", exc_info=True)
+            self.report.report_warning("usage-ingestion", str(e))
+
+    def _ingest_events(
+        self,
+        events: Iterable[AuditEvent],
+        table_refs: Collection[str],
+        usage_state: BigQueryUsageState,
+    ) -> None:
+        """Read log and store events in usage_state."""
+        num_aggregated = 0
+        for audit_event in events:
+            try:
+                num_aggregated += self._store_usage_event(
+                    audit_event, usage_state, table_refs
                 )
-                logger.debug(
-                    f"Number of buckets created = {len(aggregated_info)}. Per-bucket details:{bucket_level_stats}"
+            except Exception as e:
+                logger.warning(
+                    f"Unable to store usage event {audit_event}", exc_info=True
                 )
+                self._report_error("store-event", e)
+        logger.info(f"Total number of events aggregated = {num_aggregated}.")
 
-                self.report.usage_extraction_sec[project_id] = round(
-                    timer.elapsed_seconds(), 2
+    def _generate_operational_workunits(
+        self, usage_state: BigQueryUsageState, table_refs: Collection[str]
+    ) -> Iterable[MetadataWorkUnit]:
+        self.report.set_project_state("All", "Usage Extraction Operational Stats")
+        for audit_event in usage_state.standalone_events():
+            try:
+                operational_wu = self._create_operation_workunit(
+                    audit_event, table_refs
                 )
+                if operational_wu:
+                    yield operational_wu
+                    self.report.num_operational_stats_workunits_emitted += 1
+            except Exception as e:
+                logger.warning(
+                    f"Unable to generate operation workunit for event {audit_event}",
+                    exc_info=True,
+                )
+                self._report_error("operation-workunit", e)
 
-                yield from self.get_workunits(aggregated_info)
+    def _generate_usage_workunits(
+        self, usage_state: BigQueryUsageState
+    ) -> Iterable[MetadataWorkUnit]:
+        self.report.set_project_state("All", "Usage Extraction Usage Aggregation")
+        top_n = (
+            self.config.usage.top_n_queries
+            if self.config.usage.include_top_n_queries
+            else 0
+        )
+        for entry in usage_state.usage_statistics(top_n=top_n):
+            try:
+                yield make_usage_workunit(
+                    bucket_start_time=datetime.fromisoformat(entry.timestamp),
+                    resource=BigQueryTableRef.from_string_name(entry.resource),
+                    query_count=entry.query_count,
+                    query_freq=entry.query_freq,
+                    user_freq=entry.user_freq,
+                    column_freq=entry.column_freq,
+                    bucket_duration=self.config.bucket_duration,
+                    urn_builder=lambda resource: resource.to_urn(self.config.env),
+                    top_n_queries=self.config.usage.top_n_queries,
+                    format_sql_queries=self.config.usage.format_sql_queries,
+                )
+                self.report.num_usage_workunits_emitted += 1
             except Exception as e:
-                self.report.usage_failed_extraction.append(project_id)
-                logger.error(
-                    f"Error getting usage for project {project_id} due to error {e}"
+                logger.warning(
+                    f"Unable to generate usage workunit for bucket {entry.timestamp}, {entry.resource}",
+                    exc_info=True,
                 )
+                self._report_error("statistics-workunit", e)
 
-    def _get_bigquery_log_entries_via_exported_bigquery_audit_metadata(
-        self, client: BigQueryClient
-    ) -> Iterable[BigQueryAuditMetadata]:
-        try:
-            list_entries: Iterable[
-                BigQueryAuditMetadata
-            ] = self._get_exported_bigquery_audit_metadata(
-                client, self.config.get_table_pattern(self.config.table_pattern.allow)
-            )
-            i: int = 0
-            for i, entry in enumerate(list_entries):
-                if i == 0:
-                    logger.info(
-                        f"Starting log load from BigQuery for project {client.project}"
+    def _get_usage_events(self, projects: Iterable[str]) -> Iterable[AuditEvent]:
+        if self.config.use_exported_bigquery_audit_metadata:
+            projects = ["*"]  # project_id not used when using exported metadata
+
+        for project_id in projects:
+            with PerfTimer() as timer:
+                try:
+                    self.report.set_project_state(
+                        project_id, "Usage Extraction Ingestion"
                     )
-                yield entry
+                    yield from self._get_parsed_bigquery_log_events(project_id)
+                except Exception as e:
+                    logger.error(
+                        f"Error getting usage events for project {project_id}",
+                        exc_info=True,
+                    )
+                    self.report.usage_failed_extraction.append(project_id)
+                    self.report.report_warning(f"usage-extraction-{project_id}", str(e))
 
-            logger.info(
-                f"Finished loading {i} log entries from BigQuery for project {client.project}"
-            )
+                self.report.usage_extraction_sec[project_id] = round(
+                    timer.elapsed_seconds(), 2
+                )
 
-        except Exception as e:
-            logger.warning(
-                f"Encountered exception retrieving AuditLogEntries for project {client.project}",
-                e,
-            )
-            self.report.report_failure(
-                "lineage-extraction",
-                f"{client.project} - unable to retrieve log entries {e}",
-            )
+    def _store_usage_event(
+        self,
+        event: AuditEvent,
+        usage_state: BigQueryUsageState,
+        table_refs: Collection[str],
+    ) -> bool:
+        """Stores a usage event in `usage_state` and returns if an event was successfully processed."""
+        if event.read_event and (
+            self.config.start_time <= event.read_event.timestamp < self.config.end_time
+        ):
+            resource = event.read_event.resource
+            if str(resource) not in table_refs:
+                logger.info(f"Skipping non-existent {resource} from usage")
+                self.report.num_usage_resources_dropped += 1
+                self.report.report_dropped(str(resource))
+                return False
+            elif resource.is_temporary_table([self.config.temp_table_dataset_prefix]):
+                logger.debug(f"Dropping temporary table {resource}")
+                self.report.report_dropped(str(resource))
+                return False
+
+            # Use uuid keys to store all entries -- no overwriting
+            key = str(uuid.uuid4())
+            usage_state.read_events[key] = event.read_event
+            for field_read in event.read_event.fieldsRead:
+                usage_state.column_accesses[str(uuid.uuid4())] = key, field_read
+            return True
+        elif event.query_event and event.query_event.job_name:
+            usage_state.query_events[event.query_event.job_name] = event.query_event
+            return True
+        return False
 
     def _get_exported_bigquery_audit_metadata(
         self,
         bigquery_client: BigQueryClient,
         allow_filter: str,
         limit: Optional[int] = None,
     ) -> Iterable[BigQueryAuditMetadata]:
         if self.config.bigquery_audit_metadata_datasets is None:
             return
 
-        start_time: str = (
-            self.config.start_time - self.config.max_query_duration
-        ).strftime(
-            BQ_DATE_SHARD_FORMAT
-            if self.config.use_date_sharded_audit_log_tables
-            else BQ_DATETIME_FORMAT
-        )
+        corrected_start_time = self.config.start_time - self.config.max_query_duration
+        start_time = corrected_start_time.strftime(BQ_DATETIME_FORMAT)
+        start_date = corrected_start_time.strftime(BQ_DATE_SHARD_FORMAT)
         self.report.audit_start_time = start_time
 
-        end_time: str = (
-            self.config.end_time + self.config.max_query_duration
-        ).strftime(
-            BQ_DATE_SHARD_FORMAT
-            if self.config.use_date_sharded_audit_log_tables
-            else BQ_DATETIME_FORMAT
-        )
+        corrected_end_time = self.config.end_time + self.config.max_query_duration
+        end_time = corrected_end_time.strftime(BQ_DATETIME_FORMAT)
+        end_date = corrected_end_time.strftime(BQ_DATE_SHARD_FORMAT)
         self.report.audit_end_time = end_time
 
         for dataset in self.config.bigquery_audit_metadata_datasets:
             logger.info(
                 f"Start loading log entries from BigQueryAuditMetadata in {dataset}"
             )
 
             query = bigquery_audit_metadata_query_template(
-                dataset, self.config.use_date_sharded_audit_log_tables, allow_filter
+                dataset,
+                self.config.use_date_sharded_audit_log_tables,
+                allow_filter,
+                limit=limit,
             ).format(
                 start_time=start_time,
                 end_time=end_time,
+                start_date=start_date,
+                end_date=end_date,
             )
 
             query_job = bigquery_client.query(query)
             logger.info(
                 f"Finished loading log entries from BigQueryAuditMetadata in {dataset}"
             )
             if self.config.rate_limit:
                 with RateLimiter(max_calls=self.config.requests_per_min, period=60):
                     yield from query_job
             else:
                 yield from query_job
 
     def _get_bigquery_log_entries_via_gcp_logging(
         self, client: GCPLoggingClient, limit: Optional[int] = None
-    ) -> Iterable[Union[AuditLogEntry, BigQueryAuditMetadata]]:
+    ) -> Iterable[AuditLogEntry]:
         self.report.total_query_log_entries = 0
 
         filter = self._generate_filter(BQ_AUDIT_V2)
         logger.debug(filter)
 
-        try:
-            list_entries: Iterable[Union[AuditLogEntry, BigQueryAuditMetadata]]
-            if self.config.rate_limit:
-                with RateLimiter(max_calls=self.config.requests_per_min, period=60):
-                    list_entries = client.list_entries(
-                        filter_=filter, page_size=self.config.log_page_size
-                    )
+        list_entries: Iterable[AuditLogEntry]
+        rate_limiter: Optional[RateLimiter] = None
+        if self.config.rate_limit:
+            # client.list_entries is a generator, does api calls to GCP Logging when it runs out of entries and needs to fetch more from GCP Logging
+            # to properly ratelimit we multiply the page size by the number of requests per minute
+            rate_limiter = RateLimiter(
+                max_calls=self.config.requests_per_min * self.config.log_page_size,
+                period=60,
+            )
+
+        list_entries = client.list_entries(
+            filter_=filter,
+            page_size=self.config.log_page_size,
+            max_results=limit,
+        )
+
+        for i, entry in enumerate(list_entries):
+            if i == 0:
+                logger.info(f"Starting log load from GCP Logging for {client.project}")
+            if i % 1000 == 0:
+                logger.info(f"Loaded {i} log entries from GCP Log for {client.project}")
+            self.report.total_query_log_entries += 1
+
+            if rate_limiter:
+                with rate_limiter:
+                    yield entry
             else:
-                list_entries = client.list_entries(
-                    filter_=filter,
-                    page_size=self.config.log_page_size,
-                    max_results=limit,
-                )
-
-            for i, entry in enumerate(list_entries):
-                if i == 0:
-                    logger.info(
-                        f"Starting log load from GCP Logging for {client.project}"
-                    )
-                self.report.total_query_log_entries += 1
                 yield entry
 
-            logger.info(
-                f"Finished loading {self.report.total_query_log_entries} log entries from GCP Logging for {client.project}"
-            )
-
-        except Exception as e:
-            logger.warning(
-                f"Encountered exception retrieving AuditLogEntires for project {client.project}",
-                e,
-            )
-            self.report.report_failure(
-                "usage-extraction",
-                f"{client.project} - unable to retrive log entrires {e}",
-            )
+        logger.info(
+            f"Finished loading {self.report.total_query_log_entries} log entries from GCP Logging for {client.project}"
+        )
 
     def _generate_filter(self, audit_templates: Dict[str, str]) -> str:
         # We adjust the filter values a bit, since we need to make sure that the join
         # between query events and read events is complete. For example, this helps us
         # handle the case where the read happens within our time range but the query
         # completion event is delayed and happens after the configured end time.
         # Can safely access the first index of the allow list as it by default contains ".*"
@@ -425,17 +605,17 @@
     @staticmethod
     def _get_destination_table(event: AuditEvent) -> Optional[BigQueryTableRef]:
         if (
             not event.read_event
             and event.query_event
             and event.query_event.destinationTable
         ):
-            return event.query_event.destinationTable.get_sanitized_table_ref()
+            return event.query_event.destinationTable
         elif event.read_event:
-            return event.read_event.resource.get_sanitized_table_ref()
+            return event.read_event.resource
         else:
             # TODO: CREATE_SCHEMA operation ends up here, maybe we should capture that as well
             # but it is tricky as we only get the query so it can't be tied to anything
             # - SCRIPT statement type ends up here as well
             logger.debug(f"Unable to find destination table in event {event}")
             return None
 
@@ -480,25 +660,32 @@
                     event.query_event.timestamp.timestamp() * 1000
                 ),
                 actor_email=event.query_event.actor_email,
             )
         else:
             return None
 
-    def _create_operation_aspect_work_unit(
-        self, event: AuditEvent
+    def _create_operation_workunit(
+        self, event: AuditEvent, table_refs: Collection[str]
     ) -> Optional[MetadataWorkUnit]:
         if not event.read_event and not event.query_event:
             return None
 
         destination_table = self._get_destination_table(event)
         if destination_table is None:
             return None
 
-        if not self._is_table_allowed(destination_table):
+        if (
+            not self._is_table_allowed(destination_table)
+            or str(destination_table) not in table_refs
+        ):
+            logger.debug(
+                f"Filtering out operation {event.query_event}: invalid destination {destination_table}."
+            )
+            self.report.num_usage_operations_dropped += 1
             return None
 
         operational_meta = self._extract_operational_meta(event)
         if not operational_meta:
             return None
 
         if not self.config.usage.include_read_operational_stats and (
@@ -506,23 +693,15 @@
         ):
             return None
 
         reported_time: int = int(time.time() * 1000)
         affected_datasets = []
         if event.query_event and event.query_event.referencedTables:
             for table in event.query_event.referencedTables:
-                try:
-                    affected_datasets.append(
-                        table.get_sanitized_table_ref().to_urn(self.config.env)
-                    )
-                except Exception as e:
-                    self.report.report_warning(
-                        str(table),
-                        f"Failed to clean up table, {e}",
-                    )
+                affected_datasets.append(table.to_urn(self.config.env))
 
         operation_aspect = OperationClass(
             timestampMillis=reported_time,
             lastUpdatedTimestamp=operational_meta.last_updated_timestamp,
             actor=make_user_urn(operational_meta.actor_email.split("@")[0]),
             operationType=operational_meta.statement_type,
             customOperationType=operational_meta.custom_type,
@@ -575,258 +754,131 @@
                 if event.read_event.fieldsRead:
                     custom_properties["fieldsRead"] = ",".join(
                         event.read_event.fieldsRead
                     )
 
         return custom_properties
 
-    def _parse_bigquery_log_entries(
-        self, entries: Iterable[Union[AuditLogEntry, BigQueryAuditMetadata]]
-    ) -> Iterable[Union[ReadEvent, QueryEvent]]:
-        self.report.num_read_events = 0
-        self.report.num_query_events = 0
-        self.report.num_filtered_read_events = 0
-        self.report.num_filtered_query_events = 0
-        for entry in entries:
-            event: Optional[Union[ReadEvent, QueryEvent]] = None
-
-            missing_read_entry = ReadEvent.get_missing_key_entry(entry)
-            if missing_read_entry is None:
-                event = ReadEvent.from_entry(
-                    entry, self.config.debug_include_full_payloads
-                )
-                if not self._is_table_allowed(event.resource):
-                    self.report.num_filtered_read_events += 1
-                    continue
-
-                if event.readReason:
-                    self.report.read_reasons_stat[event.readReason] = (
-                        self.report.read_reasons_stat.get(event.readReason, 0) + 1
-                    )
-                self.report.num_read_events += 1
-
-            missing_query_entry = QueryEvent.get_missing_key_entry(entry)
-            if event is None and missing_query_entry is None:
-                event = QueryEvent.from_entry(entry)
-                self.report.num_query_events += 1
-
-            missing_query_entry_v2 = QueryEvent.get_missing_key_entry_v2(entry)
-
-            if event is None and missing_query_entry_v2 is None:
-                event = QueryEvent.from_entry_v2(
-                    entry, self.config.debug_include_full_payloads
-                )
-                self.report.num_query_events += 1
-
-            if event is None:
-                logger.warning(
-                    f"Unable to parse {type(entry)} missing read {missing_query_entry}, missing query {missing_query_entry} missing v2 {missing_query_entry_v2} for {entry}"
-                )
-            else:
-                yield event
-
-        logger.info(
-            f"Parsed {self.report.num_read_events} ReadEvents and {self.report.num_query_events} QueryEvents"
-        )
-
-    def _parse_exported_bigquery_audit_metadata(
-        self, audit_metadata_rows: Iterable[BigQueryAuditMetadata]
-    ) -> Iterable[Union[ReadEvent, QueryEvent, MetadataWorkUnit]]:
-        for audit_metadata in audit_metadata_rows:
-            event: Optional[Union[QueryEvent, ReadEvent]] = None
-            missing_query_event_exported_audit = (
-                QueryEvent.get_missing_key_exported_bigquery_audit_metadata(
-                    audit_metadata
-                )
+    def _parse_bigquery_log_entry(
+        self, entry: Union[AuditLogEntry, BigQueryAuditMetadata]
+    ) -> Optional[AuditEvent]:
+        event: Optional[Union[ReadEvent, QueryEvent]] = None
+
+        missing_read_entry = ReadEvent.get_missing_key_entry(entry)
+        if missing_read_entry is None:
+            event = ReadEvent.from_entry(entry, self.config.debug_include_full_payloads)
+            if not self._is_table_allowed(event.resource):
+                self.report.num_filtered_read_events += 1
+                return None
+
+            if event.readReason:
+                self.report.read_reasons_stat[event.readReason] += 1
+            self.report.num_read_events += 1
+
+        missing_query_entry = QueryEvent.get_missing_key_entry(entry)
+        if event is None and missing_query_entry is None:
+            event = QueryEvent.from_entry(entry)
+            self.report.num_query_events += 1
+
+        missing_query_entry_v2 = QueryEvent.get_missing_key_entry_v2(entry)
+
+        if event is None and missing_query_entry_v2 is None:
+            event = QueryEvent.from_entry_v2(
+                entry, self.config.debug_include_full_payloads
             )
-            if missing_query_event_exported_audit is None:
-                event = QueryEvent.from_exported_bigquery_audit_metadata(
-                    audit_metadata, self.config.debug_include_full_payloads
-                )
+            self.report.num_query_events += 1
 
-            missing_read_event_exported_audit = (
-                ReadEvent.get_missing_key_exported_bigquery_audit_metadata(
-                    audit_metadata
-                )
+        if event is None:
+            logger.warning(
+                f"Unable to parse {type(entry)} missing read {missing_read_entry}, "
+                f"missing query {missing_query_entry} missing v2 {missing_query_entry_v2} for {entry}"
             )
-            if missing_read_event_exported_audit is None:
-                event = ReadEvent.from_exported_bigquery_audit_metadata(
-                    audit_metadata, self.config.debug_include_full_payloads
-                )
-
-            if event is not None:
-                yield event
-            else:
-                self.error(
-                    logger,
-                    "usage-extraction",
-                    f"{audit_metadata['logName']}-{audit_metadata['insertId']} Unable to parse audit metadata missing QueryEvent keys:{str(missing_query_event_exported_audit)} ReadEvent keys: {str(missing_read_event_exported_audit)} for {audit_metadata}",
-                )
+            return None
 
-    def error(self, log: logging.Logger, key: str, reason: str) -> Any:
-        self.report.report_failure(key, reason)
-        log.error(f"{key} => {reason}")
+        return AuditEvent.create(event)
 
-    def _join_events_by_job_id(
-        self, events: Iterable[Union[ReadEvent, QueryEvent]]
-    ) -> Iterable[AuditEvent]:
-        # If caching eviction is enabled, we only store the most recently used query events,
-        # which are used when resolving job information within the read events.
-        query_jobs: MutableMapping[str, QueryEvent]
-        if self.config.usage.query_log_delay:
-            query_jobs = cachetools.LRUCache(
-                maxsize=5 * self.config.usage.query_log_delay
+    def _parse_exported_bigquery_audit_metadata(
+        self, audit_metadata: BigQueryAuditMetadata
+    ) -> Optional[AuditEvent]:
+        event: Optional[Union[ReadEvent, QueryEvent]] = None
+
+        missing_read_event = ReadEvent.get_missing_key_exported_bigquery_audit_metadata(
+            audit_metadata
+        )
+        if missing_read_event is None:
+            event = ReadEvent.from_exported_bigquery_audit_metadata(
+                audit_metadata, self.config.debug_include_full_payloads
+            )
+            if not self._is_table_allowed(event.resource):
+                self.report.num_filtered_read_events += 1
+                return None
+            if event.readReason:
+                self.report.read_reasons_stat[event.readReason] += 1
+            self.report.num_read_events += 1
+
+        missing_query_event = (
+            QueryEvent.get_missing_key_exported_bigquery_audit_metadata(audit_metadata)
+        )
+        if event is None and missing_query_event is None:
+            event = QueryEvent.from_exported_bigquery_audit_metadata(
+                audit_metadata, self.config.debug_include_full_payloads
             )
-        else:
-            query_jobs = {}
+            self.report.num_query_events += 1
 
-        def event_processor(
-            events: Iterable[Union[ReadEvent, QueryEvent]]
-        ) -> Iterable[AuditEvent]:
-            for event in events:
-                if isinstance(event, QueryEvent):
-                    if event.job_name:
-                        query_jobs[event.job_name] = event
-                        # For Insert operations we yield the query event as it is possible
-                        # there won't be any read event.
-                        if event.statementType not in READ_STATEMENT_TYPES:
-                            yield AuditEvent(query_event=event)
-                        # If destination table exists we yield the query event as it is insert operation
-                else:
-                    yield AuditEvent(read_event=event)
-
-        # TRICKY: To account for the possibility that the query event arrives after
-        # the read event in the audit logs, we wait for at least `query_log_delay`
-        # additional events to be processed before attempting to resolve BigQuery
-        # job information from the logs. If `query_log_delay` is None, it gets treated
-        # as an unlimited delay, which prioritizes correctness at the expense of memory usage.
-        original_read_events = event_processor(events)
-        delayed_read_events = delayed_iter(
-            original_read_events, self.config.usage.query_log_delay
-        )
-
-        num_joined: int = 0
-        for event in delayed_read_events:
-            # If event_processor yields a query event which is an insert operation
-            # then we should just yield it.
-            if event.query_event and not event.read_event:
-                yield event
-                continue
-            if (
-                event.read_event is None
-                or event.read_event.timestamp < self.config.start_time
-                or event.read_event.timestamp >= self.config.end_time
-                or not self._is_table_allowed(event.read_event.resource)
-            ):
-                continue
-
-            # There are some read event which does not have jobName because it was read in a different way
-            # Like https://cloud.google.com/logging/docs/reference/audit/bigquery/rest/Shared.Types/AuditData#tabledatalistrequest
-            # There are various reason to read a table
-            # https://cloud.google.com/bigquery/docs/reference/auditlogs/rest/Shared.Types/BigQueryAuditMetadata.TableDataRead.Reason
-            if event.read_event.jobName:
-                if event.read_event.jobName in query_jobs:
-                    # Join the query log event into the table read log event.
-                    num_joined += 1
-                    event.query_event = query_jobs[event.read_event.jobName]
-                else:
-                    logger.debug(
-                        f"Failed to match table read event {event.read_event.jobName} with reason {event.read_event.readReason} with job at {event.read_event.timestamp}; try increasing `query_log_delay` or `max_query_duration`"
-                    )
-            yield event
-        logger.info(f"Number of read events joined with query events: {num_joined}")
-
-    def _aggregate_enriched_read_events(
-        self,
-        datasets: Dict[datetime, Dict[BigQueryTableRef, AggregatedDataset]],
-        event: AuditEvent,
-        tables: Dict[str, List[str]],
-    ) -> Dict[datetime, Dict[BigQueryTableRef, AggregatedDataset]]:
-        if not event.read_event:
-            return datasets
-
-        floored_ts = get_time_bucket(
-            event.read_event.timestamp, self.config.bucket_duration
-        )
-        resource: Optional[BigQueryTableRef] = None
-        try:
-            resource = event.read_event.resource.get_sanitized_table_ref()
-            if (
-                resource.table_identifier.dataset not in tables
-                or resource.table_identifier.get_table_name()
-                not in tables[resource.table_identifier.dataset]
-            ):
-                logger.debug(f"Skipping non existing {resource} from usage")
-                return datasets
-        except Exception as e:
-            self.report.report_warning(
-                str(event.read_event.resource), f"Failed to clean up resource, {e}"
-            )
+        if event is None:
             logger.warning(
-                f"Failed to process event {str(event.read_event.resource)}", e
+                f"{audit_metadata['logName']}-{audit_metadata['insertId']} "
+                f"Unable to parse audit metadata missing QueryEvent keys:{str(missing_query_event)} "
+                f"ReadEvent keys: {str(missing_read_event)} for {audit_metadata}"
             )
-            return datasets
-
-        if resource.is_temporary_table([self.config.temp_table_dataset_prefix]):
-            logger.debug(f"Dropping temporary table {resource}")
-            self.report.report_dropped(str(resource))
-            return datasets
-
-        agg_bucket = datasets[floored_ts].setdefault(
-            resource,
-            AggregatedDataset(
-                bucket_start_time=floored_ts,
-                resource=resource,
-                user_email_pattern=self.config.usage.user_email_pattern,
-            ),
-        )
-
-        agg_bucket.add_read_entry(
-            event.read_event.actor_email,
-            event.query_event.query if event.query_event else None,
-            event.read_event.fieldsRead,
-        )
-
-        return datasets
-
-    def get_workunits(
-        self, aggregated_info: Dict[datetime, Dict[BigQueryTableRef, AggregatedDataset]]
-    ) -> Iterable[MetadataWorkUnit]:
-        self.report.num_usage_workunits_emitted = 0
-        for time_bucket in aggregated_info.values():
-            for aggregate in time_bucket.values():
-                yield self._make_usage_stat(aggregate)
-                self.report.num_usage_workunits_emitted += 1
+            return None
 
-    def _make_usage_stat(self, agg: AggregatedDataset) -> MetadataWorkUnit:
-        return agg.make_usage_workunit(
-            self.config.bucket_duration,
-            lambda resource: resource.to_urn(self.config.env),
-            self.config.usage.top_n_queries,
-            self.config.usage.format_sql_queries,
-            self.config.usage.include_top_n_queries,
-        )
+        return AuditEvent.create(event)
 
     def _get_parsed_bigquery_log_events(
         self, project_id: str, limit: Optional[int] = None
-    ) -> Iterable[Union[ReadEvent, QueryEvent, MetadataWorkUnit]]:
+    ) -> Iterable[AuditEvent]:
+        parse_fn: Callable[[Any], Optional[AuditEvent]]
         if self.config.use_exported_bigquery_audit_metadata:
-            _client: BigQueryClient = BigQueryClient(project=project_id)
-            return self._get_exported_bigquery_audit_metadata(
-                bigquery_client=_client,
+            bq_client = get_bigquery_client(self.config)
+            entries = self._get_exported_bigquery_audit_metadata(
+                bigquery_client=bq_client,
                 allow_filter=self.config.get_table_pattern(
                     self.config.table_pattern.allow
                 ),
                 limit=limit,
             )
+            parse_fn = self._parse_exported_bigquery_audit_metadata
         else:
-            logging_client: GCPLoggingClient = _make_gcp_logging_client(
+            logging_client = _make_gcp_logging_client(
                 project_id, self.config.extra_client_options
             )
-            return self._get_bigquery_log_entries_via_gcp_logging(
+            entries = self._get_bigquery_log_entries_via_gcp_logging(
                 logging_client, limit=limit
             )
+            parse_fn = self._parse_bigquery_log_entry
+
+        for entry in entries:
+            try:
+                event = parse_fn(entry)
+                if event:
+                    yield event
+            except Exception as e:
+                logger.warning(
+                    f"Unable to parse log entry `{entry}` for project {project_id}",
+                    exc_info=True,
+                )
+                self._report_error(
+                    f"log-parse-{project_id}", e, group="usage-log-parse"
+                )
+
+    def _report_error(
+        self, label: str, e: Exception, group: Optional[str] = None
+    ) -> None:
+        """Report an error that does not constitute a major failure."""
+        self.report.usage_error_count[label] += 1
+        self.report.report_warning(group or f"usage-{label}", str(e))
 
     def test_capability(self, project_id: str) -> None:
         for entry in self._get_parsed_bigquery_log_events(project_id, limit=1):
             logger.debug(f"Connection test got one {entry}")
             return
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/confluent_schema_registry.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/confluent_schema_registry.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,32 +1,43 @@
 import json
 import logging
+from dataclasses import dataclass
 from hashlib import md5
-from typing import List, Optional, Set, Tuple
+from typing import Any, List, Optional, Set, Tuple
 
 import confluent_kafka
+import jsonref
 from confluent_kafka.schema_registry.schema_registry_client import (
     RegisteredSchema,
     Schema,
     SchemaReference,
 )
 
 from datahub.ingestion.extractor import protobuf_util, schema_util
+from datahub.ingestion.extractor.json_schema_util import JsonSchemaTranslator
 from datahub.ingestion.extractor.protobuf_util import ProtobufSchema
 from datahub.ingestion.source.kafka import KafkaSourceConfig, KafkaSourceReport
 from datahub.ingestion.source.kafka_schema_registry_base import KafkaSchemaRegistryBase
 from datahub.metadata.com.linkedin.pegasus2avro.schema import (
     KafkaSchema,
     SchemaField,
     SchemaMetadata,
 )
 
 logger = logging.getLogger(__name__)
 
 
+@dataclass
+class JsonSchemaWrapper:
+    name: str
+    subject: str
+    content: str
+    references: List[Any]
+
+
 class ConfluentSchemaRegistry(KafkaSchemaRegistryBase):
     """
     This is confluent schema registry specific implementation of datahub.ingestion.source.kafka import SchemaRegistry
     It knows how to get SchemaMetadata of a topic from ConfluentSchemaRegistry
     """
 
     def __init__(
@@ -68,17 +79,23 @@
         subject_key: str = topic + subject_key_suffix
         if subject_key in self.source_config.topic_subject_map:
             return self.source_config.topic_subject_map[subject_key]
 
         # Subject name format when the schema registry subject name strategy is
         #  (a) TopicNameStrategy(default strategy): <topic name>-<key/value>
         #  (b) TopicRecordNameStrategy: <topic name>-<fully-qualified record name>-<key/value>
+        #  there's a third case
+        #  (c) TopicNameStrategy differing by environment name suffixes. 
+        #       e.g "a.b.c.d-value" and "a.b.c.d.qa-value"  
+        #       For such instances, the wrong schema registry entries could picked by the previous logic.      
         for subject in self.known_schema_registry_subjects:
-            if subject.startswith(topic) and subject.endswith(subject_key_suffix):
-                return subject
+            if self.source_config.disable_topic_record_naming_strategy and subject == subject_key:
+                return subject 
+            if (not self.source_config.disable_topic_record_naming_strategy) and subject.startswith(topic) and subject.endswith(subject_key_suffix):
+                return subject 
         return None
 
     @staticmethod
     def _compact_schema(schema_str: str) -> str:
         # Eliminate all white-spaces for a compact representation.
         return json.dumps(json.loads(schema_str), separators=(",", ":"))
 
@@ -98,15 +115,15 @@
 
             if ref_subject not in self.known_schema_registry_subjects:
                 logger.warning(
                     f"{ref_subject} is not present in the list of registered subjects with schema registry!"
                 )
 
             reference_schema = self.schema_registry_client.get_latest_version(
-                subject_name=ref_subject
+                subject_name=ref_subject,
             )
             schema_seen.add(ref_subject)
             logger.debug(
                 f"ref for {ref_subject} is {reference_schema.schema.schema_str}"
             )
             # Replace only external type references with the reference schema recursively.
             # NOTE: The type pattern is dependent on _compact_schema.
@@ -153,14 +170,55 @@
             all_schemas.append(
                 ProtobufSchema(
                     name=schema_ref["name"], content=reference_schema.schema.schema_str
                 )
             )
         return all_schemas
 
+    def get_schemas_from_confluent_ref_json(
+        self,
+        schema: Schema,
+        name: str,
+        subject: str,
+        schema_seen: Optional[Set[str]] = None,
+    ) -> List[JsonSchemaWrapper]:
+        """Recursively get all the referenced schemas and their references starting from this schema"""
+        all_schemas: List[JsonSchemaWrapper] = []
+        if schema_seen is None:
+            schema_seen = set()
+
+        schema_ref: SchemaReference
+        for schema_ref in schema.references:
+            ref_subject: str = schema_ref["subject"]
+            if ref_subject in schema_seen:
+                continue
+            reference_schema: RegisteredSchema = (
+                self.schema_registry_client.get_version(
+                    subject_name=ref_subject, version=schema_ref["version"]
+                )
+            )
+            schema_seen.add(ref_subject)
+            all_schemas.extend(
+                self.get_schemas_from_confluent_ref_json(
+                    reference_schema.schema,
+                    name=schema_ref["name"],
+                    subject=ref_subject,
+                    schema_seen=schema_seen,
+                )
+            )
+        all_schemas.append(
+            JsonSchemaWrapper(
+                name=name,
+                subject=subject,
+                content=schema.schema_str,
+                references=schema.references,
+            )
+        )
+        return all_schemas
+
     def _get_schema_and_fields(
         self, topic: str, is_key_schema: bool
     ) -> Tuple[Optional[Schema], List[SchemaField]]:
         schema: Optional[Schema] = None
         schema_type_str: str = "key" if is_key_schema else "value"
         topic_subject: Optional[str] = self._get_subject_for_topic(
             topic=topic, is_key_schema=is_key_schema
@@ -198,14 +256,33 @@
         fields: List[SchemaField] = []
         if schema is not None:
             fields = self._get_schema_fields(
                 topic=topic, schema=schema, is_key_schema=is_key_schema
             )
         return (schema, fields)
 
+    def _load_json_schema_with_resolved_references(
+        self, schema: Schema, name: str, subject: str
+    ) -> dict:
+        imported_json_schemas: List[
+            JsonSchemaWrapper
+        ] = self.get_schemas_from_confluent_ref_json(schema, name=name, subject=subject)
+        schema_dict = json.loads(schema.schema_str)
+        reference_map = {}
+        for imported_schema in imported_json_schemas:
+            reference_schema = json.loads(imported_schema.content)
+            if "title" not in reference_schema:
+                reference_schema["title"] = imported_schema.subject
+            reference_map[imported_schema.name] = reference_schema
+
+        jsonref_schema = jsonref.loads(
+            json.dumps(schema_dict), loader=lambda x: reference_map.get(x)
+        )
+        return jsonref_schema
+
     def _get_schema_fields(
         self, topic: str, schema: Schema, is_key_schema: bool
     ) -> List[SchemaField]:
         # Parse the schema and convert it to SchemaFields.
         fields: List[SchemaField] = []
         if schema.schema_type == "AVRO":
             cleaned_str: str = self.get_schema_str_replace_confluent_ref_avro(schema)
@@ -224,14 +301,29 @@
                     if is_key_schema
                     else f"{base_name}-value.proto",
                     schema.schema_str,
                 ),
                 imported_schemas,
                 is_key_schema=is_key_schema,
             )
+        elif schema.schema_type == "JSON":
+            base_name = topic.replace(".", "_")
+            canonical_name = (
+                f"{base_name}-key" if is_key_schema else f"{base_name}-value"
+            )
+            jsonref_schema = self._load_json_schema_with_resolved_references(
+                schema=schema,
+                name=canonical_name,
+                subject=f"{topic}-key" if is_key_schema else f"{topic}-value",
+            )
+            fields = list(
+                JsonSchemaTranslator.get_fields_from_schema(
+                    jsonref_schema, is_key_schema=is_key_schema
+                )
+            )
         elif not self.source_config.ignore_warnings_on_schema_type:
             self.report.report_warning(
                 topic,
                 f"Parsing kafka schema type {schema.schema_type} is currently not implemented",
             )
         return fields
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/csv_enricher.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/csv_enricher.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,11 +1,15 @@
 import csv
+import pathlib
 import time
 from dataclasses import dataclass
 from typing import Dict, Iterable, List, Optional, Set, Tuple, Union
+from urllib import parse
+
+import requests
 
 from datahub.configuration.common import ConfigurationError
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.decorators import (
     SupportStatus,
     config_class,
@@ -25,14 +29,15 @@
     GlossaryTermAssociationClass,
     GlossaryTermsClass,
     OwnerClass,
     OwnershipClass,
     OwnershipTypeClass,
     TagAssociationClass,
 )
+from datahub.utilities.source_helpers import auto_workunit_reporter
 from datahub.utilities.urns.dataset_urn import DatasetUrn
 from datahub.utilities.urns.urn import Urn
 
 DATASET_ENTITY_TYPE = DatasetUrn.ENTITY_TYPE
 ACTOR = "urn:li:corpuser:ingestion"
 
 
@@ -94,14 +99,18 @@
     |urn:li:dataset:(urn:li:dataPlatform:hive,SampleHiveDataset,PROD)|field_foo  |[urn:li:glossaryTerm:AccountBalance]|                   |                                                   |               |field_foo!     |                           |
     |urn:li:dataset:(urn:li:dataPlatform:hive,SampleHiveDataset,PROD)|field_bar  |                                    |[urn:li:tag:Legacy]|                                                   |               |field_bar?     |                           |
 
     Note that the first row does not have a subresource populated. That means any glossary terms, tags, and owners will
     be applied at the entity field. If a subresource IS populated (as it is for the second and third rows), glossary
     terms and tags will be applied on the subresource. Every row MUST have a resource. Also note that owners can only
     be applied at the resource level and will be ignored if populated for a row with a subresource.
+
+    :::note
+    This source will not work on very large csv files that do not fit in memory.
+    :::
     """
 
     def __init__(self, config: CSVEnricherConfig, ctx: PipelineContext):
         super().__init__(ctx)
         self.config: CSVEnricherConfig = config
         self.ctx: PipelineContext = ctx
         self.report: CSVEnricherReport = CSVEnricherReport()
@@ -297,57 +306,52 @@
             MetadataWorkUnit
         ] = self.get_resource_glossary_terms_work_unit(
             entity_urn=entity_urn,
             term_associations=term_associations,
         )
         if maybe_terms_wu:
             self.report.num_glossary_term_workunits_produced += 1
-            self.report.report_workunit(maybe_terms_wu)
             yield maybe_terms_wu
 
         maybe_tags_wu: Optional[MetadataWorkUnit] = self.get_resource_tags_work_unit(
             entity_urn=entity_urn,
             tag_associations=tag_associations,
         )
         if maybe_tags_wu:
             self.report.num_tag_workunits_produced += 1
-            self.report.report_workunit(maybe_tags_wu)
             yield maybe_tags_wu
 
         maybe_owners_wu: Optional[
             MetadataWorkUnit
         ] = self.get_resource_owners_work_unit(
             entity_urn=entity_urn,
             owners=owners,
         )
         if maybe_owners_wu:
             self.report.num_owners_workunits_produced += 1
-            self.report.report_workunit(maybe_owners_wu)
             yield maybe_owners_wu
 
         maybe_domain_wu: Optional[
             MetadataWorkUnit
         ] = self.get_resource_domain_work_unit(
             entity_urn=entity_urn,
             domain=domain,
         )
         if maybe_domain_wu:
             self.report.num_domain_workunits_produced += 1
-            self.report.report_workunit(maybe_domain_wu)
             yield maybe_domain_wu
 
         maybe_description_wu: Optional[
             MetadataWorkUnit
         ] = self.get_resource_description_work_unit(
             entity_urn=entity_urn,
             description=description,
         )
         if maybe_description_wu:
             self.report.num_description_workunits_produced += 1
-            self.report.report_workunit(maybe_description_wu)
             yield maybe_description_wu
 
     def process_sub_resource_row(
         self,
         sub_resource_row: SubResourceRow,
         current_editable_schema_metadata: EditableSchemaMetadataClass,
         needs_write: bool,
@@ -461,26 +465,25 @@
                 current_editable_schema_metadata = EditableSchemaMetadataClass(
                     editableSchemaFieldInfo=[],
                     created=get_audit_stamp(),
                 )
                 needs_write = True
 
             # Iterate over each sub resource row
-            for sub_resource_row in self.editable_schema_metadata_map[
-                entity_urn
-            ]:  # type: SubResourceRow
+            for sub_resource_row in self.editable_schema_metadata_map[entity_urn]:
                 (
                     current_editable_schema_metadata,
                     needs_write,
                 ) = self.process_sub_resource_row(
                     sub_resource_row, current_editable_schema_metadata, needs_write
                 )
 
             # Write an MCPW if needed.
             if needs_write:
+                self.report.num_editable_schema_metadata_workunits_produced += 1
                 yield MetadataChangeProposalWrapper(
                     entityUrn=entity_urn,
                     aspect=current_editable_schema_metadata,
                 ).as_workunit()
 
     def maybe_extract_glossary_terms(
         self, row: Dict[str, str]
@@ -530,89 +533,90 @@
 
         owners: List[OwnerClass] = [
             OwnerClass(owner_urn, type=ownership_type) for owner_urn in owner_urns
         ]
         return owners
 
     def get_workunits(self) -> Iterable[MetadataWorkUnit]:
+        return auto_workunit_reporter(self.report, self.get_workunits_internal())
+
+    def get_workunits_internal(self) -> Iterable[MetadataWorkUnit]:
         # As per https://stackoverflow.com/a/49150749/5004662, we want to use
         # the 'utf-8-sig' encoding to handle any BOM character that may be
         # present in the file. Excel is known to add a BOM to CSV files.
         # As per https://stackoverflow.com/a/63508823/5004662,
         # this is also safe with normal files that don't have a BOM.
-        with open(self.config.filename, mode="r", encoding="utf-8-sig") as f:
-            rows = csv.DictReader(f, delimiter=self.config.delimiter)
-            for row in rows:
-                # We need the resource to move forward
-                if not row["resource"]:
-                    continue
-
-                is_resource_row: bool = not row["subresource"]
-
-                entity_urn = row["resource"]
-                entity_type = Urn.create_from_string(row["resource"]).get_type()
-
-                term_associations: List[
-                    GlossaryTermAssociationClass
-                ] = self.maybe_extract_glossary_terms(row)
-
-                tag_associations: List[TagAssociationClass] = self.maybe_extract_tags(
-                    row
+        parsed_location = parse.urlparse(self.config.filename)
+        if parsed_location.scheme in ("file", ""):
+            with open(
+                pathlib.Path(self.config.filename), mode="r", encoding="utf-8-sig"
+            ) as f:
+                rows = list(csv.DictReader(f, delimiter=self.config.delimiter))
+        else:
+            try:
+                resp = requests.get(self.config.filename)
+                decoded_content = resp.content.decode("utf-8-sig")
+                rows = list(
+                    csv.DictReader(
+                        decoded_content.splitlines(), delimiter=self.config.delimiter
+                    )
                 )
-
-                owners: List[OwnerClass] = self.maybe_extract_owners(
-                    row, is_resource_row
+            except Exception as e:
+                raise ConfigurationError(
+                    f"Cannot read remote file {self.config.filename}, error:{e}"
                 )
 
-                domain: Optional[str] = (
-                    row["domain"]
-                    if row["domain"] and entity_type == DATASET_ENTITY_TYPE
-                    else None
-                )
+        for row in rows:
+            # We need the resource to move forward
+            if not row["resource"]:
+                continue
+
+            is_resource_row: bool = not row["subresource"]
+            entity_urn = row["resource"]
+            entity_type = Urn.create_from_string(row["resource"]).get_type()
+
+            term_associations: List[
+                GlossaryTermAssociationClass
+            ] = self.maybe_extract_glossary_terms(row)
+            tag_associations: List[TagAssociationClass] = self.maybe_extract_tags(row)
+            owners: List[OwnerClass] = self.maybe_extract_owners(row, is_resource_row)
+
+            domain: Optional[str] = (
+                row["domain"]
+                if row["domain"] and entity_type == DATASET_ENTITY_TYPE
+                else None
+            )
+            description: Optional[str] = (
+                row["description"]
+                if row["description"] and entity_type == DATASET_ENTITY_TYPE
+                else None
+            )
 
-                description: Optional[str] = (
-                    row["description"]
-                    if row["description"] and entity_type == DATASET_ENTITY_TYPE
-                    else None
+            if is_resource_row:
+                yield from self.get_resource_workunits(
+                    entity_urn=entity_urn,
+                    term_associations=term_associations,
+                    tag_associations=tag_associations,
+                    owners=owners,
+                    domain=domain,
+                    description=description,
                 )
-
-                if is_resource_row:
-                    for wu in self.get_resource_workunits(
+            elif entity_type == DATASET_ENTITY_TYPE:
+                # Only dataset sub-resources are currently supported.
+                # Add the row to the map from entity (dataset) to SubResource rows. We cannot emit work units for
+                # EditableSchemaMetadata until we parse the whole CSV due to read-modify-write issues.
+                self.editable_schema_metadata_map.setdefault(entity_urn, [])
+                self.editable_schema_metadata_map[entity_urn].append(
+                    SubResourceRow(
                         entity_urn=entity_urn,
+                        field_path=row["subresource"],
                         term_associations=term_associations,
                         tag_associations=tag_associations,
-                        owners=owners,
-                        domain=domain,
                         description=description,
-                    ):
-                        yield wu
-
-                # If this row is not applying changes at the resource level, modify the EditableSchemaMetadata map.
-                else:
-                    # Only dataset sub-resources are currently supported.
-                    if entity_type != DATASET_ENTITY_TYPE:
-                        continue
-
-                    field_path = row["subresource"]
-                    if entity_urn not in self.editable_schema_metadata_map:
-                        self.editable_schema_metadata_map[entity_urn] = []
-                    # Add the row to the map from entity (dataset) to SubResource rows. We cannot emit work units for
-                    # EditableSchemaMetadata until we parse the whole CSV due to read-modify-write issues.
-                    self.editable_schema_metadata_map[entity_urn].append(
-                        SubResourceRow(
-                            entity_urn=entity_urn,
-                            field_path=field_path,
-                            term_associations=term_associations,
-                            tag_associations=tag_associations,
-                            description=description,
-                            domain=domain,
-                        )
+                        domain=domain,
                     )
+                )
 
-        # Yield sub resource work units once the map has been fully populated.
-        for wu in self.get_sub_resource_work_units():
-            self.report.num_editable_schema_metadata_workunits_produced += 1
-            self.report.report_workunit(wu)
-            yield wu
+        yield from self.get_sub_resource_work_units()
 
     def get_report(self):
         return self.report
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/dbt/dbt_cloud.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/dbt/dbt_cloud.py`

 * *Files 2% similar despite different names*

```diff
@@ -397,26 +397,24 @@
             raw_code=raw_code,
             compiled_code=compiled_code,
             columns=columns,
             test_info=test_info,
             test_result=test_result,
         )
 
-    def _parse_into_dbt_column(self, column: Dict) -> DBTColumn:
+    def _parse_into_dbt_column(
+        self,
+        column: Dict,
+    ) -> DBTColumn:
         return DBTColumn(
             name=column["name"],
             comment=column.get("comment", ""),
             description=column["description"],
             index=column["index"],
             data_type=column["type"],
             meta=column["meta"],
             tags=column["tags"],
         )
 
     def get_external_url(self, node: DBTNode) -> Optional[str]:
         # TODO: Once dbt Cloud supports deep linking to specific files, we can use that.
-        return f"https://cloud.getdbt.com/next/accounts/{self.config.account_id}/projects/{self.config.project_id}/develop"
-
-    def get_platform_instance_id(self) -> str:
-        """The DBT project identifier is used as platform instance."""
-
-        return f"{self.platform}_{self.config.project_id}"
+        return f"https://cloud.getdbt.com/develop/{self.config.account_id}/projects/{self.config.project_id}"
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/dbt/dbt_common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/dbt/dbt_common.py`

 * *Files 2% similar despite different names*

```diff
@@ -15,34 +15,38 @@
     AllowDenyPattern,
     ConfigEnum,
     ConfigModel,
     ConfigurationError,
     LineageConfig,
 )
 from datahub.configuration.pydantic_field_deprecation import pydantic_field_deprecated
+from datahub.configuration.source_common import DatasetSourceConfigMixin
 from datahub.emitter import mce_builder
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.decorators import (
     SourceCapability,
     SupportStatus,
     capability,
     config_class,
     platform_name,
     support_status,
 )
 from datahub.ingestion.api.workunit import MetadataWorkUnit
+from datahub.ingestion.source.common.subtypes import DatasetSubTypes
 from datahub.ingestion.source.sql.sql_types import (
     BIGQUERY_TYPES_MAP,
     POSTGRES_TYPES_MAP,
     SNOWFLAKE_TYPES_MAP,
     SPARK_SQL_TYPES_MAP,
     TRINO_SQL_TYPES_MAP,
+    VERTICA_SQL_TYPES_MAP,
     resolve_postgres_modified_type,
     resolve_trino_modified_type,
+    resolve_vertica_modified_type,
 )
 from datahub.ingestion.source.state.entity_removal_state import GenericCheckpointState
 from datahub.ingestion.source.state.stale_entity_removal_handler import (
     StaleEntityRemovalHandler,
     StaleEntityRemovalSourceReport,
     StatefulStaleMetadataRemovalConfig,
 )
@@ -101,14 +105,15 @@
     TagAssociationClass,
     UpstreamLineageClass,
     ViewPropertiesClass,
 )
 from datahub.specific.dataset import DatasetPatchBuilder
 from datahub.utilities.mapping import Constants, OperationProcessor
 from datahub.utilities.source_helpers import (
+    auto_materialize_referenced_tags,
     auto_stale_entity_removal,
     auto_status_aspect,
 )
 from datahub.utilities.time import datetime_to_ts_millis
 
 logger = logging.getLogger(__name__)
 DBT_PLATFORM = "dbt"
@@ -190,15 +195,17 @@
         return allowed == EmitDirective.YES
 
     @property
     def can_emit_test_results(self) -> bool:
         return self.test_results == EmitDirective.YES
 
 
-class DBTCommonConfig(StatefulIngestionConfigBase, LineageConfig):
+class DBTCommonConfig(
+    StatefulIngestionConfigBase, DatasetSourceConfigMixin, LineageConfig
+):
     env: str = Field(
         default=mce_builder.DEFAULT_ENV,
         description="Environment to use in namespace when constructing URNs.",
     )
     target_platform: str = Field(
         description="The platform that dbt is loading onto. (e.g. bigquery / redshift / postgres etc.)",
     )
@@ -256,45 +263,46 @@
         default=True,
         description="When enabled, ownership info will be extracted from the dbt meta",
     )
     owner_extraction_pattern: Optional[str] = Field(
         default=None,
         description='Regex string to extract owner from the dbt node using the `(?P<name>...) syntax` of the [match object](https://docs.python.org/3/library/re.html#match-objects), where the group name must be `owner`. Examples: (1)`r"(?P<owner>(.*)): (\\w+) (\\w+)"` will extract `jdoe` as the owner from `"jdoe: John Doe"` (2) `r"@(?P<owner>(.*))"` will extract `alice` as the owner from `"@alice"`.',
     )
-    backcompat_skip_source_on_lineage_edge: bool = Field(
-        False,
-        description="[deprecated] Prior to version 0.8.41, lineage edges to sources were directed to the target platform node rather than the dbt source node. This contradicted the established pattern for other lineage edges to point to upstream dbt nodes. To revert lineage logic to this legacy approach, set this flag to true.",
-    )
-    _deprecate_skip_source_on_lineage_edge = pydantic_field_deprecated(
-        "backcompat_skip_source_on_lineage_edge"
-    )
 
-    incremental_lineage: bool = Field(
-        # Copied from LineageConfig, and changed the default.
-        default=False,
-        description="When enabled, emits lineage as incremental to existing lineage already in DataHub. When disabled, re-states lineage on each run.",
-    )
     include_env_in_assertion_guid: bool = Field(
         default=False,
         description="Prior to version 0.9.4.2, the assertion GUIDs did not include the environment. If you're using multiple dbt ingestion "
         "that are only distinguished by env, then you should set this flag to True.",
     )
     stateful_ingestion: Optional[StatefulStaleMetadataRemovalConfig] = pydantic.Field(
         default=None, description="DBT Stateful Ingestion Config."
     )
+    convert_column_urns_to_lowercase: bool = Field(
+        default=False,
+        description="When enabled, converts column URNs to lowercase to ensure cross-platform compatibility. "
+        "If `target_platform` is Snowflake, the default is True.",
+    )
 
     @validator("target_platform")
     def validate_target_platform_value(cls, target_platform: str) -> str:
         if target_platform.lower() == DBT_PLATFORM:
             raise ValueError(
                 "target_platform cannot be dbt. It should be the platform which dbt is operating on top of. For e.g "
                 "postgres."
             )
         return target_platform
 
+    @root_validator(pre=True)
+    def set_convert_column_urns_to_lowercase_default_for_snowflake(
+        cls, values: dict
+    ) -> dict:
+        if values.get("target_platform", "").lower() == "snowflake":
+            values.setdefault("convert_column_urns_to_lowercase", True)
+        return values
+
     @validator("write_semantics")
     def validate_write_semantics(cls, write_semantics: str) -> str:
         if write_semantics.lower() not in {"patch", "override"}:
             raise ValueError(
                 "write_semantics cannot be any other value than PATCH or OVERRIDE. Default value is PATCH. "
                 "For PATCH semantics consider using the datahub-rest sink or "
                 "provide a datahub_api: configuration on your ingestion recipe"
@@ -358,15 +366,15 @@
     language: Optional[str]
     raw_code: Optional[str]
 
     dbt_adapter: str
     dbt_name: str
     dbt_file_path: Optional[str]
 
-    node_type: str  # source, model
+    node_type: str  # source, model, snapshot, seed, test, etc
     max_loaded_at: Optional[datetime]
     materialization: Optional[str]  # table, view, ephemeral, incremental, snapshot
     # see https://docs.getdbt.com/reference/artifacts/manifest-json
     catalog_type: Optional[str]
 
     owner: Optional[str]
 
@@ -408,25 +416,24 @@
 
 def get_custom_properties(node: DBTNode) -> Dict[str, str]:
     # initialize custom properties to node's meta props
     # (dbt-native node properties)
     custom_properties = node.meta
 
     # additional node attributes to extract to custom properties
-    node_attributes = [
-        "node_type",
-        "materialization",
-        "dbt_file_path",
-        "catalog_type",
-        "language",
-    ]
-
-    for attribute in node_attributes:
-        node_attribute_value = getattr(node, attribute)
+    node_attributes = {
+        "node_type": node.node_type,
+        "materialization": node.materialization,
+        "dbt_file_path": node.dbt_file_path,
+        "catalog_type": node.catalog_type,
+        "language": node.language,
+        "dbt_unique_id": node.dbt_name,
+    }
 
+    for attribute, node_attribute_value in node_attributes.items():
         if node_attribute_value is not None:
             custom_properties[attribute] = node_attribute_value
 
     custom_properties = {key: str(value) for key, value in custom_properties.items()}
 
     return custom_properties
 
@@ -434,15 +441,14 @@
 def get_upstreams(
     upstreams: List[str],
     all_nodes: Dict[str, DBTNode],
     target_platform: str,
     target_platform_instance: Optional[str],
     environment: str,
     platform_instance: Optional[str],
-    legacy_skip_source_lineage: Optional[bool],
 ) -> List[str]:
     upstream_urns = []
 
     for upstream in sorted(upstreams):
         if upstream not in all_nodes:
             logger.debug(
                 f"Upstream node - {upstream} not found in all manifest entities."
@@ -453,18 +459,15 @@
 
         # This logic creates lineages among dbt nodes.
         platform_value = DBT_PLATFORM
         platform_instance_value = platform_instance
 
         materialized = upstream_manifest_node.materialization
 
-        resource_type = upstream_manifest_node.node_type
-        if materialized in {"view", "table", "incremental", "snapshot"} or (
-            resource_type == "source" and legacy_skip_source_lineage
-        ):
+        if materialized in {"view", "table", "incremental", "snapshot"}:
             # upstream urns point to the target platform
             platform_value = target_platform
             platform_instance_value = target_platform_instance
 
         upstream_urns.append(
             upstream_manifest_node.get_urn(
                 platform_value,
@@ -502,14 +505,15 @@
     "float8": NumberTypeClass,
     "struct": RecordType,
     **POSTGRES_TYPES_MAP,
     **SNOWFLAKE_TYPES_MAP,
     **BIGQUERY_TYPES_MAP,
     **SPARK_SQL_TYPES_MAP,
     **TRINO_SQL_TYPES_MAP,
+    **VERTICA_SQL_TYPES_MAP,
 }
 
 
 def get_column_type(
     report: DBTSourceReport, dataset_name: str, column_type: str, dbt_adapter: str
 ) -> SchemaFieldDataType:
     """
@@ -520,14 +524,16 @@
     if TypeClass is None:
         # resolve modified type
         if dbt_adapter == "trino":
             TypeClass = resolve_trino_modified_type(column_type)
         elif dbt_adapter == "postgres" or dbt_adapter == "redshift":
             # Redshift uses a variant of Postgres, so we can use the same logic.
             TypeClass = resolve_postgres_modified_type(column_type)
+        elif dbt_adapter == "vertica":
+            TypeClass = resolve_vertica_modified_type(column_type)
 
     # if still not found, report the warning
     if TypeClass is None:
         report.report_warning(
             dataset_name, f"unable to map type {column_type} to metadata schema"
         )
         TypeClass = NullTypeClass
@@ -728,15 +734,14 @@
             upstream_urns = get_upstreams(
                 upstreams=node.upstream_nodes,
                 all_nodes=all_nodes_map,
                 target_platform=self.config.target_platform,
                 target_platform_instance=self.config.target_platform_instance,
                 environment=self.config.env,
                 platform_instance=None,
-                legacy_skip_source_lineage=self.config.backcompat_skip_source_on_lineage_edge,
             )
 
             for upstream_urn in sorted(upstream_urns):
                 if self.config.entities_enabled.can_emit_node_type("test"):
                     wu = self._make_assertion_from_test(
                         custom_props,
                         node,
@@ -875,17 +880,19 @@
 
     @abstractmethod
     def load_nodes(self) -> Tuple[List[DBTNode], Dict[str, Optional[str]]]:
         # return dbt nodes + global custom properties
         raise NotImplementedError()
 
     def get_workunits(self) -> Iterable[MetadataWorkUnit]:
-        return auto_stale_entity_removal(
-            self.stale_entity_removal_handler,
-            auto_status_aspect(self.get_workunits_internal()),
+        return auto_materialize_referenced_tags(
+            auto_stale_entity_removal(
+                self.stale_entity_removal_handler,
+                auto_status_aspect(self.get_workunits_internal()),
+            )
         )
 
     def get_workunits_internal(self) -> Iterable[MetadataWorkUnit]:
         if self.config.write_semantics == "PATCH" and not self.ctx.graph:
             raise ConfigurationError(
                 "With PATCH semantics, dbt source requires a datahub_api to connect to. "
                 "Consider using the datahub-rest sink or provide a datahub_api: configuration on your ingestion recipe."
@@ -965,15 +972,14 @@
         action_processor_tag = OperationProcessor(
             self.config.query_tag_mapping,
             self.config.tag_prefix,
             "SOURCE_CONTROL",
             self.config.strip_user_ids_from_email,
         )
         for node in sorted(dbt_nodes, key=lambda n: n.dbt_name):
-
             is_primary_source = mce_platform == DBT_PLATFORM
             node_datahub_urn = node.get_urn(
                 mce_platform,
                 self.config.env,
                 mce_platform_instance,
             )
             if not self.config.entities_enabled.can_emit_node_type(node.node_type):
@@ -1253,16 +1259,20 @@
                     ]
                 )
 
             glossaryTerms = None
             if meta_aspects.get(Constants.ADD_TERM_OPERATION):
                 glossaryTerms = meta_aspects.get(Constants.ADD_TERM_OPERATION)
 
+            field_name = column.name
+            if self.config.convert_column_urns_to_lowercase:
+                field_name = field_name.lower()
+
             field = SchemaField(
-                fieldPath=column.name,
+                fieldPath=field_name,
                 nativeDataType=column.data_type,
                 type=get_column_type(
                     report, node.dbt_name, column.data_type, node.dbt_adapter
                 ),
                 description=description,
                 nullable=False,  # TODO: actually autodetect this
                 recursive=False,
@@ -1334,27 +1344,25 @@
         return sorted(tags_list)
 
     def _create_subType_wu(
         self, node: DBTNode, node_datahub_urn: str
     ) -> Optional[MetadataWorkUnit]:
         if not node.node_type:
             return None
-        subtypes: Optional[List[str]]
-        if node.node_type in {"model", "snapshot"}:
-            if node.materialization:
-                subtypes = [node.materialization, "view"]
-            else:
-                subtypes = ["model", "view"]
-        else:
-            subtypes = [node.node_type]
-        subtype_wu = MetadataChangeProposalWrapper(
+
+        subtypes: List[str] = [node.node_type.capitalize()]
+        if node.materialization == "table":
+            subtypes.append(DatasetSubTypes.TABLE)
+        elif node.materialization == "view":
+            subtypes.append(DatasetSubTypes.VIEW)
+
+        return MetadataChangeProposalWrapper(
             entityUrn=node_datahub_urn,
             aspect=SubTypesClass(typeNames=subtypes),
         ).as_workunit()
-        return subtype_wu
 
     def _create_lineage_aspect_for_dbt_node(
         self,
         node: DBTNode,
         all_nodes_map: Dict[str, DBTNode],
     ) -> Optional[UpstreamLineageClass]:
         """
@@ -1363,15 +1371,14 @@
         upstream_urns = get_upstreams(
             node.upstream_nodes,
             all_nodes_map,
             self.config.target_platform,
             self.config.target_platform_instance,
             self.config.env,
             self.config.platform_instance,
-            self.config.backcompat_skip_source_on_lineage_edge,
         )
 
         # if a node is of type source in dbt, its upstream lineage should have the corresponding table/view
         # from the platform. This code block is executed when we are generating entities of type "dbt".
         if node.node_type == "source":
             upstream_urns.append(
                 node.get_urn(
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/dbt/dbt_core.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/dbt/dbt_core.py`

 * *Files 3% similar despite different names*

```diff
@@ -5,15 +5,16 @@
 from typing import Any, Dict, List, Optional, Tuple
 from urllib.parse import urlparse
 
 import dateutil.parser
 import requests
 from pydantic import BaseModel, Field, validator
 
-from datahub.configuration.github import GitHubReference
+from datahub.configuration.git import GitReference
+from datahub.configuration.validate_field_rename import pydantic_renamed_field
 from datahub.ingestion.api.decorators import (
     SupportStatus,
     capability,
     config_class,
     platform_name,
     support_status,
 )
@@ -49,19 +50,21 @@
     )
 
     aws_connection: Optional[AwsConnectionConfig] = Field(
         default=None,
         description="When fetching manifest files from s3, configuration for aws connection details",
     )
 
-    github_info: Optional[GitHubReference] = Field(
+    git_info: Optional[GitReference] = Field(
         None,
-        description="Reference to your github location to enable easy navigation from DataHub to your dbt files.",
+        description="Reference to your git location to enable easy navigation from DataHub to your dbt files.",
     )
 
+    _github_info_deprecated = pydantic_renamed_field("github_info", "git_info")
+
     @property
     def s3_client(self):
         assert self.aws_connection
         return self.aws_connection.get_s3_client()
 
     @validator("aws_connection")
     def aws_connection_needed_if_s3_uris_present(
@@ -83,15 +86,17 @@
             raise ValueError(
                 f"Please provide aws_connection configuration, since s3 uris have been provided in fields {uri_containing_fields}"
             )
         return aws_connection
 
 
 def get_columns(
-    catalog_node: dict, manifest_node: dict, tag_prefix: str
+    catalog_node: dict,
+    manifest_node: dict,
+    tag_prefix: str,
 ) -> List[DBTColumn]:
     columns = []
 
     catalog_columns = catalog_node["columns"]
     manifest_columns = manifest_node.get("columns", {})
 
     manifest_columns_lower = {k.lower(): v for k, v in manifest_columns.items()}
@@ -155,15 +160,15 @@
 
         materialization = None
         if "materialized" in manifest_node.get("config", {}):
             # It's a model
             materialization = manifest_node["config"]["materialized"]
 
         upstream_nodes = []
-        if "depends_on" in manifest_node:
+        if "depends_on" in manifest_node and "nodes" in manifest_node["depends_on"]:
             upstream_nodes = manifest_node["depends_on"]["nodes"]
 
         # It's a source
         catalog_node = all_catalog_entities.get(key)
         catalog_type = None
 
         if catalog_node is None:
@@ -249,15 +254,19 @@
         if dbtNode.materialization not in [
             "ephemeral",
             "test",
         ]:
             logger.debug(f"Loading schema info for {dbtNode.dbt_name}")
             if catalog_node is not None:
                 # We already have done the reporting for catalog_node being None above.
-                dbtNode.columns = get_columns(catalog_node, manifest_node, tag_prefix)
+                dbtNode.columns = get_columns(
+                    catalog_node,
+                    manifest_node,
+                    tag_prefix,
+                )
 
         else:
             dbtNode.columns = []
 
         dbt_entities.append(dbtNode)
 
     return dbt_entities
@@ -472,23 +481,10 @@
                 self.load_file_as_json(self.config.test_results_path),
                 all_nodes,
             )
 
         return all_nodes, additional_custom_props
 
     def get_external_url(self, node: DBTNode) -> Optional[str]:
-        if self.config.github_info and node.dbt_file_path:
-            return self.config.github_info.get_url_for_file_path(node.dbt_file_path)
+        if self.config.git_info and node.dbt_file_path:
+            return self.config.git_info.get_url_for_file_path(node.dbt_file_path)
         return None
-
-    def get_platform_instance_id(self) -> str:
-        """The DBT project identifier is used as platform instance."""
-
-        project_id = (
-            self.load_file_as_json(self.config.manifest_path)
-            .get("metadata", {})
-            .get("project_id")
-        )
-        if project_id is None:
-            raise ValueError("DBT project identifier is not found in manifest")
-
-        return f"{self.platform}_{project_id}"
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/delta_lake/config.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/delta_lake/config.py`

 * *Files 6% similar despite different names*

```diff
@@ -4,16 +4,16 @@
 import pydantic
 from cached_property import cached_property
 from pydantic import Field
 
 from datahub.configuration.common import AllowDenyPattern
 from datahub.configuration.source_common import (
     ConfigModel,
-    EnvBasedSourceConfigBase,
-    PlatformSourceConfigBase,
+    EnvConfigMixin,
+    PlatformInstanceConfigMixin,
 )
 from datahub.ingestion.source.aws.aws_common import AwsConnectionConfig
 from datahub.ingestion.source.aws.s3_util import is_s3_uri
 
 # hide annoying debug errors from py4j
 logging.getLogger("py4j").setLevel(logging.ERROR)
 logger: logging.Logger = logging.getLogger(__name__)
@@ -31,15 +31,15 @@
     # Whether or not to create in datahub from the s3 object
     use_s3_object_tags: Optional[bool] = Field(
         False,
         description="# Whether or not to create tags in datahub from the s3 object",
     )
 
 
-class DeltaLakeSourceConfig(PlatformSourceConfigBase, EnvBasedSourceConfigBase):
+class DeltaLakeSourceConfig(PlatformInstanceConfigMixin, EnvConfigMixin):
     base_path: str = Field(
         description="Path to table (s3 or local file system). If path is not a delta table path "
         "then all subfolders will be scanned to detect and ingest delta tables."
     )
     relative_path: Optional[str] = Field(
         default=None,
         description="If set, delta-tables will be searched at location "
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/delta_lake/delta_lake_utils.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/delta_lake/delta_lake_utils.py`

 * *Files 11% similar despite different names*

```diff
@@ -15,16 +15,17 @@
         if delta_lake_config.is_s3:
             if (
                 delta_lake_config.s3 is not None
                 and delta_lake_config.s3.aws_config is not None
             ):
                 creds = delta_lake_config.s3.aws_config.get_credentials()
                 opts = {
-                    "AWS_ACCESS_KEY_ID": creds.get("aws_access_key_id", ""),
-                    "AWS_SECRET_ACCESS_KEY": creds.get("aws_secret_access_key", ""),
+                    "AWS_ACCESS_KEY_ID": creds.get("aws_access_key_id") or "",
+                    "AWS_SECRET_ACCESS_KEY": creds.get("aws_secret_access_key") or "",
+                    "AWS_SESSION_TOKEN": creds.get("aws_session_token") or "",
                     # Allow http connections, this is required for minio
                     "AWS_STORAGE_ALLOW_HTTP": "true",
                 }
                 if delta_lake_config.s3.aws_config.aws_region:
                     opts["AWS_REGION"] = delta_lake_config.s3.aws_config.aws_region
                 if delta_lake_config.s3.aws_config.aws_endpoint_url:
                     opts[
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/delta_lake/source.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/delta_lake/source.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/demo_data.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/demo_data.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,33 +1,37 @@
+from typing import Iterable
+
 from datahub.configuration.common import ConfigModel
-from datahub.ingestion.api.common import PipelineContext
+from datahub.ingestion.api.common import PipelineContext, WorkUnit
 from datahub.ingestion.api.decorators import (
     SupportStatus,
     config_class,
     platform_name,
     support_status,
 )
+from datahub.ingestion.api.source import Source, SourceReport
 from datahub.ingestion.source.file import FileSourceConfig, GenericFileSource
 from datahub.utilities.sample_data import download_sample_data
 
 
 class DemoDataConfig(ConfigModel):
     # The demo data source does not accept any configuration.
     pass
 
 
 @platform_name("Demo Data")
-@config_class(ConfigModel)
+@config_class(DemoDataConfig)
 @support_status(SupportStatus.UNKNOWN)
-class DemoDataSource(GenericFileSource):
+class DemoDataSource(Source):
     """
     This source loads sample data into DataHub. It is intended for demo and testing purposes only.
     """
 
     def __init__(self, ctx: PipelineContext, config: DemoDataConfig):
-        file_config = FileSourceConfig(path=download_sample_data())
-        super().__init__(ctx, file_config)
+        file_config = FileSourceConfig(path=str(download_sample_data()))
+        self.file_source = GenericFileSource(ctx, file_config)
+
+    def get_workunits(self) -> Iterable[WorkUnit]:
+        yield from self.file_source.get_workunits()
 
-    @classmethod
-    def create(cls, config_dict, ctx):
-        config = DemoDataConfig.parse_obj(config_dict or {})
-        return cls(ctx, config)
+    def get_report(self) -> SourceReport:
+        return self.file_source.get_report()
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/elastic_search.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/elastic_search.py`

 * *Files 1% similar despite different names*

```diff
@@ -6,15 +6,18 @@
 from typing import Any, Dict, Generator, Iterable, List, Optional, Tuple, Type
 
 from elasticsearch import Elasticsearch
 from pydantic import validator
 from pydantic.fields import Field
 
 from datahub.configuration.common import AllowDenyPattern
-from datahub.configuration.source_common import DatasetSourceConfigBase
+from datahub.configuration.source_common import (
+    EnvConfigMixin,
+    PlatformInstanceConfigMixin,
+)
 from datahub.configuration.validate_host_port import validate_host_port
 from datahub.emitter.mce_builder import (
     make_data_platform_urn,
     make_dataplatform_instance_urn,
     make_dataset_urn_with_platform_instance,
 )
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
@@ -25,14 +28,15 @@
     capability,
     config_class,
     platform_name,
     support_status,
 )
 from datahub.ingestion.api.source import Source, SourceReport
 from datahub.ingestion.api.workunit import MetadataWorkUnit
+from datahub.ingestion.source.common.subtypes import DatasetSubTypes
 from datahub.metadata.com.linkedin.pegasus2avro.common import StatusClass
 from datahub.metadata.com.linkedin.pegasus2avro.schema import (
     SchemaField,
     SchemaFieldDataType,
     SchemaMetadata,
 )
 from datahub.metadata.schema_classes import (
@@ -182,15 +186,15 @@
     def report_index_scanned(self, index: str) -> None:
         self.index_scanned += 1
 
     def report_dropped(self, index: str) -> None:
         self.filtered.append(index)
 
 
-class ElasticsearchSourceConfig(DatasetSourceConfigBase):
+class ElasticsearchSourceConfig(PlatformInstanceConfigMixin, EnvConfigMixin):
     host: str = Field(
         default="localhost:9200", description="The elastic search host URI."
     )
     username: Optional[str] = Field(
         default=None, description="The username credential."
     )
     password: Optional[str] = Field(
@@ -403,19 +407,19 @@
         )
 
         # 3. Construct and emit subtype
         yield MetadataChangeProposalWrapper(
             entityUrn=dataset_urn,
             aspect=SubTypesClass(
                 typeNames=[
-                    "Index Template"
+                    DatasetSubTypes.ELASTIC_INDEX_TEMPLATE
                     if not is_index
-                    else "Index"
+                    else DatasetSubTypes.ELASTIC_INDEX
                     if not data_stream
-                    else "Datastream"
+                    else DatasetSubTypes.ELASTIC_DATASTREAM
                 ]
             ),
         )
 
         # 4. Construct and emit properties if needed. Will attempt to get the following properties
         custom_properties: Dict[str, str] = {}
         # 4.1 aliases
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/feast.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/feast.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/file.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/file.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,22 +1,26 @@
 import datetime
 import json
 import logging
 import os.path
 import pathlib
+from collections import defaultdict
 from dataclasses import dataclass, field
 from enum import auto
 from io import BufferedReader
 from typing import Any, Dict, Iterable, Iterator, List, Optional, Tuple, Union
+from urllib import parse
 
 import ijson
+import requests
 from pydantic import validator
 from pydantic.fields import Field
 
-from datahub.configuration.common import ConfigEnum, ConfigModel
+from datahub.configuration.common import ConfigEnum, ConfigModel, ConfigurationError
+from datahub.configuration.pydantic_field_deprecation import pydantic_field_deprecated
 from datahub.configuration.validate_field_rename import pydantic_renamed_field
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.decorators import (
     SupportStatus,
     config_class,
     platform_name,
@@ -30,30 +34,32 @@
 )
 from datahub.ingestion.api.workunit import MetadataWorkUnit, UsageStatsWorkUnit
 from datahub.metadata.com.linkedin.pegasus2avro.mxe import (
     MetadataChangeEvent,
     MetadataChangeProposal,
 )
 from datahub.metadata.schema_classes import UsageAggregationClass
+from datahub.utilities.source_helpers import auto_workunit_reporter
 
 logger = logging.getLogger(__name__)
 
 
 class FileReadMode(ConfigEnum):
     STREAM = auto()
     BATCH = auto()
     AUTO = auto()
 
 
 class FileSourceConfig(ConfigModel):
-    filename: Optional[str] = Field(
-        None, description="[deprecated in favor of `path`] The file to ingest."
+    _filename = pydantic_field_deprecated(
+        "filename",
+        message="filename is deprecated. Use path instead.",
     )
-    path: pathlib.Path = Field(
-        description="Path to folder or file to ingest. If pointed to a folder, all files with extension {file_extension} (default json) within that folder will be processed."
+    path: str = Field(
+        description="File path to folder or file to ingest, or URL to a remote file. If pointed to a folder, all files with extension {file_extension} (default json) within that folder will be processed."
     )
     file_extension: str = Field(
         ".json",
         description="When providing a folder to use to read files, set this field to control file extensions that you want the source to process. * is a special value that means process every file regardless of extension",
     )
     read_mode: FileReadMode = FileReadMode.AUTO
     aspect: Optional[str] = Field(
@@ -96,16 +102,16 @@
     current_file_size: Optional[int] = None
     current_file_num_elements: Optional[int] = None
     current_file_bytes_read: Optional[int] = None
     current_file_elements_read: Optional[int] = None
     total_parse_time_in_seconds: float = 0
     total_count_time_in_seconds: float = 0
     total_deserialize_time_in_seconds: float = 0
-    aspect_counts: Dict[str, int] = field(default_factory=dict)
-    entity_type_counts: Dict[str, int] = field(default_factory=dict)
+    aspect_counts: Dict[str, int] = field(default_factory=lambda: defaultdict(int))
+    entity_type_counts: Dict[str, int] = field(default_factory=lambda: defaultdict(int))
 
     def add_deserialize_time(self, delta: datetime.timedelta) -> None:
         self.total_deserialize_time_in_seconds += round(delta.total_seconds(), 2)
 
     def add_parse_time(self, delta: datetime.timedelta) -> None:
         self.total_parse_time_in_seconds += round(delta.total_seconds(), 2)
 
@@ -174,121 +180,144 @@
 
     @classmethod
     def create(cls, config_dict, ctx):
         config = FileSourceConfig.parse_obj(config_dict)
         return cls(ctx, config)
 
     def get_filenames(self) -> Iterable[str]:
-        if self.config.path.is_file():
+        path_parsed = parse.urlparse(str(self.config.path))
+        if path_parsed.scheme in ("file", ""):
+            path = pathlib.Path(self.config.path)
+            if path.is_file():
+                self.report.total_num_files = 1
+                return [str(path)]
+            elif path.is_dir():
+                files_and_stats = [
+                    (str(x), os.path.getsize(x))
+                    for x in path.glob(f"*{self.config.file_extension}")
+                    if x.is_file()
+                ]
+                self.report.total_num_files = len(files_and_stats)
+                self.report.total_bytes_on_disk = sum([y for (x, y) in files_and_stats])
+                return [x for (x, y) in files_and_stats]
+            else:
+                raise Exception(f"Failed to process {path}")
+        else:
             self.report.total_num_files = 1
             return [str(self.config.path)]
-        elif self.config.path.is_dir():
-            files_and_stats = [
-                (str(x), os.path.getsize(x))
-                for x in list(self.config.path.glob(f"*{self.config.file_extension}"))
-                if x.is_file()
-            ]
-            self.report.total_num_files = len(files_and_stats)
-            self.report.total_bytes_on_disk = sum([y for (x, y) in files_and_stats])
-            return [x for (x, y) in files_and_stats]
-        raise Exception(f"Failed to process {self.config.path}")
 
     def get_workunits(self) -> Iterable[Union[MetadataWorkUnit, UsageStatsWorkUnit]]:
+        return auto_workunit_reporter(self.report, self.get_workunits_internal())
+
+    def get_workunits_internal(
+        self,
+    ) -> Iterable[Union[MetadataWorkUnit, UsageStatsWorkUnit]]:
         for f in self.get_filenames():
             for i, obj in self.iterate_generic_file(f):
                 id = f"file://{f}:{i}"
-                wu: Union[MetadataWorkUnit, UsageStatsWorkUnit]
                 if isinstance(obj, UsageAggregationClass):
-                    wu = UsageStatsWorkUnit(id, obj)
+                    yield UsageStatsWorkUnit(id, obj)
                 elif isinstance(
                     obj, (MetadataChangeProposalWrapper, MetadataChangeProposal)
                 ):
-                    self.report.entity_type_counts[obj.entityType] = (
-                        self.report.entity_type_counts.get(obj.entityType, 0) + 1
-                    )
+                    self.report.entity_type_counts[obj.entityType] += 1
                     if obj.aspectName is not None:
                         cur_aspect_name = str(obj.aspectName)
-                        self.report.aspect_counts[cur_aspect_name] = (
-                            self.report.aspect_counts.get(cur_aspect_name, 0) + 1
-                        )
+                        self.report.aspect_counts[cur_aspect_name] += 1
                         if (
                             self.config.aspect is not None
                             and cur_aspect_name != self.config.aspect
                         ):
                             continue
 
                     if isinstance(obj, MetadataChangeProposalWrapper):
-                        wu = MetadataWorkUnit(id, mcp=obj)
+                        yield MetadataWorkUnit(id, mcp=obj)
                     else:
-                        wu = MetadataWorkUnit(id, mcp_raw=obj)
+                        yield MetadataWorkUnit(id, mcp_raw=obj)
                 else:
-                    wu = MetadataWorkUnit(id, mce=obj)
-                self.report.report_workunit(wu)
-                yield wu
+                    yield MetadataWorkUnit(id, mce=obj)
 
     def get_report(self):
         return self.report
 
     def close(self):
         if self.fp:
             self.fp.close()
         super().close()
 
     def _iterate_file(self, path: str) -> Iterable[Tuple[int, Any]]:
         self.report.current_file_name = path
-        self.report.current_file_size = os.path.getsize(path)
-        if self.config.read_mode == FileReadMode.AUTO:
-            file_read_mode = (
-                FileReadMode.BATCH
-                if self.report.current_file_size
-                < self.config._minsize_for_streaming_mode_in_bytes
-                else FileReadMode.STREAM
-            )
-            logger.info(f"Reading file {path} in {file_read_mode} mode")
-        else:
-            file_read_mode = self.config.read_mode
-
-        if file_read_mode == FileReadMode.BATCH:
-            with open(path, "r") as f:
+        path_parsed = parse.urlparse(path)
+        if path_parsed.scheme not in ("file", ""):  # A remote file
+            try:
+                response = requests.get(path)
                 parse_start_time = datetime.datetime.now()
-                obj_list = json.load(f)
-                parse_end_time = datetime.datetime.now()
-                self.report.add_parse_time(parse_end_time - parse_start_time)
-            if not isinstance(obj_list, list):
-                obj_list = [obj_list]
-            count_start_time = datetime.datetime.now()
-            self.report.current_file_num_elements = len(obj_list)
-            self.report.add_count_time(datetime.datetime.now() - count_start_time)
+                data = response.json()
+            except Exception as e:
+                raise ConfigurationError(f"Cannot read remote file {path}, error:{e}")
+            if not isinstance(data, list):
+                data = [data]
+            parse_end_time = datetime.datetime.now()
+            self.report.add_parse_time(parse_end_time - parse_start_time)
+            self.report.current_file_size = len(response.content)
             self.report.current_file_elements_read = 0
-            for i, obj in enumerate(obj_list):
+            for i, obj in enumerate(data):
                 yield i, obj
                 self.report.current_file_elements_read += 1
         else:
-            self.fp = open(path, "rb")
-            if self.config.count_all_before_starting:
+            self.report.current_file_size = os.path.getsize(path)
+            if self.config.read_mode == FileReadMode.AUTO:
+                file_read_mode = (
+                    FileReadMode.BATCH
+                    if self.report.current_file_size
+                    < self.config._minsize_for_streaming_mode_in_bytes
+                    else FileReadMode.STREAM
+                )
+                logger.info(f"Reading file {path} in {file_read_mode} mode")
+            else:
+                file_read_mode = self.config.read_mode
+
+            if file_read_mode == FileReadMode.BATCH:
+                with open(path, "r") as f:
+                    parse_start_time = datetime.datetime.now()
+                    obj_list = json.load(f)
+                    parse_end_time = datetime.datetime.now()
+                    self.report.add_parse_time(parse_end_time - parse_start_time)
+                if not isinstance(obj_list, list):
+                    obj_list = [obj_list]
                 count_start_time = datetime.datetime.now()
+                self.report.current_file_num_elements = len(obj_list)
+                self.report.add_count_time(datetime.datetime.now() - count_start_time)
+                self.report.current_file_elements_read = 0
+                for i, obj in enumerate(obj_list):
+                    yield i, obj
+                    self.report.current_file_elements_read += 1
+            else:
+                self.fp = open(path, "rb")
+                if self.config.count_all_before_starting:
+                    count_start_time = datetime.datetime.now()
+                    parse_stream = ijson.parse(self.fp, use_float=True)
+                    total_elements = 0
+                    for row in ijson.items(parse_stream, "item", use_float=True):
+                        total_elements += 1
+                    count_end_time = datetime.datetime.now()
+                    self.report.add_count_time(count_end_time - count_start_time)
+                    self.report.current_file_num_elements = total_elements
+                self.report.current_file_elements_read = 0
+                self.fp.seek(0)
+                parse_start_time = datetime.datetime.now()
                 parse_stream = ijson.parse(self.fp, use_float=True)
-                total_elements = 0
+                rows_yielded = 0
                 for row in ijson.items(parse_stream, "item", use_float=True):
-                    total_elements += 1
-                count_end_time = datetime.datetime.now()
-                self.report.add_count_time(count_end_time - count_start_time)
-                self.report.current_file_num_elements = total_elements
-            self.report.current_file_elements_read = 0
-            self.fp.seek(0)
-            parse_start_time = datetime.datetime.now()
-            parse_stream = ijson.parse(self.fp, use_float=True)
-            rows_yielded = 0
-            for row in ijson.items(parse_stream, "item", use_float=True):
-                parse_end_time = datetime.datetime.now()
-                self.report.add_parse_time(parse_end_time - parse_start_time)
-                rows_yielded += 1
-                self.report.current_file_elements_read += 1
-                yield rows_yielded, row
-                parse_start_time = datetime.datetime.now()
+                    parse_end_time = datetime.datetime.now()
+                    self.report.add_parse_time(parse_end_time - parse_start_time)
+                    rows_yielded += 1
+                    self.report.current_file_elements_read += 1
+                    yield rows_yielded, row
+                    parse_start_time = datetime.datetime.now()
 
         self.report.files_completed.append(path)
         self.report.num_files_completed += 1
         self.report.total_bytes_read_completed_files += self.report.current_file_size
         self.report.reset_current_file_stats()
 
     def iterate_mce_file(self, path: str) -> Iterator[MetadataChangeEvent]:
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/ge_data_profiler.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/ge_data_profiler.py`

 * *Files 2% similar despite different names*

```diff
@@ -120,15 +120,15 @@
         )
         return convert_to_json_serializable(element_values.fetchone()[0])
     elif self.engine.dialect.name.lower() == "bigquery":
         element_values = self.engine.execute(
             sa.select(
                 [
                     sa.text(  # type:ignore
-                        f"APPROX_COUNT_DISTINCT(`{sa.column(column)}`)"
+                        f"APPROX_COUNT_DISTINCT(`{column}`)"
                     )
                 ]
             ).select_from(self._table)
         )
         return convert_to_json_serializable(element_values.fetchone()[0])
     elif self.engine.dialect.name.lower() == "snowflake":
         element_values = self.engine.execute(
@@ -338,15 +338,32 @@
 
         column_spec.unique_count = unique_count
 
         column_spec.cardinality = convert_to_cardinality(unique_count, pct_unique)
 
     @_run_with_query_combiner
     def _get_dataset_rows(self, dataset_profile: DatasetProfileClass) -> None:
-        dataset_profile.rowCount = self.dataset.get_row_count()
+        if (
+            self.config.profile_table_row_count_estimate_only
+            and self.dataset.engine.dialect.name.lower() == "postgresql"
+        ):
+            schema_name = self.dataset_name.split(".")[1]
+            table_name = self.dataset_name.split(".")[2]
+            logger.debug(
+                f"Getting estimated rowcounts for table:{self.dataset_name}, schema:{schema_name}, table:{table_name}"
+            )
+            dataset_profile.rowCount = int(
+                self.dataset.engine.execute(
+                    sa.text(
+                        f"SELECT c.reltuples AS estimate FROM pg_class c JOIN pg_namespace n ON n.oid = c.relnamespace WHERE  c.relname = '{table_name}' AND n.nspname = '{schema_name}'"
+                    )
+                ).scalar()
+            )
+        else:
+            dataset_profile.rowCount = self.dataset.get_row_count()
 
     @_run_with_query_combiner
     def _get_dataset_column_min(
         self, column_profile: DatasetFieldProfileClass, column: str
     ) -> None:
         if self.config.include_field_min_value:
             column_profile.min = str(self.dataset.get_column_min(column))
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/ge_profiling_config.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/ge_profiling_config.py`

 * *Files 1% similar despite different names*

```diff
@@ -111,14 +111,20 @@
     )
 
     profile_table_row_limit: Optional[int] = Field(
         default=5000000,
         description="Profile tables only if their row count is less then specified count. If set to `null`, no limit on the row count of tables to profile. Supported only in `snowflake` and `BigQuery`",
     )
 
+    profile_table_row_count_estimate_only: bool = Field(
+        default=False,
+        description="Use an approximate query for row count. This will be much faster but slightly "
+        "less accurate. Only supported for Postgres. ",
+    )
+
     # The default of (5 * cpu_count) is adopted from the default max_workers
     # parameter of ThreadPoolExecutor. Given that profiling is often an I/O-bound
     # task, it may make sense to increase this default value in the future.
     # https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor
     max_workers: int = Field(
         default=5 * (os.cpu_count() or 4),
         description="Number of worker threads to use for profiling. Set to 1 to disable.",
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/git/git_import.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/git/git_import.py`

 * *Files 5% similar despite different names*

```diff
@@ -48,15 +48,15 @@
             git_ssh_cmd += f" -i {git_ssh_identity_file}"
         if self.skip_known_host_verification:
             # Without this, the ssh command will prompt for confirmation of the host key.
             # See https://stackoverflow.com/a/28527476/5004662.
             git_ssh_cmd += (
                 " -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no"
             )
-        logger.debug("ssh_command=%s", git_ssh_cmd)
+        logger.debug(f"ssh_command={git_ssh_cmd}")
         logger.info(f" Cloning repo '{repo_url}', this can take some time...")
         self.last_repo_cloned = git.Repo.clone_from(
             repo_url,
             checkout_dir,
             env=dict(GIT_SSH_COMMAND=git_ssh_cmd),
         )
         logger.info(" Cloning complete!")
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/glue_profiling_config.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/glue_profiling_config.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/iceberg/iceberg.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/iceberg/iceberg.py`

 * *Files 2% similar despite different names*

```diff
@@ -314,18 +314,14 @@
                     "name": column.name,
                     "type": _parse_datatype(column.type, column.is_optional),
                     "doc": column.doc,
                 }
             ],
         }
 
-    def get_platform_instance_id(self) -> str:
-        assert self.config.platform_instance is not None
-        return self.config.platform_instance
-
     def get_report(self) -> SourceReport:
         return self.report
 
 
 def _parse_datatype(type: IcebergTypes.Type, nullable: bool = False) -> Dict[str, Any]:
     # Check for complex types: struct, list, map
     if type.is_list_type():
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/iceberg/iceberg_common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/iceberg/iceberg_common.py`

 * *Files 5% similar despite different names*

```diff
@@ -9,14 +9,15 @@
 from pydantic import Field, root_validator
 
 from datahub.configuration.common import (
     AllowDenyPattern,
     ConfigModel,
     ConfigurationError,
 )
+from datahub.configuration.source_common import DatasetSourceConfigMixin
 from datahub.ingestion.source.azure.azure_common import AdlsSourceConfig
 from datahub.ingestion.source.state.stale_entity_removal_handler import (
     StaleEntityRemovalSourceReport,
     StatefulStaleMetadataRemovalConfig,
 )
 from datahub.ingestion.source.state.stateful_ingestion_base import (
     StatefulIngestionConfigBase,
@@ -46,15 +47,15 @@
     # include_field_stddev_value: bool = True
     # include_field_quantiles: bool = False
     # include_field_distinct_value_frequencies: bool = False
     # include_field_histogram: bool = False
     # include_field_sample_values: bool = True
 
 
-class IcebergSourceConfig(StatefulIngestionConfigBase):
+class IcebergSourceConfig(StatefulIngestionConfigBase, DatasetSourceConfigMixin):
     # Override the stateful_ingestion config param with the Iceberg custom stateful ingestion config in the IcebergSourceConfig
     stateful_ingestion: Optional[StatefulStaleMetadataRemovalConfig] = pydantic.Field(
         default=None, description="Iceberg Stateful Ingestion Config."
     )
     adls: Optional[AdlsSourceConfig] = Field(
         default=None,
         description="[Azure Data Lake Storage](https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction) to crawl for Iceberg tables.  This is one filesystem type supported by this source and **only one can be configured**.",
@@ -77,27 +78,14 @@
     )
     group_ownership_property: Optional[str] = Field(
         default=None,
         description="Iceberg table property to look for a `CorpGroup` owner.  Can only hold a single group value.  If property has no value, no owner information will be emitted.",
     )
     profiling: IcebergProfilingConfig = IcebergProfilingConfig()
 
-    @pydantic.root_validator
-    def validate_platform_instance(cls: "IcebergSourceConfig", values: Dict) -> Dict:
-        stateful_ingestion = values.get("stateful_ingestion")
-        if (
-            stateful_ingestion
-            and stateful_ingestion.enabled
-            and not values.get("platform_instance")
-        ):
-            raise ConfigurationError(
-                "Enabling Iceberg stateful ingestion requires to specify a platform instance."
-            )
-        return values
-
     @root_validator()
     def _ensure_one_filesystem_is_configured(
         cls: "IcebergSourceConfig", values: Dict
     ) -> Dict:
         if values.get("adls") and values.get("localfs"):
             raise ConfigurationError(
                 "Only one filesystem can be configured: adls or localfs"
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/iceberg/iceberg_profiler.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/iceberg/iceberg_profiler.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/identity/azure_ad.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/identity/azure_ad.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,50 +1,68 @@
 import json
 import logging
 import re
 import urllib
 from collections import defaultdict
 from dataclasses import dataclass, field
-from typing import Any, Dict, Generator, Iterable, List
+from typing import Any, Dict, Generator, Iterable, List, Optional
 
 import click
 import requests
 from pydantic.fields import Field
 
-from datahub.configuration import ConfigModel
 from datahub.configuration.common import AllowDenyPattern
+from datahub.configuration.source_common import DatasetSourceConfigMixin
 from datahub.emitter.mce_builder import make_group_urn, make_user_urn
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.decorators import (  # SourceCapability,; capability,
     SupportStatus,
+    capability,
     config_class,
     platform_name,
     support_status,
 )
-from datahub.ingestion.api.source import Source, SourceReport
+from datahub.ingestion.api.source import SourceCapability, SourceReport
 from datahub.ingestion.api.workunit import MetadataWorkUnit
+from datahub.ingestion.source.state.sql_common_state import (
+    BaseSQLAlchemyCheckpointState,
+)
+from datahub.ingestion.source.state.stale_entity_removal_handler import (
+    StaleEntityRemovalHandler,
+    StaleEntityRemovalSourceReport,
+    StatefulStaleMetadataRemovalConfig,
+)
+from datahub.ingestion.source.state.stateful_ingestion_base import (
+    StatefulIngestionConfigBase,
+    StatefulIngestionSourceBase,
+)
 from datahub.metadata.com.linkedin.pegasus2avro.metadata.snapshot import (
     CorpGroupSnapshot,
     CorpUserSnapshot,
 )
 from datahub.metadata.com.linkedin.pegasus2avro.mxe import MetadataChangeEvent
 from datahub.metadata.schema_classes import (
     CorpGroupInfoClass,
     CorpUserInfoClass,
     GroupMembershipClass,
     OriginClass,
     OriginTypeClass,
     StatusClass,
 )
+from datahub.utilities.source_helpers import (
+    auto_stale_entity_removal,
+    auto_status_aspect,
+    auto_workunit_reporter,
+)
 
 logger = logging.getLogger(__name__)
 
 
-class AzureADConfig(ConfigModel):
+class AzureADConfig(StatefulIngestionConfigBase, DatasetSourceConfigMixin):
     """Config to create a token and connect to Azure AD instance"""
 
     # Required
     client_id: str = Field(
         description="Application ID. Found in your app registration on Azure AD Portal"
     )
     tenant_id: str = Field(
@@ -129,17 +147,22 @@
         description="Whether workunit ID's for groups should be masked to avoid leaking sensitive information.",
     )
     mask_user_id: bool = Field(
         True,
         description="Whether workunit ID's for users should be masked to avoid leaking sensitive information.",
     )
 
+    # Configuration for stateful ingestion
+    stateful_ingestion: Optional[StatefulStaleMetadataRemovalConfig] = Field(
+        default=None, description="Azure AD Stateful Ingestion Config."
+    )
+
 
 @dataclass
-class AzureADSourceReport(SourceReport):
+class AzureADSourceReport(StaleEntityRemovalSourceReport):
     filtered: List[str] = field(default_factory=list)
     filtered_tracking: bool = field(default=True, repr=False)
     filtered_count: int = field(default=0)
 
     def report_filtered(self, name: str) -> None:
         self.filtered_count += 1
         if self.filtered_tracking:
@@ -148,15 +171,18 @@
 
 # Source that extracts Azure AD users, groups and group memberships using Microsoft Graph REST API
 
 
 @platform_name("Azure AD")
 @config_class(AzureADConfig)
 @support_status(SupportStatus.CERTIFIED)
-class AzureADSource(Source):
+@capability(
+    SourceCapability.DELETION_DETECTION, "Optionally enabled via stateful_ingestion"
+)
+class AzureADSource(StatefulIngestionSourceBase):
     """
     This plugin extracts the following:
 
     - Users
     - Groups
     - Group Membership
 
@@ -223,21 +249,29 @@
 
     - `Group.Read.All`
     - `GroupMember.Read.All`
     - `User.Read.All`
 
     """
 
+    config: AzureADConfig
+    report: AzureADSourceReport
+    token_data: dict
+    token: str
+    selected_azure_ad_groups: list
+    azure_ad_groups_users: list
+    stale_entity_removal_handler: StaleEntityRemovalHandler
+
     @classmethod
     def create(cls, config_dict, ctx):
         config = AzureADConfig.parse_obj(config_dict)
         return cls(config, ctx)
 
     def __init__(self, config: AzureADConfig, ctx: PipelineContext):
-        super().__init__(ctx)
+        super(AzureADSource, self).__init__(config, ctx)
         self.config = config
         self.report = AzureADSourceReport(
             filtered_tracking=self.config.filtered_tracking
         )
         self.token_data = {
             "grant_type": "client_credentials",
             "client_id": self.config.client_id,
@@ -245,14 +279,22 @@
             "client_secret": self.config.client_secret,
             "resource": "https://graph.microsoft.com",
             "scope": "https://graph.microsoft.com/.default",
         }
         self.token = self.get_token()
         self.selected_azure_ad_groups: list = []
         self.azure_ad_groups_users: list = []
+        # Create and register the stateful ingestion use-case handler.
+        self.stale_entity_removal_handler = StaleEntityRemovalHandler(
+            source=self,
+            config=self.config,
+            state_type_class=BaseSQLAlchemyCheckpointState,
+            pipeline_name=ctx.pipeline_name,
+            run_id=ctx.run_id,
+        )
 
     def get_token(self):
         token_response = requests.post(self.config.token_url, data=self.token_data)
         if token_response.status_code == 200:
             token = token_response.json().get("access_token")
             return token
         else:
@@ -261,15 +303,15 @@
                 f"Token response content: {str(token_response.content)}"
             )
             logger.error(error_str)
             self.report.report_failure("get_token", error_str)
             click.echo("Error: Token response invalid")
             exit()
 
-    def get_workunits(self) -> Iterable[MetadataWorkUnit]:
+    def get_workunits_internal(self) -> Iterable[MetadataWorkUnit]:
         # for future developers: The actual logic of this ingestion wants to be executed, in order:
         # 1) the groups
         # 2) the groups' memberships
         # 3) the users
 
         # Create MetadataWorkUnits for CorpGroups
         if self.config.ingest_groups:
@@ -359,14 +401,22 @@
                 # azure_ad_users = next(self._get_azure_ad_users())
                 datahub_corp_user_snapshots = self._map_azure_ad_users(azure_ad_users)
                 yield from self.ingest_ad_users(
                     datahub_corp_user_snapshots,
                     datahub_corp_user_urn_to_group_membership,
                 )
 
+    def get_workunits(self) -> Iterable[MetadataWorkUnit]:
+        return auto_stale_entity_removal(
+            self.stale_entity_removal_handler,
+            auto_workunit_reporter(
+                self.report, auto_status_aspect(self.get_workunits_internal())
+            ),
+        )
+
     def _add_group_members_to_group_membership(
         self,
         parent_corp_group_urn: str,
         azure_ad_group: dict,
         user_urn_to_group_membership: Dict[str, GroupMembershipClass],
     ) -> None:
         # Extract and map members for each group
@@ -482,14 +532,15 @@
                     url = False  # type: ignore
                 yield json_data["value"]
             else:
                 error_str = (
                     f"Response status code: {str(response.status_code)}. "
                     f"Response content: {str(response.content)}"
                 )
+                logger.debug(f"URL = {url}")
                 logger.error(error_str)
                 self.report.report_failure("_get_azure_ad_data_", error_str)
                 continue
 
     def _map_identity_to_urn(self, func, id_to_extract, mapping_identifier, id_type):
         result, error_str = None, None
         try:
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/identity/okta.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/identity/okta.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,47 +9,61 @@
 
 from okta.client import Client as OktaClient
 from okta.exceptions import OktaAPIException
 from okta.models import Group, GroupProfile, User, UserProfile, UserStatus
 from pydantic import validator
 from pydantic.fields import Field
 
-from datahub.configuration import ConfigModel
 from datahub.configuration.common import ConfigurationError
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.decorators import (
     SourceCapability,
     SupportStatus,
     capability,
     config_class,
     platform_name,
     support_status,
 )
-from datahub.ingestion.api.source import Source, SourceReport
 from datahub.ingestion.api.workunit import MetadataWorkUnit
+from datahub.ingestion.source.state.sql_common_state import (
+    BaseSQLAlchemyCheckpointState,
+)
+from datahub.ingestion.source.state.stale_entity_removal_handler import (
+    StaleEntityRemovalHandler,
+    StaleEntityRemovalSourceReport,
+    StatefulStaleMetadataRemovalConfig,
+)
+from datahub.ingestion.source.state.stateful_ingestion_base import (
+    StatefulIngestionConfigBase,
+    StatefulIngestionSourceBase,
+)
 from datahub.metadata.com.linkedin.pegasus2avro.metadata.snapshot import (
     CorpGroupSnapshot,
     CorpUserSnapshot,
 )
 from datahub.metadata.com.linkedin.pegasus2avro.mxe import MetadataChangeEvent
 from datahub.metadata.schema_classes import (
     ChangeTypeClass,
     CorpGroupInfoClass,
     CorpUserInfoClass,
     GroupMembershipClass,
     OriginClass,
     OriginTypeClass,
     StatusClass,
 )
+from datahub.utilities.source_helpers import (
+    auto_stale_entity_removal,
+    auto_status_aspect,
+)
 
 logger = logging.getLogger(__name__)
 
 
-class OktaConfig(ConfigModel):
+class OktaConfig(StatefulIngestionConfigBase):
     # Required: Domain of the Okta deployment. Example: dev-33231928.okta.com
     okta_domain: str = Field(
         description="The location of your Okta Domain, without a protocol. Can be found in Okta Developer console. e.g. dev-33231928.okta.com",
     )
     # Required: An API token generated from Okta.
     okta_api_token: str = Field(
         description="An API token generated for the DataHub application inside your Okta Developer Console. e.g. 00be4R_M2MzDqXawbWgfKGpKee0kuEOfX1RCQSRx00",
@@ -67,15 +81,15 @@
         description="Whether group membership should be ingested into DataHub. ingest_groups must be True if this is True.",
     )
 
     # Optional: Customize the mapping to DataHub Username from an attribute appearing in the Okta User
     # profile. Reference: https://developer.okta.com/docs/reference/api/users/
     okta_profile_to_username_attr: str = Field(
         default="login",
-        description="Which Okta User Profile attribute to use as input to DataHub username mapping.",
+        description="Which Okta User Profile attribute to use as input to DataHub username mapping. Common values used are - login, email.",
     )
     okta_profile_to_username_regex: str = Field(
         default="([^@]+)",
         description="A regex used to parse the DataHub username from the attribute specified in `okta_profile_to_username_attr`.",
     )
 
     # Optional: Customize the mapping to DataHub Group from an attribute appearing in the Okta Group
@@ -127,14 +141,19 @@
         description="Okta filter expression (not regex) for ingesting groups. Only one of `okta_groups_filter` and `okta_groups_search` can be set. See (https://developer.okta.com/docs/reference/api/groups/#filters) for more info.",
     )
     okta_groups_search: Optional[str] = Field(
         default=None,
         description="Okta search expression (not regex) for ingesting groups. Only one of `okta_groups_filter` and `okta_groups_search` can be set. See (https://developer.okta.com/docs/reference/api/groups/#list-groups-with-search) for more info.",
     )
 
+    # Configuration for stateful ingestion
+    stateful_ingestion: Optional[StatefulStaleMetadataRemovalConfig] = Field(
+        default=None, description="Okta Stateful Ingestion Config."
+    )
+
     # Optional: Whether to mask sensitive information from workunit ID's. On by default.
     mask_group_id: bool = True
     mask_user_id: bool = True
 
     @validator("okta_users_search")
     def okta_users_one_of_filter_or_search(cls, v, values):
         if v and values["okta_users_filter"]:
@@ -149,15 +168,15 @@
             raise ConfigurationError(
                 "Only one of okta_groups_filter or okta_groups_search can be set"
             )
         return v
 
 
 @dataclass
-class OktaSourceReport(SourceReport):
+class OktaSourceReport(StaleEntityRemovalSourceReport):
     filtered: List[str] = field(default_factory=list)
 
     def report_filtered(self, name: str) -> None:
         self.filtered.append(name)
 
 
 #
@@ -174,15 +193,18 @@
 #
 
 
 @platform_name("Okta")
 @config_class(OktaConfig)
 @support_status(SupportStatus.CERTIFIED)
 @capability(SourceCapability.DESCRIPTIONS, "Optionally enabled via configuration")
-class OktaSource(Source):
+@capability(
+    SourceCapability.DELETION_DETECTION, "Optionally enabled via stateful_ingestion"
+)
+class OktaSource(StatefulIngestionSourceBase):
     """
     This plugin extracts the following:
 
     - Users
     - Groups
     - Group Membership
 
@@ -200,15 +222,19 @@
     #### Usernames
 
     Usernames serve as unique identifiers for users on DataHub. This connector extracts usernames using the
     "login" field of an [Okta User Profile](https://developer.okta.com/docs/reference/api/users/#profile-object).
     By default, the 'login' attribute, which contains an email, is parsed to extract the text before the "@" and map that to the DataHub username.
 
     If this is not how you wish to map to DataHub usernames, you can provide a custom mapping using the configurations options detailed below. Namely, `okta_profile_to_username_attr`
-    and `okta_profile_to_username_regex`.
+    and `okta_profile_to_username_regex`. e.g. if you want to map emails to urns then you may use the following configuration:
+    ```yaml
+    okta_profile_to_username_attr: "email"
+    okta_profile_to_username_regex: ".*"
+    ```
 
     #### Profiles
 
     This connector also extracts basic user profile information from Okta. The following fields of the Okta User Profile are extracted
     and mapped to the DataHub `CorpUserInfo` aspect:
 
     - display name
@@ -248,26 +274,40 @@
     users, set either the `okta_users_filter` or `okta_users_search` flag (only one can be set at a time). For groups, set
     either the `okta_groups_filter` or `okta_groups_search` flag. Note that these are not regular expressions. See [below](#config-details) for full configuration
     options.
 
 
     """
 
+    config: OktaConfig
+    report: OktaSourceReport
+    okta_client: OktaClient
+    stale_entity_removal_handler: StaleEntityRemovalHandler
+
     @classmethod
     def create(cls, config_dict, ctx):
         config = OktaConfig.parse_obj(config_dict)
         return cls(config, ctx)
 
     def __init__(self, config: OktaConfig, ctx: PipelineContext):
-        super().__init__(ctx)
+        super(OktaSource, self).__init__(config, ctx)
         self.config = config
         self.report = OktaSourceReport()
         self.okta_client = self._create_okta_client()
 
-    def get_workunits(self) -> Iterable[MetadataWorkUnit]:
+        # Create and register the stateful ingestion use-case handler.
+        self.stale_entity_removal_handler = StaleEntityRemovalHandler(
+            source=self,
+            config=self.config,
+            state_type_class=BaseSQLAlchemyCheckpointState,
+            pipeline_name=ctx.pipeline_name,
+            run_id=ctx.run_id,
+        )
+
+    def get_workunits_internal(self) -> Iterable[MetadataWorkUnit]:
         # Step 0: get or create the event loop
         # This method can be called on the main thread or an async thread, so we must create a new loop if one doesn't exist
         # See https://docs.python.org/3/library/asyncio-eventloop.html for more info.
 
         try:
             event_loop: asyncio.AbstractEventLoop = asyncio.get_event_loop()
         except RuntimeError:
@@ -399,14 +439,20 @@
                 )
                 self.report.report_workunit(user_status_wu)
                 yield user_status_wu
 
         # Step 4: Close the event loop
         event_loop.close()
 
+    def get_workunits(self) -> Iterable[MetadataWorkUnit]:
+        return auto_stale_entity_removal(
+            self.stale_entity_removal_handler,
+            auto_status_aspect(self.get_workunits_internal()),
+        )
+
     def get_report(self):
         return self.report
 
     # Instantiates Okta SDK Client.
     def _create_okta_client(self):
         config = {
             "orgUrl": f"https://{self.config.okta_domain}",
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/kafka.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/kafka.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,34 +11,37 @@
 from confluent_kafka.admin import (
     AdminClient,
     ConfigEntry,
     ConfigResource,
     TopicMetadata,
 )
 
-from datahub.configuration.common import AllowDenyPattern, ConfigurationError
+from datahub.configuration.common import AllowDenyPattern
 from datahub.configuration.kafka import KafkaConsumerConnectionConfig
-from datahub.configuration.source_common import DatasetSourceConfigBase
+from datahub.configuration.source_common import DatasetSourceConfigMixin
 from datahub.emitter.mce_builder import (
     make_data_platform_urn,
     make_dataplatform_instance_urn,
     make_dataset_urn_with_platform_instance,
     make_domain_urn,
 )
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.emitter.mcp_builder import add_domain_to_entity_wu
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.decorators import (
     SupportStatus,
+    capability,
     config_class,
     platform_name,
     support_status,
 )
 from datahub.ingestion.api.registry import import_path
+from datahub.ingestion.api.source import SourceCapability
 from datahub.ingestion.api.workunit import MetadataWorkUnit
+from datahub.ingestion.source.common.subtypes import DatasetSubTypes
 from datahub.ingestion.source.kafka_schema_registry_base import KafkaSchemaRegistryBase
 from datahub.ingestion.source.state.entity_removal_state import GenericCheckpointState
 from datahub.ingestion.source.state.stale_entity_removal_handler import (
     StaleEntityRemovalHandler,
     StaleEntityRemovalSourceReport,
     StatefulStaleMetadataRemovalConfig,
 )
@@ -69,15 +72,15 @@
     RETENTION_SIZE_CONFIG = "retention.bytes"
     RETENTION_TIME_CONFIG = "retention.ms"
     CLEANUP_POLICY_CONFIG = "cleanup.policy"
     MAX_MESSAGE_SIZE_CONFIG = "max.message.bytes"
     UNCLEAN_LEADER_ELECTION_CONFIG = "unclean.leader.election.enable"
 
 
-class KafkaSourceConfig(StatefulIngestionConfigBase, DatasetSourceConfigBase):
+class KafkaSourceConfig(StatefulIngestionConfigBase, DatasetSourceConfigMixin):
     connection: KafkaConsumerConnectionConfig = KafkaConsumerConnectionConfig()
 
     topic_patterns: AllowDenyPattern = AllowDenyPattern(allow=[".*"], deny=["^_.*"])
     domain: Dict[str, AllowDenyPattern] = pydantic.Field(
         default={},
         description="A map of domain names to allow deny patterns. Domains can be urn-based (`urn:li:domain:13ae4d85-d955-49fc-8474-9004c663a810`) or bare (`13ae4d85-d955-49fc-8474-9004c663a810`).",
     )
@@ -90,27 +93,18 @@
         default="datahub.ingestion.source.confluent_schema_registry.ConfluentSchemaRegistry",
         description="The fully qualified implementation class(custom) that implements the KafkaSchemaRegistryBase interface.",
     )
     ignore_warnings_on_schema_type: bool = pydantic.Field(
         default=False,
         description="Disables warnings reported for non-AVRO/Protobuf value or key schemas if set.",
     )
-
-    @pydantic.root_validator
-    def validate_platform_instance(cls: "KafkaSourceConfig", values: Dict) -> Dict:
-        stateful_ingestion = values.get("stateful_ingestion")
-        if (
-            stateful_ingestion
-            and stateful_ingestion.enabled
-            and not values.get("platform_instance")
-        ):
-            raise ConfigurationError(
-                "Enabling kafka stateful ingestion requires to specify a platform instance."
-            )
-        return values
+    disable_topic_record_naming_strategy: bool = pydantic.Field(
+        default=False,
+        description="Disables the utilization of the TopicRecordNameStrategy for Schema Registry subjects. For more information, visit: https://docs.confluent.io/platform/current/schema-registry/serdes-develop/index.html#handling-differences-between-preregistered-and-client-derived-schemas:~:text=io.confluent.kafka.serializers.subject.TopicRecordNameStrategy",
+    )
 
 
 @dataclass
 class KafkaSourceReport(StaleEntityRemovalSourceReport):
     topics_scanned: int = 0
     filtered: List[str] = field(default_factory=list)
 
@@ -120,19 +114,27 @@
     def report_dropped(self, topic: str) -> None:
         self.filtered.append(topic)
 
 
 @platform_name("Kafka")
 @config_class(KafkaSourceConfig)
 @support_status(SupportStatus.CERTIFIED)
+@capability(
+    SourceCapability.PLATFORM_INSTANCE,
+    "For multiple Kafka clusters, use the platform_instance configuration",
+)
+@capability(
+    SourceCapability.SCHEMA_METADATA,
+    "Schemas associated with each topic are extracted from the schema registry. Avro and Protobuf (certified), JSON (incubating). Schema references are supported.",
+)
 class KafkaSource(StatefulIngestionSourceBase):
     """
     This plugin extracts the following:
     - Topics from the Kafka broker
-    - Schemas associated with each topic from the schema registry (only Avro schemas are currently supported)
+    - Schemas associated with each topic from the schema registry (Avro, Protobuf and JSON schemas are supported)
     """
 
     platform: str = "kafka"
 
     @classmethod
     def create_schema_registry(
         cls, config: KafkaSourceConfig, report: KafkaSourceReport
@@ -185,18 +187,14 @@
         except Exception as e:
             logger.debug(e, exc_info=e)
             self.report.report_warning(
                 "kafka-admin-client",
                 f"Failed to create Kafka Admin Client due to error {e}.",
             )
 
-    def get_platform_instance_id(self) -> str:
-        assert self.source_config.platform_instance is not None
-        return self.source_config.platform_instance
-
     @classmethod
     def create(cls, config_dict: Dict, ctx: PipelineContext) -> "KafkaSource":
         config: KafkaSourceConfig = KafkaSourceConfig.parse_obj(config_dict)
         return cls(config, ctx)
 
     def get_workunits(self) -> Iterable[MetadataWorkUnit]:
         return auto_stale_entity_removal(
@@ -243,22 +241,18 @@
         schema_metadata = self.schema_registry_client.get_schema_metadata(
             topic, platform_urn
         )
         if schema_metadata is not None:
             dataset_snapshot.aspects.append(schema_metadata)
 
         # 3. Attach browsePaths aspect
-        browse_path_suffix = (
-            f"{self.source_config.platform_instance}/{topic}"
-            if self.source_config.platform_instance
-            else topic
-        )
-        browse_path = BrowsePathsClass(
-            [f"/{self.source_config.env.lower()}/{self.platform}/{browse_path_suffix}"]
-        )
+        browse_path_str = f"/{self.source_config.env.lower()}/{self.platform}"
+        if self.source_config.platform_instance:
+            browse_path_str += f"/{self.source_config.platform_instance}"
+        browse_path = BrowsePathsClass([browse_path_str])
         dataset_snapshot.aspects.append(browse_path)
 
         custom_props = self.build_custom_properties(
             topic, topic_detail, extra_topic_config
         )
 
         dataset_properties = DatasetPropertiesClass(
@@ -285,15 +279,15 @@
         yield wu
 
         # 5. Add the subtype aspect marking this as a "topic"
         subtype_wu = MetadataWorkUnit(
             id=f"{topic}-subtype",
             mcp=MetadataChangeProposalWrapper(
                 entityUrn=dataset_urn,
-                aspect=SubTypesClass(typeNames=["topic"]),
+                aspect=SubTypesClass(typeNames=[DatasetSubTypes.TOPIC]),
             ),
         )
         self.report.report_workunit(subtype_wu)
         yield subtype_wu
 
         domain_urn: Optional[str] = None
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/kafka_connect.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/kafka_connect.py`

 * *Files 6% similar despite different names*

```diff
@@ -9,28 +9,45 @@
 import requests
 from pydantic.fields import Field
 from sqlalchemy.engine.url import make_url
 
 import datahub.emitter.mce_builder as builder
 import datahub.metadata.schema_classes as models
 from datahub.configuration.common import AllowDenyPattern, ConfigModel
-from datahub.configuration.source_common import DatasetLineageProviderConfigBase
+from datahub.configuration.source_common import (
+    DatasetLineageProviderConfigBase,
+    PlatformInstanceConfigMixin,
+)
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.decorators import (
     SourceCapability,
     SupportStatus,
     capability,
     config_class,
     platform_name,
     support_status,
 )
-from datahub.ingestion.api.source import Source, SourceReport
+from datahub.ingestion.api.source import Source
 from datahub.ingestion.api.workunit import MetadataWorkUnit
 from datahub.ingestion.source.sql.sql_common import get_platform_from_sqlalchemy_uri
+from datahub.ingestion.source.state.entity_removal_state import GenericCheckpointState
+from datahub.ingestion.source.state.stale_entity_removal_handler import (
+    StaleEntityRemovalHandler,
+    StaleEntityRemovalSourceReport,
+    StatefulStaleMetadataRemovalConfig,
+)
+from datahub.ingestion.source.state.stateful_ingestion_base import (
+    StatefulIngestionConfigBase,
+    StatefulIngestionSourceBase,
+)
+from datahub.utilities.source_helpers import (
+    auto_stale_entity_removal,
+    auto_status_aspect,
+)
 
 logger = logging.getLogger(__name__)
 
 
 class ProvidedConfig(ConfigModel):
     provider: str
     path_key: str
@@ -39,28 +56,33 @@
 
 class GenericConnectorConfig(ConfigModel):
     connector_name: str
     source_dataset: str
     source_platform: str
 
 
-class KafkaConnectSourceConfig(DatasetLineageProviderConfigBase):
+class KafkaConnectSourceConfig(
+    PlatformInstanceConfigMixin,
+    DatasetLineageProviderConfigBase,
+    StatefulIngestionConfigBase,
+):
     # See the Connect REST Interface for details
     # https://docs.confluent.io/platform/current/connect/references/restapi.html#
     connect_uri: str = Field(
         default="http://localhost:8083/", description="URI to connect to."
     )
     username: Optional[str] = Field(default=None, description="Kafka Connect username.")
     password: Optional[str] = Field(default=None, description="Kafka Connect password.")
     cluster_name: Optional[str] = Field(
         default="connect-cluster", description="Cluster to ingest from."
     )
-    construct_lineage_workunits: bool = Field(
-        default=True,
-        description="Whether to create the input and output Dataset entities",
+    # convert lineage dataset's urns to lowercase
+    convert_lineage_urns_to_lowercase: bool = Field(
+        default=False,
+        description="Whether to convert the urns of ingested lineage dataset to lowercase",
     )
     connector_patterns: AllowDenyPattern = Field(
         default=AllowDenyPattern.allow_all(),
         description="regex patterns for connectors to filter for ingestion.",
     )
     provided_configs: Optional[List[ProvidedConfig]] = Field(
         default=None, description="Provided Configurations"
@@ -74,17 +96,19 @@
         description='Platform instance mapping to use when constructing URNs. e.g.`platform_instance_map: { "hive": "warehouse" }`',
     )
     generic_connectors: List[GenericConnectorConfig] = Field(
         default=[],
         description="Provide lineage graph for sources connectors other than Confluent JDBC Source Connector, Debezium Source Connector, and Mongo Source Connector",
     )
 
+    stateful_ingestion: Optional[StatefulStaleMetadataRemovalConfig] = None
+
 
 @dataclass
-class KafkaConnectSourceReport(SourceReport):
+class KafkaConnectSourceReport(StaleEntityRemovalSourceReport):
     connectors_scanned: int = 0
     filtered: List[str] = field(default_factory=list)
 
     def report_connector_scanned(self, connector: str) -> None:
         self.connectors_scanned += 1
 
     def report_dropped(self, connector: str) -> None:
@@ -488,15 +512,15 @@
                 topic = topic_prefix + source_table if topic_prefix else source_table
 
                 transform_regex = Pattern.compile(transforms[0]["regex"])
                 transform_replacement = transforms[0]["replacement"]
 
                 matcher = transform_regex.matcher(topic)
                 if matcher.matches():
-                    topic = matcher.replaceFirst(transform_replacement)
+                    topic = str(matcher.replaceFirst(transform_replacement))
 
                 # Additional check to confirm that the topic present
                 # in connector topics
 
                 if topic in self.connector_manifest.topic_names:
                     # include schema name for three-level hierarchies
                     if has_three_level_hierarchy(source_platform) and len(table) > 1:
@@ -893,31 +917,40 @@
                 connector_config[k] = v.replace(key, value)
 
 
 @platform_name("Kafka Connect")
 @config_class(KafkaConnectSourceConfig)
 @support_status(SupportStatus.CERTIFIED)
 @capability(SourceCapability.PLATFORM_INSTANCE, "Enabled by default")
-class KafkaConnectSource(Source):
-
+class KafkaConnectSource(StatefulIngestionSourceBase):
     config: KafkaConnectSourceConfig
     report: KafkaConnectSourceReport
+    platform: str = "kafka-connect"
 
     def __init__(self, config: KafkaConnectSourceConfig, ctx: PipelineContext):
-        super().__init__(ctx)
+        super().__init__(config, ctx)
         self.config = config
         self.report = KafkaConnectSourceReport()
         self.session = requests.Session()
         self.session.headers.update(
             {
                 "Accept": "application/json",
                 "Content-Type": "application/json",
             }
         )
 
+        # Create and register the stateful ingestion use-case handlers.
+        self.stale_entity_removal_handler = StaleEntityRemovalHandler(
+            source=self,
+            config=self.config,
+            state_type_class=GenericCheckpointState,
+            pipeline_name=self.ctx.pipeline_name,
+            run_id=self.ctx.run_id,
+        )
+
         # Test the connection
         if self.config.username is not None and self.config.password is not None:
             logger.info(
                 f"Connecting to {self.config.connect_uri} with Authentication..."
             )
             self.session.auth = (self.config.username, self.config.password)
 
@@ -943,17 +976,20 @@
         )
 
         payload = connector_response.json()
 
         for c in payload:
             connector_url = f"{self.config.connect_uri}/connectors/{c}"
             connector_response = self.session.get(connector_url)
-
             manifest = connector_response.json()
             connector_manifest = ConnectorManifest(**manifest)
+            if not self.config.connector_patterns.allowed(connector_manifest.name):
+                self.report.report_dropped(connector_manifest.name)
+                continue
+
             if self.config.provided_configs:
                 transform_connector_config(
                     connector_manifest.config, self.config.provided_configs
                 )
             # Initialize connector lineages
             connector_manifest.lineages = list()
             connector_manifest.url = connector_url
@@ -1040,86 +1076,79 @@
     ) -> Iterable[MetadataWorkUnit]:
         connector_name = connector.name
         connector_type = connector.type
         connector_class = connector.config.get("connector.class")
         flow_property_bag = connector.flow_property_bag
         # connector_url = connector.url  # NOTE: this will expose connector credential when used
         flow_urn = builder.make_data_flow_urn(
-            "kafka-connect", connector_name, self.config.env
+            self.platform,
+            connector_name,
+            self.config.env,
+            self.config.platform_instance,
         )
 
         mcp = MetadataChangeProposalWrapper(
             entityUrn=flow_urn,
             aspect=models.DataFlowInfoClass(
                 name=connector_name,
                 description=f"{connector_type.capitalize()} connector using `{connector_class}` plugin.",
                 customProperties=flow_property_bag,
                 # externalUrl=connector_url, # NOTE: this will expose connector credential when used
             ),
         )
 
         for proposal in [mcp]:
             wu = MetadataWorkUnit(
-                id=f"kafka-connect.{connector_name}.{proposal.aspectName}", mcp=proposal
+                id=f"{self.platform}.{connector_name}.{proposal.aspectName}",
+                mcp=proposal,
             )
             self.report.report_workunit(wu)
             yield wu
 
     def construct_job_workunits(
         self, connector: ConnectorManifest
     ) -> Iterable[MetadataWorkUnit]:
         connector_name = connector.name
         flow_urn = builder.make_data_flow_urn(
-            "kafka-connect", connector_name, self.config.env
+            self.platform,
+            connector_name,
+            self.config.env,
+            self.config.platform_instance,
         )
 
         lineages = connector.lineages
         if lineages:
             for lineage in lineages:
                 source_dataset = lineage.source_dataset
                 source_platform = lineage.source_platform
-                source_platform_instance = (
-                    self.config.platform_instance_map.get(source_platform)
-                    if self.config.platform_instance_map
-                    else None
-                )
                 target_dataset = lineage.target_dataset
                 target_platform = lineage.target_platform
-                target_platform_instance = (
-                    self.config.platform_instance_map.get(target_platform)
-                    if self.config.platform_instance_map
-                    else None
-                )
                 job_property_bag = lineage.job_property_bag
 
                 job_id = (
                     source_dataset
                     if source_dataset
                     else f"unknown_source.{target_dataset}"
                 )
                 job_urn = builder.make_data_job_urn_with_flow(flow_urn, job_id)
 
                 inlets = (
                     [
-                        builder.make_dataset_urn_with_platform_instance(
+                        self.make_lineage_dataset_urn(
                             source_platform,
                             source_dataset,
-                            platform_instance=source_platform_instance,
-                            env=self.config.env,
                         )
                     ]
                     if source_dataset
                     else []
                 )
                 outlets = [
-                    builder.make_dataset_urn_with_platform_instance(
+                    self.make_lineage_dataset_urn(
                         target_platform,
                         target_dataset,
-                        platform_instance=target_platform_instance,
-                        env=self.config.env,
                     )
                 ]
 
                 mcp = MetadataChangeProposalWrapper(
                     entityUrn=job_urn,
                     aspect=models.DataJobInfoClass(
                         name=f"{connector_name}:{job_id}",
@@ -1127,113 +1156,64 @@
                         description=None,
                         customProperties=job_property_bag
                         # externalUrl=job_url,
                     ),
                 )
 
                 wu = MetadataWorkUnit(
-                    id=f"kafka-connect.{connector_name}.{job_id}.{mcp.aspectName}",
+                    id=f"{self.platform}.{connector_name}.{job_id}.{mcp.aspectName}",
                     mcp=mcp,
                 )
                 self.report.report_workunit(wu)
                 yield wu
 
                 mcp = MetadataChangeProposalWrapper(
                     entityUrn=job_urn,
                     aspect=models.DataJobInputOutputClass(
                         inputDatasets=inlets,
                         outputDatasets=outlets,
                     ),
                 )
 
                 wu = MetadataWorkUnit(
-                    id=f"kafka-connect.{connector_name}.{job_id}.{mcp.aspectName}",
+                    id=f"{self.platform}.{connector_name}.{job_id}.{mcp.aspectName}",
                     mcp=mcp,
                 )
                 self.report.report_workunit(wu)
                 yield wu
 
-    def construct_lineage_workunits(
-        self, connector: ConnectorManifest
-    ) -> Iterable[MetadataWorkUnit]:
-        lineages = connector.lineages
-        if lineages:
-            for lineage in lineages:
-                source_dataset = lineage.source_dataset
-                source_platform = lineage.source_platform
-                source_platform_instance = (
-                    self.config.platform_instance_map.get(source_platform)
-                    if self.config.platform_instance_map
-                    else None
-                )
-                target_dataset = lineage.target_dataset
-                target_platform = lineage.target_platform
-                target_platform_instance = (
-                    self.config.platform_instance_map.get(target_platform)
-                    if self.config.platform_instance_map
-                    else None
-                )
-
-                mcp = MetadataChangeProposalWrapper(
-                    entityUrn=builder.make_dataset_urn_with_platform_instance(
-                        target_platform,
-                        target_dataset,
-                        platform_instance=target_platform_instance,
-                        env=self.config.env,
-                    ),
-                    aspect=models.DataPlatformInstanceClass(
-                        platform=builder.make_data_platform_urn(target_platform),
-                        instance=builder.make_dataplatform_instance_urn(
-                            target_platform, target_platform_instance
-                        )
-                        if target_platform_instance
-                        else None,
-                    ),
-                )
-
-                wu = MetadataWorkUnit(id=target_dataset, mcp=mcp)
-                self.report.report_workunit(wu)
-                yield wu
-                if source_dataset:
-                    mcp = MetadataChangeProposalWrapper(
-                        entityUrn=builder.make_dataset_urn_with_platform_instance(
-                            source_platform,
-                            source_dataset,
-                            platform_instance=source_platform_instance,
-                            env=self.config.env,
-                        ),
-                        aspect=models.DataPlatformInstanceClass(
-                            platform=builder.make_data_platform_urn(source_platform),
-                            instance=builder.make_dataplatform_instance_urn(
-                                source_platform, source_platform_instance
-                            )
-                            if source_platform_instance
-                            else None,
-                        ),
-                    )
-
-                    wu = MetadataWorkUnit(id=source_dataset, mcp=mcp)
-                    self.report.report_workunit(wu)
-                    yield wu
-
     def get_workunits(self) -> Iterable[MetadataWorkUnit]:
+        return auto_stale_entity_removal(
+            self.stale_entity_removal_handler,
+            auto_status_aspect(self.get_workunits_internal()),
+        )
+
+    def get_workunits_internal(self) -> Iterable[MetadataWorkUnit]:
         connectors_manifest = self.get_connectors_manifest()
         for connector in connectors_manifest:
             name = connector.name
-            if self.config.connector_patterns.allowed(name):
-                yield from self.construct_flow_workunit(connector)
-                yield from self.construct_job_workunits(connector)
-                if self.config.construct_lineage_workunits:
-                    yield from self.construct_lineage_workunits(connector)
 
-                self.report.report_connector_scanned(name)
-
-            else:
-                self.report.report_dropped(name)
+            yield from self.construct_flow_workunit(connector)
+            yield from self.construct_job_workunits(connector)
+            self.report.report_connector_scanned(name)
 
     def get_report(self) -> KafkaConnectSourceReport:
         return self.report
 
+    def make_lineage_dataset_urn(self, platform: str, name: str) -> str:
+        if self.config.convert_lineage_urns_to_lowercase:
+            name = name.lower()
+
+        platform_instance = (
+            self.config.platform_instance_map.get(platform)
+            if self.config.platform_instance_map
+            else None
+        )
+
+        return builder.make_dataset_urn_with_platform_instance(
+            platform, name, platform_instance, self.config.env
+        )
+
 
 # TODO: Find a more automated way to discover new platforms with 3 level naming hierarchy.
 def has_three_level_hierarchy(platform: str) -> bool:
     return platform in ["postgres", "trino", "redshift", "snowflake"]
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/ldap.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/ldap.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,14 +4,15 @@
 from typing import Any, Dict, Iterable, List, Optional
 
 import ldap
 from ldap.controls import SimplePagedResultsControl
 from pydantic.fields import Field
 
 from datahub.configuration.common import ConfigurationError
+from datahub.configuration.source_common import DatasetSourceConfigMixin
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.decorators import (
     SupportStatus,
     config_class,
     platform_name,
     support_status,
 )
@@ -95,15 +96,15 @@
     """
 
     cookie = pctrls[0].cookie
     lc_object.cookie = cookie
     return bool(cookie)
 
 
-class LDAPSourceConfig(StatefulIngestionConfigBase):
+class LDAPSourceConfig(StatefulIngestionConfigBase, DatasetSourceConfigMixin):
     """Config used by the LDAP Source."""
 
     # Server configuration.
     ldap_server: str = Field(description="LDAP server URL.")
     ldap_user: str = Field(description="LDAP user.")
     ldap_password: str = Field(description="LDAP password.")
 
@@ -283,21 +284,14 @@
                 self.report.report_failure(
                     "ldap-control", "Server ignores RFC 2696 control."
                 )
                 break
 
             cookie = set_cookie(self.lc, pctrls)
 
-    def get_platform_instance_id(self) -> str:
-        """
-        The source identifier such as the specific source host address required for stateful ingestion.
-        Individual subclasses need to override this method appropriately.
-        """
-        return self.config.ldap_server
-
     def handle_user(self, dn: str, attrs: Dict[str, Any]) -> Iterable[MetadataWorkUnit]:
         """
         Handle a DN and attributes by adding manager info and constructing a
         work unit based on the information.
         """
         manager_ldap = None
         if self.config.user_attrs_map["managerUrn"] in attrs:
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/looker/looker_common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/looker_common.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 from __future__ import print_function
 
 import dataclasses
 import datetime
+import itertools
 import logging
 import re
 from dataclasses import dataclass, field as dataclasses_field
 from enum import Enum
 from functools import lru_cache
 from typing import (
     TYPE_CHECKING,
@@ -17,26 +18,27 @@
     Set,
     Tuple,
     Union,
 )
 
 import pydantic
 from looker_sdk.error import SDKError
-from looker_sdk.sdk.api31.models import User, WriteQuery
+from looker_sdk.sdk.api40.models import User, WriteQuery
 from pydantic import Field
 from pydantic.class_validators import validator
 
 import datahub.emitter.mce_builder as builder
 from datahub.configuration import ConfigModel
 from datahub.configuration.common import ConfigurationError
-from datahub.configuration.source_common import DatasetSourceConfigBase
+from datahub.configuration.source_common import DatasetSourceConfigMixin
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.emitter.mcp_builder import create_embed_mcp
 from datahub.ingestion.api.report import Report
 from datahub.ingestion.api.source import SourceReport
+from datahub.ingestion.source.common.subtypes import DatasetSubTypes
 from datahub.ingestion.source.looker.looker_lib_wrapper import LookerAPI
 from datahub.ingestion.source.sql.sql_types import (
     POSTGRES_TYPES_MAP,
     SNOWFLAKE_TYPES_MAP,
     resolve_postgres_modified_type,
 )
 from datahub.ingestion.source.state.stale_entity_removal_handler import (
@@ -63,15 +65,14 @@
     SchemaMetadata,
     StringTypeClass,
     TimeTypeClass,
     UnionTypeClass,
 )
 from datahub.metadata.schema_classes import (
     BrowsePathsClass,
-    ChangeTypeClass,
     DatasetPropertiesClass,
     EnumTypeClass,
     FineGrainedLineageClass,
     GlobalTagsClass,
     OwnerClass,
     OwnershipClass,
     OwnershipTypeClass,
@@ -137,15 +138,16 @@
             raise ConfigurationError(
                 f"Failed to find any variable assigned to pattern {self.pattern}. Must have at least one. {self.allowed_docstring()}"
             )
         return True
 
     def replace_variables(self, values: Union[Dict[str, Optional[str]], object]) -> str:
         if not isinstance(values, dict):
-            assert dataclasses.is_dataclass(values)
+            # Check that this is a dataclass instance (not a dataclass type).
+            assert dataclasses.is_dataclass(values) and not isinstance(values, type)
             values = dataclasses.asdict(values)
         values = {k: v for k, v in values.items() if v is not None}
         return self.pattern.format(**values)
 
 
 @dataclass
 class NamingPatternMapping:
@@ -156,15 +158,15 @@
     name: str
 
 
 class LookerNamingPattern(NamingPattern):
     ALLOWED_VARS = [field.name for field in dataclasses.fields(NamingPatternMapping)]
 
 
-class LookerCommonConfig(DatasetSourceConfigBase):
+class LookerCommonConfig(DatasetSourceConfigMixin):
     explore_naming_pattern: LookerNamingPattern = pydantic.Field(
         description=f"Pattern for providing dataset names to explores. {LookerNamingPattern.allowed_docstring()}",
         default=LookerNamingPattern(pattern="{model}.explore.{name}"),
     )
     explore_browse_pattern: LookerNamingPattern = pydantic.Field(
         description=f"Pattern for providing browse paths to explores. {LookerNamingPattern.allowed_docstring()}",
         default=LookerNamingPattern(pattern="/{env}/{platform}/{project}/explores"),
@@ -532,19 +534,24 @@
     def from_dict(
         cls,
         model_name: str,
         dict: Dict,
         resolved_includes: List[ProjectInclude],
         looker_viewfile_loader: "LookerViewFileLoader",
         reporter: "LookMLSourceReport",
+        model_explores_map: Dict[str, dict],
     ) -> "LookerExplore":
-        view_names = set()
+        view_names: Set[str] = set()
         joins = None
-        # always add the explore's name or the name from the from clause as the view on which this explore is built
-        view_names.add(dict.get("from", dict.get("name")))
+        assert "name" in dict, "Explore doesn't have a name field, this isn't allowed"
+        # The view name that the explore refers to is resolved in the following order of priority:
+        # 1. view_name: https://cloud.google.com/looker/docs/reference/param-explore-view-name
+        # 2. from: https://cloud.google.com/looker/docs/reference/param-explore-from
+        # 3. default to the name of the explore
+        view_names.add(dict.get("view_name") or dict.get("from") or dict["name"])
 
         if dict.get("joins", {}) != {}:
             # additionally for join-based explores, pull in the linked views
             assert "joins" in dict
             for join in dict["joins"]:
                 join_from = join.get("from")
                 view_names.add(join_from or join["name"])
@@ -555,31 +562,55 @@
 
         # HACK: We shouldn't be doing imports here. We also have
         # circular imports that don't belong.
         from datahub.ingestion.source.looker.lookml_source import (
             _find_view_from_resolved_includes,
         )
 
-        upstream_views = []
-        for view_name in view_names:
-            info = _find_view_from_resolved_includes(
-                None,
-                resolved_includes,
-                looker_viewfile_loader,
-                view_name,
-                reporter,
+        upstream_views: List[ProjectInclude] = []
+        # create the list of extended explores
+        extends = list(
+            itertools.chain.from_iterable(
+                dict.get("extends", dict.get("extends__all", []))
             )
-            if not info:
-                logger.warning(
-                    f'Could not resolve view {view_name} for explore {dict["name"]} in model {model_name}'
-                )
-            else:
-                upstream_views.append(
-                    ProjectInclude(project=info[0].project, include=view_name)
+        )
+        if extends:
+            for extended_explore in extends:
+                if extended_explore in model_explores_map:
+                    parsed_explore = LookerExplore.from_dict(
+                        model_name,
+                        model_explores_map[extended_explore],
+                        resolved_includes,
+                        looker_viewfile_loader,
+                        reporter,
+                        model_explores_map,
+                    )
+                    upstream_views.extend(parsed_explore.upstream_views or [])
+                else:
+                    logger.warning(
+                        f'Could not find extended explore {extended_explore} for explore {dict["name"]} in model {model_name}'
+                    )
+        else:
+            # we only fallback to the view_names list if this is not an extended explore
+            for view_name in view_names:
+                info = _find_view_from_resolved_includes(
+                    None,
+                    resolved_includes,
+                    looker_viewfile_loader,
+                    view_name,
+                    reporter,
                 )
+                if not info:
+                    logger.warning(
+                        f'Could not resolve view {view_name} for explore {dict["name"]} in model {model_name}'
+                    )
+                else:
+                    upstream_views.append(
+                        ProjectInclude(project=info[0].project, include=view_name)
+                    )
 
         return LookerExplore(
             model_name=model_name,
             name=dict["name"],
             label=dict.get("label"),
             description=dict.get("description"),
             upstream_views=upstream_views,
@@ -855,19 +886,16 @@
                 reporter=reporter,
             )
             if schema_metadata is not None:
                 dataset_snapshot.aspects.append(schema_metadata)
 
         mce = MetadataChangeEvent(proposedSnapshot=dataset_snapshot)
         mcp = MetadataChangeProposalWrapper(
-            entityType="dataset",
-            changeType=ChangeTypeClass.UPSERT,
             entityUrn=dataset_snapshot.urn,
-            aspectName="subTypes",
-            aspect=SubTypesClass(typeNames=["explore"]),
+            aspect=SubTypesClass(typeNames=[DatasetSubTypes.LOOKER_EXPLORE]),
         )
 
         proposals: List[Union[MetadataChangeEvent, MetadataChangeProposalWrapper]] = [
             mce,
             mcp,
         ]
 
@@ -1013,15 +1041,15 @@
         if self._looker_api:
             self.looker_api_stats = self._looker_api.compute_stats()
         return super().compute_stats()
 
 
 @dataclass
 class LookerUser:
-    id: int
+    id: str
     email: Optional[str]
     display_name: Optional[str]
     first_name: Optional[str]
     last_name: Optional[str]
 
     @classmethod
     def create_looker_user(cls, raw_user: User) -> "LookerUser":
@@ -1031,20 +1059,21 @@
             raw_user.email,
             raw_user.display_name,
             raw_user.first_name,
             raw_user.last_name,
         )
 
     def get_urn(self, strip_user_ids_from_email: bool) -> Optional[str]:
-        if self.email is None:
+        user = self.email
+        if user and strip_user_ids_from_email:
+            user = user.split("@")[0]
+
+        if not user:
             return None
-        if strip_user_ids_from_email:
-            return builder.make_user_urn(self.email.split("@")[0])
-        else:
-            return builder.make_user_urn(self.email)
+        return builder.make_user_urn(user)
 
 
 @dataclass
 class InputFieldElement:
     name: str
     view_field: Optional[ViewField]
     model: str = ""
@@ -1131,18 +1160,18 @@
 class LookerUserRegistry:
     looker_api_wrapper: LookerAPI
     fields: str = ",".join(["id", "email", "display_name", "first_name", "last_name"])
 
     def __init__(self, looker_api: LookerAPI):
         self.looker_api_wrapper = looker_api
 
-    def get_by_id(self, id_: int) -> Optional[LookerUser]:
+    def get_by_id(self, id_: str) -> Optional[LookerUser]:
         logger.debug(f"Will get user {id_}")
 
         raw_user: Optional[User] = self.looker_api_wrapper.get_user(
-            id_, user_fields=self.fields
+            str(id_), user_fields=self.fields
         )
         if raw_user is None:
             return None
 
         looker_user = LookerUser.create_looker_user(raw_user)
         return looker_user
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/looker/looker_lib_wrapper.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/looker_lib_wrapper.py`

 * *Files 1% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 import os
 from functools import lru_cache
 from typing import Dict, List, MutableMapping, Optional, Sequence, Set, Union, cast
 
 import looker_sdk
 from looker_sdk.error import SDKError
 from looker_sdk.rtl.transport import TransportOptions
-from looker_sdk.sdk.api31.models import (
+from looker_sdk.sdk.api40.models import (
     Dashboard,
     DashboardBase,
     DBConnection,
     Folder,
     LookmlModel,
     User,
     WriteQuery,
@@ -64,15 +64,15 @@
     def __init__(self, config: LookerAPIConfig) -> None:
         self.config = config
         # The Looker SDK looks wants these as environment variables
         os.environ["LOOKERSDK_CLIENT_ID"] = config.client_id
         os.environ["LOOKERSDK_CLIENT_SECRET"] = config.client_secret
         os.environ["LOOKERSDK_BASE_URL"] = config.base_url
 
-        self.client = looker_sdk.init31()
+        self.client = looker_sdk.init40()
         self.transport_options = (
             config.transport_options.get_transport_options()
             if config.transport_options is not None
             else None
         )
         # try authenticating current user to check connectivity
         # (since it's possible to initialize an invalid client without any complaints)
@@ -104,15 +104,15 @@
         for role in roles:
             if role.permission_set and role.permission_set.permissions:
                 permissions.update(role.permission_set.permissions)
 
         return permissions
 
     @lru_cache(maxsize=2000)
-    def get_user(self, id_: int, user_fields: str) -> Optional[User]:
+    def get_user(self, id_: str, user_fields: str) -> Optional[User]:
         self.client_stats.user_calls += 1
         try:
             return self.client.user(
                 id_,
                 fields=cast(str, user_fields),
                 transport_options=self.transport_options,
             )
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/looker/looker_query_model.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/looker_query_model.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from dataclasses import dataclass, field
 from enum import Enum
 from typing import Dict, List, cast
 
-from looker_sdk.sdk.api31.models import WriteQuery
+from looker_sdk.sdk.api40.models import WriteQuery
 
 
 # Enum whose value is string and compatible with dictionary having string value as key
 class StrEnum(str, Enum):
     pass
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/looker/looker_source.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/looker_source.py`

 * *Files 0% similar despite different names*

```diff
@@ -15,19 +15,20 @@
     Set,
     Tuple,
     Union,
     cast,
 )
 
 from looker_sdk.error import SDKError
-from looker_sdk.sdk.api31.models import Dashboard, DashboardElement, FolderBase, Query
+from looker_sdk.sdk.api40.models import Dashboard, DashboardElement, FolderBase, Query
 from pydantic import Field, validator
 
 import datahub.emitter.mce_builder as builder
-from datahub.configuration.common import AllowDenyPattern, ConfigurationError
+from datahub.configuration.common import AllowDenyPattern
+from datahub.configuration.source_common import EnvConfigMixin
 from datahub.configuration.validate_field_removal import pydantic_removed_field
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.emitter.mcp_builder import create_embed_mcp
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.decorators import (
     SupportStatus,
     capability,
@@ -79,15 +80,14 @@
 from datahub.metadata.com.linkedin.pegasus2avro.metadata.snapshot import (
     ChartSnapshot,
     DashboardSnapshot,
 )
 from datahub.metadata.com.linkedin.pegasus2avro.mxe import MetadataChangeEvent
 from datahub.metadata.schema_classes import (
     BrowsePathsClass,
-    ChangeTypeClass,
     ChartInfoClass,
     ChartTypeClass,
     DashboardInfoClass,
     InputFieldClass,
     InputFieldsClass,
     OwnerClass,
     OwnershipClass,
@@ -98,15 +98,18 @@
     auto_status_aspect,
 )
 
 logger = logging.getLogger(__name__)
 
 
 class LookerDashboardSourceConfig(
-    LookerAPIConfig, LookerCommonConfig, StatefulIngestionConfigBase
+    LookerAPIConfig,
+    LookerCommonConfig,
+    StatefulIngestionConfigBase,
+    EnvConfigMixin,
 ):
     _removed_github_info = pydantic_removed_field("github_info")
 
     dashboard_pattern: AllowDenyPattern = Field(
         AllowDenyPattern.allow_all(),
         description="Patterns for selecting dashboard ids that are to be included",
     )
@@ -138,15 +141,15 @@
         description="Max parallelism for Looker API calls. Defaults to cpuCount or 40",
     )
     external_base_url: Optional[str] = Field(
         None,
         description="Optional URL to use when constructing external URLs to Looker if the `base_url` is not the correct one to use. For example, `https://looker-public.company.com`. If not provided, the external base URL will default to `base_url`.",
     )
     extract_usage_history: bool = Field(
-        False,
+        True,
         description="Whether to ingest usage statistics for dashboards. Setting this to True will query looker system activity explores to fetch historical dashboard usage.",
     )
     # TODO - stateful ingestion to autodetect usage history interval
     extract_usage_history_for_interval: str = Field(
         "30 days",
         description="Used only if extract_usage_history is set to True. Interval to extract looker dashboard usage history for. See https://docs.looker.com/reference/filter-expressions#date_and_time.",
     )
@@ -160,31 +163,26 @@
 
     @validator("external_base_url", pre=True, always=True)
     def external_url_defaults_to_api_config_base_url(
         cls, v: Optional[str], *, values: Dict[str, Any], **kwargs: Dict[str, Any]
     ) -> Optional[str]:
         return v or values.get("base_url")
 
-    @validator("platform_instance")
-    def platform_instance_not_supported(cls, v: Optional[str]) -> Optional[str]:
-        if v is not None:
-            raise ConfigurationError("Looker Source doesn't support platform instances")
-        return v
-
 
 @platform_name("Looker")
 @support_status(SupportStatus.CERTIFIED)
 @config_class(LookerDashboardSourceConfig)
 @capability(SourceCapability.DESCRIPTIONS, "Enabled by default")
-@capability(SourceCapability.PLATFORM_INSTANCE, "Enabled by default")
+@capability(SourceCapability.PLATFORM_INSTANCE, "Not supported", supported=False)
 @capability(
     SourceCapability.OWNERSHIP, "Enabled by default, configured using `extract_owners`"
 )
 @capability(
-    SourceCapability.USAGE_STATS, "Can be enabled using `extract_usage_history`"
+    SourceCapability.USAGE_STATS,
+    "Enabled by default, configured using `extract_usage_history`",
 )
 class LookerDashboardSource(TestableSource, StatefulIngestionSourceBase):
     """
     This plugin extracts the following:
     - Looker dashboards, dashboard elements (charts) and explores
     - Names, descriptions, URLs, chart types, input explores for the charts
     - Schemas and input views for explores
@@ -246,24 +244,19 @@
             run_id=self.ctx.run_id,
         )
 
     @staticmethod
     def test_connection(config_dict: dict) -> TestConnectionReport:
         test_report = TestConnectionReport()
         try:
-            self = cast(
-                LookerDashboardSource,
-                LookerDashboardSource.create(
-                    config_dict, PipelineContext("looker-test-connection")
-                ),
-            )
             test_report.basic_connectivity = CapabilityReport(capable=True)
             test_report.capability_report = {}
 
-            permissions = self.looker_api.get_available_permissions()
+            config = LookerDashboardSourceConfig.parse_obj_allow_extras(config_dict)
+            permissions = LookerAPI(config).get_available_permissions()
 
             BASIC_INGEST_REQUIRED_PERMISSIONS = {
                 # TODO: Make this a bit more granular.
                 "access_data",
                 "explore",
                 "manage_models",
                 "see_datagroups",
@@ -587,15 +580,15 @@
                                     listener.field,
                                     view_field=None,
                                     model=query.model if query is not None else "",
                                     explore=query.view if query is not None else "",
                                 )
                             )
 
-            explores = list(set(explores))  # dedup the list of views
+            explores = sorted(list(set(explores)))  # dedup the list of views
 
             return LookerDashboardElement(
                 id=element.id,
                 title=element.title if element.title is not None else "",
                 type=element.type,
                 description=element.subtitle_text,
                 look_id=element.look_id,
@@ -821,15 +814,14 @@
             return event.proposedSnapshot.urn
         else:
             return event.entityUrn
 
     def _make_dashboard_and_chart_mces(
         self, looker_dashboard: LookerDashboard
     ) -> Iterable[Union[MetadataChangeEvent, MetadataChangeProposalWrapper]]:
-
         # Step 1: Emit metadata for each Chart inside the Dashboard.
         chart_events = []
         for element in looker_dashboard.dashboard_elements:
             if element.type == "vis":
                 chart_events.extend(
                     self._make_chart_metadata_events(element, looker_dashboard)
                 )
@@ -963,15 +955,15 @@
             deleted_by=self._get_looker_user(dashboard.deleter_id),
             favorite_count=dashboard.favorite_count,
             view_count=dashboard.view_count,
             last_viewed_at=dashboard.last_viewed_at,
         )
         return looker_dashboard
 
-    def _get_looker_user(self, user_id: Optional[int]) -> Optional[LookerUser]:
+    def _get_looker_user(self, user_id: Optional[str]) -> Optional[LookerUser]:
         user = (
             self.user_registry.get_by_id(
                 user_id,
             )
             if self.source_config.extract_owners and user_id is not None
             else None
         )
@@ -1077,36 +1069,30 @@
             all_fields.extend(
                 self._input_fields_from_dashboard_element(dashboard_element)
             )
 
         input_fields_aspect = InputFieldsClass(fields=all_fields)
 
         return MetadataChangeProposalWrapper(
-            entityType="dashboard",
             entityUrn=dashboard_urn,
-            changeType=ChangeTypeClass.UPSERT,
-            aspectName="inputFields",
             aspect=input_fields_aspect,
         )
 
     def _make_metrics_dimensions_chart_mcp(
         self, dashboard_element: LookerDashboardElement, dashboard: LookerDashboard
     ) -> MetadataChangeProposalWrapper:
         chart_urn = builder.make_chart_urn(
             self.source_config.platform_name, dashboard_element.get_urn_element_id()
         )
         input_fields_aspect = InputFieldsClass(
             fields=self._input_fields_from_dashboard_element(dashboard_element)
         )
 
         return MetadataChangeProposalWrapper(
-            entityType="chart",
             entityUrn=chart_urn,
-            changeType=ChangeTypeClass.UPSERT,
-            aspectName="inputFields",
             aspect=input_fields_aspect,
         )
 
     def process_dashboard(
         self, dashboard_id: str, fields: List[str]
     ) -> Tuple[
         List[MetadataWorkUnit],
@@ -1354,12 +1340,9 @@
                 self.reporter.report_workunit(workunit)
                 yield workunit
             self.reporter.report_stage_end("usage_extraction")
 
     def get_report(self) -> SourceReport:
         return self.reporter
 
-    def get_platform_instance_id(self) -> str:
-        return self.source_config.platform_instance or self.platform
-
     def close(self):
         self.prepare_for_commit()
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/looker/looker_usage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/looker_usage.py`

 * *Files 0% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 import datetime
 import logging
 from abc import ABC, abstractmethod
 from dataclasses import dataclass
 from enum import Enum
 from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, cast
 
-from looker_sdk.sdk.api31.models import Dashboard, LookWithQuery
+from looker_sdk.sdk.api40.models import Dashboard, LookWithQuery
 
 import datahub.emitter.mce_builder as builder
 from datahub.emitter.mce_builder import Aspect, AspectAbstract
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.ingestion.source.looker import looker_common
 from datahub.ingestion.source.looker.looker_common import (
     LookerDashboardSourceReport,
@@ -235,15 +235,15 @@
         return round(
             datetime.datetime.strptime(date_time, "%Y-%m-%d")
             .replace(tzinfo=datetime.timezone.utc)
             .timestamp()
             * 1000
         )
 
-    def _get_user_identifier(self, row: Dict) -> int:
+    def _get_user_identifier(self, row: Dict) -> str:
         return row[UserViewField.USER_ID]
 
     def _process_entity_timeseries_rows(
         self, rows: List[Dict]
     ) -> Dict[Tuple[str, str], AspectAbstract]:
         # Convert Looker entity stat i.e. rows to DataHub stat aspect
         entity_stat_aspect: Dict[Tuple[str, str], AspectAbstract] = {}
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/looker/lookml_source.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/looker/lookml_source.py`

 * *Files 4% similar despite different names*

```diff
@@ -8,34 +8,38 @@
 from datetime import datetime, timedelta
 from typing import Any, Dict, Iterable, List, Optional, Set, Tuple, Type, Union
 
 import lkml
 import lkml.simple
 import pydantic
 from looker_sdk.error import SDKError
-from looker_sdk.sdk.api31.models import DBConnection
+from looker_sdk.sdk.api40.models import DBConnection
 from pydantic import root_validator, validator
 from pydantic.fields import Field
 
 import datahub.emitter.mce_builder as builder
 from datahub.configuration import ConfigModel
 from datahub.configuration.common import AllowDenyPattern, ConfigurationError
-from datahub.configuration.github import GitHubInfo
-from datahub.configuration.source_common import EnvBasedSourceConfigBase
+from datahub.configuration.git import GitInfo
+from datahub.configuration.source_common import EnvConfigMixin
+from datahub.configuration.validate_field_rename import pydantic_renamed_field
 from datahub.emitter.mce_builder import make_schema_field_urn
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.decorators import (
     SupportStatus,
+    capability,
     config_class,
     platform_name,
     support_status,
 )
 from datahub.ingestion.api.registry import import_path
+from datahub.ingestion.api.source import SourceCapability
 from datahub.ingestion.api.workunit import MetadataWorkUnit
+from datahub.ingestion.source.common.subtypes import DatasetSubTypes
 from datahub.ingestion.source.git.git_import import GitClone
 from datahub.ingestion.source.looker.looker_common import (
     LookerCommonConfig,
     LookerExplore,
     LookerUtil,
     LookerViewId,
     ProjectInclude,
@@ -64,15 +68,14 @@
     UpstreamClass,
     UpstreamLineage,
     ViewProperties,
 )
 from datahub.metadata.com.linkedin.pegasus2avro.metadata.snapshot import DatasetSnapshot
 from datahub.metadata.com.linkedin.pegasus2avro.mxe import MetadataChangeEvent
 from datahub.metadata.schema_classes import (
-    ChangeTypeClass,
     DatasetPropertiesClass,
     FineGrainedLineageClass,
     FineGrainedLineageUpstreamTypeClass,
     SubTypesClass,
 )
 from datahub.utilities.lossy_collections import LossyList
 from datahub.utilities.source_helpers import (
@@ -88,14 +91,18 @@
 # Patch lkml to support the local_dependency and remote_dependency keywords.
 lkml.simple.PLURAL_KEYS = (
     *lkml.simple.PLURAL_KEYS,
     "local_dependency",
     "remote_dependency",
 )
 
+_EXPLORE_FILE_EXTENSION = ".explore.lkml"
+_VIEW_FILE_EXTENSION = ".view.lkml"
+_MODEL_FILE_EXTENSION = ".model.lkml"
+
 
 def _get_bigquery_definition(
     looker_connection: DBConnection,
 ) -> Tuple[str, Optional[str], Optional[str]]:
     platform = "bigquery"
     # bigquery project ids are returned in the host field
     db = looker_connection.host
@@ -132,15 +139,15 @@
         default=None,
         description="The environment that the platform is located in. Leaving this empty will inherit defaults from the top level Looker configuration",
     )
 
     @validator("platform_env")
     def platform_env_must_be_one_of(cls, v: Optional[str]) -> Optional[str]:
         if v is not None:
-            return EnvBasedSourceConfigBase.env_must_be_one_of(v)
+            return EnvConfigMixin.env_must_be_one_of(v)
         return v
 
     @validator("platform", "default_db", "default_schema")
     def lower_everything(cls, v):
         """We lower case all strings passed in to avoid casing issues later"""
         if v is not None:
             return v.lower()
@@ -164,24 +171,27 @@
                 (platform, db, schema) = extracting_function(looker_connection)
                 return cls(platform=platform, default_db=db, default_schema=schema)
         raise ConfigurationError(
             f"Could not find an appropriate platform for looker_connection: {looker_connection.name} with dialect: {looker_connection.dialect_name}"
         )
 
 
-class LookMLSourceConfig(LookerCommonConfig, StatefulIngestionConfigBase):
-    github_info: Optional[GitHubInfo] = Field(
+class LookMLSourceConfig(
+    LookerCommonConfig, StatefulIngestionConfigBase, EnvConfigMixin
+):
+    git_info: Optional[GitInfo] = Field(
         None,
-        description="Reference to your github location. If present, supplies handy links to your lookml on the dataset entity page.",
+        description="Reference to your git location. If present, supplies handy links to your lookml on the dataset entity page.",
     )
+    _github_info_deprecated = pydantic_renamed_field("github_info", "git_info")
     base_folder: Optional[pydantic.DirectoryPath] = Field(
         None,
         description="Required if not providing github configuration and deploy keys. A pointer to a local directory (accessible to the ingestion system) where the root of the LookML repo has been checked out (typically via a git clone). This is typically the root folder where the `*.model.lkml` and `*.view.lkml` files are stored. e.g. If you have checked out your LookML repo under `/Users/jdoe/workspace/my-lookml-repo`, then set `base_folder` to `/Users/jdoe/workspace/my-lookml-repo`.",
     )
-    project_dependencies: Dict[str, Union[pydantic.DirectoryPath, GitHubInfo]] = Field(
+    project_dependencies: Dict[str, Union[pydantic.DirectoryPath, GitInfo]] = Field(
         {},
         description="A map of project_name to local directory (accessible to the ingestion system) or Git credentials. "
         "Every local_dependencies or private remote_dependency listed in the main project's manifest.lkml file should have a corresponding entry here. "
         "If a deploy key is not provided, the ingestion system will use the same deploy key as the main project. ",
     )
     connection_to_platform_map: Optional[Dict[str, LookerConnectionDefinition]] = Field(
         None,
@@ -224,22 +234,14 @@
         False,
         description="When enabled, sql parsing will be executed in a separate process to prevent memory leaks.",
     )
     stateful_ingestion: Optional[StatefulStaleMetadataRemovalConfig] = Field(
         default=None, description=""
     )
 
-    @validator("platform_instance")
-    def platform_instance_not_supported(cls, v: Optional[str]) -> Optional[str]:
-        if v is not None:
-            raise ConfigurationError(
-                "LookML Source doesn't support platform instance at the top level. However connection-specific platform instances are supported for generating lineage edges. Read the documentation to find out more."
-            )
-        return v
-
     @validator("connection_to_platform_map", pre=True)
     def convert_string_to_connection_def(cls, conn_map):
         # Previous version of config supported strings in connection map. This upconverts strings to ConnectionMap
         for key in conn_map:
             if isinstance(conn_map[key], str):
                 platform = conn_map[key]
                 if "." in platform:
@@ -280,17 +282,17 @@
         return values
 
     @validator("base_folder", always=True)
     def check_base_folder_if_not_provided(
         cls, v: Optional[pydantic.DirectoryPath], values: Dict[str, Any]
     ) -> Optional[pydantic.DirectoryPath]:
         if v is None:
-            github_info: Optional[GitHubInfo] = values.get("github_info", None)
-            if github_info and github_info.deploy_key:
-                # We have github_info populated correctly, base folder is not needed
+            git_info: Optional[GitInfo] = values.get("git_info")
+            if git_info and git_info.deploy_key:
+                # We have git_info populated correctly, base folder is not needed
                 pass
             else:
                 raise ValueError(
                     "base_folder is not provided. Neither has a github deploy_key or deploy_key_file been provided"
                 )
         return v
 
@@ -336,86 +338,127 @@
     explores: List[dict]
     resolved_includes: List[ProjectInclude]
 
     @staticmethod
     def from_looker_dict(
         looker_model_dict: dict,
         base_project_name: str,
-        base_folder: str,
+        root_project_name: Optional[str],
         base_projects_folders: Dict[str, pathlib.Path],
         path: str,
         reporter: LookMLSourceReport,
     ) -> "LookerModel":
         logger.debug(f"Loading model from {path}")
         connection = looker_model_dict["connection"]
         includes = looker_model_dict.get("includes", [])
         resolved_includes = LookerModel.resolve_includes(
             includes,
             base_project_name,
-            base_folder,
+            root_project_name,
             base_projects_folders,
             path,
             reporter,
             seen_so_far=set(),
             traversal_path=pathlib.Path(path).stem,
         )
         logger.debug(f"{path} has resolved_includes: {resolved_includes}")
         explores = looker_model_dict.get("explores", [])
 
+        explore_files = [
+            x.include
+            for x in resolved_includes
+            if x.include.endswith(_EXPLORE_FILE_EXTENSION)
+        ]
+        for included_file in explore_files:
+            try:
+                with open(included_file, "r") as file:
+                    parsed = lkml.load(file)
+                    included_explores = parsed.get("explores", [])
+                    explores.extend(included_explores)
+            except Exception as e:
+                reporter.report_warning(
+                    path, f"Failed to load {included_file} due to {e}"
+                )
+                # continue in this case, as it might be better to load and resolve whatever we can
+
         return LookerModel(
             connection=connection,
             includes=includes,
             resolved_includes=resolved_includes,
             explores=explores,
         )
 
     @staticmethod
     def resolve_includes(
         includes: List[str],
         project_name: str,
-        base_folder: str,
+        root_project_name: Optional[str],
         base_projects_folder: Dict[str, pathlib.Path],
         path: str,
         reporter: LookMLSourceReport,
         seen_so_far: Set[str],
         traversal_path: str = "",  # a cosmetic parameter to aid debugging
     ) -> List[ProjectInclude]:
         """Resolve ``include`` statements in LookML model files to a list of ``.lkml`` files.
 
         For rules on how LookML ``include`` statements are written, see
             https://docs.looker.com/data-modeling/getting-started/ide-folders#wildcard_examples
         """
+
         resolved = []
         for inc in includes:
-            resolved_project_name = project_name
             # Filter out dashboards - we get those through the looker source.
             if (
                 inc.endswith(".dashboard")
                 or inc.endswith(".dashboard.lookml")
                 or inc.endswith(".dashboard.lkml")
             ):
                 logger.debug(f"include '{inc}' is a dashboard, skipping it")
                 continue
 
+            resolved_project_name = project_name
+            resolved_project_folder = str(base_projects_folder[project_name])
+
             # Massage the looker include into a valid glob wildcard expression
             if inc.startswith("//"):
                 # remote include, let's see if we have the project checked out locally
                 (remote_project, project_local_path) = inc[2:].split("/", maxsplit=1)
                 if remote_project in base_projects_folder:
-                    glob_expr = (
-                        f"{base_projects_folder[remote_project]}/{project_local_path}"
-                    )
+                    resolved_project_folder = str(base_projects_folder[remote_project])
+                    glob_expr = f"{resolved_project_folder}/{project_local_path}"
                     resolved_project_name = remote_project
                 else:
                     logger.warning(
                         f"Resolving {inc} failed. Could not find a locally checked out reference for {remote_project}"
                     )
                     continue
             elif inc.startswith("/"):
-                glob_expr = f"{base_folder}{inc}"
+                glob_expr = f"{resolved_project_folder}{inc}"
+
+                # The include path is sometimes '/{project_name}/{path_within_project}'
+                # instead of '//{project_name}/{path_within_project}' or '/{path_within_project}'.
+                #
+                # TODO: I can't seem to find any documentation on this pattern, but we definitely
+                # have seen it in the wild. Example from Mozilla's public looker-hub repo:
+                # https://github.com/mozilla/looker-hub/blob/f491ca51ce1add87c338e6723fd49bc6ae4015ca/fenix/explores/activation.explore.lkml#L7
+                # As such, we try to handle it but are as defensive as possible.
+
+                non_base_project_name = project_name
+                if project_name == _BASE_PROJECT_NAME and root_project_name is not None:
+                    non_base_project_name = root_project_name
+                if non_base_project_name != _BASE_PROJECT_NAME and inc.startswith(
+                    f"/{non_base_project_name}/"
+                ):
+                    # This might be a local include. Let's make sure that '/{project_name}' doesn't
+                    # exist as normal include in the project.
+                    if not pathlib.Path(
+                        f"{resolved_project_folder}/{non_base_project_name}"
+                    ).exists():
+                        path_within_project = pathlib.Path(*pathlib.Path(inc).parts[2:])
+                        glob_expr = f"{resolved_project_folder}/{path_within_project}"
             else:
                 # Need to handle a relative path.
                 glob_expr = str(pathlib.Path(path).parent / inc)
             # "**" matches an arbitrary number of directories in LookML
             # we also resolve these paths to absolute paths so we can de-dup effectively later on
             included_files = [
                 str(p.resolve())
@@ -461,30 +504,29 @@
                         parsed = lkml.load(file)
                         seen_so_far.add(included_file)
                         if "includes" in parsed:  # we have more includes to resolve!
                             resolved.extend(
                                 LookerModel.resolve_includes(
                                     parsed["includes"],
                                     resolved_project_name,
-                                    base_folder,
+                                    root_project_name,
                                     base_projects_folder,
                                     included_file,
                                     reporter,
                                     seen_so_far,
                                     traversal_path=traversal_path
                                     + "."
                                     + pathlib.Path(included_file).stem,
                                 )
                             )
                 except Exception as e:
                     reporter.report_warning(
                         path, f"Failed to load {included_file} due to {e}"
                     )
                     # continue in this case, as it might be better to load and resolve whatever we can
-                    pass
 
             resolved.extend(
                 [
                     ProjectInclude(project=resolved_project_name, include=f)
                     for f in included_files
                 ]
             )
@@ -502,28 +544,28 @@
 
     @classmethod
     def from_looker_dict(
         cls,
         absolute_file_path: str,
         looker_view_file_dict: dict,
         project_name: str,
-        base_folder: str,
+        root_project_name: Optional[str],
         base_projects_folder: Dict[str, pathlib.Path],
         raw_file_content: str,
         reporter: LookMLSourceReport,
     ) -> "LookerViewFile":
         logger.debug(f"Loading view file at {absolute_file_path}")
         includes = looker_view_file_dict.get("includes", [])
         resolved_path = str(pathlib.Path(absolute_file_path).resolve())
         seen_so_far = set()
         seen_so_far.add(resolved_path)
         resolved_includes = LookerModel.resolve_includes(
             includes,
             project_name,
-            base_folder,
+            root_project_name,
             base_projects_folder,
             absolute_file_path,
             reporter,
             seen_so_far=seen_so_far,
         )
         logger.debug(
             f"resolved_includes for {absolute_file_path} is {resolved_includes}"
@@ -550,35 +592,39 @@
     """
     Loads the looker viewfile at a :path and caches the LookerViewFile in memory
     This is to avoid reloading the same file off of disk many times during the recursive include resolution process
     """
 
     def __init__(
         self,
-        base_folder: str,
+        root_project_name: Optional[str],
         base_projects_folder: Dict[str, pathlib.Path],
         reporter: LookMLSourceReport,
     ) -> None:
         self.viewfile_cache: Dict[str, LookerViewFile] = {}
-        self._base_folder = base_folder
+        self._root_project_name = root_project_name
         self._base_projects_folder = base_projects_folder
         self.reporter = reporter
 
     def is_view_seen(self, path: str) -> bool:
         return path in self.viewfile_cache
 
     def _load_viewfile(
         self, project_name: str, path: str, reporter: LookMLSourceReport
     ) -> Optional[LookerViewFile]:
         # always fully resolve paths to simplify de-dup
         path = str(pathlib.Path(path).resolve())
-        if not path.endswith(".view.lkml"):
+        allowed_extensions = [_VIEW_FILE_EXTENSION, _EXPLORE_FILE_EXTENSION]
+        matched_any_extension = [
+            match for match in [path.endswith(x) for x in allowed_extensions] if match
+        ]
+        if not matched_any_extension:
             # not a view file
             logger.debug(
-                f"Skipping file {path} because it doesn't appear to be a view file"
+                f"Skipping file {path} because it doesn't appear to be a view file. Matched extensions {allowed_extensions}"
             )
             return None
 
         if self.is_view_seen(str(path)):
             return self.viewfile_cache[path]
 
         try:
@@ -591,15 +637,15 @@
             with open(path, "r") as file:
                 logger.debug(f"Loading viewfile {path}")
                 parsed = lkml.load(file)
                 looker_viewfile = LookerViewFile.from_looker_dict(
                     absolute_file_path=path,
                     looker_view_file_dict=parsed,
                     project_name=project_name,
-                    base_folder=self._base_folder,
+                    root_project_name=self._root_project_name,
                     base_projects_folder=self._base_projects_folder,
                     raw_file_content=raw_file_content,
                     reporter=reporter,
                 )
                 logger.debug(f"adding viewfile for path {path} to the cache")
                 self.viewfile_cache[path] = looker_viewfile
                 return looker_viewfile
@@ -656,14 +702,15 @@
 
 @dataclass
 class LookerView:
     id: LookerViewId
     absolute_file_path: str
     connection: LookerConnectionDefinition
     sql_table_names: List[str]
+    upstream_explores: List[str]
     fields: List[ViewField]
     raw_file_content: str
     view_details: Optional[ViewProperties] = None
 
     @classmethod
     def _import_sql_parser_cls(cls, sql_parser_path: str) -> Type[SQLParser]:
         assert "." in sql_parser_path, "sql_parser-path must contain a ."
@@ -742,14 +789,19 @@
                     ):
                         matched_field = upstream_field_match.group(1)
                         # Remove quotes from field names
                         matched_field = (
                             matched_field.replace('"', "").replace("`", "").lower()
                         )
                         upstream_fields.append(matched_field)
+                else:
+                    # If no SQL is specified, we assume this is referencing an upstream field
+                    # with the same name. This commonly happens for extends and derived tables.
+                    upstream_fields.append(name)
+
             upstream_fields = sorted(list(set(upstream_fields)))
 
             field = ViewField(
                 name=name,
                 type=native_type,
                 label=label,
                 description=description,
@@ -823,163 +875,211 @@
             looker_view.get("measures", []),
             ViewFieldType.MEASURE,
             extract_col_level_lineage,
             populate_sql_logic_in_descriptions=populate_sql_logic_in_descriptions,
         )
         fields: List[ViewField] = dimensions + dimension_groups + measures
 
-        # also store the view logic and materialization
+        # Prep "default" values for the view, which will be overridden by the logic below.
         view_logic = looker_viewfile.raw_file_content[:max_file_snippet_length]
+        sql_table_names: List[str] = []
+        upstream_explores: List[str] = []
 
-        # Parse SQL from derived tables to extract dependencies
         if derived_table is not None:
-            fields, sql_table_names = cls._extract_metadata_from_sql_query(
-                reporter,
-                parse_table_names_from_sql,
-                sql_parser_path,
-                view_name,
-                sql_table_name,
-                derived_table,
-                fields,
-                use_external_process=process_isolation_for_sql_parsing,
-            )
+            # Derived tables can either be a SQL query or a LookML explore.
+            # See https://cloud.google.com/looker/docs/derived-tables.
+
             if "sql" in derived_table:
                 view_logic = derived_table["sql"]
                 view_lang = VIEW_LANGUAGE_SQL
-            if "explore_source" in derived_table:
-                view_logic = str(derived_table["explore_source"])
+
+                # Parse SQL to extract dependencies.
+                if parse_table_names_from_sql:
+                    (
+                        fields,
+                        sql_table_names,
+                    ) = cls._extract_metadata_from_derived_table_sql(
+                        reporter,
+                        sql_parser_path,
+                        view_name,
+                        sql_table_name,
+                        view_logic,
+                        fields,
+                        use_external_process=process_isolation_for_sql_parsing,
+                    )
+
+            elif "explore_source" in derived_table:
+                # This is called a "native derived table".
+                # See https://cloud.google.com/looker/docs/creating-ndts.
+                explore_source = derived_table["explore_source"]
+
+                # We want this to render the full lkml block
+                # e.g. explore_source: source_name { ... }
+                # As such, we use the full derived_table instead of the explore_source.
+                view_logic = str(lkml.dump(derived_table))[:max_file_snippet_length]
                 view_lang = VIEW_LANGUAGE_LOOKML
 
+                (
+                    fields,
+                    upstream_explores,
+                ) = cls._extract_metadata_from_derived_table_explore(
+                    reporter, view_name, explore_source, fields
+                )
+
             materialized = False
             for k in derived_table:
                 if k in ["datagroup_trigger", "sql_trigger_value", "persist_for"]:
                     materialized = True
             if "materialized_view" in derived_table:
                 materialized = derived_table["materialized_view"] == "yes"
 
             view_details = ViewProperties(
                 materialized=materialized, viewLogic=view_logic, viewLanguage=view_lang
             )
+        else:
+            # If not a derived table, then this view essentially wraps an existing
+            # object in the database. If sql_table_name is set, there is a single
+            # dependency in the view, on the sql_table_name.
+            # Otherwise, default to the view name as per the docs:
+            # https://docs.looker.com/reference/view-params/sql_table_name-for-view
+            sql_table_names = (
+                [view_name] if sql_table_name is None else [sql_table_name]
+            )
+            view_details = ViewProperties(
+                materialized=False,
+                viewLogic=view_logic,
+                viewLanguage=VIEW_LANGUAGE_LOOKML,
+            )
 
-            return LookerView(
-                id=LookerViewId(
-                    project_name=project_name,
-                    model_name=model_name,
-                    view_name=view_name,
-                ),
-                absolute_file_path=looker_viewfile.absolute_file_path,
-                connection=connection,
-                sql_table_names=sql_table_names,
-                fields=fields,
-                raw_file_content=looker_viewfile.raw_file_content,
-                view_details=view_details,
-            )
-
-        # If not a derived table, then this view essentially wraps an existing
-        # object in the database. If sql_table_name is set, there is a single
-        # dependency in the view, on the sql_table_name.
-        # Otherwise, default to the view name as per the docs:
-        # https://docs.looker.com/reference/view-params/sql_table_name-for-view
-        sql_table_names = [view_name] if sql_table_name is None else [sql_table_name]
-        output_looker_view = LookerView(
+        return LookerView(
             id=LookerViewId(
                 project_name=project_name, model_name=model_name, view_name=view_name
             ),
             absolute_file_path=looker_viewfile.absolute_file_path,
-            sql_table_names=sql_table_names,
             connection=connection,
+            sql_table_names=sql_table_names,
+            upstream_explores=upstream_explores,
             fields=fields,
             raw_file_content=looker_viewfile.raw_file_content,
-            view_details=ViewProperties(
-                materialized=False,
-                viewLogic=view_logic,
-                viewLanguage=VIEW_LANGUAGE_LOOKML,
-            ),
+            view_details=view_details,
         )
-        return output_looker_view
 
     @classmethod
-    def _extract_metadata_from_sql_query(
-        cls: Type,
+    def _extract_metadata_from_derived_table_sql(
+        cls,
         reporter: LookMLSourceReport,
-        parse_table_names_from_sql: bool,
         sql_parser_path: str,
         view_name: str,
         sql_table_name: Optional[str],
-        derived_table: dict,
+        sql_query: str,
         fields: List[ViewField],
         use_external_process: bool,
     ) -> Tuple[List[ViewField], List[str]]:
         sql_table_names: List[str] = []
-        if parse_table_names_from_sql and "sql" in derived_table:
-            logger.debug(f"Parsing sql from derived table section of view: {view_name}")
-            sql_query = derived_table["sql"]
-            reporter.query_parse_attempts += 1
-
-            # Skip queries that contain liquid variables. We currently don't parse them correctly.
-            # Docs: https://cloud.google.com/looker/docs/liquid-variable-reference.
-            # TODO: also support ${EXTENDS} and ${TABLE}
-            if "{%" in sql_query:
-                try:
-                    # test if parsing works
-                    sql_info: SQLInfo = cls._get_sql_info(
-                        sql_query, sql_parser_path, use_external_process
-                    )
-                    if not sql_info.table_names:
-                        raise Exception("Failed to find any tables")
-                except Exception:
-                    logger.debug(
-                        f"{view_name}: SQL Parsing didn't return any tables, trying a hail-mary"
-                    )
-                    # A hail-mary simple parse.
-                    for maybe_table_match in re.finditer(
-                        r"FROM\s*([a-zA-Z0-9_.`]+)", sql_query
-                    ):
-                        if maybe_table_match.group(1) not in sql_table_names:
-                            sql_table_names.append(maybe_table_match.group(1))
-                    return fields, sql_table_names
-            # Looker supports sql fragments that omit the SELECT and FROM parts of the query
-            # Add those in if we detect that it is missing
-            if not re.search(r"SELECT\s", sql_query, flags=re.I):
-                # add a SELECT clause at the beginning
-                sql_query = f"SELECT {sql_query}"
-            if not re.search(r"FROM\s", sql_query, flags=re.I):
-                # add a FROM clause at the end
-                sql_query = f"{sql_query} FROM {sql_table_name if sql_table_name is not None else view_name}"
-                # Get the list of tables in the query
+
+        logger.debug(f"Parsing sql from derived table section of view: {view_name}")
+        reporter.query_parse_attempts += 1
+
+        # Skip queries that contain liquid variables. We currently don't parse them correctly.
+        # Docs: https://cloud.google.com/looker/docs/liquid-variable-reference.
+        # TODO: also support ${EXTENDS} and ${TABLE}
+        if "{%" in sql_query:
             try:
-                sql_info = cls._get_sql_info(
+                # test if parsing works
+                sql_info: SQLInfo = cls._get_sql_info(
                     sql_query, sql_parser_path, use_external_process
                 )
-                sql_table_names = sql_info.table_names
-                column_names = sql_info.column_names
-                if not fields:
-                    # it seems like the view is defined purely as sql, let's try using the column names to populate the schema
-                    fields = [
-                        # set types to unknown for now as our sql parser doesn't give us column types yet
-                        ViewField(c, "", "unknown", "", ViewFieldType.UNKNOWN)
-                        for c in sorted(column_names)
-                    ]
                 if not sql_info.table_names:
-                    reporter.query_parse_failures += 1
-                    reporter.query_parse_failure_views.append(view_name)
-            except Exception as e:
-                reporter.query_parse_failures += 1
-                reporter.report_warning(
-                    f"looker-view-{view_name}",
-                    f"Failed to parse sql query, lineage will not be accurate. Exception: {e}",
+                    raise Exception("Failed to find any tables")
+            except Exception:
+                logger.debug(
+                    f"{view_name}: SQL Parsing didn't return any tables, trying a hail-mary"
                 )
+                # A hail-mary simple parse.
+                for maybe_table_match in re.finditer(
+                    r"FROM\s*([a-zA-Z0-9_.`]+)", sql_query
+                ):
+                    if maybe_table_match.group(1) not in sql_table_names:
+                        sql_table_names.append(maybe_table_match.group(1))
+                return fields, sql_table_names
+
+        # Looker supports sql fragments that omit the SELECT and FROM parts of the query
+        # Add those in if we detect that it is missing
+        if not re.search(r"SELECT\s", sql_query, flags=re.I):
+            # add a SELECT clause at the beginning
+            sql_query = f"SELECT {sql_query}"
+        if not re.search(r"FROM\s", sql_query, flags=re.I):
+            # add a FROM clause at the end
+            sql_query = f"{sql_query} FROM {sql_table_name if sql_table_name is not None else view_name}"
+            # Get the list of tables in the query
+        try:
+            sql_info = cls._get_sql_info(
+                sql_query, sql_parser_path, use_external_process
+            )
+            sql_table_names = sql_info.table_names
+            column_names = sql_info.column_names
+
+            if not fields:
+                # it seems like the view is defined purely as sql, let's try using the column names to populate the schema
+                fields = [
+                    # set types to unknown for now as our sql parser doesn't give us column types yet
+                    ViewField(c, "", "unknown", "", ViewFieldType.UNKNOWN)
+                    for c in sorted(column_names)
+                ]
+                # remove fields or sql tables that contain liquid variables
+                fields = [f for f in fields if "{%" not in f.name]
+
+            if not sql_info.table_names:
+                reporter.query_parse_failures += 1
+                reporter.query_parse_failure_views.append(view_name)
+        except Exception as e:
+            reporter.query_parse_failures += 1
+            reporter.report_warning(
+                f"looker-view-{view_name}",
+                f"Failed to parse sql query, lineage will not be accurate. Exception: {e}",
+            )
 
-        # remove fields or sql tables that contain liquid variables
-        fields = [f for f in fields if "{%" not in f.name]
         sql_table_names = [table for table in sql_table_names if "{%" not in table]
 
         return fields, sql_table_names
 
     @classmethod
+    def _extract_metadata_from_derived_table_explore(
+        cls,
+        reporter: LookMLSourceReport,
+        view_name: str,
+        explore_source: dict,
+        fields: List[ViewField],
+    ) -> Tuple[List[ViewField], List[str]]:
+        logger.debug(
+            f"Parsing explore_source from derived table section of view: {view_name}"
+        )
+
+        upstream_explores = [explore_source["name"]]
+
+        explore_columns = explore_source.get("columns", [])
+        # TODO: We currently don't support column-level lineage for derived_column.
+        # In order to support it, we'd need to parse the `sql` field of the derived_column.
+
+        # The fields in the view are actually references to the fields in the explore.
+        # As such, we need to perform an extra mapping step to update
+        # the upstream column names.
+        for field in fields:
+            for i, upstream_field in enumerate(field.upstream_fields):
+                # Find the matching column in the explore.
+                for explore_column in explore_columns:
+                    if explore_column["name"] == upstream_field:
+                        field.upstream_fields[i] = explore_column.get(
+                            "field", explore_column["name"]
+                        )
+                        break
+
+        return fields, upstream_explores
+
+    @classmethod
     def resolve_extends_view_name(
         cls,
         connection: LookerConnectionDefinition,
         looker_viewfile: LookerViewFile,
         looker_viewfile_loader: LookerViewFileLoader,
         target_view_name: str,
         reporter: LookMLSourceReport,
@@ -1059,14 +1159,23 @@
     local_dependencies: List[str]
     remote_dependencies: List[LookerRemoteDependency]
 
 
 @platform_name("Looker")
 @config_class(LookMLSourceConfig)
 @support_status(SupportStatus.CERTIFIED)
+@capability(
+    SourceCapability.PLATFORM_INSTANCE,
+    "Supported using the `connection_to_platform_map`",
+)
+@capability(SourceCapability.LINEAGE_COARSE, "Supported by default")
+@capability(
+    SourceCapability.LINEAGE_FINE,
+    "Enabled by default, configured using `extract_column_level_lineage`",
+)
 class LookMLSource(StatefulIngestionSourceBase):
     """
     This plugin extracts the following:
     - LookML views from model files in a project
     - Name, upstream table names, metadata for dimensions, measures, and dimension groups attached as tags
     - If API integration is enabled (recommended), resolves table and view names by calling the Looker API, otherwise supports offline resolution of these names.
 
@@ -1078,15 +1187,15 @@
     platform = "lookml"
     source_config: LookMLSourceConfig
     reporter: LookMLSourceReport
     looker_client: Optional[LookerAPI] = None
 
     # This is populated during the git clone step.
     base_projects_folder: Dict[str, pathlib.Path] = {}
-    remote_projects_github_info: Dict[str, GitHubInfo] = {}
+    remote_projects_git_info: Dict[str, GitInfo] = {}
 
     def __init__(self, config: LookMLSourceConfig, ctx: PipelineContext):
         super().__init__(config, ctx)
         self.source_config = config
         self.reporter = LookMLSourceReport()
         if self.source_config.api:
             self.looker_client = LookerAPI(self.source_config.api)
@@ -1109,15 +1218,15 @@
     def _load_model(self, path: str) -> LookerModel:
         with open(path, "r") as file:
             logger.debug(f"Loading model from file {path}")
             parsed = lkml.load(file)
             looker_model = LookerModel.from_looker_dict(
                 parsed,
                 _BASE_PROJECT_NAME,
-                str(self.source_config.base_folder),
+                self.source_config.project_name,
                 self.base_projects_folder,
                 path,
                 self.reporter,
             )
         return looker_model
 
     def _platform_names_have_2_parts(self, platform: str) -> bool:
@@ -1200,21 +1309,23 @@
     ) -> Optional[LookerConnectionDefinition]:
         if self.source_config.connection_to_platform_map is None:
             self.source_config.connection_to_platform_map = {}
         assert self.source_config.connection_to_platform_map is not None
         if connection in self.source_config.connection_to_platform_map:
             return self.source_config.connection_to_platform_map[connection]
         elif self.looker_client:
-            looker_connection: Optional[DBConnection] = None
             try:
-                looker_connection = self.looker_client.connection(connection)
+                looker_connection: DBConnection = self.looker_client.connection(
+                    connection
+                )
             except SDKError:
-                logger.error(f"Failed to retrieve connection {connection} from Looker")
-
-            if looker_connection:
+                logger.error(
+                    f"Failed to retrieve connection {connection} from Looker. This usually happens when the credentials provided are not admin credentials."
+                )
+            else:
                 try:
                     connection_def: LookerConnectionDefinition = (
                         LookerConnectionDefinition.from_looker_connection(
                             looker_connection
                         )
                     )
 
@@ -1230,21 +1341,39 @@
                     )
 
         return None
 
     def _get_upstream_lineage(
         self, looker_view: LookerView
     ) -> Optional[UpstreamLineage]:
-        upstreams = []
+        # Merge dataset upstreams with sql table upstreams.
+        upstream_dataset_urns = []
+        for upstream_explore in looker_view.upstream_explores:
+            # We're creating a "LookerExplore" just to use the urn generator.
+            upstream_dataset_urn = LookerExplore(
+                name=upstream_explore, model_name=looker_view.id.model_name
+            ).get_explore_urn(self.source_config)
+            upstream_dataset_urns.append(upstream_dataset_urn)
         for sql_table_name in looker_view.sql_table_names:
             sql_table_name = sql_table_name.replace('"', "").replace("`", "")
-            upstream_dataset_urn: str = self._construct_datalineage_urn(
+            upstream_dataset_urn = self._construct_datalineage_urn(
                 sql_table_name, looker_view
             )
-            fine_grained_lineages: List[FineGrainedLineageClass] = []
+            upstream_dataset_urns.append(upstream_dataset_urn)
+
+        # Generate the upstream + fine grained lineage objects.
+        upstreams = []
+        fine_grained_lineages: List[FineGrainedLineageClass] = []
+        for upstream_dataset_urn in upstream_dataset_urns:
+            upstream = UpstreamClass(
+                dataset=upstream_dataset_urn,
+                type=DatasetLineageTypeClass.VIEW,
+            )
+            upstreams.append(upstream)
+
             if self.source_config.extract_column_level_lineage and (
                 looker_view.view_details is not None
                 and looker_view.view_details.viewLanguage
                 != VIEW_LANGUAGE_SQL  # we currently only map col-level lineage for views without sql
             ):
                 for field in looker_view.fields:
                     if field.upstream_fields:
@@ -1262,35 +1391,26 @@
                                     looker_view.id.get_urn(self.source_config),
                                     field.name,
                                 )
                             ],
                         )
                         fine_grained_lineages.append(fine_grained_lineage)
 
-            upstream = UpstreamClass(
-                dataset=upstream_dataset_urn,
-                type=DatasetLineageTypeClass.VIEW,
-            )
-            upstreams.append(upstream)
-
         if upstreams != []:
             return UpstreamLineage(
                 upstreams=upstreams, fineGrainedLineages=fine_grained_lineages or None
             )
         else:
             return None
 
     def _get_custom_properties(self, looker_view: LookerView) -> DatasetPropertiesClass:
         assert self.source_config.base_folder  # this is always filled out
-        if looker_view.id.project_name == _BASE_PROJECT_NAME:
-            base_folder = self.source_config.base_folder
-        else:
-            base_folder = self.base_projects_folder.get(
-                looker_view.id.project_name, self.source_config.base_folder
-            )
+        base_folder = self.base_projects_folder.get(
+            looker_view.id.project_name, self.source_config.base_folder
+        )
         try:
             file_path = str(
                 pathlib.Path(looker_view.absolute_file_path).relative_to(
                     base_folder.resolve()
                 )
             )
         except Exception:
@@ -1302,46 +1422,40 @@
         custom_properties = {
             "looker.file.path": file_path or looker_view.absolute_file_path,
         }
         dataset_props = DatasetPropertiesClass(
             name=looker_view.id.view_name, customProperties=custom_properties
         )
 
-        maybe_github_info = self.source_config.project_dependencies.get(
+        maybe_git_info = self.source_config.project_dependencies.get(
             looker_view.id.project_name,
-            self.remote_projects_github_info.get(looker_view.id.project_name),
+            self.remote_projects_git_info.get(looker_view.id.project_name),
         )
-        if isinstance(maybe_github_info, GitHubInfo):
-            github_info: Optional[GitHubInfo] = maybe_github_info
+        if isinstance(maybe_git_info, GitInfo):
+            git_info: Optional[GitInfo] = maybe_git_info
         else:
-            github_info = self.source_config.github_info
-        if github_info is not None and file_path:
+            git_info = self.source_config.git_info
+        if git_info is not None and file_path:
             # It should be that looker_view.id.project_name is the base project.
-            github_file_url = github_info.get_url_for_file_path(file_path)
+            github_file_url = git_info.get_url_for_file_path(file_path)
             dataset_props.externalUrl = github_file_url
 
         return dataset_props
 
     def _build_dataset_mcps(
         self, looker_view: LookerView
     ) -> List[MetadataChangeProposalWrapper]:
         subTypeEvent = MetadataChangeProposalWrapper(
-            entityType="dataset",
-            changeType=ChangeTypeClass.UPSERT,
             entityUrn=looker_view.id.get_urn(self.source_config),
-            aspectName="subTypes",
-            aspect=SubTypesClass(typeNames=["view"]),
+            aspect=SubTypesClass(typeNames=[DatasetSubTypes.VIEW]),
         )
         events = [subTypeEvent]
         if looker_view.view_details is not None:
             viewEvent = MetadataChangeProposalWrapper(
-                entityType="dataset",
-                changeType=ChangeTypeClass.UPSERT,
                 entityUrn=looker_view.id.get_urn(self.source_config),
-                aspectName="viewProperties",
                 aspect=looker_view.view_details,
             )
             events.append(viewEvent)
 
         return events
 
     def _build_dataset_mce(self, looker_view: LookerView) -> MetadataChangeEvent:
@@ -1420,62 +1534,62 @@
             auto_status_aspect(self.get_workunits_internal()),
         )
 
     def get_workunits_internal(self) -> Iterable[MetadataWorkUnit]:
         with tempfile.TemporaryDirectory("lookml_tmp") as tmp_dir:
             # Clone the base_folder if necessary.
             if not self.source_config.base_folder:
-                assert self.source_config.github_info
+                assert self.source_config.git_info
                 # we don't have a base_folder, so we need to clone the repo and process it locally
                 start_time = datetime.now()
                 git_clone = GitClone(tmp_dir)
                 # github info deploy key is always populated
-                assert self.source_config.github_info.deploy_key
-                assert self.source_config.github_info.repo_ssh_locator
+                assert self.source_config.git_info.deploy_key
+                assert self.source_config.git_info.repo_ssh_locator
                 checkout_dir = git_clone.clone(
-                    ssh_key=self.source_config.github_info.deploy_key,
-                    repo_url=self.source_config.github_info.repo_ssh_locator,
-                    branch=self.source_config.github_info.branch_for_clone,
+                    ssh_key=self.source_config.git_info.deploy_key,
+                    repo_url=self.source_config.git_info.repo_ssh_locator,
+                    branch=self.source_config.git_info.branch_for_clone,
                 )
                 self.reporter.git_clone_latency = datetime.now() - start_time
                 self.source_config.base_folder = checkout_dir.resolve()
 
             self.base_projects_folder[
                 _BASE_PROJECT_NAME
             ] = self.source_config.base_folder
+
             visited_projects: Set[str] = set()
 
             # We clone everything that we're pointed at.
             for project, p_ref in self.source_config.project_dependencies.items():
                 # If we were given GitHub info, we need to clone the project.
-                if isinstance(p_ref, GitHubInfo):
+                if isinstance(p_ref, GitInfo):
                     assert p_ref.repo_ssh_locator
 
                     p_cloner = GitClone(f"{tmp_dir}/_included_/{project}")
                     try:
                         p_checkout_dir = p_cloner.clone(
                             ssh_key=(
                                 # If a deploy key was provided, use it. Otherwise, fall back
                                 # to the main project deploy key.
                                 p_ref.deploy_key
                                 or (
-                                    self.source_config.github_info.deploy_key
-                                    if self.source_config.github_info
+                                    self.source_config.git_info.deploy_key
+                                    if self.source_config.git_info
                                     else None
                                 )
                             ),
                             repo_url=p_ref.repo_ssh_locator,
                             branch=p_ref.branch_for_clone,
                         )
 
                         p_ref = p_checkout_dir.resolve()
                     except Exception as e:
                         logger.warning(
-                            f"Failed to clone remote project {project}. This can lead to failures in parsing lookml files later on",
-                            e,
+                            f"Failed to clone remote project {project}. This can lead to failures in parsing lookml files later on: {e}",
                         )
                         visited_projects.add(project)
                         continue
 
                 self.base_projects_folder[project] = p_ref
 
             self._recursively_check_manifests(
@@ -1496,49 +1610,61 @@
             logger.warning(
                 f"Could not find {project_name} in the project_dependencies config. This can lead to failures in parsing lookml files later on.",
             )
             return
 
         manifest = self.get_manifest_if_present(project_path)
         if manifest:
+            # Special case handling if the root project has a name in the manifest file.
+            if project_name == _BASE_PROJECT_NAME and manifest.project_name:
+                if (
+                    self.source_config.project_name is not None
+                    and manifest.project_name != self.source_config.project_name
+                ):
+                    logger.warning(
+                        f"The project name in the manifest file '{manifest.project_name}'"
+                        f"does not match the configured project name '{self.source_config.project_name}'. "
+                        "This can lead to failures in LookML include resolution and lineage generation."
+                    )
+                elif self.source_config.project_name is None:
+                    self.source_config.project_name = manifest.project_name
+
             # Clone the remote project dependencies.
             for remote_project in manifest.remote_dependencies:
                 if remote_project.name in project_visited:
                     continue
 
                 p_cloner = GitClone(f"{tmp_dir}/_remote_/{project_name}")
                 try:
                     # TODO: For 100% correctness, we should be consulting
                     # the manifest lock file for the exact ref to use.
 
                     p_checkout_dir = p_cloner.clone(
                         ssh_key=(
-                            self.source_config.github_info.deploy_key
-                            if self.source_config.github_info
+                            self.source_config.git_info.deploy_key
+                            if self.source_config.git_info
                             else None
                         ),
                         repo_url=remote_project.url,
                     )
 
                     self.base_projects_folder[
                         remote_project.name
                     ] = p_checkout_dir.resolve()
                     repo = p_cloner.get_last_repo_cloned()
                     assert repo
-                    remote_github_info = GitHubInfo(
+                    remote_git_info = GitInfo(
                         url_template=remote_project.url,
                         repo="dummy/dummy",  # set to dummy values to bypass validation
                         branch=repo.active_branch.name,
                     )
-                    remote_github_info.repo = (
+                    remote_git_info.repo = (
                         ""  # set to empty because url already contains the full path
                     )
-                    self.remote_projects_github_info[
-                        remote_project.name
-                    ] = remote_github_info
+                    self.remote_projects_git_info[remote_project.name] = remote_git_info
 
                 except Exception as e:
                     logger.warning(
                         f"Failed to clone remote project {project_name}. This can lead to failures in parsing lookml files later on",
                         e,
                     )
                     project_visited.add(project_name)
@@ -1550,27 +1676,29 @@
             for project in manifest.local_dependencies:
                 self._recursively_check_manifests(tmp_dir, project, project_visited)
 
     def get_internal_workunits(self) -> Iterable[MetadataWorkUnit]:  # noqa: C901
         assert self.source_config.base_folder
 
         viewfile_loader = LookerViewFileLoader(
-            str(self.source_config.base_folder),
+            self.source_config.project_name,
             self.base_projects_folder,
             self.reporter,
         )
 
         # some views can be mentioned by multiple 'include' statements and can be included via different connections.
         # So this set is used to prevent creating duplicate events
         processed_view_map: Dict[str, Set[str]] = {}
         view_connection_map: Dict[str, Tuple[str, str]] = {}
 
         # The ** means "this directory and all subdirectories", and hence should
         # include all the files we want.
-        model_files = sorted(self.source_config.base_folder.glob("**/*.model.lkml"))
+        model_files = sorted(
+            self.source_config.base_folder.glob(f"**/*{_MODEL_FILE_EXTENSION}")
+        )
         model_suffix_len = len(".model")
 
         for file_path in model_files:
             self.reporter.report_models_scanned()
             model_name = file_path.stem[:-model_suffix_len]
 
             if not self.source_config.model_pattern.allowed(model_name):
@@ -1596,22 +1724,24 @@
                     f"Failed to load connection {model.connection}. Check your API key permissions.",
                 )
                 self.reporter.report_models_dropped(model_name)
                 continue
 
             explore_reachable_views: Set[ProjectInclude] = set()
             if self.source_config.emit_reachable_views_only:
+                model_explores_map = {d["name"]: d for d in model.explores}
                 for explore_dict in model.explores:
                     try:
                         explore: LookerExplore = LookerExplore.from_dict(
                             model_name,
                             explore_dict,
                             model.resolved_includes,
                             viewfile_loader,
                             self.reporter,
+                            model_explores_map,
                         )
                         if explore.upstream_views:
                             for view_name in explore.upstream_views:
                                 explore_reachable_views.add(view_name)
                     except Exception as e:
                         self.reporter.report_warning(
                             f"{model}.explores",
@@ -1742,12 +1872,9 @@
                 )
                 self.reporter.report_workunit(workunit)
                 yield workunit
 
     def get_report(self):
         return self.reporter
 
-    def get_platform_instance_id(self) -> str:
-        return self.source_config.platform_instance or self.platform
-
     def close(self):
         self.prepare_for_commit()
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/metabase.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/metabase.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/metadata/business_glossary.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/metadata/business_glossary.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,51 +1,42 @@
 import logging
 import pathlib
 import time
 from dataclasses import dataclass, field
 from typing import Any, Dict, Iterable, List, Optional, Union
 
-import pydantic
 from pydantic import validator
 from pydantic.fields import Field
 
 import datahub.metadata.schema_classes as models
 from datahub.configuration.common import ConfigModel
 from datahub.configuration.config_loader import load_config_file
-from datahub.emitter.mce_builder import (
-    datahub_guid,
-    get_sys_time,
-    make_group_urn,
-    make_user_urn,
-)
+from datahub.emitter.mce_builder import datahub_guid, make_group_urn, make_user_urn
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.decorators import (  # SourceCapability,; capability,
     SupportStatus,
     config_class,
     platform_name,
     support_status,
 )
 from datahub.ingestion.api.source import Source, SourceReport
 from datahub.ingestion.api.workunit import MetadataWorkUnit, UsageStatsWorkUnit
 from datahub.ingestion.graph.client import DataHubGraph
 from datahub.utilities.registries.domain_registry import DomainRegistry
+from datahub.utilities.source_helpers import auto_workunit_reporter
 from datahub.utilities.urn_encoder import UrnEncoder
 
 logger = logging.getLogger(__name__)
 
 valid_status: models.StatusClass = models.StatusClass(removed=False)
 
 # This needed to map path presents in inherits, contains, values, and related_terms to terms' optional id
 path_vs_id: Dict[str, Optional[str]] = {}
 
-auditStamp = models.AuditStampClass(
-    time=get_sys_time(), actor="urn:li:corpUser:restEmitter"
-)
-
 
 class Owners(ConfigModel):
     users: Optional[List[str]]
     groups: Optional[List[str]]
 
 
 class KnowledgeCard(ConfigModel):
@@ -89,16 +80,16 @@
     source: str
     owners: Owners
     url: Optional[str] = None
     source_type: Optional[str] = "INTERNAL"
 
 
 class BusinessGlossarySourceConfig(ConfigModel):
-    file: pydantic.FilePath = Field(
-        description="Path to business glossary file to ingest."
+    file: Union[str, pathlib.Path] = Field(
+        description="File path or URL to business glossary file to ingest."
     )
     enable_auto_id: bool = Field(
         description="Generate id field from GlossaryNode and GlossaryTerm's name field",
         default=False,
     )
 
 
@@ -477,31 +468,32 @@
     report: SourceReport = field(default_factory=SourceReport)
 
     @classmethod
     def create(cls, config_dict, ctx):
         config = BusinessGlossarySourceConfig.parse_obj(config_dict)
         return cls(ctx, config)
 
-    def load_glossary_config(self, file_name: pathlib.Path) -> BusinessGlossaryConfig:
+    def load_glossary_config(
+        self, file_name: Union[str, pathlib.Path]
+    ) -> BusinessGlossaryConfig:
         config = load_config_file(file_name)
         glossary_cfg = BusinessGlossaryConfig.parse_obj(config)
         return glossary_cfg
 
     def get_workunits(self) -> Iterable[Union[MetadataWorkUnit, UsageStatsWorkUnit]]:
+        return auto_workunit_reporter(self.report, self.get_workunits_internal())
+
+    def get_workunits_internal(
+        self,
+    ) -> Iterable[Union[MetadataWorkUnit, UsageStatsWorkUnit]]:
         glossary_config = self.load_glossary_config(self.config.file)
         populate_path_vs_id(glossary_config)
         for event in get_mces(
             glossary_config, ingestion_config=self.config, ctx=self.ctx
         ):
             if isinstance(event, models.MetadataChangeEventClass):
-                wu = MetadataWorkUnit(f"{event.proposedSnapshot.urn}", mce=event)
-                self.report.report_workunit(wu)
-                yield wu
+                yield MetadataWorkUnit(f"{event.proposedSnapshot.urn}", mce=event)
             elif isinstance(event, MetadataChangeProposalWrapper):
-                wu = MetadataWorkUnit(
-                    id=f"{event.entityType}-{event.aspectName}-{event.entityUrn}",
-                    mcp=event,
-                )
-                yield wu
+                yield event.as_workunit()
 
     def get_report(self):
         return self.report
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/metadata/lineage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/metadata/lineage.py`

 * *Files 21% similar despite different names*

```diff
@@ -9,38 +9,35 @@
 from datahub.cli.cli_utils import get_aspects_for_entity
 from datahub.configuration.common import (
     ConfigModel,
     ConfigurationError,
     VersionedConfig,
 )
 from datahub.configuration.config_loader import load_config_file
-from datahub.configuration.source_common import EnvBasedSourceConfigBase
+from datahub.configuration.source_common import EnvConfigMixin
 from datahub.emitter.mce_builder import (
     get_sys_time,
     make_dataset_urn_with_platform_instance,
 )
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.decorators import (
     SupportStatus,
     config_class,
     platform_name,
     support_status,
 )
 from datahub.ingestion.api.source import Source, SourceReport
 from datahub.ingestion.api.workunit import MetadataWorkUnit, UsageStatsWorkUnit
+from datahub.utilities.source_helpers import auto_workunit_reporter
 
 logger = logging.getLogger(__name__)
 
-auditStamp = models.AuditStampClass(
-    time=get_sys_time(), actor="urn:li:corpUser:pythonEmitter"
-)
-
 
-class EntityConfig(EnvBasedSourceConfigBase):
+class EntityConfig(EnvConfigMixin):
     name: str
     type: str
     platform: str
     platform_instance: Optional[str]
 
     @validator("type")
     def type_must_be_supported(cls, v: str) -> str:
@@ -58,15 +55,15 @@
 
 
 # https://pydantic-docs.helpmanual.io/usage/postponed_annotations/ required for when you reference a model within itself
 EntityNodeConfig.update_forward_refs()
 
 
 class LineageFileSourceConfig(ConfigModel):
-    file: str = Field(description="Path to lineage file to ingest.")
+    file: str = Field(description="File path or URL to lineage file to ingest.")
     preserve_upstream: bool = Field(
         default=True,
         description="Whether we want to query datahub-gms for upstream data. False means it will hard replace upstream data for a given entity. True means it will query the backend for existing upstreams and include it in the ingestion run",
     )
 
 
 class LineageConfig(VersionedConfig):
@@ -99,108 +96,91 @@
 
     @staticmethod
     def load_lineage_config(file_name: str) -> LineageConfig:
         config = load_config_file(file_name)
         lineage_config = LineageConfig.parse_obj(config)
         return lineage_config
 
-    @staticmethod
-    def get_lineage_metadata_change_event_proposal(
-        entities: List[EntityNodeConfig], preserve_upstream: bool
-    ) -> Iterable[MetadataChangeProposalWrapper]:
-        """
-        Builds a list of events to be emitted to datahub by going through each entity and its upstream nodes
-        :param preserve_upstream: This field determines if we want to query the datahub backend to extract
-        the existing upstream lineages for each entity and preserve it
-        :param entities: A list of entities we want to build a proposal on
-        :return: Returns a list of metadata change event proposals to be emitted to datahub
-        """
-
-        def _get_entity_urn(entity_config: EntityConfig) -> Optional[str]:
-            """Helper inner function to extract a given entity_urn
-            A return value of None represents an unsupported entity type
-            """
-            if entity_config.type == "dataset":
-                return make_dataset_urn_with_platform_instance(
-                    platform=entity_config.platform,
-                    name=entity_config.name,
-                    env=entity_config.env,
-                    platform_instance=entity_config.platform_instance,
-                )
-            logger.warning(f"Entity type: {entity_config.type} is not supported!")
-            return None
-
-        # loop through all the entities
-        for entity_node in entities:
-            new_upstreams: List[models.UpstreamClass] = []
-            # if this entity has upstream nodes defined, we'll want to do some work.
-            # if no upstream nodes are present, we don't emit an MCP for it.
-            if entity_node.upstream:
-                entity = entity_node.entity
-                logger.info(f"Upstream detected for {entity}. Extracting urn...")
-                entity_urn = _get_entity_urn(entity)
-                if entity_urn:
-                    # extract the old lineage and save it for the new mcp
-                    if preserve_upstream:
-                        old_upstream_lineage = get_aspects_for_entity(
-                            entity_urn=entity_urn,
-                            aspects=["upstreamLineage"],
-                            typed=True,
-                        ).get("upstreamLineage")
-                        if old_upstream_lineage:
-                            # Can't seem to get mypy to be happy about
-                            # `Argument 1 to "list" has incompatible type "Optional[Any]";
-                            # expected "Iterable[UpstreamClass]"`
-                            new_upstreams.extend(
-                                old_upstream_lineage.get("upstreams")  # type: ignore
-                            )
-                    for upstream_entity_node in entity_node.upstream:
-                        upstream_entity = upstream_entity_node.entity
-                        upstream_entity_urn = _get_entity_urn(upstream_entity)
-                        if upstream_entity_urn:
-                            new_upstream = models.UpstreamClass(
-                                dataset=upstream_entity_urn,
-                                type=models.DatasetLineageTypeClass.TRANSFORMED,
-                                auditStamp=auditStamp,
-                            )
-                            new_upstreams.append(new_upstream)
-                        else:
-                            logger.warning(
-                                f"Entity type: {upstream_entity.type} is unsupported. Upstream lineage will be skipped "
-                                f"for {upstream_entity.name}->{entity.name}"
-                            )
-                    new_upstream_lineage = models.UpstreamLineageClass(
-                        upstreams=new_upstreams
-                    )
-                    yield MetadataChangeProposalWrapper(
-                        entityType=entity.type,
-                        changeType=models.ChangeTypeClass.UPSERT,
-                        entityUrn=entity_urn,
-                        aspectName="upstreamLineage",
-                        aspect=new_upstream_lineage,
-                    )
-                else:
-                    logger.warning(
-                        f"Entity type: {entity.type} is unsupported. Entity node {entity.name} and its "
-                        f"upstream lineages will be skipped"
-                    )
-
     def get_workunits(self) -> Iterable[Union[MetadataWorkUnit, UsageStatsWorkUnit]]:
-        lineage_config = self.load_lineage_config(self.config.file)
-        lineage = lineage_config.lineage
-        preserve_upstream = self.config.preserve_upstream
-        logger.debug(lineage_config)
-        logger.info(f"preserve_upstream is set to {self.config.preserve_upstream}")
-        for (
-            metadata_change_event_proposal
-        ) in self.get_lineage_metadata_change_event_proposal(
-            lineage, preserve_upstream
-        ):
-            work_unit = MetadataWorkUnit(
-                f"lineage-{metadata_change_event_proposal.entityUrn}",
-                mcp=metadata_change_event_proposal,
-            )
-            self.report.report_workunit(work_unit)
-            yield work_unit
+        return auto_workunit_reporter(self.report, self.get_workunits_internal())
+
+    def get_workunits_internal(
+        self,
+    ) -> Iterable[Union[MetadataWorkUnit, UsageStatsWorkUnit]]:
+        config = self.load_lineage_config(self.config.file)
+        logger.debug(config)
+        for entity_node in config.lineage:
+            mcp = _get_lineage_mcp(entity_node, self.config.preserve_upstream)
+            if mcp:
+                yield mcp.as_workunit()
 
     def get_report(self):
         return self.report
+
+
+def _get_entity_urn(entity_config: EntityConfig) -> Optional[str]:
+    """A return value of None represents an unsupported entity type."""
+    if entity_config.type == "dataset":
+        return make_dataset_urn_with_platform_instance(
+            platform=entity_config.platform,
+            name=entity_config.name,
+            env=entity_config.env,
+            platform_instance=entity_config.platform_instance,
+        )
+    return None
+
+
+def _get_lineage_mcp(
+    entity_node: EntityNodeConfig, preserve_upstream: bool
+) -> Optional[MetadataChangeProposalWrapper]:
+    new_upstreams: List[models.UpstreamClass] = []
+    # if this entity has upstream nodes defined, we'll want to do some work.
+    # if no upstream nodes are present, we don't emit an MCP for it.
+    if not entity_node.upstream:
+        return None
+
+    entity = entity_node.entity
+    logger.info(f"Upstream detected for {entity}. Extracting urn...")
+    entity_urn = _get_entity_urn(entity)
+    if not entity_urn:
+        logger.warning(
+            f"Entity type: {entity.type} is unsupported. "
+            f"Entity node {entity.name} and its upstream lineages will be skipped"
+        )
+        return None
+
+    # extract the old lineage and save it for the new mcp
+    if preserve_upstream:
+        old_upstream_lineage = get_aspects_for_entity(
+            entity_urn=entity_urn,
+            aspects=["upstreamLineage"],
+            typed=True,
+        ).get("upstreamLineage")
+        if old_upstream_lineage:
+            # Can't seem to get mypy to be happy about
+            # `Argument 1 to "list" has incompatible type "Optional[Any]";
+            # expected "Iterable[UpstreamClass]"`
+            new_upstreams.extend(old_upstream_lineage.get("upstreams"))  # type: ignore
+
+    for upstream_entity_node in entity_node.upstream:
+        upstream_entity = upstream_entity_node.entity
+        upstream_entity_urn = _get_entity_urn(upstream_entity)
+        if upstream_entity_urn:
+            new_upstreams.append(
+                models.UpstreamClass(
+                    dataset=upstream_entity_urn,
+                    type=models.DatasetLineageTypeClass.TRANSFORMED,
+                    auditStamp=models.AuditStampClass(
+                        time=get_sys_time(), actor="urn:li:corpUser:ingestion"
+                    ),
+                )
+            )
+        else:
+            logger.warning(
+                f"Entity type: {upstream_entity.type} is unsupported. "
+                f"Upstream lineage will be skipped for {upstream_entity.name}->{entity.name}"
+            )
+
+    return MetadataChangeProposalWrapper(
+        entityUrn=entity_urn,
+        aspect=models.UpstreamLineageClass(upstreams=new_upstreams),
+    )
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/mode.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/mode.py`

 * *Files 0% similar despite different names*

```diff
@@ -210,14 +210,17 @@
 
         last_modified = ChangeAuditStamps()
         creator = self._get_creator(
             report_info.get("_links", {}).get("creator", {}).get("href", "")
         )
         if creator is not None:
             modified_actor = builder.make_user_urn(creator)
+            if report_info.get("last_saved_at") is None:
+                report_info["last_saved_at"] = report_info.get("created_at")
+
             modified_ts = int(
                 dp.parse(f"{report_info.get('last_saved_at', 'now')}").timestamp()
                 * 1000
             )
             created_ts = int(
                 dp.parse(f"{report_info.get('created_at', 'now')}").timestamp() * 1000
             )
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/mongodb.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/mongodb.py`

 * *Files 1% similar despite different names*

```diff
@@ -6,15 +6,15 @@
 import pymongo
 from packaging import version
 from pydantic import PositiveInt, validator
 from pydantic.fields import Field
 from pymongo.mongo_client import MongoClient
 
 from datahub.configuration.common import AllowDenyPattern
-from datahub.configuration.source_common import EnvBasedSourceConfigBase
+from datahub.configuration.source_common import EnvConfigMixin
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.decorators import (
     SourceCapability,
     SupportStatus,
     capability,
     config_class,
     platform_name,
@@ -50,15 +50,15 @@
 # These are MongoDB-internal databases, which we want to skip.
 # See https://docs.mongodb.com/manual/reference/local-database/ and
 # https://docs.mongodb.com/manual/reference/config-database/ and
 # https://stackoverflow.com/a/48273736/5004662.
 DENY_DATABASE_LIST = set(["admin", "config", "local"])
 
 
-class MongoDBConfig(EnvBasedSourceConfigBase):
+class MongoDBConfig(EnvConfigMixin):
     # See the MongoDB authentication docs for details and examples.
     # https://pymongo.readthedocs.io/en/stable/examples/authentication.html
     connect_uri: str = Field(
         default="mongodb://localhost", description="MongoDB connection URI."
     )
     username: Optional[str] = Field(default=None, description="MongoDB username.")
     password: Optional[str] = Field(default=None, description="MongoDB password.")
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/nifi.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/nifi.py`

 * *Files 1% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 from dateutil import parser
 from packaging import version
 from pydantic.fields import Field
 from requests.adapters import HTTPAdapter
 
 import datahub.emitter.mce_builder as builder
 from datahub.configuration.common import AllowDenyPattern
-from datahub.configuration.source_common import EnvBasedSourceConfigBase
+from datahub.configuration.source_common import EnvConfigMixin
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.decorators import (
     SupportStatus,
     config_class,
     platform_name,
     support_status,
@@ -58,15 +58,15 @@
 
 class NifiAuthType(Enum):
     NO_AUTH = "NO_AUTH"
     SINGLE_USER = "SINGLE_USER"
     CLIENT_CERT = "CLIENT_CERT"
 
 
-class NifiSourceConfig(EnvBasedSourceConfigBase):
+class NifiSourceConfig(EnvConfigMixin):
     site_url: str = Field(description="URI to connect")
 
     auth: NifiAuthType = Field(
         default=NifiAuthType.NO_AUTH,
         description="Nifi authentication. must be one of : NO_AUTH, SINGLE_USER, CLIENT_CERT",
     )
 
@@ -297,22 +297,22 @@
     filtered: List[str] = field(default_factory=list)
 
     def report_dropped(self, ent_name: str) -> None:
         self.filtered.append(ent_name)
 
 
 # allowRemoteAccess
-@platform_name("Nifi")
+@platform_name("NiFi", id="nifi")
 @config_class(NifiSourceConfig)
 @support_status(SupportStatus.CERTIFIED)
 class NifiSource(Source):
     """
     This plugin extracts the following:
 
-    - Nifi flow as `DataFlow` entity
+    - NiFi flow as `DataFlow` entity
     - Ingress, egress processors, remote input and output ports as `DataJob` entity
     - Input and output ports receiving remote connections as `Dataset` entity
     - Lineage information between external datasets and ingress/egress processors by analyzing provenance events
 
     Current limitations:
 
     - Limited ingress/egress processors are supported
@@ -595,21 +595,21 @@
                 c.name.startswith("Get")
                 or c.name.startswith("List")
                 or c.name.startswith("Fetch")
                 or c.name.startswith("Put")
             ):
                 self.report_warning(
                     self.config.site_url,
-                    f"Dropping Nifi Processor of type {c.type}, id {c.id}, name {c.name} from lineage view. \
+                    f"Dropping NiFi Processor of type {c.type}, id {c.id}, name {c.name} from lineage view. \
                     This is likely an Ingress or Egress node which may be reading to/writing from external datasets \
                     However not currently supported in datahub",
                 )
             else:
                 logger.debug(
-                    f"Dropping Nifi Component of type {c.type}, id {c.id}, name {c.name} from lineage view."
+                    f"Dropping NiFi Component of type {c.type}, id {c.id}, name {c.name} from lineage view."
                 )
 
             del self.nifi_flow.components[c.id]
 
     def create_nifi_flow(self):
         about_response = self.session.get(
             url=urljoin(self.config.site_url, ABOUT_ENDPOINT)
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/openapi.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/openapi.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/openapi_parser.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/openapi_parser.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,23 +1,25 @@
 import json
+import logging
 import re
-import warnings
 from typing import Any, Dict, Generator, List, Optional, Tuple
 
 import requests
 import yaml
 from requests.auth import HTTPBasicAuth
 
 from datahub.metadata.com.linkedin.pegasus2avro.schema import (
     OtherSchemaClass,
     SchemaField,
     SchemaMetadata,
 )
 from datahub.metadata.schema_classes import SchemaFieldDataTypeClass, StringTypeClass
 
+logger = logging.getLogger(__name__)
+
 
 def flatten(d: dict, prefix: str = "") -> Generator:
     for k, v in d.items():
         if isinstance(v, dict):
             yield from flatten(v, f"{prefix}.{k}")
         else:
             yield f"{prefix}-{k}".strip(".")
@@ -158,15 +160,15 @@
                             ]
                         elif isinstance(res_cont["application/json"][ex_field], list):
                             # taking the first example
                             url_details[p_k]["data"] = res_cont["application/json"][
                                 ex_field
                             ][0]
                     else:
-                        warnings.warn(
+                        logger.warning(
                             f"Field in swagger file does not give consistent data --- {p_k}"
                         )
                 elif "text/csv" in res_cont.keys():
                     url_details[p_k]["data"] = res_cont["text/csv"]["schema"]
             elif "examples" in base_res.keys():
                 url_details[p_k]["data"] = base_res["examples"]["application/json"]
 
@@ -292,20 +294,20 @@
 
     The list in the output tuple will contain the fields name.
     The dict in the output tuple will contain a sample of data.
     """
     dict_data = json.loads(response.content)
     if isinstance(dict_data, str):
         # no sense
-        warnings.warn(f"Empty data --- {dataset_name}")
+        logger.warning(f"Empty data --- {dataset_name}")
         return [], {}
     elif isinstance(dict_data, list):
         # it's maybe just a list
         if len(dict_data) == 0:
-            warnings.warn(f"Empty data --- {dataset_name}")
+            logger.warning(f"Empty data --- {dataset_name}")
             return [], {}
         # so we take the fields of the first element,
         # if it's a dict
         if isinstance(dict_data[0], dict):
             return flatten2list(dict_data[0]), dict_data[0]
         elif isinstance(dict_data[0], str):
             # this is actually data
@@ -326,15 +328,15 @@
         # ..but will take the keys of the first element (to be improved)
         if isinstance(dict_data[dst_key], list):
             if len(dict_data[dst_key]) > 0:
                 return flatten2list(dict_data[dst_key][0]), dict_data[dst_key][0]
             else:
                 return [], {}  # it's empty!
         else:
-            warnings.warn(f"Unable to get the attributes --- {dataset_name}")
+            logger.warning(f"Unable to get the attributes --- {dataset_name}")
             return [], {}
 
 
 def get_tok(
     url: str,
     username: str = "",
     password: str = "",
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi/m_query/native_sql_parser.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/native_sql_parser.py`

 * *Files 14% similar despite different names*

```diff
@@ -13,35 +13,35 @@
         native_query = native_query.replace(char, " ")
 
     return native_query
 
 
 def get_tables(native_query: str) -> List[str]:
     native_query = remove_special_characters(native_query)
-    logger.debug("Processing query = %s", native_query)
+    logger.debug(f"Processing query = {native_query}")
     tables: List[str] = []
     parsed = sqlparse.parse(native_query)[0]
     tokens: List[sqlparse.sql.Token] = list(parsed.tokens)
     length: int = len(tokens)
     from_index: int = -1
     for index, token in enumerate(tokens):
-        logger.debug("%s=%s", token.value, token.ttype)
+        logger.debug(f"{token.value}={token.ttype}")
         if (
             token.value.lower().strip() == "from"
             and str(token.ttype) == "Token.Keyword"
         ):
             from_index = index + 1
             break
 
     # Collect all identifier after FROM clause till we reach to the end or WHERE clause encounter
     while (
         from_index < length
         and isinstance(tokens[from_index], sqlparse.sql.Where) is not True
     ):
-        logger.debug("%s=%s", tokens[from_index].value, tokens[from_index].ttype)
-        logger.debug("Type=%s", type(tokens[from_index]))
+        logger.debug(f"{tokens[from_index].value}={tokens[from_index].ttype}")
+        logger.debug(f"Type={type(tokens[from_index])}")
         if isinstance(tokens[from_index], sqlparse.sql.Identifier):
             # Split on as keyword and collect the table name from 0th position. strip any spaces
             tables.append(tokens[from_index].value.split("as")[0].strip())
         from_index = from_index + 1
 
     return tables
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi/m_query/resolver.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/resolver.py`

 * *Files 13% similar despite different names*

```diff
@@ -2,96 +2,96 @@
 from abc import ABC, abstractmethod
 from dataclasses import dataclass
 from enum import Enum
 from typing import Any, Dict, List, Optional, Tuple, Type, Union, cast
 
 from lark import Tree
 
-from datahub.ingestion.source.powerbi.config import PowerBiDashboardSourceReport
+from datahub.ingestion.source.powerbi.config import (
+    DataPlatformPair,
+    PowerBiDashboardSourceReport,
+    SupportedDataPlatform,
+)
 from datahub.ingestion.source.powerbi.m_query import native_sql_parser, tree_function
 from datahub.ingestion.source.powerbi.m_query.data_classes import (
+    TRACE_POWERBI_MQUERY_PARSER,
     DataAccessFunctionDetail,
     IdentifierAccessor,
 )
-from datahub.ingestion.source.powerbi.proxy import PowerBiAPI
+from datahub.ingestion.source.powerbi.rest_api_wrapper.data_classes import Table
 
 logger = logging.getLogger(__name__)
 
 
 @dataclass
-class DataPlatformPair:
-    datahub_data_platform_name: str
-    powerbi_data_platform_name: str
-
-
-@dataclass
 class DataPlatformTable:
     name: str
     full_name: str
+    datasource_server: str
     data_platform_pair: DataPlatformPair
 
 
-class SupportedDataPlatform(Enum):
-    POSTGRES_SQL = DataPlatformPair(
-        powerbi_data_platform_name="PostgreSQL", datahub_data_platform_name="postgres"
-    )
-
-    ORACLE = DataPlatformPair(
-        powerbi_data_platform_name="Oracle", datahub_data_platform_name="oracle"
-    )
-
-    SNOWFLAKE = DataPlatformPair(
-        powerbi_data_platform_name="Snowflake", datahub_data_platform_name="snowflake"
-    )
-
-    MS_SQL = DataPlatformPair(
-        powerbi_data_platform_name="Sql", datahub_data_platform_name="mssql"
-    )
-
-
-class AbstractTableFullNameCreator(ABC):
+class AbstractDataPlatformTableCreator(ABC):
     @abstractmethod
-    def get_full_table_names(
+    def create_dataplatform_tables(
         self, data_access_func_detail: DataAccessFunctionDetail
-    ) -> List[str]:
+    ) -> List[DataPlatformTable]:
         pass
 
     @abstractmethod
     def get_platform_pair(self) -> DataPlatformPair:
         pass
 
+    @staticmethod
+    def get_db_detail_from_argument(
+        arg_list: Tree,
+    ) -> Tuple[Optional[str], Optional[str]]:
+        arguments: List[str] = tree_function.strip_char_from_list(
+            values=tree_function.remove_whitespaces_from_list(
+                tree_function.token_values(arg_list)
+            ),
+        )
+
+        if len(arguments) < 2:
+            logger.debug(f"Expected minimum 2 arguments, but got {len(arguments)}")
+            return None, None
+
+        return arguments[0], arguments[1]
+
 
 class AbstractDataAccessMQueryResolver(ABC):
-    table: PowerBiAPI.Table
+    table: Table
     parse_tree: Tree
+    parameters: Dict[str, str]
     reporter: PowerBiDashboardSourceReport
     data_access_functions: List[str]
 
     def __init__(
         self,
-        table: PowerBiAPI.Table,
+        table: Table,
         parse_tree: Tree,
         reporter: PowerBiDashboardSourceReport,
+        parameters: Dict[str, str],
     ):
         self.table = table
         self.parse_tree = parse_tree
         self.reporter = reporter
+        self.parameters = parameters
         self.data_access_functions = SupportedResolver.get_function_names()
 
     @abstractmethod
     def resolve_to_data_platform_table_list(self) -> List[DataPlatformTable]:
         pass
 
 
 class MQueryResolver(AbstractDataAccessMQueryResolver, ABC):
-    @staticmethod
     def get_item_selector_tokens(
+        self,
         expression_tree: Tree,
     ) -> Tuple[Optional[str], Optional[Dict[str, str]]]:
-
         item_selector: Optional[Tree] = tree_function.first_item_selector_func(
             expression_tree
         )
         if item_selector is None:
             logger.debug("Item Selector not found in tree")
             logger.debug(expression_tree.pretty())
             return None, None
@@ -103,17 +103,18 @@
             logger.debug("Identifier not found in tree")
             logger.debug(item_selector.pretty())
             return None, None
 
         # remove whitespaces and quotes from token
         tokens: List[str] = tree_function.strip_char_from_list(
             tree_function.remove_whitespaces_from_list(
-                tree_function.token_values(cast(Tree, item_selector))
+                tree_function.token_values(
+                    cast(Tree, item_selector), parameters=self.parameters
+                )
             ),
-            '"',
         )
         identifier: List[str] = tree_function.token_values(
             cast(Tree, identifier_tree)
         )  # type :ignore
 
         # convert tokens to dict
         iterator = iter(tokens)
@@ -130,15 +131,14 @@
             return None
 
         return argument_list
 
     def _process_invoke_expression(
         self, invoke_expression: Tree
     ) -> Union[DataAccessFunctionDetail, List[str], None]:
-
         letter_tree: Tree = invoke_expression.children[0]
         data_access_func: str = tree_function.make_function_name(letter_tree)
         # The invoke function is either DataAccess function like PostgreSQL.Database(<argument-list>) or
         # some other function like Table.AddColumn or Table.Combine and so on
         if data_access_func in self.data_access_functions:
             arg_list: Optional[Tree] = MQueryResolver.get_argument_list(
                 invoke_expression
@@ -159,49 +159,53 @@
         # function is not data-access function, lets process function argument
         first_arg_tree: Optional[Tree] = tree_function.first_arg_list_func(
             invoke_expression
         )
 
         if first_arg_tree is None:
             logger.debug(
-                "Function invocation without argument in expression = %s",
-                invoke_expression.pretty(),
+                f"Function invocation without argument in expression = {invoke_expression.pretty()}"
             )
             self.reporter.report_warning(
                 f"{self.table.full_name}-variable-statement",
                 "Function invocation without argument",
             )
             return None
 
-        first_argument: Tree = tree_function.flat_argument_list(first_arg_tree)[
-            0
-        ]  # take first argument only
+        flat_arg_list: List[Tree] = tree_function.flat_argument_list(first_arg_tree)
+        if len(flat_arg_list) == 0:
+            logger.debug("flat_arg_list is zero")
+            return None
+
+        first_argument: Tree = flat_arg_list[0]  # take first argument only
         expression: Optional[Tree] = tree_function.first_list_expression_func(
             first_argument
         )
 
-        logger.debug("Extracting token from tree %s", first_argument.pretty())
+        if TRACE_POWERBI_MQUERY_PARSER:
+            logger.debug(f"Extracting token from tree {first_argument.pretty()}")
+        else:
+            logger.debug(f"Extracting token from tree {first_argument}")
         if expression is None:
             expression = tree_function.first_type_expression_func(first_argument)
             if expression is None:
                 logger.debug(
-                    "Either list_expression or type_expression is not found = %s",
-                    invoke_expression.pretty(),
+                    f"Either list_expression or type_expression is not found = {invoke_expression.pretty()}"
                 )
                 self.reporter.report_warning(
                     f"{self.table.full_name}-variable-statement",
                     "Function argument expression is not supported",
                 )
                 return None
 
         tokens: List[str] = tree_function.remove_whitespaces_from_list(
             tree_function.token_values(expression)
         )
 
-        logger.debug("Tokens in invoke expression are %s", tokens)
+        logger.debug(f"Tokens in invoke expression are {tokens}")
         return tokens
 
     def _process_item_selector_expression(
         self, rh_tree: Tree
     ) -> Tuple[Optional[str], Optional[Dict[str, str]]]:
         new_identifier, key_vs_value = self.get_item_selector_tokens(  # type: ignore
             cast(Tree, tree_function.first_expression_func(rh_tree))
@@ -211,15 +215,14 @@
 
     @staticmethod
     def _create_or_update_identifier_accessor(
         identifier_accessor: Optional[IdentifierAccessor],
         new_identifier: str,
         key_vs_value: Dict[str, Any],
     ) -> IdentifierAccessor:
-
         # It is first identifier_accessor
         if identifier_accessor is None:
             return IdentifierAccessor(
                 identifier=new_identifier, items=key_vs_value, next=None
             )
 
         new_identifier_accessor: IdentifierAccessor = IdentifierAccessor(
@@ -278,17 +281,15 @@
             if invoke_expression is not None:
                 result: Union[
                     DataAccessFunctionDetail, List[str], None
                 ] = self._process_invoke_expression(invoke_expression)
                 if result is None:
                     return None  # No need to process some un-expected grammar found while processing invoke_expression
                 if isinstance(result, DataAccessFunctionDetail):
-                    cast(
-                        DataAccessFunctionDetail, result
-                    ).identifier_accessor = identifier_accessor
+                    result.identifier_accessor = identifier_accessor
                     table_links.append(result)  # Link of a table is completed
                     identifier_accessor = (
                         None  # reset the identifier_accessor for other table
                     )
                     return None
                 # Process first argument of the function.
                 # The first argument can be a single table argument or list of table.
@@ -337,309 +338,424 @@
         # Each item is data-access function
         for f_detail in table_links:
             supported_resolver = SupportedResolver.get_resolver(
                 f_detail.data_access_function_name
             )
             if supported_resolver is None:
                 logger.debug(
-                    "Resolver not found for the data-access-function %s",
-                    f_detail.data_access_function_name,
+                    f"Resolver not found for the data-access-function {f_detail.data_access_function_name}"
                 )
                 self.reporter.report_warning(
                     f"{self.table.full_name}-data-access-function",
                     f"Resolver not found for data-access-function = {f_detail.data_access_function_name}",
                 )
                 continue
 
-            table_full_name_creator: AbstractTableFullNameCreator = (
+            table_full_name_creator: AbstractDataPlatformTableCreator = (
                 supported_resolver.get_table_full_name_creator()()
             )
 
-            for table_full_name in table_full_name_creator.get_full_table_names(
-                f_detail
-            ):
-                data_platform_tables.append(
-                    DataPlatformTable(
-                        name=table_full_name.split(".")[-1],
-                        full_name=table_full_name,
-                        data_platform_pair=table_full_name_creator.get_platform_pair(),
-                    )
-                )
+            data_platform_tables.extend(
+                table_full_name_creator.create_dataplatform_tables(f_detail)
+            )
 
         return data_platform_tables
 
 
-class DefaultTwoStepDataAccessSources(AbstractTableFullNameCreator, ABC):
+class DefaultTwoStepDataAccessSources(AbstractDataPlatformTableCreator, ABC):
     """
     These are the DataSource for which PowerBI Desktop generates default M-Query of following pattern
         let
             Source = Sql.Database("localhost", "library"),
             dbo_book_issue = Source{[Schema="dbo",Item="book_issue"]}[Data]
         in
             dbo_book_issue
     """
 
     def two_level_access_pattern(
         self, data_access_func_detail: DataAccessFunctionDetail
-    ) -> List[str]:
-        full_table_names: List[str] = []
-
+    ) -> List[DataPlatformTable]:
         logger.debug(
-            "Processing PostgreSQL data-access function detail %s",
-            data_access_func_detail,
+            f"Processing {self.get_platform_pair().powerbi_data_platform_name} data-access function detail {data_access_func_detail}"
         )
-        arguments: List[str] = tree_function.strip_char_from_list(
-            values=tree_function.remove_whitespaces_from_list(
-                tree_function.token_values(data_access_func_detail.arg_list)
-            ),
-            char='"',
-        )
-
-        if len(arguments) != 2:
-            logger.debug("Expected 2 arguments, but got {%s}", len(arguments))
-            return full_table_names
 
-        db_name: str = arguments[1]
+        server, db_name = self.get_db_detail_from_argument(
+            data_access_func_detail.arg_list
+        )
+        if server is None or db_name is None:
+            return []  # Return empty list
 
         schema_name: str = cast(
             IdentifierAccessor, data_access_func_detail.identifier_accessor
         ).items["Schema"]
 
         table_name: str = cast(
             IdentifierAccessor, data_access_func_detail.identifier_accessor
         ).items["Item"]
 
-        full_table_names.append(f"{db_name}.{schema_name}.{table_name}")
+        full_table_name: str = f"{db_name}.{schema_name}.{table_name}"
 
         logger.debug(
-            "Platform(%s) full-table-names = %s",
-            self.get_platform_pair().datahub_data_platform_name,
-            full_table_names,
+            f"Platform({self.get_platform_pair().datahub_data_platform_name}) full_table_name= {full_table_name}"
         )
 
-        return full_table_names
+        return [
+            DataPlatformTable(
+                name=table_name,
+                full_name=full_table_name,
+                datasource_server=server,
+                data_platform_pair=self.get_platform_pair(),
+            )
+        ]
 
 
-class PostgresTableFullNameCreator(DefaultTwoStepDataAccessSources):
-    def get_full_table_names(
+class PostgresDataPlatformTableCreator(DefaultTwoStepDataAccessSources):
+    def create_dataplatform_tables(
         self, data_access_func_detail: DataAccessFunctionDetail
-    ) -> List[str]:
+    ) -> List[DataPlatformTable]:
         return self.two_level_access_pattern(data_access_func_detail)
 
     def get_platform_pair(self) -> DataPlatformPair:
         return SupportedDataPlatform.POSTGRES_SQL.value
 
 
-class MSSqlTableFullNameCreator(DefaultTwoStepDataAccessSources):
+class MSSqlDataPlatformTableCreator(DefaultTwoStepDataAccessSources):
     def get_platform_pair(self) -> DataPlatformPair:
         return SupportedDataPlatform.MS_SQL.value
 
-    def get_full_table_names(
+    def create_dataplatform_tables(
         self, data_access_func_detail: DataAccessFunctionDetail
-    ) -> List[str]:
-        full_table_names: List[str] = []
+    ) -> List[DataPlatformTable]:
+        dataplatform_tables: List[DataPlatformTable] = []
         arguments: List[str] = tree_function.strip_char_from_list(
             values=tree_function.remove_whitespaces_from_list(
                 tree_function.token_values(data_access_func_detail.arg_list)
             ),
-            char='"',
         )
 
         if len(arguments) == 2:
             # It is regular case of MS-SQL
             logger.debug("Handling with regular case")
             return self.two_level_access_pattern(data_access_func_detail)
 
         if len(arguments) >= 4 and arguments[2] != "Query":
             logger.debug("Unsupported case is found. Second index is not the Query")
-            return full_table_names
+            return dataplatform_tables
 
         db_name: str = arguments[1]
+
         tables: List[str] = native_sql_parser.get_tables(arguments[3])
         for table in tables:
             schema_and_table: List[str] = table.split(".")
             if len(schema_and_table) == 1:
                 # schema name is not present. Default schema name in MS-SQL is dbo
                 # https://learn.microsoft.com/en-us/sql/relational-databases/security/authentication-access/ownership-and-user-schema-separation?view=sql-server-ver16
                 schema_and_table.insert(0, "dbo")
 
-            full_table_names.append(
-                f"{db_name}.{schema_and_table[0]}.{schema_and_table[1]}"
+            dataplatform_tables.append(
+                DataPlatformTable(
+                    name=schema_and_table[1],
+                    full_name=f"{db_name}.{schema_and_table[0]}.{schema_and_table[1]}",
+                    datasource_server=arguments[0],
+                    data_platform_pair=self.get_platform_pair(),
+                )
             )
 
-        logger.debug("MS-SQL full-table-names %s", full_table_names)
+        logger.debug("MS-SQL full-table-names %s", dataplatform_tables)
 
-        return full_table_names
+        return dataplatform_tables
 
 
-class OracleTableFullNameCreator(AbstractTableFullNameCreator):
+class OracleDataPlatformTableCreator(AbstractDataPlatformTableCreator):
     def get_platform_pair(self) -> DataPlatformPair:
         return SupportedDataPlatform.ORACLE.value
 
-    def _get_db_name(self, value: str) -> Optional[str]:
-        error_message: str = f"The target argument ({value}) should in the format of <host-name>:<port>/<db-name>[.<domain>]"
+    @staticmethod
+    def _get_server_and_db_name(value: str) -> Tuple[Optional[str], Optional[str]]:
+        error_message: str = (
+            f"The target argument ({value}) should in the format of <host-name>:<port>/<db-name>["
+            ".<domain>]"
+        )
         splitter_result: List[str] = value.split("/")
         if len(splitter_result) != 2:
             logger.debug(error_message)
-            return None
+            return None, None
 
         db_name = splitter_result[1].split(".")[0]
 
-        return db_name
+        return tree_function.strip_char_from_list([splitter_result[0]])[0], db_name
 
-    def get_full_table_names(
+    def create_dataplatform_tables(
         self, data_access_func_detail: DataAccessFunctionDetail
-    ) -> List[str]:
-        full_table_names: List[str] = []
-
+    ) -> List[DataPlatformTable]:
         logger.debug(
-            "Processing Oracle data-access function detail %s", data_access_func_detail
+            f"Processing Oracle data-access function detail {data_access_func_detail}"
         )
 
         arguments: List[str] = tree_function.remove_whitespaces_from_list(
             tree_function.token_values(data_access_func_detail.arg_list)
         )
 
-        db_name: Optional[str] = self._get_db_name(arguments[0])
-        if db_name is None:
-            return full_table_names
+        server, db_name = self._get_server_and_db_name(arguments[0])
+
+        if db_name is None or server is None:
+            return []
 
         schema_name: str = cast(
             IdentifierAccessor, data_access_func_detail.identifier_accessor
         ).items["Schema"]
 
         table_name: str = cast(
             IdentifierAccessor,
             cast(IdentifierAccessor, data_access_func_detail.identifier_accessor).next,
         ).items["Name"]
 
-        full_table_names.append(f"{db_name}.{schema_name}.{table_name}")
-
-        return full_table_names
-
+        return [
+            DataPlatformTable(
+                name=table_name,
+                full_name=f"{db_name}.{schema_name}.{table_name}",
+                datasource_server=server,
+                data_platform_pair=self.get_platform_pair(),
+            )
+        ]
+
+
+class DefaultThreeStepDataAccessSources(AbstractDataPlatformTableCreator, ABC):
+    def get_datasource_server(
+        self, arguments: List[str], data_access_func_detail: DataAccessFunctionDetail
+    ) -> str:
+        return tree_function.strip_char_from_list([arguments[0]])[0]
 
-class SnowflakeTableFullNameCreator(AbstractTableFullNameCreator):
-    def get_platform_pair(self) -> DataPlatformPair:
-        return SupportedDataPlatform.SNOWFLAKE.value
-
-    def get_full_table_names(
+    def create_dataplatform_tables(
         self, data_access_func_detail: DataAccessFunctionDetail
-    ) -> List[str]:
+    ) -> List[DataPlatformTable]:
+        logger.debug(
+            f"Processing {self.get_platform_pair().datahub_data_platform_name} function detail {data_access_func_detail}"
+        )
 
-        logger.debug("Processing Snowflake function detail %s", data_access_func_detail)
+        arguments: List[str] = tree_function.remove_whitespaces_from_list(
+            tree_function.token_values(data_access_func_detail.arg_list)
+        )
         # First is database name
         db_name: str = data_access_func_detail.identifier_accessor.items["Name"]  # type: ignore
         # Second is schema name
         schema_name: str = cast(
             IdentifierAccessor, data_access_func_detail.identifier_accessor.next  # type: ignore
         ).items["Name"]
         # Third is table name
         table_name: str = cast(
             IdentifierAccessor, data_access_func_detail.identifier_accessor.next.next  # type: ignore
         ).items["Name"]
 
         full_table_name: str = f"{db_name}.{schema_name}.{table_name}"
 
-        logger.debug("Snowflake full-table-name %s", full_table_name)
+        logger.debug(
+            f"{self.get_platform_pair().datahub_data_platform_name} full-table-name {full_table_name}"
+        )
 
-        return [full_table_name]
+        return [
+            DataPlatformTable(
+                name=table_name,
+                full_name=full_table_name,
+                datasource_server=self.get_datasource_server(
+                    arguments, data_access_func_detail
+                ),
+                data_platform_pair=self.get_platform_pair(),
+            )
+        ]
 
 
-class NativeQueryTableFullNameCreator(AbstractTableFullNameCreator):
+class SnowflakeDataPlatformTableCreator(DefaultThreeStepDataAccessSources):
     def get_platform_pair(self) -> DataPlatformPair:
         return SupportedDataPlatform.SNOWFLAKE.value
 
-    def get_full_table_names(
+
+class GoogleBigQueryDataPlatformTableCreator(DefaultThreeStepDataAccessSources):
+    def get_platform_pair(self) -> DataPlatformPair:
+        return SupportedDataPlatform.GOOGLE_BIGQUERY.value
+
+    def get_datasource_server(
+        self, arguments: List[str], data_access_func_detail: DataAccessFunctionDetail
+    ) -> str:
+        # In Google BigQuery server is project-name
+        # condition to silent lint, it is not going to be None
+        return (
+            data_access_func_detail.identifier_accessor.items["Name"]
+            if data_access_func_detail.identifier_accessor is not None
+            else str()
+        )
+
+
+class AmazonRedshiftDataPlatformTableCreator(AbstractDataPlatformTableCreator):
+    def get_platform_pair(self) -> DataPlatformPair:
+        return SupportedDataPlatform.AMAZON_REDSHIFT.value
+
+    def create_dataplatform_tables(
         self, data_access_func_detail: DataAccessFunctionDetail
-    ) -> List[str]:
-        full_table_names: List[str] = []
+    ) -> List[DataPlatformTable]:
+        logger.debug(
+            f"Processing AmazonRedshift data-access function detail {data_access_func_detail}"
+        )
+
+        server, db_name = self.get_db_detail_from_argument(
+            data_access_func_detail.arg_list
+        )
+        if db_name is None or server is None:
+            return []  # Return empty list
+
+        schema_name: str = cast(
+            IdentifierAccessor, data_access_func_detail.identifier_accessor
+        ).items["Name"]
+
+        table_name: str = cast(
+            IdentifierAccessor,
+            cast(IdentifierAccessor, data_access_func_detail.identifier_accessor).next,
+        ).items["Name"]
+
+        return [
+            DataPlatformTable(
+                name=table_name,
+                full_name=f"{db_name}.{schema_name}.{table_name}",
+                datasource_server=server,
+                data_platform_pair=self.get_platform_pair(),
+            )
+        ]
+
+
+class NativeQueryDataPlatformTableCreator(AbstractDataPlatformTableCreator):
+    SUPPORTED_NATIVE_QUERY_DATA_PLATFORM: dict = {
+        SupportedDataPlatform.SNOWFLAKE.value.powerbi_data_platform_name: SupportedDataPlatform.SNOWFLAKE,
+        SupportedDataPlatform.AMAZON_REDSHIFT.value.powerbi_data_platform_name: SupportedDataPlatform.AMAZON_REDSHIFT,
+    }
+    current_data_platform: SupportedDataPlatform = SupportedDataPlatform.SNOWFLAKE
+
+    def get_platform_pair(self) -> DataPlatformPair:
+        return self.current_data_platform.value
+
+    @staticmethod
+    def is_native_parsing_supported(data_access_function_name: str) -> bool:
+        return (
+            data_access_function_name
+            in NativeQueryDataPlatformTableCreator.SUPPORTED_NATIVE_QUERY_DATA_PLATFORM
+        )
+
+    def create_dataplatform_tables(
+        self, data_access_func_detail: DataAccessFunctionDetail
+    ) -> List[DataPlatformTable]:
+        dataplatform_tables: List[DataPlatformTable] = []
         t1: Tree = cast(
             Tree, tree_function.first_arg_list_func(data_access_func_detail.arg_list)
         )
         flat_argument_list: List[Tree] = tree_function.flat_argument_list(t1)
 
         if len(flat_argument_list) != 2:
             logger.debug(
-                "Expecting 2 argument, actual argument count is %s",
-                len(flat_argument_list),
+                f"Expecting 2 argument, actual argument count is {len(flat_argument_list)}"
             )
-            logger.debug("Flat argument list = %s", flat_argument_list)
-            return full_table_names
-
+            logger.debug(f"Flat argument list = {flat_argument_list}")
+            return dataplatform_tables
         data_access_tokens: List[str] = tree_function.remove_whitespaces_from_list(
             tree_function.token_values(flat_argument_list[0])
         )
-        if (
-            data_access_tokens[0]
-            != SupportedDataPlatform.SNOWFLAKE.value.powerbi_data_platform_name
-        ):
+
+        if not self.is_native_parsing_supported(data_access_tokens[0]):
+            logger.debug(
+                f"Unsupported native-query data-platform = {data_access_tokens[0]}"
+            )
             logger.debug(
-                "Provided native-query data-platform = %s", data_access_tokens[0]
+                f"NativeQuery is supported only for {self.SUPPORTED_NATIVE_QUERY_DATA_PLATFORM}"
             )
-            logger.debug("Only Snowflake is supported in NativeQuery")
-            return full_table_names
 
+        if len(data_access_tokens[0]) < 3:
+            logger.debug(
+                f"Server is not available in argument list for data-platform {data_access_tokens[0]}. Returning empty "
+                "list"
+            )
+            return dataplatform_tables
+
+        self.current_data_platform = self.SUPPORTED_NATIVE_QUERY_DATA_PLATFORM[
+            data_access_tokens[0]
+        ]
         # First argument is the query
         sql_query: str = tree_function.strip_char_from_list(
             values=tree_function.remove_whitespaces_from_list(
                 tree_function.token_values(flat_argument_list[1])
             ),
-            char='"',
         )[
             0
         ]  # Remove any whitespaces and double quotes character
 
         for table in native_sql_parser.get_tables(sql_query):
             if len(table.split(".")) != 3:
                 logger.debug(
-                    "Skipping table (%s) as it is not as per full_table_name format",
-                    table,
+                    f"Skipping table {table} as it is not as per full_table_name format"
                 )
                 continue
 
-            full_table_names.append(table)
+            dataplatform_tables.append(
+                DataPlatformTable(
+                    name=table.split(".")[2],
+                    full_name=table,
+                    datasource_server=tree_function.strip_char_from_list(
+                        [data_access_tokens[2]]
+                    )[0],
+                    data_platform_pair=self.get_platform_pair(),
+                )
+            )
 
-        return full_table_names
+        return dataplatform_tables
 
 
 class FunctionName(Enum):
     NATIVE_QUERY = "Value.NativeQuery"
     POSTGRESQL_DATA_ACCESS = "PostgreSQL.Database"
     ORACLE_DATA_ACCESS = "Oracle.Database"
     SNOWFLAKE_DATA_ACCESS = "Snowflake.Databases"
     MSSQL_DATA_ACCESS = "Sql.Database"
+    GOOGLE_BIGQUERY_DATA_ACCESS = "GoogleBigQuery.Database"
+    AMAZON_REDSHIFT_DATA_ACCESS = "AmazonRedshift.Database"
 
 
 class SupportedResolver(Enum):
     POSTGRES_SQL = (
-        PostgresTableFullNameCreator,
+        PostgresDataPlatformTableCreator,
         FunctionName.POSTGRESQL_DATA_ACCESS,
     )
 
     ORACLE = (
-        OracleTableFullNameCreator,
+        OracleDataPlatformTableCreator,
         FunctionName.ORACLE_DATA_ACCESS,
     )
 
     SNOWFLAKE = (
-        SnowflakeTableFullNameCreator,
+        SnowflakeDataPlatformTableCreator,
         FunctionName.SNOWFLAKE_DATA_ACCESS,
     )
 
     MS_SQL = (
-        MSSqlTableFullNameCreator,
+        MSSqlDataPlatformTableCreator,
         FunctionName.MSSQL_DATA_ACCESS,
     )
 
+    GOOGLE_BIG_QUERY = (
+        GoogleBigQueryDataPlatformTableCreator,
+        FunctionName.GOOGLE_BIGQUERY_DATA_ACCESS,
+    )
+
+    AMAZON_REDSHIFT = (
+        AmazonRedshiftDataPlatformTableCreator,
+        FunctionName.AMAZON_REDSHIFT_DATA_ACCESS,
+    )
+
     NATIVE_QUERY = (
-        NativeQueryTableFullNameCreator,
+        NativeQueryDataPlatformTableCreator,
         FunctionName.NATIVE_QUERY,
     )
 
-    def get_table_full_name_creator(self) -> Type[AbstractTableFullNameCreator]:
+    def get_table_full_name_creator(self) -> Type[AbstractDataPlatformTableCreator]:
         return self.value[0]
 
     def get_function_name(self) -> str:
         return self.value[1].value
 
     @staticmethod
     def get_function_names() -> List[str]:
@@ -647,13 +763,13 @@
         for supported_resolver in SupportedResolver:
             functions.append(supported_resolver.get_function_name())
 
         return functions
 
     @staticmethod
     def get_resolver(function_name: str) -> Optional["SupportedResolver"]:
-        logger.debug("Looking for resolver %s", function_name)
+        logger.debug(f"Looking for resolver {function_name}")
         for supported_resolver in SupportedResolver:
             if function_name == supported_resolver.get_function_name():
                 return supported_resolver
-        logger.debug("Looking not found for resolver %s", function_name)
+        logger.debug(f"Resolver not found for function_name {function_name}")
         return None
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi/m_query/tree_function.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/tree_function.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,13 +1,17 @@
 import logging
 from functools import partial
-from typing import Any, List, Optional, Union, cast
+from typing import Any, Dict, List, Optional, Union, cast
 
 from lark import Token, Tree
 
+from datahub.ingestion.source.powerbi.m_query.data_classes import (
+    TRACE_POWERBI_MQUERY_PARSER,
+)
+
 logger = logging.getLogger(__name__)
 
 
 def get_output_variable(root: Tree) -> Optional[str]:
     in_expression_tree: Optional[Tree] = get_first_rule(root, "in_expression")
     if in_expression_tree is None:
         return None
@@ -24,21 +28,22 @@
 def get_variable_statement(parse_tree: Tree, variable: str) -> Optional[Tree]:
     _filter = parse_tree.find_data("variable")
     # filter will return statement of the form <variable-name> = <expression>
     # We are searching for Tree where variable-name is matching with provided variable
     for tree in _filter:
         values: List[str] = token_values(tree.children[0])
         actual_value: str = "".join(strip_char_from_list(values, " "))
-        logger.debug("Actual Value = %s", actual_value)
-        logger.debug("Expected Value = %s", variable)
+        if TRACE_POWERBI_MQUERY_PARSER:
+            logger.debug(f"Actual Value = {actual_value}")
+            logger.debug(f"Expected Value = {variable}")
 
         if actual_value.lower() == variable.lower():
             return tree
 
-    logger.info("Provided variable(%s) not found in variable rule", variable)
+    logger.debug(f"Provided variable({variable}) not found in variable rule")
 
     return None
 
 
 def get_first_rule(tree: Tree, rule: str) -> Optional[Tree]:
     """
     Lark library doesn't have advance search function.
@@ -61,29 +66,49 @@
         return None
 
     expression_tree: Optional[Tree] = internal(tree)
 
     return expression_tree
 
 
-def token_values(tree: Tree) -> List[str]:
+def token_values(tree: Tree, parameters: Dict[str, str] = {}) -> List[str]:
     """
-
     :param tree: Tree to traverse
+    :param parameters: If parameters is not an empty dict, it will try to resolve identifier variable references
+                       using the values in 'parameters'.
     :return: List of leaf token data
     """
     values: List[str] = []
 
     def internal(node: Union[Tree, Token]) -> None:
-        if isinstance(node, Token):
+        if parameters and isinstance(node, Tree) and node.data == "identifier":
+            # This is the case where they reference a variable using
+            # the `#"Name of variable"` or `Variable` syntax. It can be
+            # a quoted_identifier or a regular_identifier.
+
+            ref = make_function_name(node)
+
+            # For quoted_identifier, ref will have quotes around it.
+            if ref.startswith('"') and ref[1:-1] in parameters:
+                resolved = parameters[ref[1:-1]]
+                values.append(resolved)
+            elif ref in parameters:
+                resolved = parameters[ref]
+                values.append(resolved)
+            else:
+                # If we can't resolve, fall back to the name of the variable.
+                logger.debug(f"Unable to resolve parameter reference to {ref}")
+                values.append(ref)
+        elif isinstance(node, Token):
+            # This means we're probably looking at a literal.
             values.append(cast(Token, node).value)
             return
-
-        for child in node.children:
-            internal(child)
+        else:
+            for child in node.children:
+                internal(child)
 
     internal(tree)
 
     return values
 
 
 def remove_whitespaces_from_list(values: List[str]) -> List[str]:
@@ -91,15 +116,15 @@
     for item in values:
         if item.strip() not in ("", "\n", "\t"):
             result.append(item)
 
     return result
 
 
-def strip_char_from_list(values: List[str], char: str) -> List[str]:
+def strip_char_from_list(values: List[str], char: str = '"') -> List[str]:
     result: List[str] = []
     for item in values:
         result.append(item.strip(char))
 
     return result
 
 
@@ -116,15 +141,16 @@
     """
     functions: List[str] = []
 
     # List the all invoke_expression in the Tree
     _filter: Any = tree.find_data("invoke_expression")
 
     for node in _filter:
-        logger.debug("Tree = %s", node.pretty())
+        if TRACE_POWERBI_MQUERY_PARSER:
+            logger.debug(f"Tree = {node.pretty()}")
         primary_expression_node: Optional[Tree] = first_primary_expression_func(node)
         if primary_expression_node is None:
             continue
 
         identifier_node: Optional[Tree] = first_identifier_func(primary_expression_node)
         if identifier_node is None:
             continue
@@ -149,11 +175,10 @@
 
 
 first_expression_func = partial(get_first_rule, rule="expression")
 first_item_selector_func = partial(get_first_rule, rule="item_selector")
 first_arg_list_func = partial(get_first_rule, rule="argument_list")
 first_identifier_func = partial(get_first_rule, rule="identifier")
 first_primary_expression_func = partial(get_first_rule, rule="primary_expression")
-first_identifier_func = partial(get_first_rule, rule="identifier")
 first_invoke_expression_func = partial(get_first_rule, rule="invoke_expression")
 first_type_expression_func = partial(get_first_rule, rule="type_expression")
 first_list_expression_func = partial(get_first_rule, rule="list_expression")
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi/m_query/validator.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/m_query/validator.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi/powerbi-lexical-grammar.rule` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/powerbi-lexical-grammar.rule`

 * *Files 2% similar despite different names*

```diff
@@ -1,7 +1,16 @@
+// MODIFICATIONS:
+// - The let_expression definition has added the whitespace rule instead of the required newline.
+//   This allows the parser to be less strict about whitespace.
+// - Add inline whitespace to item_selection and optional_item_selection.
+// - Tweak unary_expression to allow arbitrary operators within it.
+//   This is necessary because unary_expression is the base for the
+//   whole relational_expression parse tree.
+
+
 lexical_unit:   lexical_elements?
 
 lexical_elements:   lexical_element 
                 |   lexical_elements?
 
 lexical_element:    whitespace
                 |   token comment
@@ -292,14 +301,15 @@
                 |       "meta" 
                 |       unary_expression
 
 unary_expression:   type_expression
                 |   "+" unary_expression
                 |   "_" unary_expression
                 |   "not" unary_expression
+                |   expression
 
 primary_expression: literal_expression
                 |   list_expression
                 |   record_expression
                 |   identifier_expression
                 |   section_access_expression
                 |   parenthesized_expression
@@ -357,17 +367,17 @@
 
 field_name: generalized_identifier
         |   quoted_identifier
 
 item_access_expression: item_selection
                     |   optional_item_selection
 
-item_selection: primary_expression "{" item_selector "}"
+item_selection: primary_expression whitespace? "{" item_selector "}"
 
-optional_item_selection:    primary_expression "{" item_selector "}" "?"
+optional_item_selection:    primary_expression whitespace? "{" item_selector "}" "?"
 
 item_selector:  expression
 
 field_access_expression:    field_selection
                         |   implicit_target_field_selection
                         |   projection
                         |   implicit_target_projection
@@ -422,15 +432,15 @@
 
 optional_parameter: "optional" WS_INLINE parameter
 
 each_expression:    "each" WS_INLINE each_expression_body
 
 each_expression_body:   function_body
 
-let_expression: "let" NEWLINE WS_INLINE? variable_list WS_INLINE? NEWLINE? in_expression
+let_expression: "let" whitespace variable_list WS_INLINE? NEWLINE? in_expression
 
 in_expression: "in" NEWLINE? WS_INLINE? NEWLINE? expression
 
 variable_list:    variable
             |     variable NEWLINE? WS_INLINE? "," NEWLINE? WS_INLINE? variable_list
 
 variable:   variable_name WS_INLINE? "=" WS_INLINE? expression
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi/powerbi.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi/powerbi.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,39 +1,53 @@
 #########################################################
 #
 # Meta Data Ingestion From the Power BI Source
 #
 #########################################################
 
 import logging
-from typing import Iterable, List, Optional, Tuple, Union, cast
+from typing import Iterable, List, Optional, Tuple
 
 import datahub.emitter.mce_builder as builder
-from datahub.configuration.source_common import DEFAULT_ENV
+import datahub.ingestion.source.powerbi.rest_api_wrapper.data_classes as powerbi_data_classes
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.emitter.mcp_builder import gen_containers
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.decorators import (
     SourceCapability,
     SupportStatus,
     capability,
     config_class,
     platform_name,
     support_status,
 )
-from datahub.ingestion.api.source import Source, SourceReport
+from datahub.ingestion.api.source import SourceReport
 from datahub.ingestion.api.workunit import MetadataWorkUnit
+from datahub.ingestion.source.common.subtypes import BIContainerSubTypes
 from datahub.ingestion.source.powerbi.config import (
     Constant,
     PlatformDetail,
     PowerBiDashboardSourceConfig,
     PowerBiDashboardSourceReport,
 )
+from datahub.ingestion.source.powerbi.dataplatform_instance_resolver import (
+    AbstractDataPlatformInstanceResolver,
+    create_dataplatform_instance_resolver,
+)
 from datahub.ingestion.source.powerbi.m_query import parser, resolver
-from datahub.ingestion.source.powerbi.proxy import PowerBiAPI
+from datahub.ingestion.source.powerbi.rest_api_wrapper.powerbi_api import PowerBiAPI
+from datahub.ingestion.source.state.sql_common_state import (
+    BaseSQLAlchemyCheckpointState,
+)
+from datahub.ingestion.source.state.stale_entity_removal_handler import (
+    StaleEntityRemovalHandler,
+)
+from datahub.ingestion.source.state.stateful_ingestion_base import (
+    StatefulIngestionSourceBase,
+)
 from datahub.metadata.com.linkedin.pegasus2avro.common import ChangeAuditStamps
 from datahub.metadata.schema_classes import (
     BrowsePathsClass,
     ChangeTypeClass,
     ChartInfoClass,
     ChartKeyClass,
     ContainerClass,
@@ -49,14 +63,19 @@
     StatusClass,
     SubTypesClass,
     TagAssociationClass,
     UpstreamClass,
     UpstreamLineageClass,
 )
 from datahub.utilities.dedup_list import deduplicate_list
+from datahub.utilities.source_helpers import (
+    auto_stale_entity_removal,
+    auto_status_aspect,
+    auto_workunit_reporter,
+)
 
 # Logger instance
 logger = logging.getLogger(__name__)
 
 
 class Mapper:
     """
@@ -75,17 +94,19 @@
         def __hash__(self):
             return id(self.id)
 
     def __init__(
         self,
         config: PowerBiDashboardSourceConfig,
         reporter: PowerBiDashboardSourceReport,
+        dataplatform_instance_resolver: AbstractDataPlatformInstanceResolver,
     ):
         self.__config = config
         self.__reporter = reporter
+        self.__dataplatform_instance_resolver = dataplatform_instance_resolver
 
     @staticmethod
     def urn_to_lowercase(value: str, flag: bool) -> str:
         if flag is True:
             return value.lower()
 
         return value
@@ -126,81 +147,73 @@
                 ENTITY_URN=mcp.entityUrn,
                 ASPECT_NAME=mcp.aspectName,
             ),
             mcp=mcp,
         )
 
     def extract_lineage(
-        self, table: PowerBiAPI.Table, ds_urn: str
+        self, table: powerbi_data_classes.Table, ds_urn: str
     ) -> List[MetadataChangeProposalWrapper]:
         mcps: List[MetadataChangeProposalWrapper] = []
 
+        # table.dataset should always be set, but we check it just in case.
+        parameters = table.dataset.parameters if table.dataset else {}
+
         upstreams: List[UpstreamClass] = []
         upstream_tables: List[resolver.DataPlatformTable] = parser.get_upstream_tables(
-            table, self.__reporter
+            table, self.__reporter, parameters=parameters
+        )
+        logger.debug(
+            f"PowerBI virtual table {table.full_name} and it's upstream dataplatform tables = {upstream_tables}"
         )
-
         for upstream_table in upstream_tables:
             if (
                 upstream_table.data_platform_pair.powerbi_data_platform_name
                 not in self.__config.dataset_type_mapping.keys()
             ):
                 logger.debug(
-                    "Skipping upstream table for %s. The platform (%s) is not part of dataset_type_mapping",
-                    ds_urn,
-                    upstream_table.data_platform_pair.powerbi_data_platform_name,
+                    f"Skipping upstream table for {ds_urn}. The platform {upstream_table.data_platform_pair.powerbi_data_platform_name} is not part of dataset_type_mapping",
                 )
                 continue
 
-            platform: Union[str, PlatformDetail] = self.__config.dataset_type_mapping[
-                upstream_table.data_platform_pair.powerbi_data_platform_name
-            ]
-
-            platform_name: str = (
-                upstream_table.data_platform_pair.datahub_data_platform_name
+            platform_detail: PlatformDetail = (
+                self.__dataplatform_instance_resolver.get_platform_instance(
+                    upstream_table
+                )
             )
-
-            platform_instance_name: Optional[str] = None
-            platform_env: str = DEFAULT_ENV
-            # Determine if PlatformDetail is provided
-            if isinstance(platform, PlatformDetail):
-                platform_instance_name = cast(
-                    PlatformDetail, platform
-                ).platform_instance
-                platform_env = cast(PlatformDetail, platform).env
-
             upstream_urn = builder.make_dataset_urn_with_platform_instance(
-                platform=platform_name,
-                platform_instance=platform_instance_name,
-                env=platform_env,
+                platform=upstream_table.data_platform_pair.datahub_data_platform_name,
+                platform_instance=platform_detail.platform_instance,
+                env=platform_detail.env,
                 name=self.lineage_urn_to_lowercase(upstream_table.full_name),
             )
 
             upstream_table_class = UpstreamClass(
                 upstream_urn,
                 DatasetLineageTypeClass.TRANSFORMED,
             )
             upstreams.append(upstream_table_class)
 
-            if len(upstreams) > 0:
-                upstream_lineage = UpstreamLineageClass(upstreams=upstreams)
-                mcp = MetadataChangeProposalWrapper(
-                    entityType="dataset",
-                    changeType=ChangeTypeClass.UPSERT,
-                    entityUrn=ds_urn,
-                    aspect=upstream_lineage,
-                )
-                mcps.append(mcp)
+        if len(upstreams) > 0:
+            upstream_lineage = UpstreamLineageClass(upstreams=upstreams)
+            logger.debug(f"Dataset urn = {ds_urn} and its lineage = {upstream_lineage}")
+            mcp = MetadataChangeProposalWrapper(
+                entityType=Constant.DATASET,
+                changeType=ChangeTypeClass.UPSERT,
+                entityUrn=ds_urn,
+                aspect=upstream_lineage,
+            )
+            mcps.append(mcp)
 
         return mcps
 
     def to_datahub_dataset(
         self,
-        dataset: Optional[PowerBiAPI.PowerBIDataset],
-        workspace: PowerBiAPI.Workspace,
+        dataset: Optional[powerbi_data_classes.PowerBIDataset],
+        workspace: powerbi_data_classes.Workspace,
     ) -> List[MetadataChangeProposalWrapper]:
         """
         Map PowerBi dataset to datahub dataset. Here we are mapping each table of PowerBi Dataset to Datahub dataset.
         In PowerBi Tile would be having single dataset, However corresponding Datahub's chart might have many input sources.
         """
 
         dataset_mcps: List[MetadataChangeProposalWrapper] = []
@@ -209,24 +222,26 @@
 
         logger.debug(
             f"Mapping dataset={dataset.name}(id={dataset.id}) to datahub dataset"
         )
 
         for table in dataset.tables:
             # Create a URN for dataset
-            ds_urn = builder.make_dataset_urn(
+            ds_urn = builder.make_dataset_urn_with_platform_instance(
                 platform=self.__config.platform_name,
                 name=self.assets_urn_to_lowercase(table.full_name),
+                platform_instance=self.__config.platform_instance,
                 env=self.__config.env,
             )
 
             logger.debug(f"{Constant.Dataset_URN}={ds_urn}")
             # Create datasetProperties mcp
             ds_properties = DatasetPropertiesClass(
-                name=table.name, description=table.name
+                name=table.name,
+                description=dataset.description,
             )
 
             info_mcp = self.new_mcp(
                 entity_type=Constant.DATASET,
                 entity_urn=ds_urn,
                 aspect_name=Constant.DATASET_PROPERTIES,
                 aspect=ds_properties,
@@ -266,52 +281,58 @@
                 TagAssociationClass(builder.make_tag_urn(tag_to_add))
                 for tag_to_add in tags
             ]
         )
 
     def to_datahub_chart_mcp(
         self,
-        tile: PowerBiAPI.Tile,
+        tile: powerbi_data_classes.Tile,
         ds_mcps: List[MetadataChangeProposalWrapper],
-        workspace: PowerBiAPI.Workspace,
+        workspace: powerbi_data_classes.Workspace,
     ) -> List[MetadataChangeProposalWrapper]:
         """
         Map PowerBi tile to datahub chart
         """
-        logger.info("Converting tile {}(id={}) to chart".format(tile.title, tile.id))
+        logger.info(f"Converting tile {tile.title}(id={tile.id}) to chart")
         # Create a URN for chart
         chart_urn = builder.make_chart_urn(
-            self.__config.platform_name, tile.get_urn_part()
+            platform=self.__config.platform_name,
+            platform_instance=self.__config.platform_instance,
+            name=tile.get_urn_part(),
         )
 
-        logger.info("{}={}".format(Constant.CHART_URN, chart_urn))
+        logger.info(f"{Constant.CHART_URN}={chart_urn}")
 
         ds_input: List[str] = self.to_urn_set(ds_mcps)
 
-        def tile_custom_properties(tile: PowerBiAPI.Tile) -> dict:
-            custom_properties = {
-                "datasetId": tile.dataset.id if tile.dataset else "",
-                "reportId": tile.report.id if tile.report else "",
-                "datasetWebUrl": tile.dataset.webUrl
-                if tile.dataset is not None
-                else "",
-                "createdFrom": tile.createdFrom.value,
+        def tile_custom_properties(tile: powerbi_data_classes.Tile) -> dict:
+            custom_properties: dict = {
+                Constant.CREATED_FROM: tile.createdFrom.value,
             }
 
+            if tile.dataset_id is not None:
+                custom_properties[Constant.DATASET_ID] = tile.dataset_id
+
+            if tile.dataset is not None and tile.dataset.webUrl is not None:
+                custom_properties[Constant.DATASET_WEB_URL] = tile.dataset.webUrl
+
+            if tile.report is not None and tile.report.id is not None:
+                custom_properties[Constant.REPORT_ID] = tile.report.id
+
             return custom_properties
 
         # Create chartInfo mcp
         # Set chartUrl only if tile is created from Report
         chart_info_instance = ChartInfoClass(
             title=tile.title or "",
             description=tile.title or "",
             lastModified=ChangeAuditStamps(),
             inputs=ds_input,
             externalUrl=tile.report.webUrl if tile.report else None,
-            customProperties={**tile_custom_properties(tile)},
+            customProperties=tile_custom_properties(tile),
         )
 
         info_mcp = self.new_mcp(
             entity_type=Constant.CHART,
             entity_urn=chart_urn,
             aspect_name=Constant.CHART_INFO,
             aspect=chart_info_instance,
@@ -327,22 +348,28 @@
 
         # ChartKey status
         chart_key_instance = ChartKeyClass(
             dashboardTool=self.__config.platform_name,
             chartId=Constant.CHART_ID.format(tile.id),
         )
 
-        chartkey_mcp = self.new_mcp(
+        chart_key_mcp = self.new_mcp(
             entity_type=Constant.CHART,
             entity_urn=chart_urn,
             aspect_name=Constant.CHART_KEY,
             aspect=chart_key_instance,
         )
-
-        result_mcps = [info_mcp, status_mcp, chartkey_mcp]
+        browse_path = BrowsePathsClass(paths=["/powerbi/{}".format(workspace.name)])
+        browse_path_mcp = self.new_mcp(
+            entity_type=Constant.CHART,
+            entity_urn=chart_urn,
+            aspect_name=Constant.BROWSERPATH,
+            aspect=browse_path,
+        )
+        result_mcps = [info_mcp, status_mcp, chart_key_mcp, browse_path_mcp]
 
         self.append_container_mcp(
             result_mcps,
             workspace,
             chart_urn,
         )
 
@@ -356,40 +383,42 @@
                 for mcp in mcps
                 if mcp is not None and mcp.entityUrn is not None
             ]
         )
 
     def to_datahub_dashboard_mcp(
         self,
-        dashboard: PowerBiAPI.Dashboard,
-        workspace: PowerBiAPI.Workspace,
+        dashboard: powerbi_data_classes.Dashboard,
+        workspace: powerbi_data_classes.Workspace,
         chart_mcps: List[MetadataChangeProposalWrapper],
         user_mcps: List[MetadataChangeProposalWrapper],
     ) -> List[MetadataChangeProposalWrapper]:
         """
         Map PowerBi dashboard to Datahub dashboard
         """
 
         dashboard_urn = builder.make_dashboard_urn(
-            self.__config.platform_name, dashboard.get_urn_part()
+            platform=self.__config.platform_name,
+            platform_instance=self.__config.platform_instance,
+            name=dashboard.get_urn_part(),
         )
 
         chart_urn_list: List[str] = self.to_urn_set(chart_mcps)
         user_urn_list: List[str] = self.to_urn_set(user_mcps)
 
-        def chart_custom_properties(dashboard: PowerBiAPI.Dashboard) -> dict:
+        def chart_custom_properties(dashboard: powerbi_data_classes.Dashboard) -> dict:
             return {
-                "chartCount": str(len(dashboard.tiles)),
-                "workspaceName": dashboard.workspace_name,
-                "workspaceId": dashboard.workspace_id,
+                Constant.CHART_COUNT: str(len(dashboard.tiles)),
+                Constant.WORKSPACE_NAME: dashboard.workspace_name,
+                Constant.WORKSPACE_ID: dashboard.workspace_id,
             }
 
         # DashboardInfo mcp
         dashboard_info_cls = DashboardInfoClass(
-            description=dashboard.displayName or "",
+            description=dashboard.description,
             title=dashboard.displayName or "",
             charts=chart_urn_list,
             lastModified=ChangeAuditStamps(),
             dashboardUrl=dashboard.webUrl,
             customProperties={**chart_custom_properties(dashboard)},
         )
 
@@ -438,15 +467,15 @@
                 entity_urn=dashboard_urn,
                 aspect_name=Constant.OWNERSHIP,
                 aspect=ownership,
             )
 
         # Dashboard browsePaths
         browse_path = BrowsePathsClass(
-            paths=["/powerbi/{}".format(dashboard.workspace_name)]
+            paths=[f"/{Constant.PLATFORM_NAME}/{dashboard.workspace_name}"]
         )
         browse_path_mcp = self.new_mcp(
             entity_type=Constant.DASHBOARD,
             entity_urn=dashboard_urn,
             aspect_name=Constant.BROWSERPATH,
             aspect=browse_path,
         )
@@ -475,38 +504,41 @@
         )
 
         return list_of_mcps
 
     def append_container_mcp(
         self,
         list_of_mcps: List[MetadataChangeProposalWrapper],
-        workspace: PowerBiAPI.Workspace,
+        workspace: powerbi_data_classes.Workspace,
         entity_urn: str,
     ) -> None:
         if self.__config.extract_workspaces_to_containers:
-            container_key = workspace.get_workspace_key(self.__config.platform_name)
+            container_key = workspace.get_workspace_key(
+                platform_name=self.__config.platform_name,
+                platform_instance=self.__config.platform_instance,
+            )
             container_urn = builder.make_container_urn(
                 guid=container_key.guid(),
             )
 
             mcp = MetadataChangeProposalWrapper(
                 changeType=ChangeTypeClass.UPSERT,
                 entityUrn=entity_urn,
                 aspect=ContainerClass(container=f"{container_urn}"),
             )
             list_of_mcps.append(mcp)
 
     def generate_container_for_workspace(
-        self, workspace: PowerBiAPI.Workspace
+        self, workspace: powerbi_data_classes.Workspace
     ) -> Iterable[MetadataWorkUnit]:
         workspace_key = workspace.get_workspace_key(self.__config.platform_name)
         container_work_units = gen_containers(
             container_key=workspace_key,
             name=workspace.name,
-            sub_types=["Workspace"],
+            sub_types=[BIContainerSubTypes.POWERBI_WORKSPACE],
         )
         return container_work_units
 
     def append_tag_mcp(
         self,
         list_of_mcps: List[MetadataChangeProposalWrapper],
         entity_urn: str,
@@ -519,15 +551,15 @@
                 entity_urn=entity_urn,
                 aspect_name=Constant.GLOBAL_TAGS,
                 aspect=self.transform_tags(tags),
             )
             list_of_mcps.append(tags_mcp)
 
     def to_datahub_user(
-        self, user: PowerBiAPI.User
+        self, user: powerbi_data_classes.User
     ) -> List[MetadataChangeProposalWrapper]:
         """
         Map PowerBi user to datahub user
         """
 
         logger.debug(f"Mapping user {user.displayName}(id={user.id}) to datahub's user")
 
@@ -542,27 +574,27 @@
             aspect_name=Constant.CORP_USER_KEY,
             aspect=user_key,
         )
 
         return [user_key_mcp]
 
     def to_datahub_users(
-        self, users: List[PowerBiAPI.User]
+        self, users: List[powerbi_data_classes.User]
     ) -> List[MetadataChangeProposalWrapper]:
         user_mcps = []
 
         for user in users:
             user_mcps.extend(self.to_datahub_user(user))
 
         return user_mcps
 
     def to_datahub_chart(
         self,
-        tiles: List[PowerBiAPI.Tile],
-        workspace: PowerBiAPI.Workspace,
+        tiles: List[powerbi_data_classes.Tile],
+        workspace: powerbi_data_classes.Workspace,
     ) -> Tuple[
         List[MetadataChangeProposalWrapper], List[MetadataChangeProposalWrapper]
     ]:
         ds_mcps = []
         chart_mcps = []
 
         # Return empty list if input list is empty
@@ -584,16 +616,16 @@
 
         # Return dataset and chart MCPs
 
         return ds_mcps, chart_mcps
 
     def to_datahub_work_units(
         self,
-        dashboard: PowerBiAPI.Dashboard,
-        workspace: PowerBiAPI.Workspace,
+        dashboard: powerbi_data_classes.Dashboard,
+        workspace: powerbi_data_classes.Workspace,
     ) -> List[EquableMetadataWorkUnit]:
         mcps = []
 
         logger.info(
             f"Converting dashboard={dashboard.displayName} to datahub dashboard"
         )
 
@@ -617,49 +649,50 @@
         # Convert MCP to work_units
         work_units = map(self._to_work_unit, mcps)
         # Return set of work_unit
         return deduplicate_list([wu for wu in work_units if wu is not None])
 
     def pages_to_chart(
         self,
-        pages: List[PowerBiAPI.Page],
-        workspace: PowerBiAPI.Workspace,
+        pages: List[powerbi_data_classes.Page],
+        workspace: powerbi_data_classes.Workspace,
         ds_mcps: List[MetadataChangeProposalWrapper],
     ) -> List[MetadataChangeProposalWrapper]:
-
         chart_mcps = []
 
         # Return empty list if input list is empty
         if not pages:
             return []
 
         logger.debug(f"Converting pages(count={len(pages)}) to charts")
 
         def to_chart_mcps(
-            page: PowerBiAPI.Page,
+            page: powerbi_data_classes.Page,
             ds_mcps: List[MetadataChangeProposalWrapper],
         ) -> List[MetadataChangeProposalWrapper]:
-            logger.debug("Converting page {} to chart".format(page.displayName))
+            logger.debug(f"Converting page {page.displayName} to chart")
             # Create a URN for chart
             chart_urn = builder.make_chart_urn(
-                self.__config.platform_name, page.get_urn_part()
+                platform=self.__config.platform_name,
+                platform_instance=self.__config.platform_instance,
+                name=page.get_urn_part(),
             )
 
-            logger.debug("{}={}".format(Constant.CHART_URN, chart_urn))
+            logger.debug(f"{Constant.CHART_URN}={chart_urn}")
 
             ds_input: List[str] = self.to_urn_set(ds_mcps)
 
             # Create chartInfo mcp
             # Set chartUrl only if tile is created from Report
             chart_info_instance = ChartInfoClass(
                 title=page.displayName or "",
                 description=page.displayName or "",
                 lastModified=ChangeAuditStamps(),
                 inputs=ds_input,
-                customProperties={"order": str(page.order)},
+                customProperties={Constant.ORDER: str(page.order)},
             )
 
             info_mcp = self.new_mcp(
                 entity_type=Constant.CHART,
                 entity_urn=chart_urn,
                 aspect_name=Constant.CHART_INFO,
                 aspect=chart_info_instance,
@@ -668,15 +701,22 @@
             # removed status mcp
             status_mcp = self.new_mcp(
                 entity_type=Constant.CHART,
                 entity_urn=chart_urn,
                 aspect_name=Constant.STATUS,
                 aspect=StatusClass(removed=False),
             )
-            list_of_mcps = [info_mcp, status_mcp]
+            browse_path = BrowsePathsClass(paths=["/powerbi/{}".format(workspace.name)])
+            browse_path_mcp = self.new_mcp(
+                entity_type=Constant.CHART,
+                entity_urn=chart_urn,
+                aspect_name=Constant.BROWSERPATH,
+                aspect=browse_path,
+            )
+            list_of_mcps = [info_mcp, status_mcp, browse_path_mcp]
 
             self.append_container_mcp(
                 list_of_mcps,
                 workspace,
                 chart_urn,
             )
 
@@ -689,33 +729,35 @@
             chart_mcp = to_chart_mcps(page, ds_mcps)
             chart_mcps.extend(chart_mcp)
 
         return chart_mcps
 
     def report_to_dashboard(
         self,
-        workspace: PowerBiAPI.Workspace,
-        report: PowerBiAPI.Report,
+        workspace: powerbi_data_classes.Workspace,
+        report: powerbi_data_classes.Report,
         chart_mcps: List[MetadataChangeProposalWrapper],
         user_mcps: List[MetadataChangeProposalWrapper],
     ) -> List[MetadataChangeProposalWrapper]:
         """
         Map PowerBi report to Datahub dashboard
         """
 
         dashboard_urn = builder.make_dashboard_urn(
-            self.__config.platform_name, report.get_urn_part()
+            platform=self.__config.platform_name,
+            platform_instance=self.__config.platform_instance,
+            name=report.get_urn_part(),
         )
 
         chart_urn_list: List[str] = self.to_urn_set(chart_mcps)
         user_urn_list: List[str] = self.to_urn_set(user_mcps)
 
         # DashboardInfo mcp
         dashboard_info_cls = DashboardInfoClass(
-            description=report.description or "",
+            description=report.description,
             title=report.name or "",
             charts=chart_urn_list,
             lastModified=ChangeAuditStamps(),
             dashboardUrl=report.webUrl,
         )
 
         info_mcp = self.new_mcp(
@@ -742,47 +784,48 @@
         # Dashboard key
         dashboard_key_mcp = self.new_mcp(
             entity_type=Constant.DASHBOARD,
             entity_urn=dashboard_urn,
             aspect_name=Constant.DASHBOARD_KEY,
             aspect=dashboard_key_cls,
         )
-
-        # Dashboard Ownership
+        # Report Ownership
         owners = [
             OwnerClass(owner=user_urn, type=OwnershipTypeClass.NONE)
             for user_urn in user_urn_list
             if user_urn is not None
         ]
 
         owner_mcp = None
         if len(owners) > 0:
-            # Dashboard owner MCP
+            # Report owner MCP
             ownership = OwnershipClass(owners=owners)
             owner_mcp = self.new_mcp(
                 entity_type=Constant.DASHBOARD,
                 entity_urn=dashboard_urn,
                 aspect_name=Constant.OWNERSHIP,
                 aspect=ownership,
             )
 
-        # Dashboard browsePaths
-        browse_path = BrowsePathsClass(paths=["/powerbi/{}".format(workspace.name)])
+        # Report browsePaths
+        browse_path = BrowsePathsClass(
+            paths=[f"/{Constant.PLATFORM_NAME}/{workspace.name}"]
+        )
         browse_path_mcp = self.new_mcp(
             entity_type=Constant.DASHBOARD,
             entity_urn=dashboard_urn,
             aspect_name=Constant.BROWSERPATH,
             aspect=browse_path,
         )
 
         sub_type_mcp = self.new_mcp(
             entity_type=Constant.DASHBOARD,
             entity_urn=dashboard_urn,
             aspect_name=SubTypesClass.ASPECT_NAME,
-            aspect=SubTypesClass(typeNames=["Report"]),
+            aspect=SubTypesClass(typeNames=[Constant.REPORT_TYPE_NAME]),
         )
 
         list_of_mcps = [
             browse_path_mcp,
             info_mcp,
             removed_status_mcp,
             dashboard_key_mcp,
@@ -805,16 +848,16 @@
             report.tags,
         )
 
         return list_of_mcps
 
     def report_to_datahub_work_units(
         self,
-        report: PowerBiAPI.Report,
-        workspace: PowerBiAPI.Workspace,
+        report: powerbi_data_classes.Report,
+        workspace: powerbi_data_classes.Workspace,
     ) -> Iterable[MetadataWorkUnit]:
         mcps: List[MetadataChangeProposalWrapper] = []
 
         logger.debug(f"Converting dashboard={report.name} to datahub dashboard")
 
         # Convert user to CorpUser
         user_mcps = self.to_datahub_users(report.users)
@@ -835,107 +878,142 @@
         work_units = map(self._to_work_unit, mcps)
         return work_units
 
 
 @platform_name("PowerBI")
 @config_class(PowerBiDashboardSourceConfig)
 @support_status(SupportStatus.CERTIFIED)
+@capability(SourceCapability.DESCRIPTIONS, "Enabled by default")
+@capability(SourceCapability.PLATFORM_INSTANCE, "Enabled by default")
 @capability(
-    SourceCapability.OWNERSHIP, "On by default but can disabled by configuration"
+    SourceCapability.OWNERSHIP,
+    "Disabled by default, configured using `extract_ownership`",
 )
-class PowerBiDashboardSource(Source):
+class PowerBiDashboardSource(StatefulIngestionSourceBase):
     """
     This plugin extracts the following:
     - Power BI dashboards, tiles and datasets
     - Names, descriptions and URLs of dashboard and tile
     - Owners of dashboards
     """
 
     source_config: PowerBiDashboardSourceConfig
     reporter: PowerBiDashboardSourceReport
+    dataplatform_instance_resolver: AbstractDataPlatformInstanceResolver
     accessed_dashboards: int = 0
+    platform: str = "powerbi"
 
     def __init__(self, config: PowerBiDashboardSourceConfig, ctx: PipelineContext):
-        super().__init__(ctx)
+        super(PowerBiDashboardSource, self).__init__(config, ctx)
         self.source_config = config
         self.reporter = PowerBiDashboardSourceReport()
-        self.auth_token = PowerBiAPI(self.source_config).get_access_token()
-        self.powerbi_client = PowerBiAPI(self.source_config)
-        self.mapper = Mapper(config, self.reporter)
+        self.dataplatform_instance_resolver = create_dataplatform_instance_resolver(
+            self.source_config
+        )
+        try:
+            self.powerbi_client = PowerBiAPI(self.source_config)
+        except Exception as e:
+            logger.warning(e)
+            exit(
+                1
+            )  # Exit pipeline as we are not able to connect to PowerBI API Service. This exit will avoid raising
+            # unwanted stacktrace on console
+
+        self.mapper = Mapper(config, self.reporter, self.dataplatform_instance_resolver)
+
+        # Create and register the stateful ingestion use-case handler.
+        self.stale_entity_removal_handler = StaleEntityRemovalHandler(
+            source=self,
+            config=self.source_config,
+            state_type_class=BaseSQLAlchemyCheckpointState,
+            pipeline_name=ctx.pipeline_name,
+            run_id=ctx.run_id,
+        )
 
     @classmethod
     def create(cls, config_dict, ctx):
         config = PowerBiDashboardSourceConfig.parse_obj(config_dict)
         return cls(config, ctx)
 
-    def get_workspace_ids(self) -> Iterable[str]:
+    def get_allowed_workspaces(self) -> Iterable[powerbi_data_classes.Workspace]:
         all_workspaces = self.powerbi_client.get_workspaces()
-        return [
-            workspace.id
+        allowed_wrk = [
+            workspace
             for workspace in all_workspaces
             if self.source_config.workspace_id_pattern.allowed(workspace.id)
         ]
 
+        logger.info(f"Number of workspaces = {len(all_workspaces)}")
+        self.reporter.report_number_of_workspaces(len(all_workspaces))
+        logger.info(f"Number of allowed workspaces = {len(allowed_wrk)}")
+        logger.debug(f"Workspaces = {all_workspaces}")
+
+        return allowed_wrk
+
     def validate_dataset_type_mapping(self):
         powerbi_data_platforms: List[str] = [
             data_platform.value.powerbi_data_platform_name
             for data_platform in resolver.SupportedDataPlatform
         ]
 
         for key in self.source_config.dataset_type_mapping.keys():
             if key not in powerbi_data_platforms:
                 raise ValueError(f"PowerBI DataPlatform {key} is not supported")
 
-    def get_workunits(self) -> Iterable[MetadataWorkUnit]:
+        logger.debug(
+            f"Dataset lineage would get ingested for data-platform = {self.source_config.dataset_type_mapping}"
+        )
+
+    def get_workunits_internal(self) -> Iterable[MetadataWorkUnit]:
         """
         Datahub Ingestion framework invoke this method
         """
         logger.info("PowerBi plugin execution is started")
         # Validate dataset type mapping
         self.validate_dataset_type_mapping()
         # Fetch PowerBi workspace for given workspace identifier
-        for workspace_id in self.get_workspace_ids():
-            logger.info(f"Scanning workspace id: {workspace_id}")
-            workspace = self.powerbi_client.get_workspace(workspace_id, self.reporter)
+        for workspace in self.get_allowed_workspaces():
+            logger.info(f"Scanning workspace id: {workspace.id}")
+            self.powerbi_client.fill_workspace(workspace, self.reporter)
 
             if self.source_config.extract_workspaces_to_containers:
                 workspace_workunits = self.mapper.generate_container_for_workspace(
                     workspace
                 )
 
                 for workunit in workspace_workunits:
-                    # Add workunit to report
-                    self.reporter.report_workunit(workunit)
                     # Return workunit to Datahub Ingestion framework
                     yield workunit
-
             for dashboard in workspace.dashboards:
-
                 try:
                     # Fetch PowerBi users for dashboards
                     dashboard.users = self.powerbi_client.get_dashboard_users(dashboard)
                     # Increase dashboard and tiles count in report
                     self.reporter.report_dashboards_scanned()
                     self.reporter.report_charts_scanned(count=len(dashboard.tiles))
                 except Exception as e:
                     message = f"Error ({e}) occurred while loading dashboard {dashboard.displayName}(id={dashboard.id}) tiles."
 
                     logger.exception(message, e)
                     self.reporter.report_warning(dashboard.id, message)
                 # Convert PowerBi Dashboard and child entities to Datahub work unit to ingest into Datahub
                 workunits = self.mapper.to_datahub_work_units(dashboard, workspace)
                 for workunit in workunits:
-                    # Add workunit to report
-                    self.reporter.report_workunit(workunit)
                     # Return workunit to Datahub Ingestion framework
                     yield workunit
 
-            if self.source_config.extract_reports:
-                for report in self.powerbi_client.get_reports(workspace=workspace):
-                    for work_unit in self.mapper.report_to_datahub_work_units(
-                        report, workspace
-                    ):
-                        self.reporter.report_workunit(work_unit)
-                        yield work_unit
+            for report in workspace.reports:
+                for work_unit in self.mapper.report_to_datahub_work_units(
+                    report, workspace
+                ):
+                    yield work_unit
+
+    def get_workunits(self) -> Iterable[MetadataWorkUnit]:
+        return auto_stale_entity_removal(
+            self.stale_entity_removal_handler,
+            auto_workunit_reporter(
+                self.reporter, auto_status_aspect(self.get_workunits_internal())
+            ),
+        )
 
     def get_report(self) -> SourceReport:
         return self.reporter
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi_report_server/constants.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi_report_server/constants.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi_report_server/report_server.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi_report_server/report_server.py`

 * *Files 1% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 import pydantic
 import requests
 from requests.exceptions import ConnectionError
 from requests_ntlm import HttpNtlmAuth
 
 import datahub.emitter.mce_builder as builder
 from datahub.configuration.common import AllowDenyPattern
-from datahub.configuration.source_common import EnvBasedSourceConfigBase
+from datahub.configuration.source_common import EnvConfigMixin
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.decorators import (
     SourceCapability,
     SupportStatus,
     capability,
     config_class,
@@ -54,15 +54,15 @@
     StatusClass,
 )
 from datahub.utilities.dedup_list import deduplicate_list
 
 LOGGER = logging.getLogger(__name__)
 
 
-class PowerBiReportServerAPIConfig(EnvBasedSourceConfigBase):
+class PowerBiReportServerAPIConfig(EnvConfigMixin):
     username: str = pydantic.Field(description="Windows account username")
     password: str = pydantic.Field(description="Windows account password")
     workstation_name: str = pydantic.Field(
         default="localhost", description="Workstation name"
     )
     host_port: str = pydantic.Field(description="Power BI Report Server host URL")
     server_alias: str = pydantic.Field(
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/powerbi_report_server/report_server_domain.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/powerbi_report_server/report_server_domain.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/profiling/common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/profiling/common.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/pulsar.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/pulsar.py`

 * *Files 1% similar despite different names*

```diff
@@ -23,14 +23,15 @@
     capability,
     config_class,
     platform_name,
     support_status,
 )
 from datahub.ingestion.api.workunit import MetadataWorkUnit
 from datahub.ingestion.extractor import schema_util
+from datahub.ingestion.source.common.subtypes import DatasetSubTypes
 from datahub.ingestion.source.state.entity_removal_state import GenericCheckpointState
 from datahub.ingestion.source.state.stale_entity_removal_handler import (
     StaleEntityRemovalHandler,
 )
 from datahub.ingestion.source.state.stateful_ingestion_base import (
     StatefulIngestionSourceBase,
 )
@@ -219,18 +220,14 @@
                 message = f"An HTTP error occurred: {http_error}"
                 self.report.report_warning("HTTPError", message)
         except requests.exceptions.RequestException as e:
             raise Exception(
                 f"An ambiguous exception occurred while handling the request: {e}"
             )
 
-    def get_platform_instance_id(self) -> str:
-        assert self.config.platform_instance is not None
-        return self.config.platform_instance
-
     @classmethod
     def create(cls, config_dict, ctx):
         config = PulsarSourceConfig.parse_obj(config_dict)
 
         # Do not include each individual partition for partitioned topics,
         if config.exclude_individual_partitions:
             config.topic_patterns.deny.append(r".*-partition-[0-9]+")
@@ -481,15 +478,15 @@
             ).as_workunit()
             self.report.report_workunit(platform_instance_wu)
             yield platform_instance_wu
 
         # 6. Emit subtype aspect marking this as a "topic"
         subtype_wu = MetadataChangeProposalWrapper(
             entityUrn=dataset_urn,
-            aspect=SubTypesClass(typeNames=["topic"]),
+            aspect=SubTypesClass(typeNames=[DatasetSubTypes.TOPIC]),
         ).as_workunit()
         self.report.report_workunit(subtype_wu)
         yield subtype_wu
 
         # 7. Emit domains aspect
         domain_urn: Optional[str] = None
         for domain, pattern in self.config.domain.items():
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/redash.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redash.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/s3/config.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/s3/config.py`

 * *Files 7% similar despite different names*

```diff
@@ -2,34 +2,32 @@
 from typing import Any, Dict, List, Optional, Union
 
 import pydantic
 from pydantic.fields import Field
 
 from datahub.configuration.common import AllowDenyPattern
 from datahub.configuration.source_common import (
-    EnvBasedSourceConfigBase,
-    PlatformSourceConfigBase,
+    EnvConfigMixin,
+    PlatformInstanceConfigMixin,
 )
 from datahub.configuration.validate_field_rename import pydantic_renamed_field
 from datahub.ingestion.source.aws.aws_common import AwsConnectionConfig
 from datahub.ingestion.source.aws.path_spec import PathSpec
-from datahub.ingestion.source.aws.s3_util import get_bucket_name
 from datahub.ingestion.source.s3.profiling import DataLakeProfilerConfig
 
 # hide annoying debug errors from py4j
 logging.getLogger("py4j").setLevel(logging.ERROR)
 logger: logging.Logger = logging.getLogger(__name__)
 
 
-class DataLakeSourceConfig(PlatformSourceConfigBase, EnvBasedSourceConfigBase):
+class DataLakeSourceConfig(PlatformInstanceConfigMixin, EnvConfigMixin):
     path_specs: List[PathSpec] = Field(
         description="List of PathSpec. See [below](#path-spec) the details about PathSpec"
     )
     platform: str = Field(
-        # The platform field already exists, but we want to override the type/default/docs.
         default="",
         description="The platform that this source connects to (either 's3' or 'file'). "
         "If not specified, the platform will be inferred from the path_specs.",
     )
     aws_config: Optional[AwsConnectionConfig] = Field(
         default=None, description="AWS configuration"
     )
@@ -40,14 +38,20 @@
     )
     # Whether or not to create in datahub from the s3 object
     use_s3_object_tags: Optional[bool] = Field(
         None,
         description="# Whether or not to create tags in datahub from the s3 object",
     )
 
+    # Whether to update the table schema when schema in files within the partitions are updated
+    update_schema_on_partition_file_updates: Optional[bool] = Field(
+        default=False,
+        description="Whether to update the table schema when schema in files within the partitions are updated.",
+    )
+
     profile_patterns: AllowDenyPattern = Field(
         default=AllowDenyPattern.allow_all(),
         description="regex patterns for tables to profile ",
     )
     profiling: DataLakeProfilerConfig = Field(
         default=DataLakeProfilerConfig(), description="Data profiling configuration"
     )
@@ -83,24 +87,14 @@
         )
         if len(guessed_platforms) > 1:
             raise ValueError(
                 f"Cannot have multiple platforms in path_specs: {guessed_platforms}"
             )
         guessed_platform = guessed_platforms.pop()
 
-        # If platform is s3, check that they're all the same bucket.
-        if guessed_platform == "s3":
-            bucket_names = set(
-                get_bucket_name(path_spec.include) for path_spec in path_specs
-            )
-            if len(bucket_names) > 1:
-                raise ValueError(
-                    f"All path_specs should reference the same s3 bucket. Got {bucket_names}"
-                )
-
         # Ensure s3 configs aren't used for file sources.
         if guessed_platform != "s3" and (
             values.get("use_s3_object_tags") or values.get("use_s3_bucket_tags")
         ):
             raise ValueError(
                 "Cannot grab s3 object/bucket tags when platform is not s3. Remove the flag or use s3."
             )
@@ -112,14 +106,24 @@
             )
         else:
             logger.debug(f'Setting config "platform": {guessed_platform}')
             values["platform"] = guessed_platform
 
         return path_specs
 
+    @pydantic.validator("platform", always=True)
+    def platform_not_empty(cls, platform: str, values: dict) -> str:
+        inferred_platform = values.get(
+            "platform", None
+        )  # we may have inferred it above
+        platform = platform or inferred_platform
+        if not platform:
+            raise ValueError("platform must not be empty")
+        return platform
+
     @pydantic.root_validator()
     def ensure_profiling_pattern_is_passed_to_profiling(
         cls, values: Dict[str, Any]
     ) -> Dict[str, Any]:
         profiling: Optional[DataLakeProfilerConfig] = values.get("profiling")
         if profiling is not None and profiling.enabled:
             profiling._allow_deny_patterns = values["profile_patterns"]
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/s3/data_lake_utils.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/s3/data_lake_utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,14 +11,15 @@
 )
 from datahub.ingestion.api.workunit import MetadataWorkUnit
 from datahub.ingestion.source.aws.s3_util import (
     get_bucket_name,
     get_bucket_relative_path,
     is_s3_uri,
 )
+from datahub.ingestion.source.common.subtypes import DatasetContainerSubTypes
 
 # hide annoying debug errors from py4j
 logging.getLogger("py4j").setLevel(logging.ERROR)
 logger: logging.Logger = logging.getLogger(__name__)
 
 
 class ContainerWUCreator:
@@ -75,15 +76,15 @@
         parent_key = None
         if is_s3_uri(path):
             bucket_name = get_bucket_name(path)
             bucket_key = self.gen_bucket_key(bucket_name)
             yield from self.create_emit_containers(
                 container_key=bucket_key,
                 name=bucket_name,
-                sub_types=["S3 bucket"],
+                sub_types=[DatasetContainerSubTypes.S3_BUCKET],
                 parent_container_key=None,
             )
             parent_key = bucket_key
             base_full_path = get_bucket_relative_path(path)
 
         parent_folder_path = (
             base_full_path[: base_full_path.rfind("/")]
@@ -107,14 +108,14 @@
                 elif isinstance(parent_key, FolderKey):
                     prefix = parent_key.folder_abs_path
                 abs_path = prefix + "/" + folder
             folder_key = self.gen_folder_key(abs_path)
             yield from self.create_emit_containers(
                 container_key=folder_key,
                 name=folder,
-                sub_types=["Folder"],
+                sub_types=[DatasetContainerSubTypes.S3_FOLDER],
                 parent_container_key=parent_key,
             )
             parent_key = folder_key
 
         assert parent_key is not None
         yield from add_dataset_to_container(parent_key, dataset_urn)
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/s3/profiling.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/s3/profiling.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/s3/source.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/s3/source.py`

 * *Files 3% similar despite different names*

```diff
@@ -75,15 +75,14 @@
     RecordTypeClass,
     SchemaFieldDataType,
     SchemaMetadata,
     StringTypeClass,
     TimeTypeClass,
 )
 from datahub.metadata.schema_classes import (
-    ChangeTypeClass,
     DatasetPropertiesClass,
     MapTypeClass,
     OtherSchemaClass,
 )
 from datahub.telemetry import stats, telemetry
 from datahub.utilities.perf_timer import PerfTimer
 
@@ -303,14 +302,23 @@
             else:
                 # if no explicit AWS config is provided, use a default AWS credentials provider
                 conf.set(
                     "spark.hadoop.fs.s3a.aws.credentials.provider",
                     "org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider",
                 )
 
+            if self.source_config.aws_config.aws_endpoint_url is not None:
+                conf.set(
+                    "fs.s3a.endpoint", self.source_config.aws_config.aws_endpoint_url
+                )
+            if self.source_config.aws_config.aws_region is not None:
+                conf.set(
+                    "fs.s3a.endpoint.region", self.source_config.aws_config.aws_region
+                )
+
         conf.set("spark.jars.excludes", pydeequ.f2j_maven_coord)
         conf.set("spark.driver.memory", self.source_config.spark_driver_memory)
 
         self.spark = SparkSession.builder.config(conf=conf).getOrCreate()
 
     @classmethod
     def create(cls, config_dict, ctx):
@@ -492,18 +500,15 @@
             logger.info(
                 f"Finished profiling {table_data.full_path}; took {time_taken:.3f} seconds"
             )
 
             self.profiling_times_taken.append(time_taken)
 
         mcp = MetadataChangeProposalWrapper(
-            entityType="dataset",
             entityUrn=dataset_urn,
-            changeType=ChangeTypeClass.UPSERT,
-            aspectName="datasetProfile",
             aspect=table_profiler.profile,
         )
         wu = MetadataWorkUnit(
             id=f"profile-{self.source_config.platform}-{table_data.table_path}", mcp=mcp
         )
         self.report.report_workunit(wu)
         yield wu
@@ -737,14 +742,25 @@
                         continue
                     table_data = self.extract_table_data(
                         path_spec, file, timestamp, size
                     )
                     if table_data.table_path not in table_dict:
                         table_dict[table_data.table_path] = table_data
                     else:
+                        logger.debug(
+                            f"Update schema on partition file updates is set to: {self.source_config.update_schema_on_partition_file_updates!s}"
+                        )
+                        if (
+                            self.source_config.update_schema_on_partition_file_updates
+                            and not path_spec.sample_files
+                        ):
+                            logger.info(
+                                "Will update table schema as file within the partitions has an updated schema."
+                            )
+                            table_dict[table_data.table_path] = table_data
                         table_dict[table_data.table_path].number_of_files = (
                             table_dict[table_data.table_path].number_of_files + 1
                         )
                         table_dict[table_data.table_path].size_in_bytes = (
                             table_dict[table_data.table_path].size_in_bytes
                             + table_data.size_in_bytes
                         )
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/salesforce.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/salesforce.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,27 +11,28 @@
 
 import datahub.emitter.mce_builder as builder
 from datahub.configuration.common import (
     AllowDenyPattern,
     ConfigModel,
     ConfigurationError,
 )
-from datahub.configuration.source_common import DatasetSourceConfigBase
+from datahub.configuration.source_common import DatasetSourceConfigMixin
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.emitter.mcp_builder import add_domain_to_entity_wu
 from datahub.ingestion.api.common import PipelineContext, WorkUnit
 from datahub.ingestion.api.decorators import (
     SourceCapability,
     SupportStatus,
     capability,
     config_class,
     platform_name,
     support_status,
 )
 from datahub.ingestion.api.source import Source, SourceReport
+from datahub.ingestion.source.common.subtypes import DatasetSubTypes
 from datahub.metadata.schema_classes import (
     AuditStampClass,
     BooleanTypeClass,
     BytesTypeClass,
     DataPlatformInstanceClass,
     DatasetProfileClass,
     DatasetPropertiesClass,
@@ -67,15 +68,15 @@
         default=False,
         description="Whether profiling should be done. Supports only table-level profiling at this stage",
     )
 
     # TODO - support field level profiling
 
 
-class SalesforceConfig(DatasetSourceConfigBase):
+class SalesforceConfig(DatasetSourceConfigMixin):
     platform = "salesforce"
 
     auth: SalesforceAuthType = SalesforceAuthType.USERNAME_PASSWORD
 
     # Username, Password Auth
     username: Optional[str] = Field(description="Salesforce username")
     password: Optional[str] = Field(description="Password for Salesforce user")
@@ -444,19 +445,19 @@
             customProperties=sObjectProperties,
         )
         return MetadataChangeProposalWrapper(
             entityUrn=datasetUrn, aspect=datasetProperties
         ).as_workunit()
 
     def get_subtypes_workunit(self, sObjectName: str, datasetUrn: str) -> WorkUnit:
-        subtypes = []
+        subtypes: List[str] = []
         if sObjectName.endswith("__c"):
-            subtypes.append("Custom Object")
+            subtypes.append(DatasetSubTypes.SALESFORCE_CUSTOM_OBJECT)
         else:
-            subtypes.append("Standard Object")
+            subtypes.append(DatasetSubTypes.SALESFORCE_STANDARD_OBJECT)
 
         return MetadataChangeProposalWrapper(
             entityUrn=datasetUrn, aspect=SubTypesClass(typeNames=subtypes)
         ).as_workunit()
 
     def get_profile_workunit(
         self, sObjectName: str, datasetUrn: str
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/schema_inference/avro.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/avro.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/schema_inference/csv_tsv.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/csv_tsv.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/schema_inference/json.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/json.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/schema_inference/object.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/object.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/schema_inference/parquet.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/schema_inference/parquet.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/snowflake/constants.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/constants.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/snowflake/snowflake_config.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_config.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,23 +1,24 @@
 import logging
 from enum import Enum
 from typing import Dict, Optional, cast
 
 from pydantic import Field, SecretStr, root_validator, validator
 
 from datahub.configuration.common import AllowDenyPattern
+from datahub.configuration.validate_field_removal import pydantic_removed_field
+from datahub.configuration.validate_field_rename import pydantic_renamed_field
 from datahub.ingestion.glossary.classifier import ClassificationConfig
 from datahub.ingestion.source.state.stateful_ingestion_base import (
-    ProfilingStatefulIngestionConfig,
-    UsageStatefulIngestionConfig,
+    StatefulProfilingConfigMixin,
+    StatefulUsageConfigMixin,
 )
 from datahub.ingestion.source_config.sql.snowflake import (
     BaseSnowflakeConfig,
     SnowflakeConfig,
-    SnowflakeProvisionRoleConfig,
 )
 from datahub.ingestion.source_config.usage.snowflake_usage import SnowflakeUsageConfig
 
 logger = logging.Logger(__name__)
 
 
 class TagOption(str, Enum):
@@ -25,16 +26,16 @@
     without_lineage = "without_lineage"
     skip = "skip"
 
 
 class SnowflakeV2Config(
     SnowflakeConfig,
     SnowflakeUsageConfig,
-    UsageStatefulIngestionConfig,
-    ProfilingStatefulIngestionConfig,
+    StatefulUsageConfigMixin,
+    StatefulProfilingConfigMixin,
 ):
     convert_urns_to_lowercase: bool = Field(
         default=True,
     )
 
     include_usage_stats: bool = Field(
         default=True,
@@ -47,22 +48,20 @@
     )
 
     include_column_lineage: bool = Field(
         default=True,
         description="If enabled, populates the column lineage. Supported only for snowflake table-to-table and view-to-table lineage edge (not supported in table-to-view or view-to-view lineage edge yet). Requires appropriate grants given to the role.",
     )
 
-    check_role_grants: bool = Field(
-        default=False,
-        description="Not supported",
-    )
+    _check_role_grants_removed = pydantic_removed_field("check_role_grants")
+    _provision_role_removed = pydantic_removed_field("provision_role")
 
-    provision_role: Optional[SnowflakeProvisionRoleConfig] = Field(
-        default=None, description="Not supported"
-    )
+    # FIXME: This validator already exists in one of the parent classes, but for some reason it
+    # does not have any effect there. As such, we have to re-add it here.
+    rename_host_port_to_account_id = pydantic_renamed_field("host_port", "account_id")
 
     extract_tags: TagOption = Field(
         default=TagOption.skip,
         description="""Optional. Allowed values are `without_lineage`, `with_lineage`, and `skip` (default). `without_lineage` only extracts tags that have been applied directly to the given entity. `with_lineage` extracts both directly applied and propagated tags, but will be significantly slower. See the [Snowflake documentation](https://docs.snowflake.com/en/user-guide/object-tagging.html#tag-lineage) for information about tag lineage/propagation. """,
     )
 
     classification: Optional[ClassificationConfig] = Field(
@@ -76,14 +75,19 @@
     )
 
     match_fully_qualified_names: bool = Field(
         default=False,
         description="Whether `schema_pattern` is matched against fully qualified schema name `<catalog>.<schema>`.",
     )
 
+    use_legacy_lineage_method: bool = Field(
+        default=True,
+        description="Whether to use the legacy lineage computation method. If set to False, ingestion uses new optimised lineage extraction method that requires less ingestion process memory.",
+    )
+
     @validator("include_column_lineage")
     def validate_include_column_lineage(cls, v, values):
         if not values.get("include_table_lineage") and v:
             raise ValueError(
                 "include_table_lineage must be True for include_column_lineage to be set."
             )
         return v
@@ -91,26 +95,14 @@
     tag_pattern: AllowDenyPattern = Field(
         default=AllowDenyPattern.allow_all(),
         description="List of regex patterns for tags to include in ingestion. Only used if `extract_tags` is enabled.",
     )
 
     @root_validator(pre=False)
     def validate_unsupported_configs(cls, values: Dict) -> Dict:
-        value = values.get("provision_role")
-        if value is not None and value.enabled:
-            raise ValueError(
-                "Provision role is currently not supported. Set `provision_role.enabled` to False."
-            )
-
-        value = values.get("check_role_grants")
-        if value is not None and value:
-            raise ValueError(
-                "Check role grants is not supported. Set `check_role_grants` to False.",
-            )
-
         value = values.get("include_read_operational_stats")
         if value is not None and value:
             raise ValueError(
                 "include_read_operational_stats is not supported. Set `include_read_operational_stats` to False.",
             )
 
         match_fully_qualified_names = values.get("match_fully_qualified_names")
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/snowflake/snowflake_lineage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_lineage_legacy.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/snowflake/snowflake_profiler.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_profiler.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 import dataclasses
 import logging
 from datetime import datetime
-from typing import Callable, Iterable, List, Optional, cast
+from typing import Callable, Dict, Iterable, List, Optional, cast
 
 from snowflake.sqlalchemy import snowdialect
 from sqlalchemy import create_engine, inspect
 from sqlalchemy.sql import sqltypes
 
 from datahub.configuration.pattern_utils import is_schema_allowed
 from datahub.emitter.mce_builder import make_dataset_urn_with_platform_instance
@@ -49,77 +49,74 @@
     ) -> None:
         super().__init__(config, report, self.platform, state_handler)
         self.config: SnowflakeV2Config = config
         self.report: SnowflakeV2Report = report
         self.logger = logger
 
     def get_workunits(
-        self, databases: List[SnowflakeDatabase]
+        self, database: SnowflakeDatabase, db_tables: Dict[str, List[SnowflakeTable]]
     ) -> Iterable[MetadataWorkUnit]:
         # Extra default SQLAlchemy option for better connection pooling and threading.
         # https://docs.sqlalchemy.org/en/14/core/pooling.html#sqlalchemy.pool.QueuePool.params.max_overflow
         if self.config.profiling.enabled:
             self.config.options.setdefault(
                 "max_overflow", self.config.profiling.max_workers
             )
 
-        for db in databases:
-            if not self.config.database_pattern.allowed(db.name):
+        profile_requests = []
+        for schema in database.schemas:
+            if not is_schema_allowed(
+                self.config.schema_pattern,
+                schema.name,
+                database.name,
+                self.config.match_fully_qualified_names,
+            ):
                 continue
-            profile_requests = []
-            for schema in db.schemas:
-                if not is_schema_allowed(
-                    self.config.schema_pattern,
-                    schema.name,
-                    db.name,
-                    self.config.match_fully_qualified_names,
-                ):
-                    continue
-                for table in schema.tables:
-                    # Emit the profile work unit
-                    profile_request = self.get_snowflake_profile_request(
-                        table, schema.name, db.name
-                    )
-                    if profile_request is not None:
-                        profile_requests.append(profile_request)
 
-            if len(profile_requests) == 0:
-                continue
+            for table in db_tables[schema.name]:
+                profile_request = self.get_snowflake_profile_request(
+                    table, schema.name, database.name
+                )
+                if profile_request is not None:
+                    profile_requests.append(profile_request)
 
-            table_profile_requests = cast(List[TableProfilerRequest], profile_requests)
+        if len(profile_requests) == 0:
+            return
 
-            for request, profile in self.generate_profiles(
-                table_profile_requests,
-                self.config.profiling.max_workers,
-                db.name,
-                platform=self.platform,
-                profiler_args=self.get_profile_args(),
-            ):
-                if profile is None:
-                    continue
-                profile.sizeInBytes = cast(
-                    SnowflakeProfilerRequest, request
-                ).table.size_in_bytes
-                dataset_name = request.pretty_name
-                dataset_urn = make_dataset_urn_with_platform_instance(
-                    self.platform,
-                    dataset_name,
-                    self.config.platform_instance,
-                    self.config.env,
+        table_profile_requests = cast(List[TableProfilerRequest], profile_requests)
+
+        for request, profile in self.generate_profiles(
+            table_profile_requests,
+            self.config.profiling.max_workers,
+            database.name,
+            platform=self.platform,
+            profiler_args=self.get_profile_args(),
+        ):
+            if profile is None:
+                continue
+            profile.sizeInBytes = cast(
+                SnowflakeProfilerRequest, request
+            ).table.size_in_bytes
+            dataset_name = request.pretty_name
+            dataset_urn = make_dataset_urn_with_platform_instance(
+                self.platform,
+                dataset_name,
+                self.config.platform_instance,
+                self.config.env,
+            )
+
+            # We don't add to the profiler state if we only do table level profiling as it always happens
+            if self.state_handler:
+                self.state_handler.add_to_state(
+                    dataset_urn, int(datetime.now().timestamp() * 1000)
                 )
 
-                # We don't add to the profiler state if we only do table level profiling as it always happens
-                if self.state_handler:
-                    self.state_handler.add_to_state(
-                        dataset_urn, int(datetime.now().timestamp() * 1000)
-                    )
-
-                yield MetadataChangeProposalWrapper(
-                    entityUrn=dataset_urn, aspect=profile
-                ).as_workunit()
+            yield MetadataChangeProposalWrapper(
+                entityUrn=dataset_urn, aspect=profile
+            ).as_workunit()
 
     def get_snowflake_profile_request(
         self,
         table: SnowflakeTable,
         schema_name: str,
         db_name: str,
     ) -> Optional[SnowflakeProfilerRequest]:
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/snowflake/snowflake_query.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_query.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,11 +1,28 @@
 from typing import Optional
 
+from datahub.ingestion.source.snowflake.constants import SnowflakeObjectDomain
+
 
 class SnowflakeQuery:
+    ACCESS_HISTORY_TABLE_VIEW_DOMAINS_FILTER = (
+        "("
+        f"'{SnowflakeObjectDomain.TABLE.capitalize()}',"
+        f"'{SnowflakeObjectDomain.EXTERNAL_TABLE.capitalize()}',"
+        f"'{SnowflakeObjectDomain.VIEW.capitalize()}',"
+        f"'{SnowflakeObjectDomain.MATERIALIZED_VIEW.capitalize()}'"
+        ")"
+    )
+    ACCESS_HISTORY_TABLE_DOMAINS_FILTER = (
+        "("
+        f"'{SnowflakeObjectDomain.TABLE.capitalize()}',"
+        f"'{SnowflakeObjectDomain.VIEW.capitalize()}'"
+        ")"
+    )
+
     @staticmethod
     def current_account() -> str:
         return "select CURRENT_ACCOUNT()"
 
     @staticmethod
     def current_region() -> str:
         return "select CURRENT_REGION()"
@@ -116,43 +133,53 @@
         tag_name AS "TAG_NAME",
         tag_value AS "TAG_VALUE"
         FROM table("{db_name}".information_schema.tag_references('{quoted_identifier}', '{domain}'));
         """
 
     @staticmethod
     def get_all_tags_in_database_without_propagation(db_name: str) -> str:
+
+        allowed_object_domains = (
+            "("
+            f"'{SnowflakeObjectDomain.DATABASE.upper()}',"
+            f"'{SnowflakeObjectDomain.SCHEMA.upper()}',"
+            f"'{SnowflakeObjectDomain.TABLE.upper()}',"
+            f"'{SnowflakeObjectDomain.COLUMN.upper()}'"
+            ")"
+        )
+
         # https://docs.snowflake.com/en/sql-reference/account-usage/tag_references.html
         return f"""
         SELECT tag_database as "TAG_DATABASE",
         tag_schema AS "TAG_SCHEMA",
         tag_name AS "TAG_NAME",
         tag_value AS "TAG_VALUE",
         object_database as "OBJECT_DATABASE",
         object_schema AS "OBJECT_SCHEMA",
         object_name AS "OBJECT_NAME",
         column_name AS "COLUMN_NAME",
         domain as "DOMAIN"
         FROM snowflake.account_usage.tag_references
         WHERE (object_database = '{db_name}' OR object_name = '{db_name}')
-        AND domain in ('DATABASE', 'SCHEMA', 'TABLE', 'COLUMN')
+        AND domain in {allowed_object_domains}
         AND object_deleted IS NULL;
         """
 
     @staticmethod
     def get_tags_on_columns_with_propagation(
         db_name: str, quoted_table_identifier: str
     ) -> str:
         # https://docs.snowflake.com/en/sql-reference/functions/tag_references_all_columns.html
         return f"""
         SELECT tag_database as "TAG_DATABASE",
         tag_schema AS "TAG_SCHEMA",
         tag_name AS "TAG_NAME",
         tag_value AS "TAG_VALUE",
         column_name AS "COLUMN_NAME"
-        FROM table("{db_name}".information_schema.tag_references_all_columns('{quoted_table_identifier}', 'table'));
+        FROM table("{db_name}".information_schema.tag_references_all_columns('{quoted_table_identifier}', '{SnowflakeObjectDomain.TABLE}'));
         """
 
     # View definition is retrived in information_schema query only if role is owner of view. Hence this query is not used.
     # https://community.snowflake.com/s/article/Is-it-possible-to-see-the-view-definition-in-information-schema-views-from-a-non-owner-role
     @staticmethod
     def views_for_database(db_name: Optional[str]) -> str:
         db_clause = f'"{db_name}".' if db_name is not None else ""
@@ -396,14 +423,56 @@
             downstream_table_name {", downstream_table_columns" if include_column_lineage else ""}
             ORDER BY
               query_start_time DESC
           ) = 1
         """
 
     @staticmethod
+    def table_to_table_lineage_history_v2(
+        start_time_millis: int,
+        end_time_millis: int,
+        include_view_lineage: bool = True,
+        include_column_lineage: bool = True,
+    ) -> str:
+        if include_column_lineage:
+            return SnowflakeQuery.table_upstreams_with_column_lineage(
+                start_time_millis, end_time_millis, include_view_lineage
+            )
+        else:
+            return SnowflakeQuery.table_upstreams_only(
+                start_time_millis, end_time_millis, include_view_lineage
+            )
+
+    @staticmethod
+    def view_dependencies_v2() -> str:
+        return """
+        SELECT
+            ARRAY_UNIQUE_AGG(
+                OBJECT_CONSTRUCT(
+                    'upstream_object_name', concat(
+                                    referenced_database, '.', referenced_schema,
+                                    '.', referenced_object_name
+                                ),
+                    'upstream_object_domain', referenced_object_domain
+                )
+                ) as "UPSTREAM_TABLES",
+          concat(
+            referencing_database, '.', referencing_schema,
+            '.', referencing_object_name
+          ) AS "DOWNSTREAM_TABLE_NAME",
+          ANY_VALUE(referencing_object_domain) AS "DOWNSTREAM_TABLE_DOMAIN"
+        FROM
+          snowflake.account_usage.object_dependencies
+        WHERE
+          referencing_object_domain in ('VIEW', 'MATERIALIZED VIEW')
+        GROUP BY
+            DOWNSTREAM_TABLE_NAME
+        """
+
+    @staticmethod
     def show_external_tables() -> str:
         return "show external tables in account"
 
     @staticmethod
     def external_table_lineage_history(
         start_time_millis: int, end_time_millis: int
     ) -> str:
@@ -424,15 +493,15 @@
             AND t.query_start_time >= to_timestamp_ltz({start_time_millis}, 3)
             AND t.query_start_time < to_timestamp_ltz({end_time_millis}, 3))
         SELECT
         upstream_locations AS "UPSTREAM_LOCATIONS",
         downstream_table_name AS "DOWNSTREAM_TABLE_NAME",
         downstream_table_columns AS "DOWNSTREAM_TABLE_COLUMNS"
         FROM external_table_lineage_history
-        WHERE downstream_table_domain = 'Table'
+        WHERE downstream_table_domain = '{SnowflakeObjectDomain.TABLE.capitalize()}'
         QUALIFY ROW_NUMBER() OVER (PARTITION BY downstream_table_name ORDER BY query_start_time DESC) = 1"""
 
     @staticmethod
     def get_access_history_date_range() -> str:
         return """
             select
                 min(query_start_time) as "MIN_TIME",
@@ -451,14 +520,15 @@
     ) -> str:
         if not include_top_n_queries:
             top_n_queries = 0
         assert time_bucket_size == "DAY" or time_bucket_size == "HOUR"
         objects_column = (
             "BASE_OBJECTS_ACCESSED" if use_base_objects else "DIRECT_OBJECTS_ACCESSED"
         )
+
         return f"""
         WITH object_access_history AS
         (
             select
                 object.value : "objectName"::varchar AS object_name,
                 object.value : "objectDomain"::varchar AS object_domain,
                 object.value : "columns" AS object_columns,
@@ -581,20 +651,161 @@
                 on basic_usage_counts.bucket_start_time = field_usage_counts.bucket_start_time
                 and basic_usage_counts.object_name = field_usage_counts.object_name
             left join
                 user_usage_counts user_usage_counts
                 on basic_usage_counts.bucket_start_time = user_usage_counts.bucket_start_time
                 and basic_usage_counts.object_name = user_usage_counts.object_name
         where
-            basic_usage_counts.object_domain in
-            (
-                'Table',
-                'View',
-                'Materialized view',
-                'External table'
-            ) and basic_usage_counts.object_name is not null
+            basic_usage_counts.object_domain in {SnowflakeQuery.ACCESS_HISTORY_TABLE_VIEW_DOMAINS_FILTER}
+            and basic_usage_counts.object_name is not null
         group by
             basic_usage_counts.object_name,
             basic_usage_counts.bucket_start_time
         order by
             basic_usage_counts.bucket_start_time
         """
+
+    @staticmethod
+    def table_upstreams_with_column_lineage(
+        start_time_millis: int,
+        end_time_millis: int,
+        include_view_lineage: bool = True,
+    ) -> str:
+        allowed_upstream_table_domains = (
+            SnowflakeQuery.ACCESS_HISTORY_TABLE_VIEW_DOMAINS_FILTER
+            if include_view_lineage
+            else SnowflakeQuery.ACCESS_HISTORY_TABLE_DOMAINS_FILTER
+        )
+        return f"""
+        WITH column_lineage_history AS (
+            SELECT
+                r.value : "objectName" :: varchar AS upstream_table_name,
+                r.value : "objectDomain" :: varchar AS upstream_table_domain,
+                w.value : "objectName" :: varchar AS downstream_table_name,
+                w.value : "objectDomain" :: varchar AS downstream_table_domain,
+                wcols.value : "columnName" :: varchar AS downstream_column_name,
+                wcols_directSources.value : "objectName" as upstream_column_table_name,
+                wcols_directSources.value : "columnName" as upstream_column_name,
+                wcols_directSources.value : "objectDomain" as upstream_column_object_domain,
+                t.query_start_time AS query_start_time,
+                t.query_id AS query_id
+            FROM
+                (SELECT * from snowflake.account_usage.access_history) t,
+                lateral flatten(input => t.DIRECT_OBJECTS_ACCESSED) r,
+                lateral flatten(input => t.OBJECTS_MODIFIED) w,
+                lateral flatten(input => w.value : "columns", outer => true) wcols,
+                lateral flatten(input => wcols.value : "directSourceColumns", outer => true) wcols_directSources
+            WHERE
+                r.value : "objectId" IS NOT NULL
+                AND w.value : "objectId" IS NOT NULL
+                AND w.value : "objectName" NOT LIKE '%.GE_TMP_%'
+                AND w.value : "objectName" NOT LIKE '%.GE_TEMP_%'
+                AND t.query_start_time >= to_timestamp_ltz({start_time_millis}, 3)
+                AND t.query_start_time < to_timestamp_ltz({end_time_millis}, 3)
+                AND upstream_table_domain in {allowed_upstream_table_domains}
+                AND downstream_table_domain = '{SnowflakeObjectDomain.TABLE.capitalize()}'
+            ),
+        column_upstream_jobs AS (
+            SELECT
+                downstream_table_name,
+                downstream_column_name,
+                ANY_VALUE(query_start_time),
+                query_id,
+                ARRAY_AGG(
+                    OBJECT_CONSTRUCT(
+                        'object_name', upstream_column_table_name,
+                        'object_domain', upstream_column_object_domain,
+                        'column_name', upstream_column_name
+                    )
+                ) as upstream_columns_for_job
+            FROM
+                column_lineage_history
+            WHERE
+                upstream_column_name is not null
+                and upstream_column_table_name is not null
+            GROUP BY
+                downstream_table_name,
+                downstream_column_name,
+                query_id
+            ),
+        column_upstreams AS (
+            SELECT
+                downstream_table_name,
+                downstream_column_name,
+                ARRAY_UNIQUE_AGG(upstream_columns_for_job) as upstreams
+            FROM
+                column_upstream_jobs
+            GROUP BY
+                downstream_table_name,
+                downstream_column_name
+            )
+        SELECT
+            h.downstream_table_name AS "DOWNSTREAM_TABLE_NAME",
+            ANY_VALUE(h.downstream_table_domain) AS "DOWNSTREAM_TABLE_DOMAIN",
+            ARRAY_UNIQUE_AGG(
+                OBJECT_CONSTRUCT(
+                    'upstream_object_name', h.upstream_table_name,
+                    'upstream_object_domain', h.upstream_table_domain
+                )
+            ) AS "UPSTREAM_TABLES",
+            ARRAY_AGG(
+                OBJECT_CONSTRUCT(
+                'column_name', column_upstreams.downstream_column_name,
+                'upstreams', column_upstreams.upstreams
+                )
+            ) AS "UPSTREAM_COLUMNS"
+            FROM
+                column_lineage_history h
+            LEFT JOIN column_upstreams column_upstreams
+                on h.downstream_table_name = column_upstreams.downstream_table_name
+            GROUP BY
+                h.downstream_table_name
+        """
+
+    @staticmethod
+    def table_upstreams_only(
+        start_time_millis: int,
+        end_time_millis: int,
+        include_view_lineage: bool = True,
+    ) -> str:
+        allowed_upstream_table_domains = (
+            SnowflakeQuery.ACCESS_HISTORY_TABLE_VIEW_DOMAINS_FILTER
+            if include_view_lineage
+            else SnowflakeQuery.ACCESS_HISTORY_TABLE_DOMAINS_FILTER
+        )
+        return f"""
+            WITH table_lineage_history AS (
+                SELECT
+                    r.value:"objectName"::varchar AS upstream_table_name,
+                    r.value:"objectDomain"::varchar AS upstream_table_domain,
+                    r.value:"columns" AS upstream_table_columns,
+                    w.value:"objectName"::varchar AS downstream_table_name,
+                    w.value:"objectDomain"::varchar AS downstream_table_domain,
+                    w.value:"columns" AS downstream_table_columns,
+                    t.query_start_time AS query_start_time
+                FROM
+                    (SELECT * from snowflake.account_usage.access_history) t,
+                    lateral flatten(input => t.DIRECT_OBJECTS_ACCESSED) r,
+                    lateral flatten(input => t.OBJECTS_MODIFIED) w
+                WHERE r.value:"objectId" IS NOT NULL
+                AND w.value:"objectId" IS NOT NULL
+                AND w.value:"objectName" NOT LIKE '%.GE_TMP_%'
+                AND w.value:"objectName" NOT LIKE '%.GE_TEMP_%'
+                AND t.query_start_time >= to_timestamp_ltz({start_time_millis}, 3)
+                AND t.query_start_time < to_timestamp_ltz({end_time_millis}, 3)
+                AND upstream_table_domain in {allowed_upstream_table_domains}
+                AND downstream_table_domain = '{SnowflakeObjectDomain.TABLE.capitalize()}'
+                )
+            SELECT
+                downstream_table_name AS "DOWNSTREAM_TABLE_NAME",
+                ANY_VALUE(downstream_table_domain) as "DOWNSTREAM_TABLE_DOMAIN",
+                ARRAY_UNIQUE_AGG(
+                    OBJECT_CONSTRUCT(
+                        'upstream_object_name', upstream_table_name,
+                        'upstream_object_domain', upstream_table_domain
+                    )
+                ) as "UPSTREAM_TABLES"
+                FROM
+                    table_lineage_history
+                GROUP BY
+                    downstream_table_name
+            """
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/snowflake/snowflake_report.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_report.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,15 +1,17 @@
-from typing import MutableSet, Optional
+from dataclasses import dataclass, field
+from typing import Dict, MutableSet, Optional
 
 from datahub.ingestion.source.snowflake.constants import SnowflakeEdition
 from datahub.ingestion.source.sql.sql_generic_profiler import ProfilingSqlReport
 from datahub.ingestion.source_report.sql.snowflake import SnowflakeReport
 from datahub.ingestion.source_report.usage.snowflake_usage import SnowflakeUsageReport
 
 
+@dataclass
 class SnowflakeV2Report(SnowflakeReport, SnowflakeUsageReport, ProfilingSqlReport):
     account_locator: Optional[str] = None
     region: Optional[str] = None
 
     schemas_scanned: int = 0
     databases_scanned: int = 0
     tags_scanned: int = 0
@@ -21,33 +23,43 @@
 
     usage_aggregation_query_secs: float = -1
     table_lineage_query_secs: float = -1
     view_upstream_lineage_query_secs: float = -1
     view_downstream_lineage_query_secs: float = -1
     external_lineage_queries_secs: float = -1
 
+    # Reports how many times we reset in-memory `functools.lru_cache` caches of data,
+    # which occurs when we occur a different database / schema.
+    # Should not be more than the number of databases / schemas scanned.
+    # Maps (function name) -> (stat_name) -> (stat_value)
+    lru_cache_info: Dict[str, Dict[str, int]] = field(default_factory=dict)
+
     # These will be non-zero if snowflake information_schema queries fail with error -
     # "Information schema query returned too much data. Please repeat query with more selective predicates.""
     # This will result in overall increase in time complexity
     num_get_tables_for_schema_queries: int = 0
     num_get_views_for_schema_queries: int = 0
     num_get_columns_for_table_queries: int = 0
 
     # these will be non-zero if the user choses to enable the extract_tags = "with_lineage" option, which requires
     # individual queries per object (database, schema, table) and an extra query per table to get the tags on the columns.
     num_get_tags_for_object_queries: int = 0
     num_get_tags_on_columns_for_table_queries: int = 0
 
     rows_zero_objects_modified: int = 0
 
-    _processed_tags: MutableSet[str] = set()
-    _scanned_tags: MutableSet[str] = set()
+    _processed_tags: MutableSet[str] = field(default_factory=set)
+    _scanned_tags: MutableSet[str] = field(default_factory=set)
 
     edition: Optional[SnowflakeEdition] = None
 
+    num_tables_with_external_upstreams_only: int = 0
+    num_tables_with_upstreams: int = 0
+    num_views_with_upstreams: int = 0
+
     def report_entity_scanned(self, name: str, ent_type: str = "table") -> None:
         """
         Entity could be a view or a table or a schema or a database
         """
         if ent_type == "table":
             self.tables_scanned += 1
         elif ent_type == "view":
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/snowflake/snowflake_schema.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_schema.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 import logging
 from collections import defaultdict
 from dataclasses import dataclass, field
 from datetime import datetime
+from functools import lru_cache
 from typing import Dict, List, Optional
 
 import pandas as pd
 from snowflake.connector import SnowflakeConnection
 
 from datahub.ingestion.source.snowflake.constants import SnowflakeObjectDomain
 from datahub.ingestion.source.snowflake.snowflake_query import SnowflakeQuery
@@ -34,14 +35,17 @@
 @dataclass
 class SnowflakeTag:
     database: str
     schema: str
     name: str
     value: str
 
+    def display_name(self) -> str:
+        return f"{self.name}: {self.value}"
+
     def identifier(self) -> str:
         return f"{self._id_prefix_as_str()}:{self.value}"
 
     def _id_prefix_as_str(self) -> str:
         return f"{self.database}.{self.schema}.{self.name}"
 
 
@@ -91,16 +95,16 @@
 
 @dataclass
 class SnowflakeSchema:
     name: str
     created: Optional[datetime]
     last_altered: Optional[datetime]
     comment: Optional[str]
-    tables: List[SnowflakeTable] = field(default_factory=list)
-    views: List[SnowflakeView] = field(default_factory=list)
+    tables: List[str] = field(default_factory=list)
+    views: List[str] = field(default_factory=list)
     tags: Optional[List[SnowflakeTag]] = None
 
 
 @dataclass
 class SnowflakeDatabase:
     name: str
     created: Optional[datetime]
@@ -233,14 +237,15 @@
                 created=schema["CREATED"],
                 last_altered=schema["LAST_ALTERED"],
                 comment=schema["COMMENT"],
             )
             snowflake_schemas.append(snowflake_schema)
         return snowflake_schemas
 
+    @lru_cache(maxsize=1)
     def get_tables_for_database(
         self, db_name: str
     ) -> Optional[Dict[str, List[SnowflakeTable]]]:
         tables: Dict[str, List[SnowflakeTable]] = {}
         try:
             cur = self.query(
                 SnowflakeQuery.tables_for_database(db_name),
@@ -287,14 +292,15 @@
                     rows_count=table["ROW_COUNT"],
                     comment=table["COMMENT"],
                     clustering_key=table["CLUSTERING_KEY"],
                 )
             )
         return tables
 
+    @lru_cache(maxsize=1)
     def get_views_for_database(
         self, db_name: str
     ) -> Optional[Dict[str, List[SnowflakeView]]]:
         views: Dict[str, List[SnowflakeView]] = {}
         try:
             cur = self.query(SnowflakeQuery.show_views_for_database(db_name))
         except Exception as e:
@@ -334,14 +340,15 @@
                     comment=table["comment"],
                     view_definition=table["text"],
                     last_altered=table["created_on"],
                 )
             )
         return views
 
+    @lru_cache(maxsize=1)
     def get_columns_for_schema(
         self, schema_name: str, db_name: str
     ) -> Optional[Dict[str, List[SnowflakeColumn]]]:
         columns: Dict[str, List[SnowflakeColumn]] = {}
         try:
             cur = self.query(SnowflakeQuery.columns_for_schema(schema_name, db_name))
         except Exception as e:
@@ -389,14 +396,15 @@
                     character_maximum_length=column["CHARACTER_MAXIMUM_LENGTH"],
                     numeric_precision=column["NUMERIC_PRECISION"],
                     numeric_scale=column["NUMERIC_SCALE"],
                 )
             )
         return columns
 
+    @lru_cache(maxsize=1)
     def get_pk_constraints_for_schema(
         self, schema_name: str, db_name: str
     ) -> Dict[str, SnowflakePK]:
         constraints: Dict[str, SnowflakePK] = {}
         cur = self.query(
             SnowflakeQuery.show_primary_keys_for_schema(schema_name, db_name),
         )
@@ -405,14 +413,15 @@
             if row["table_name"] not in constraints:
                 constraints[row["table_name"]] = SnowflakePK(
                     name=row["constraint_name"], column_names=[]
                 )
             constraints[row["table_name"]].column_names.append(row["column_name"])
         return constraints
 
+    @lru_cache(maxsize=1)
     def get_fk_constraints_for_schema(
         self, schema_name: str, db_name: str
     ) -> Dict[str, List[SnowflakeFK]]:
         constraints: Dict[str, List[SnowflakeFK]] = {}
         fk_constraints_map: Dict[str, SnowflakeFK] = {}
 
         cur = self.query(
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/snowflake/snowflake_tag.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_tag.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/snowflake/snowflake_usage_v2.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_usage_v2.py`

 * *Files 3% similar despite different names*

```diff
@@ -19,25 +19,22 @@
 from datahub.ingestion.source.snowflake.snowflake_report import SnowflakeV2Report
 from datahub.ingestion.source.snowflake.snowflake_utils import (
     SnowflakeCommonMixin,
     SnowflakeConnectionMixin,
     SnowflakePermissionError,
     SnowflakeQueryMixin,
 )
+from datahub.ingestion.source.usage.usage_common import TOTAL_BUDGET_FOR_QUERY_LIST
 from datahub.metadata.com.linkedin.pegasus2avro.dataset import (
     DatasetFieldUsageCounts,
     DatasetUsageStatistics,
     DatasetUserUsageCounts,
 )
 from datahub.metadata.com.linkedin.pegasus2avro.timeseries import TimeWindowSize
-from datahub.metadata.schema_classes import (
-    ChangeTypeClass,
-    OperationClass,
-    OperationTypeClass,
-)
+from datahub.metadata.schema_classes import OperationClass, OperationTypeClass
 from datahub.utilities.perf_timer import PerfTimer
 from datahub.utilities.sql_formatter import format_sql_query, trim_query
 
 logger: logging.Logger = logging.getLogger(__name__)
 
 OPERATION_STATEMENT_TYPES = {
     "INSERT": OperationTypeClass.INSERT,
@@ -61,16 +58,17 @@
     objectDomain: Optional[str] = None
     objectId: Optional[int] = None
 
 
 class SnowflakeObjectAccessEntry(PermissiveModel):
     columns: Optional[List[SnowflakeColumnReference]]
     objectDomain: str
-    objectId: int
     objectName: str
+    # Seems like it should never be null, but in practice have seen null objectIds
+    objectId: Optional[int]
     stageKind: Optional[str]
 
 
 class SnowflakeJoinedAccessEvent(PermissiveModel):
     query_start_time: datetime
     query_text: str
     query_type: str
@@ -171,15 +169,15 @@
                 continue
 
             dataset_identifier = self.get_dataset_identifier_from_qualified_name(
                 row["OBJECT_NAME"]
             )
             if dataset_identifier not in discovered_datasets:
                 logger.debug(
-                    f"Skipping usage for table {dataset_identifier}, as table schema is not accessible"
+                    f"Skipping usage for table {dataset_identifier}, as table schema is not accessible or not allowed by recipe."
                 )
                 continue
 
             yield from self.build_usage_statistics_for_dataset(dataset_identifier, row)
 
     def build_usage_statistics_for_dataset(self, dataset_identifier, row):
         try:
@@ -214,17 +212,16 @@
                 exc_info=e,
             )
             self.report_warning(
                 "Failed to parse usage statistics for dataset", dataset_identifier
             )
 
     def _map_top_sql_queries(self, top_sql_queries: Dict) -> List[str]:
-        total_budget_for_query_list: int = 24000
         budget_per_query: int = int(
-            total_budget_for_query_list / self.config.top_n_queries
+            TOTAL_BUDGET_FOR_QUERY_LIST / self.config.top_n_queries
         )
         return sorted(
             [
                 trim_query(format_sql_query(query), budget_per_query)
                 if self.config.format_sql_queries
                 else trim_query(query, budget_per_query)
                 for query in top_sql_queries
@@ -364,25 +361,21 @@
                 operation_aspect = OperationClass(
                     timestampMillis=reported_time,
                     lastUpdatedTimestamp=last_updated_timestamp,
                     actor=user_urn,
                     operationType=operation_type,
                 )
                 mcp = MetadataChangeProposalWrapper(
-                    entityType="dataset",
-                    aspectName="operation",
-                    changeType=ChangeTypeClass.UPSERT,
                     entityUrn=dataset_urn,
                     aspect=operation_aspect,
                 )
                 wu = MetadataWorkUnit(
                     id=f"{start_time.isoformat()}-operation-aspect-{resource}",
                     mcp=mcp,
                 )
-                self.report.report_workunit(wu)
                 yield wu
 
     def _process_snowflake_history_row(
         self, row: Any
     ) -> Iterable[SnowflakeJoinedAccessEvent]:
         try:  # big hammer try block to ensure we don't fail on parsing events
             self.report.rows_processed += 1
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/snowflake/snowflake_utils.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_utils.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/snowflake/snowflake_v2.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/snowflake/snowflake_v2.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 import json
 import logging
 import os
 import os.path
 import platform
 from dataclasses import dataclass
-from typing import Dict, Iterable, List, Optional, Tuple, Union, cast
+from typing import Callable, Dict, Iterable, List, Optional, Union, cast
 
 import pandas as pd
 from snowflake.connector import SnowflakeConnection
 
 from datahub.configuration.pattern_utils import is_schema_allowed
 from datahub.emitter.mce_builder import (
     make_data_platform_urn,
@@ -32,25 +32,32 @@
     SourceCapability,
     SourceReport,
     TestableSource,
     TestConnectionReport,
 )
 from datahub.ingestion.api.workunit import MetadataWorkUnit
 from datahub.ingestion.glossary.classification_mixin import ClassificationMixin
+from datahub.ingestion.source.common.subtypes import (
+    DatasetContainerSubTypes,
+    DatasetSubTypes,
+)
 from datahub.ingestion.source.snowflake.constants import (
     GENERIC_PERMISSION_ERROR_KEY,
     SNOWFLAKE_DATABASE,
     SnowflakeEdition,
     SnowflakeObjectDomain,
 )
 from datahub.ingestion.source.snowflake.snowflake_config import (
     SnowflakeV2Config,
     TagOption,
 )
-from datahub.ingestion.source.snowflake.snowflake_lineage import (
+from datahub.ingestion.source.snowflake.snowflake_lineage_legacy import (
+    SnowflakeLineageExtractor as SnowflakeLineageLegacyExtractor,
+)
+from datahub.ingestion.source.snowflake.snowflake_lineage_v2 import (
     SnowflakeLineageExtractor,
 )
 from datahub.ingestion.source.snowflake.snowflake_profiler import SnowflakeProfiler
 from datahub.ingestion.source.snowflake.snowflake_report import SnowflakeV2Report
 from datahub.ingestion.source.snowflake.snowflake_schema import (
     SnowflakeColumn,
     SnowflakeDatabase,
@@ -69,15 +76,14 @@
 )
 from datahub.ingestion.source.snowflake.snowflake_utils import (
     SnowflakeCommonMixin,
     SnowflakeConnectionMixin,
     SnowflakePermissionError,
     SnowflakeQueryMixin,
 )
-from datahub.ingestion.source.sql.sql_common import SqlContainerSubTypes
 from datahub.ingestion.source.sql.sql_utils import (
     add_table_to_schema_container,
     gen_database_container,
     gen_database_key,
     gen_schema_container,
     gen_schema_key,
     get_dataplatform_instance_aspect,
@@ -124,14 +130,15 @@
     TimeType,
 )
 from datahub.metadata.com.linkedin.pegasus2avro.tag import TagProperties
 from datahub.utilities.registries.domain_registry import DomainRegistry
 from datahub.utilities.source_helpers import (
     auto_stale_entity_removal,
     auto_status_aspect,
+    auto_workunit_reporter,
 )
 from datahub.utilities.time import datetime_to_ts_millis
 
 logger: logging.Logger = logging.getLogger(__name__)
 
 # https://docs.snowflake.com/en/sql-reference/intro-summary-data-types.html
 SNOWFLAKE_FIELD_TYPE_MAPPINGS = {
@@ -243,17 +250,25 @@
             self.domain_registry = DomainRegistry(
                 cached_domains=[k for k in self.config.domain], graph=self.ctx.graph
             )
 
         # For database, schema, tables, views, etc
         self.data_dictionary = SnowflakeDataDictionary()
 
+        self.lineage_extractor: Union[
+            SnowflakeLineageExtractor, SnowflakeLineageLegacyExtractor
+        ]
         if config.include_table_lineage:
             # For lineage
-            self.lineage_extractor = SnowflakeLineageExtractor(config, self.report)
+            if self.config.use_legacy_lineage_method:
+                self.lineage_extractor = SnowflakeLineageLegacyExtractor(
+                    config, self.report
+                )
+            else:
+                self.lineage_extractor = SnowflakeLineageExtractor(config, self.report)
 
         if config.include_usage_stats or config.include_operational_stats:
             # For usage stats
             self.usage_extractor = SnowflakeUsageExtractor(config, self.report)
 
         self.tag_extractor = SnowflakeTagExtractor(
             config, self.data_dictionary, self.report
@@ -273,28 +288,16 @@
             self.profiler = SnowflakeProfiler(
                 config, self.report, self.profiling_state_handler
             )
 
         if self.is_classification_enabled():
             self.classifiers = self.get_classifiers()
 
-        # Currently caching using instance variables
-        # TODO - rewrite cache for readability or use out of the box solution
-        self.db_tables: Dict[str, Optional[Dict[str, List[SnowflakeTable]]]] = {}
-        self.db_views: Dict[str, Optional[Dict[str, List[SnowflakeView]]]] = {}
-
-        # For column related queries and constraints, we currently query at schema level
-        # TODO: In future, we may consider using queries and caching at database level first
-        self.schema_columns: Dict[
-            Tuple[str, str], Optional[Dict[str, List[SnowflakeColumn]]]
-        ] = {}
-        self.schema_pk_constraints: Dict[Tuple[str, str], Dict[str, SnowflakePK]] = {}
-        self.schema_fk_constraints: Dict[
-            Tuple[str, str], Dict[str, List[SnowflakeFK]]
-        ] = {}
+        # Caches tables for a single database. Consider moving to disk or S3 when possible.
+        self.db_tables: Dict[str, List[SnowflakeTable]] = {}
 
     @classmethod
     def create(cls, config_dict: dict, ctx: PipelineContext) -> "Source":
         config = SnowflakeV2Config.parse_obj(config_dict)
         return cls(ctx, config)
 
     @staticmethod
@@ -319,16 +322,16 @@
             if test_report.basic_connectivity is None:
                 test_report.basic_connectivity = CapabilityReport(
                     capable=False, failure_reason=f"{e}"
                 )
             else:
                 test_report.internal_failure = True
                 test_report.internal_failure_reason = f"{e}"
-        finally:
-            return test_report
+
+        return test_report
 
     @staticmethod
     def check_capabilities(
         conn: SnowflakeConnection, connection_conf: SnowflakeV2Config
     ) -> Dict[Union[SourceCapability, str], CapabilityReport]:
         # Currently only overall capabilities are reported.
         # Resource level variations in capabilities are not considered.
@@ -500,37 +503,52 @@
             return
 
         for snowflake_db in databases:
             try:
                 yield from self._process_database(snowflake_db)
 
             except SnowflakePermissionError as e:
-                # FIXME - This may break satetful ingestion if new tables than previous run are emitted above
+                # FIXME - This may break stateful ingestion if new tables than previous run are emitted above
                 # and stateful ingestion is enabled
                 self.report_error(GENERIC_PERMISSION_ERROR_KEY, str(e))
                 return
 
         self.connection.close()
 
-        # TODO: The checkpoint state for stale entity detection can be comitted here.
+        lru_cache_functions: List[Callable] = [
+            self.data_dictionary.get_tables_for_database,
+            self.data_dictionary.get_views_for_database,
+            self.data_dictionary.get_columns_for_schema,
+            self.data_dictionary.get_pk_constraints_for_schema,
+            self.data_dictionary.get_fk_constraints_for_schema,
+        ]
+        for func in lru_cache_functions:
+            self.report.lru_cache_info[func.__name__] = func.cache_info()._asdict()  # type: ignore
 
-        if self.config.profiling.enabled and len(databases) != 0:
-            yield from self.profiler.get_workunits(databases)
+        # TODO: The checkpoint state for stale entity detection can be committed here.
 
         discovered_tables: List[str] = [
-            self.get_dataset_identifier(table.name, schema.name, db.name)
+            self.get_dataset_identifier(table_name, schema.name, db.name)
             for db in databases
             for schema in db.schemas
-            for table in schema.tables
+            for table_name in schema.tables
+            if self._is_dataset_pattern_allowed(
+                self.get_dataset_identifier(table_name, schema.name, db.name),
+                SnowflakeObjectDomain.TABLE,
+            )
         ]
         discovered_views: List[str] = [
-            self.get_dataset_identifier(table.name, schema.name, db.name)
+            self.get_dataset_identifier(table_name, schema.name, db.name)
             for db in databases
             for schema in db.schemas
-            for table in schema.views
+            for table_name in schema.views
+            if self._is_dataset_pattern_allowed(
+                self.get_dataset_identifier(table_name, schema.name, db.name),
+                SnowflakeObjectDomain.VIEW,
+            )
         ]
 
         if len(discovered_tables) == 0 and len(discovered_views) == 0:
             self.report_error(
                 GENERIC_PERMISSION_ERROR_KEY,
                 "No tables/views found. Please check permissions.",
             )
@@ -565,15 +583,18 @@
                 )
 
             yield from self.usage_extractor.get_workunits(discovered_datasets)
 
     def get_workunits(self) -> Iterable[MetadataWorkUnit]:
         return auto_stale_entity_removal(
             self.stale_entity_removal_handler,
-            auto_status_aspect(self.get_workunits_internal()),
+            auto_workunit_reporter(
+                self.report,
+                auto_status_aspect(self.get_workunits_internal()),
+            ),
         )
 
     def report_warehouse_failure(self):
         if self.config.warehouse is not None:
             self.report_error(
                 GENERIC_PERMISSION_ERROR_KEY,
                 f"Current role does not have permissions to use warehouse {self.config.warehouse}. Please update permissions.",
@@ -672,17 +693,21 @@
 
         self.fetch_schemas_for_database(snowflake_db, db_name)
 
         if self.config.include_technical_schema and snowflake_db.tags:
             for tag in snowflake_db.tags:
                 yield from self._process_tag(tag)
 
+        self.db_tables = {}
         for snowflake_schema in snowflake_db.schemas:
             yield from self._process_schema(snowflake_schema, db_name)
 
+        if self.config.profiling.enabled and self.db_tables:
+            yield from self.profiler.get_workunits(snowflake_db, self.db_tables)
+
     def fetch_schemas_for_database(self, snowflake_db, db_name):
         try:
             snowflake_db.schemas = self.data_dictionary.get_schemas_for_database(
                 db_name
             )
         except Exception as e:
             if isinstance(e, SnowflakePermissionError):
@@ -725,41 +750,45 @@
                 schema_name=schema_name, db_name=db_name, domain="schema"
             )
 
         if self.config.include_technical_schema:
             yield from self.gen_schema_containers(snowflake_schema, db_name)
 
         if self.config.include_tables:
-            self.fetch_tables_for_schema(snowflake_schema, db_name, schema_name)
+            tables = self.fetch_tables_for_schema(
+                snowflake_schema, db_name, schema_name
+            )
+            self.db_tables[schema_name] = tables
 
             if self.config.include_technical_schema:
-                for table in snowflake_schema.tables:
+                for table in tables:
                     yield from self._process_table(table, schema_name, db_name)
 
         if self.config.include_views:
-            self.fetch_views_for_schema(snowflake_schema, db_name, schema_name)
+            views = self.fetch_views_for_schema(snowflake_schema, db_name, schema_name)
 
             if self.config.include_technical_schema:
-                for view in snowflake_schema.views:
+                for view in views:
                     yield from self._process_view(view, schema_name, db_name)
 
         if self.config.include_technical_schema and snowflake_schema.tags:
             for tag in snowflake_schema.tags:
                 yield from self._process_tag(tag)
 
         if not snowflake_schema.views and not snowflake_schema.tables:
             self.report_warning(
                 "No tables/views found in schema. If tables exist, please grant REFERENCES or SELECT permissions on them.",
                 f"{db_name}.{schema_name}",
             )
 
     def fetch_views_for_schema(self, snowflake_schema, db_name, schema_name):
         try:
-            snowflake_schema.views = self.get_views_for_schema(schema_name, db_name)
-
+            views = self.get_views_for_schema(schema_name, db_name)
+            snowflake_schema.views = [view.name for view in views]
+            return views
         except Exception as e:
             if isinstance(e, SnowflakePermissionError):
                 # Ideal implementation would use PEP 678  Enriching Exceptions with Notes
                 error_msg = f"Failed to get views for schema {db_name}.{schema_name}. Please check permissions."
 
                 raise SnowflakePermissionError(error_msg) from e.__cause__
             else:
@@ -770,15 +799,17 @@
                 self.report_warning(
                     "Failed to get views for schema",
                     f"{db_name}.{schema_name}",
                 )
 
     def fetch_tables_for_schema(self, snowflake_schema, db_name, schema_name):
         try:
-            snowflake_schema.tables = self.get_tables_for_schema(schema_name, db_name)
+            tables = self.get_tables_for_schema(schema_name, db_name)
+            snowflake_schema.tables = [table.name for table in tables]
+            return tables
         except Exception as e:
             if isinstance(e, SnowflakePermissionError):
                 # Ideal implementation would use PEP 678  Enriching Exceptions with Notes
                 error_msg = f"Failed to get tables for schema {db_name}.{schema_name}. Please check permissions."
                 raise SnowflakePermissionError(error_msg) from e.__cause__
             else:
                 logger.debug(
@@ -883,14 +914,15 @@
                 exc_info=e,
             )
             self.report_warning("Failed to get primary key for table", table_identifier)
 
     def fetch_columns_for_table(self, table, schema_name, db_name, table_identifier):
         try:
             table.columns = self.get_columns_for_table(table.name, schema_name, db_name)
+            table.column_count = len(table.columns)
             if self.config.extract_tags != TagOption.skip:
                 table.column_tags = self.tag_extractor.get_column_tags_for_table(
                     table.name, schema_name, db_name
                 )
         except Exception as e:
             logger.debug(
                 f"Failed to get columns for table {table_identifier} due to error {e}",
@@ -992,44 +1024,47 @@
             platform_instance=self.config.platform_instance,
             env=self.config.env,
         )
 
         yield from add_table_to_schema_container(
             dataset_urn=dataset_urn,
             parent_container_key=schema_container_key,
-            report=self.report,
         )
         dpi_aspect = get_dataplatform_instance_aspect(
             dataset_urn=dataset_urn,
             platform=self.platform,
             platform_instance=self.config.platform_instance,
         )
         if dpi_aspect:
             yield dpi_aspect
 
         subTypes = SubTypes(
-            typeNames=["view"] if isinstance(table, SnowflakeView) else ["table"]
+            typeNames=[DatasetSubTypes.VIEW]
+            if isinstance(table, SnowflakeView)
+            else [DatasetSubTypes.TABLE]
         )
 
         yield MetadataChangeProposalWrapper(
             entityUrn=dataset_urn, aspect=subTypes
         ).as_workunit()
 
         if self.domain_registry:
             yield from get_domain_wu(
                 dataset_name=dataset_name,
                 entity_urn=dataset_urn,
                 domain_config=self.config.domain,
                 domain_registry=self.domain_registry,
-                report=self.report,
             )
 
         if table.tags:
             tag_associations = [
-                TagAssociation(tag=make_tag_urn(tag.identifier())) for tag in table.tags
+                TagAssociation(
+                    tag=make_tag_urn(self.snowflake_identifier(tag.identifier()))
+                )
+                for tag in table.tags
             ]
             global_tags = GlobalTags(tag_associations)
             yield MetadataChangeProposalWrapper(
                 entityUrn=dataset_urn, aspect=global_tags
             ).as_workunit()
 
         if (
@@ -1070,19 +1105,18 @@
                 else SnowflakeObjectDomain.VIEW,
             )
             if self.config.include_external_url
             else None,
         )
 
     def gen_tag_workunits(self, tag: SnowflakeTag) -> Iterable[MetadataWorkUnit]:
-        tag_key = tag.identifier()
-        tag_urn = make_tag_urn(self.snowflake_identifier(tag_key))
+        tag_urn = make_tag_urn(self.snowflake_identifier(tag.identifier()))
 
         tag_properties_aspect = TagProperties(
-            name=tag_key,
+            name=tag.display_name(),
             description=f"Represents the Snowflake tag `{tag._id_prefix_as_str()}` with value `{tag.value}`.",
         )
 
         yield MetadataChangeProposalWrapper(
             entityUrn=tag_urn, aspect=tag_properties_aspect
         ).as_workunit()
 
@@ -1212,18 +1246,17 @@
             env=self.config.env,
         )
 
         yield from gen_database_container(
             name=database.name,
             database=self.snowflake_identifier(database.name),
             database_container_key=database_container_key,
-            sub_types=[SqlContainerSubTypes.DATABASE],
+            sub_types=[DatasetContainerSubTypes.DATABASE],
             domain_registry=self.domain_registry,
             domain_config=self.config.domain,
-            report=self.report,
             external_url=self.get_external_url_for_database(database.name)
             if self.config.include_external_url
             else None,
             description=database.comment,
             created=int(database.created.timestamp() * 1000)
             if database.created is not None
             else None,
@@ -1259,16 +1292,15 @@
         yield from gen_schema_container(
             name=schema.name,
             schema=self.snowflake_identifier(schema.name),
             database=self.snowflake_identifier(db_name),
             database_container_key=database_container_key,
             domain_config=self.config.domain,
             schema_container_key=schema_container_key,
-            sub_types=[SqlContainerSubTypes.SCHEMA],
-            report=self.report,
+            sub_types=[DatasetContainerSubTypes.SCHEMA],
             domain_registry=self.domain_registry,
             description=schema.comment,
             external_url=self.get_external_url_for_schema(schema.name, db_name)
             if self.config.include_external_url
             else None,
             created=int(schema.created.timestamp() * 1000)
             if schema.created is not None
@@ -1282,55 +1314,43 @@
             if schema.tags
             else None,
         )
 
     def get_tables_for_schema(
         self, schema_name: str, db_name: str
     ) -> List[SnowflakeTable]:
-        if db_name not in self.db_tables.keys():
-            tables = self.data_dictionary.get_tables_for_database(db_name)
-            self.db_tables[db_name] = tables
-        else:
-            tables = self.db_tables[db_name]
+        tables = self.data_dictionary.get_tables_for_database(db_name)
 
         # get all tables for database failed,
         # falling back to get tables for schema
         if tables is None:
             self.report.num_get_tables_for_schema_queries += 1
             return self.data_dictionary.get_tables_for_schema(schema_name, db_name)
 
         # Some schema may not have any table
         return tables.get(schema_name, [])
 
     def get_views_for_schema(
         self, schema_name: str, db_name: str
     ) -> List[SnowflakeView]:
-        if db_name not in self.db_views.keys():
-            views = self.data_dictionary.get_views_for_database(db_name)
-            self.db_views[db_name] = views
-        else:
-            views = self.db_views[db_name]
+        views = self.data_dictionary.get_views_for_database(db_name)
 
         # get all views for database failed,
         # falling back to get views for schema
         if views is None:
             self.report.num_get_views_for_schema_queries += 1
             return self.data_dictionary.get_views_for_schema(schema_name, db_name)
 
         # Some schema may not have any table
         return views.get(schema_name, [])
 
     def get_columns_for_table(
         self, table_name: str, schema_name: str, db_name: str
     ) -> List[SnowflakeColumn]:
-        if (db_name, schema_name) not in self.schema_columns.keys():
-            columns = self.data_dictionary.get_columns_for_schema(schema_name, db_name)
-            self.schema_columns[(db_name, schema_name)] = columns
-        else:
-            columns = self.schema_columns[(db_name, schema_name)]
+        columns = self.data_dictionary.get_columns_for_schema(schema_name, db_name)
 
         # get all columns for schema failed,
         # falling back to get columns for table
         if columns is None:
             self.report.num_get_columns_for_table_queries += 1
             return self.data_dictionary.get_columns_for_table(
                 table_name, schema_name, db_name
@@ -1338,47 +1358,38 @@
 
         # Access to table but none of its columns - is this possible ?
         return columns.get(table_name, [])
 
     def get_pk_constraints_for_table(
         self, table_name: str, schema_name: str, db_name: str
     ) -> Optional[SnowflakePK]:
-        if (db_name, schema_name) not in self.schema_pk_constraints.keys():
-            constraints = self.data_dictionary.get_pk_constraints_for_schema(
-                schema_name, db_name
-            )
-            self.schema_pk_constraints[(db_name, schema_name)] = constraints
-        else:
-            constraints = self.schema_pk_constraints[(db_name, schema_name)]
+        constraints = self.data_dictionary.get_pk_constraints_for_schema(
+            schema_name, db_name
+        )
 
         # Access to table but none of its constraints - is this possible ?
         return constraints.get(table_name)
 
     def get_fk_constraints_for_table(
         self, table_name: str, schema_name: str, db_name: str
     ) -> List[SnowflakeFK]:
-        if (db_name, schema_name) not in self.schema_fk_constraints.keys():
-            constraints = self.data_dictionary.get_fk_constraints_for_schema(
-                schema_name, db_name
-            )
-            self.schema_fk_constraints[(db_name, schema_name)] = constraints
-        else:
-            constraints = self.schema_fk_constraints[(db_name, schema_name)]
+        constraints = self.data_dictionary.get_fk_constraints_for_schema(
+            schema_name, db_name
+        )
 
         # Access to table but none of its constraints - is this possible ?
         return constraints.get(table_name, [])
 
     def add_config_to_report(self):
         self.report.cleaned_account_id = self.config.get_account()
         self.report.ignore_start_time_lineage = self.config.ignore_start_time_lineage
         self.report.upstream_lineage_in_report = self.config.upstream_lineage_in_report
         if not self.report.ignore_start_time_lineage:
             self.report.lineage_start_time = self.config.start_time
         self.report.lineage_end_time = self.config.end_time
-        self.report.check_role_grants = self.config.check_role_grants
         self.report.include_technical_schema = self.config.include_technical_schema
         self.report.include_usage_stats = self.config.include_usage_stats
         self.report.include_operational_stats = self.config.include_operational_stats
         self.report.include_column_lineage = self.config.include_column_lineage
         if self.report.include_usage_stats or self.config.include_operational_stats:
             self.report.window_start_time = self.config.start_time
             self.report.window_end_time = self.config.end_time
@@ -1408,18 +1419,14 @@
             if self.is_standard_edition():
                 self.report.edition = SnowflakeEdition.STANDARD
             else:
                 self.report.edition = SnowflakeEdition.ENTERPRISE
         except Exception:
             self.report.edition = None
 
-    # Stateful Ingestion Overrides.
-    def get_platform_instance_id(self) -> str:
-        return self.config.get_account()
-
     # Ideally we do not want null values in sample data for a column.
     # However that would require separate query per column and
     # that would be expensive, hence not done.
     def get_sample_values_for_table(self, table_name, schema_name, db_name):
         # Create a cursor object.
         cur = self.get_connection().cursor()
         NUM_SAMPLED_ROWS = 1000
@@ -1549,11 +1556,12 @@
             if os.path.exists(file_path):
                 os.remove(file_path)
         except Exception:
             logger.debug(f'Failed to remove OCSP cache file at "{file_path}"')
 
     def close(self) -> None:
         super().close()
+        StatefulIngestionSourceBase.close(self)
         if hasattr(self, "lineage_extractor"):
             self.lineage_extractor.close()
         if hasattr(self, "usage_extractor"):
             self.usage_extractor.close()
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/source_registry.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/source_registry.py`

 * *Files 16% similar despite different names*

```diff
@@ -3,32 +3,41 @@
 from datahub.configuration.common import ConfigurationWarning
 from datahub.ingestion.api.registry import PluginRegistry
 from datahub.ingestion.api.source import Source
 
 source_registry = PluginRegistry[Source]()
 source_registry.register_from_entrypoint("datahub.ingestion.source.plugins")
 
-# This source is always enabled
-assert source_registry.get("file")
-
 # Deprecations.
 source_registry.register_alias(
     "snowflake-beta",
     "snowflake",
     lambda: warnings.warn(
         "source type snowflake-beta is deprecated, use snowflake instead",
         ConfigurationWarning,
+        stacklevel=3,
     ),
 )
 source_registry.register_alias(
     "bigquery-beta",
     "bigquery",
     lambda: warnings.warn(
         "source type bigquery-beta is deprecated, use bigquery instead",
         ConfigurationWarning,
+        stacklevel=3,
+    ),
+)
+
+source_registry.register_alias(
+    "redshift-usage",
+    "redshift-usage-legacy",
+    lambda: warnings.warn(
+        "source type redshift-usage is deprecated, use redshift source instead as usage was merged into the main source",
+        ConfigurationWarning,
+        stacklevel=3,
     ),
 )
 
 # The MSSQL source has two possible sets of dependencies. We alias
 # the second to the first so that we maintain the 1:1 mapping between
 # source type and pip extra.
 source_registry.register_alias(
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/athena.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/athena.py`

 * *Files 2% similar despite different names*

```diff
@@ -16,18 +16,16 @@
     capability,
     config_class,
     platform_name,
     support_status,
 )
 from datahub.ingestion.api.workunit import MetadataWorkUnit
 from datahub.ingestion.source.aws.s3_util import make_s3_urn
-from datahub.ingestion.source.sql.sql_common import (
-    SQLAlchemySource,
-    SqlContainerSubTypes,
-)
+from datahub.ingestion.source.common.subtypes import DatasetContainerSubTypes
+from datahub.ingestion.source.sql.sql_common import SQLAlchemySource
 from datahub.ingestion.source.sql.sql_config import (
     SQLAlchemyConfig,
     make_sqlalchemy_uri,
 )
 from datahub.ingestion.source.sql.sql_utils import (
     add_table_to_schema_container,
     gen_database_container,
@@ -203,40 +201,44 @@
             platform_instance=self.config.platform_instance,
             env=self.config.env,
         )
 
         yield from gen_database_container(
             database=database,
             database_container_key=database_container_key,
-            sub_types=[SqlContainerSubTypes.DATABASE],
+            sub_types=[DatasetContainerSubTypes.DATABASE],
             domain_registry=self.domain_registry,
             domain_config=self.config.domain,
             report=self.report,
             extra_properties=extra_properties,
         )
 
     def get_database_container_key(self, db_name: str, schema: str) -> PlatformKey:
         # Because our overridden get_allowed_schemas method returns db_name as the schema name,
         # the db_name and schema here will be the same. Hence, we just ignore the schema parameter.
-        assert db_name == schema
+        # Based on community feedback, db_name only available if it is explicitly specified in the connection string.
+        # If it is not available then we should use schema as db_name
+
+        if not db_name:
+            db_name = schema
+
         return gen_database_key(
             db_name,
             platform=self.platform,
             platform_instance=self.config.platform_instance,
             env=self.config.env,
         )
 
     def add_table_to_schema_container(
         self,
         dataset_urn: str,
         db_name: str,
         schema: str,
         schema_container_key: Optional[PlatformKey] = None,
     ) -> Iterable[MetadataWorkUnit]:
-
         yield from add_table_to_schema_container(
             dataset_urn=dataset_urn,
             parent_container_key=self.get_database_container_key(db_name, schema),
             report=self.report,
         )
 
     # It seems like database/schema filter in the connection string does not work and this to work around that
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/clickhouse.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/clickhouse.py`

 * *Files 1% similar despite different names*

```diff
@@ -44,15 +44,14 @@
     ArrayTypeClass,
     MapTypeClass,
     NumberTypeClass,
     StringTypeClass,
     UnionTypeClass,
 )
 from datahub.metadata.schema_classes import (
-    ChangeTypeClass,
     DatasetLineageTypeClass,
     DatasetPropertiesClass,
     DatasetSnapshotClass,
     UpstreamClass,
 )
 
 # adding extra types not handled by clickhouse-sqlalchemy 0.1.8
@@ -119,15 +118,15 @@
 
 
 class ClickHouseConfig(
     BasicSQLAlchemyConfig, BaseTimeWindowConfig, DatasetLineageProviderConfigBase
 ):
     # defaults
     host_port = Field(default="localhost:8123", description="ClickHouse host URL.")
-    scheme = Field(default="clickhouse", description="", hidden_from_schema=True)
+    scheme = Field(default="clickhouse", description="", hidden_from_docs=True)
     password: pydantic.SecretStr = Field(
         default=pydantic.SecretStr(""), description="password"
     )
 
     secure: Optional[bool] = Field(default=None, description="")
     protocol: Optional[str] = Field(default=None, description="")
 
@@ -647,15 +646,12 @@
         if custom_properties:
             properties = DatasetPropertiesClass(customProperties=custom_properties)
 
         if not upstream_lineage:
             return None, properties
 
         mcp = MetadataChangeProposalWrapper(
-            entityType="dataset",
-            changeType=ChangeTypeClass.UPSERT,
             entityUrn=dataset_urn,
-            aspectName="upstreamLineage",
             aspect=UpstreamLineage(upstreams=upstream_lineage),
         )
 
         return mcp, properties
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/druid.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/druid.py`

 * *Files 3% similar despite different names*

```diff
@@ -40,15 +40,15 @@
             if self.platform_instance
             else f"{table}"
         )
 
 
 @platform_name("Druid")
 @config_class(DruidConfig)
-@support_status(SupportStatus.CERTIFIED)
+@support_status(SupportStatus.INCUBATING)
 @capability(SourceCapability.PLATFORM_INSTANCE, "Enabled by default")
 class DruidSource(SQLAlchemySource):
     """
     This plugin extracts the following:
     - Metadata for databases, schemas, and tables
     - Column types associated with each table
     - Table, row, and column statistics via optional SQL profiling.
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/hana.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/hana.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/hive.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/hive.py`

 * *Files 4% similar despite different names*

```diff
@@ -88,22 +88,22 @@
     pass
 except Exception as e:
     logger.warning(f"Failed to patch method due to {e}")
 
 
 class HiveConfig(BasicSQLAlchemyConfig):
     # defaults
-    scheme = Field(default="hive", hidden_from_schema=True)
+    scheme = Field(default="hive", hidden_from_docs=True)
 
     # Hive SQLAlchemy connector returns views as tables.
     # See https://github.com/dropbox/PyHive/blob/b21c507a24ed2f2b0cf15b0b6abb1c43f31d3ee0/pyhive/sqlalchemy_hive.py#L270-L273.
     # Disabling views helps us prevent this duplication.
     include_views = Field(
         default=False,
-        hidden_from_schema=True,
+        hidden_from_docs=True,
         description="Hive SQLAlchemy connector returns views as tables. See https://github.com/dropbox/PyHive/blob/b21c507a24ed2f2b0cf15b0b6abb1c43f31d3ee0/pyhive/sqlalchemy_hive.py#L270-L273. Disabling views helps us prevent this duplication.",
     )
 
     @validator("host_port")
     def clean_host_port(cls, v):
         return config_clean.remove_protocol(v)
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/mariadb.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/mariadb.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/mssql.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/mssql.py`

 * *Files 1% similar despite different names*

```diff
@@ -37,15 +37,15 @@
 register_custom_type(sqlalchemy.dialects.mssql.BIT, BooleanTypeClass)
 register_custom_type(sqlalchemy.dialects.mssql.SQL_VARIANT, UnionTypeClass)
 
 
 class SQLServerConfig(BasicSQLAlchemyConfig):
     # defaults
     host_port: str = Field(default="localhost:1433", description="MSSQL host URL.")
-    scheme: str = Field(default="mssql+pytds", description="", hidden_from_schema=True)
+    scheme: str = Field(default="mssql+pytds", description="", hidden_from_docs=True)
     use_odbc: bool = Field(
         default=False,
         description="See https://docs.sqlalchemy.org/en/14/dialects/mssql.html#module-sqlalchemy.dialects.mssql.pyodbc.",
     )
     uri_args: Dict[str, str] = Field(
         default={},
         description="Arguments to URL-encode when connecting. See https://docs.microsoft.com/en-us/sql/connect/odbc/dsn-connection-string-attribute?view=sql-server-ver15.",
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/mysql.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/mysql.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/oauth_generator.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/oauth_generator.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/oracle.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/oracle.py`

 * *Files 8% similar despite different names*

```diff
@@ -61,14 +61,18 @@
 
     service_name: Optional[str] = Field(
         default=None, description="Oracle service name. If using, omit `database`."
     )
     database: Optional[str] = Field(
         default=None, description="If using, omit `service_name`."
     )
+    add_database_name_to_urn: Optional[bool] = Field(
+        default=False,
+        description="Add oracle database name to urn, default urn is schema.table",
+    )
 
     @pydantic.validator("service_name")
     def check_service_name(cls, v, values):
         if values.get("database") and v:
             raise ValueError(
                 "specify one of 'database' and 'service_name', but not both"
             )
@@ -77,14 +81,25 @@
     def get_sql_alchemy_url(self):
         url = super().get_sql_alchemy_url()
         if self.service_name:
             assert not self.database
             url = f"{url}/?service_name={self.service_name}"
         return url
 
+    def get_identifier(self, schema: str, table: str) -> str:
+        regular = f"{schema}.{table}"
+        if self.add_database_name_to_urn:
+            if self.database_alias:
+                return f"{self.database_alias}.{regular}"
+            if self.database:
+                return f"{self.database}.{regular}"
+            return regular
+        else:
+            return regular
+
 
 class OracleInspectorObjectWrapper:
     """
     Inspector class wrapper, which queries DBA_TABLES instead of ALL_TABLES
     """
 
     def __init__(self, inspector_instance: Inspector):
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/postgres.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/postgres.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,23 +1,25 @@
+import logging
 from collections import defaultdict
-from typing import Dict, Iterable, List, Tuple, Union
+from typing import Any, Dict, Iterable, List, Optional, Tuple, Union
 
 # This import verifies that the dependencies are available.
 import psycopg2  # noqa: F401
 import sqlalchemy.dialects.postgresql as custom_types
 
 # GeoAlchemy adds support for PostGIS extensions in SQLAlchemy. In order to
 # activate it, we must import it so that it can hook into SQLAlchemy. While
 # we don't use the Geometry type that we import, we do care about the side
 # effects of the import. For more details, see here:
 # https://geoalchemy-2.readthedocs.io/en/latest/core_tutorial.html#reflecting-tables.
 from geoalchemy2 import Geometry  # noqa: F401
 from pydantic import BaseModel
 from pydantic.fields import Field
-from sqlalchemy import create_engine
+from sqlalchemy import create_engine, inspect
+from sqlalchemy.engine.reflection import Inspector
 
 from datahub.configuration.common import AllowDenyPattern
 from datahub.emitter import mce_builder
 from datahub.emitter.mcp_builder import mcps_from_mce
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.decorators import (
     SourceCapability,
@@ -36,14 +38,16 @@
 from datahub.ingestion.source.sql.sql_config import BasicSQLAlchemyConfig
 from datahub.metadata.com.linkedin.pegasus2avro.schema import (
     ArrayTypeClass,
     BytesTypeClass,
     MapTypeClass,
 )
 
+logger: logging.Logger = logging.getLogger(__name__)
+
 register_custom_type(custom_types.ARRAY, ArrayTypeClass)
 register_custom_type(custom_types.JSON, BytesTypeClass)
 register_custom_type(custom_types.JSONB, BytesTypeClass)
 register_custom_type(custom_types.HSTORE, MapTypeClass)
 
 
 VIEW_LINEAGE_QUERY = """
@@ -97,21 +101,22 @@
     # defaults
     scheme = Field(default="postgresql+psycopg2", description="database scheme")
     schema_pattern = Field(default=AllowDenyPattern(deny=["information_schema"]))
     include_view_lineage = Field(
         default=False, description="Include table lineage for views"
     )
 
-    def get_identifier(self: BasicSQLAlchemyConfig, schema: str, table: str) -> str:
-        regular = f"{schema}.{table}"
-        if self.database_alias:
-            return f"{self.database_alias}.{regular}"
-        if self.database:
-            return f"{self.database}.{regular}"
-        return regular
+    database_pattern: AllowDenyPattern = Field(
+        default=AllowDenyPattern.allow_all(),
+        description="Regex patterns for databases to filter in ingestion.",
+    )
+    database: Optional[str] = Field(
+        default=None,
+        description="database (catalog). If set to Null, all databases will be considered for ingestion.",
+    )
 
 
 @platform_name("Postgres")
 @config_class(PostgresConfig)
 @support_status(SupportStatus.CERTIFIED)
 @capability(SourceCapability.DOMAINS, "Enabled by default")
 @capability(SourceCapability.PLATFORM_INSTANCE, "Enabled by default")
@@ -134,26 +139,50 @@
         super().__init__(config, ctx, "postgres")
 
     @classmethod
     def create(cls, config_dict, ctx):
         config = PostgresConfig.parse_obj(config_dict)
         return cls(config, ctx)
 
-    def get_workunits(self) -> Iterable[Union[MetadataWorkUnit, SqlWorkUnit]]:
-        yield from super().get_workunits()
-
-        if self.config.include_view_lineage:
-            yield from self._get_view_lineage_workunits()
-
-    def _get_view_lineage_elements(self) -> Dict[Tuple[str, str], List[str]]:
+    def get_inspectors(self) -> Iterable[Inspector]:
+        # This method can be overridden in the case that you want to dynamically
+        # run on multiple databases.
         url = self.config.get_sql_alchemy_url()
+        logger.debug(f"sql_alchemy_url={url}")
         engine = create_engine(url, **self.config.options)
+        with engine.connect() as conn:
+            if self.config.database and self.config.database != "":
+                inspector = inspect(conn)
+                yield inspector
+            else:
+                # pg_database catalog -  https://www.postgresql.org/docs/current/catalog-pg-database.html
+                # exclude template databases - https://www.postgresql.org/docs/current/manage-ag-templatedbs.html
+                databases = conn.execute(
+                    "SELECT datname from pg_database where datname not in ('template0', 'template1')"
+                )
+                for db in databases:
+                    if self.config.database_pattern.allowed(db["datname"]):
+                        url = self.config.get_sql_alchemy_url(database=db["datname"])
+                        inspector = inspect(
+                            create_engine(url, **self.config.options).connect()
+                        )
+                        yield inspector
 
+    def get_workunits(self) -> Iterable[Union[MetadataWorkUnit, SqlWorkUnit]]:
+        yield from super().get_workunits()
+
+        for inspector in self.get_inspectors():
+            if self.config.include_view_lineage:
+                yield from self._get_view_lineage_workunits(inspector)
+
+    def _get_view_lineage_elements(
+        self, inspector: Inspector
+    ) -> Dict[Tuple[str, str], List[str]]:
         data: List[ViewLineageEntry] = []
-        with engine.connect() as conn:
+        with inspector.engine.connect() as conn:
             results = conn.execute(VIEW_LINEAGE_QUERY)
             if results.returns_rows is False:
                 return {}
 
             for row in results:
                 data.append(ViewLineageEntry.parse_obj(row))
 
@@ -173,41 +202,43 @@
                 continue
 
             key = (lineage.dependent_view, lineage.dependent_schema)
             # Append the source table to the list.
             lineage_elements[key].append(
                 mce_builder.make_dataset_urn(
                     self.platform,
-                    self.config.get_identifier(
-                        lineage.source_schema,
-                        lineage.source_table,
+                    self.get_identifier(
+                        schema=lineage.source_schema,
+                        entity=lineage.source_table,
+                        inspector=inspector,
                     ),
                     self.config.env,
                 )
             )
 
         return lineage_elements
 
-    def _get_view_lineage_workunits(self) -> Iterable[MetadataWorkUnit]:
-        lineage_elements = self._get_view_lineage_elements()
+    def _get_view_lineage_workunits(
+        self, inspector: Inspector
+    ) -> Iterable[MetadataWorkUnit]:
+        lineage_elements = self._get_view_lineage_elements(inspector)
 
         if not lineage_elements:
             return None
 
         # Loop over the lineage elements dictionary.
         for key, source_tables in lineage_elements.items():
             # Split the key into dependent view and dependent schema
             dependent_view, dependent_schema = key
 
             # Construct a lineage object.
             urn = mce_builder.make_dataset_urn(
                 self.platform,
-                self.config.get_identifier(
-                    dependent_schema,
-                    dependent_view,
+                self.get_identifier(
+                    schema=dependent_schema, entity=dependent_view, inspector=inspector
                 ),
                 self.config.env,
             )
 
             # use the mce_builder to ensure that the change proposal inherits
             # the correct defaults for auditHeader and systemMetadata
             lineage_mce = mce_builder.make_lineage_mce(
@@ -215,7 +246,18 @@
                 urn,
             )
 
             for item in mcps_from_mce(lineage_mce):
                 wu = item.as_workunit()
                 self.report.report_workunit(wu)
                 yield wu
+
+    def get_identifier(
+        self, *, schema: str, entity: str, inspector: Inspector, **kwargs: Any
+    ) -> str:
+        regular = f"{schema}.{entity}"
+        if self.config.database:
+            if self.config.database_alias:
+                return f"{self.config.database_alias}.{regular}"
+            return f"{self.config.database}.{regular}"
+        current_database = self.get_db_name(inspector)
+        return f"{current_database}.{regular}"
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/presto.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/presto.py`

 * *Files 2% similar despite different names*

```diff
@@ -81,15 +81,15 @@
 PrestoDialect.get_table_comment = get_table_comment
 PrestoDialect.get_columns = _get_columns
 PrestoDialect._get_full_table = _get_full_table
 
 
 class PrestoConfig(TrinoConfig):
     # defaults
-    scheme = Field(default="presto", description="", hidden_from_schema=True)
+    scheme = Field(default="presto", description="", hidden_from_docs=True)
 
 
 @platform_name("Presto", doc_order=1)
 @config_class(PrestoConfig)
 @support_status(SupportStatus.CERTIFIED)
 @capability(SourceCapability.DOMAINS, "Supported via the `domain` config field")
 @capability(SourceCapability.DATA_PROFILING, "Optionally enabled via configuration")
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/presto_on_hive.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/presto_on_hive.py`

 * *Files 1% similar despite different names*

```diff
@@ -23,14 +23,18 @@
     SupportStatus,
     capability,
     config_class,
     platform_name,
     support_status,
 )
 from datahub.ingestion.api.workunit import MetadataWorkUnit
+from datahub.ingestion.source.common.subtypes import (
+    DatasetContainerSubTypes,
+    DatasetSubTypes,
+)
 from datahub.ingestion.source.sql.sql_common import (
     SQLAlchemySource,
     SqlWorkUnit,
     get_schema_metadata,
 )
 from datahub.ingestion.source.sql.sql_config import (
     BasicSQLAlchemyConfig,
@@ -59,25 +63,14 @@
 from datahub.utilities.hive_schema_to_avro import get_schema_fields_for_hive_column
 
 logger: logging.Logger = logging.getLogger(__name__)
 
 TableKey = namedtuple("TableKey", ["schema", "table"])
 
 
-class PrestoOnHiveContainerSubTypes(str, Enum):
-    DATABASE = "Database"
-    CATALOG = "Catalog"
-    SCHEMA = "Schema"
-
-
-class PrestoOnHiveDatasetSubTypes(str, Enum):
-    VIEW = "View"
-    TABLE = "Table"
-
-
 class PrestoOnHiveConfigMode(str, Enum):
     hive: str = "hive"  # noqa: F811
     presto: str = "presto"
     presto_on_hive: str = "presto-on-hive"
     trino: str = "trino"
 
 
@@ -103,17 +96,15 @@
         description="Where clause to specify what Hive schemas should be ingested.",
     )
     ingestion_job_id: str = ""
     host_port: str = Field(
         default="localhost:3306",
         description="Host URL and port to connect to. Example: localhost:3306",
     )
-    scheme: str = Field(
-        default="mysql+pymysql", description="", hidden_from_schema=True
-    )
+    scheme: str = Field(default="mysql+pymysql", description="", hidden_from_docs=True)
 
     database_pattern: AllowDenyPattern = Field(
         default=AllowDenyPattern.allow_all(),
         description="Regex patterns for hive/presto database to filter in ingestion. Specify regex to only match the database name. e.g. to match all tables in database analytics, use the regex 'analytics'",
     )
 
     metastore_db_name: Optional[str] = Field(
@@ -134,15 +125,17 @@
     )
 
     include_catalog_name_in_ids: bool = Field(
         default=False,
         description="Add the Presto catalog name (e.g. hive) to the generated dataset urns. `urn:li:dataset:(urn:li:dataPlatform:hive,hive.user.logging_events,PROD)` versus `urn:li:dataset:(urn:li:dataPlatform:hive,user.logging_events,PROD)`",
     )
 
-    def get_sql_alchemy_url(self, uri_opts: Optional[Dict[str, Any]] = None) -> str:
+    def get_sql_alchemy_url(
+        self, uri_opts: Optional[Dict[str, Any]] = None, database: Optional[str] = None
+    ) -> str:
         if not ((self.host_port and self.scheme) or self.sqlalchemy_uri):
             raise ValueError("host_port and schema or connect_uri required.")
 
         return self.sqlalchemy_uri or make_sqlalchemy_uri(
             self.scheme,
             self.username,
             self.password.get_secret_value() if self.password is not None else None,
@@ -290,27 +283,27 @@
     """
 
     def __init__(self, config: PrestoOnHiveConfig, ctx: PipelineContext) -> None:
         super().__init__(config, ctx, config.mode.value)
         self.config: PrestoOnHiveConfig = config
         self._alchemy_client = SQLAlchemyClient(config)
         self.database_container_subtype = (
-            PrestoOnHiveContainerSubTypes.CATALOG
+            DatasetContainerSubTypes.PRESTO_CATALOG
             if config.use_catalog_subtype
-            else PrestoOnHiveContainerSubTypes.DATABASE
+            else DatasetContainerSubTypes.DATABASE
         )
         self.view_subtype = (
-            PrestoOnHiveDatasetSubTypes.VIEW
+            DatasetSubTypes.VIEW.title()
             if config.use_dataset_pascalcase_subtype
-            else PrestoOnHiveDatasetSubTypes.VIEW.lower()
+            else DatasetSubTypes.VIEW.lower()
         )
         self.table_subtype = (
-            PrestoOnHiveDatasetSubTypes.TABLE
+            DatasetSubTypes.TABLE.title()
             if config.use_dataset_pascalcase_subtype
-            else PrestoOnHiveDatasetSubTypes.TABLE.lower()
+            else DatasetSubTypes.TABLE.lower()
         )
 
     def get_db_name(self, inspector: Inspector) -> str:
         if self.config.database_alias:
             return f"{self.config.database_alias}"
         if self.config.database:
             return f"{self.config.database}"
@@ -387,15 +380,15 @@
                 platform_instance=self.config.platform_instance,
                 env=self.config.env,
             )
 
             yield from gen_schema_container(
                 database=database,
                 schema=schema,
-                sub_types=["Schema"],
+                sub_types=[DatasetContainerSubTypes.SCHEMA],
                 database_container_key=database_container_key,
                 schema_container_key=schema_container_key,
                 domain_registry=self.domain_registry,
                 domain_config=self.config.domain,
                 report=self.report,
                 extra_properties=extra_properties,
             )
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/redshift.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/redshift.py`

 * *Files 1% similar despite different names*

```diff
@@ -3,15 +3,14 @@
 from dataclasses import dataclass, field
 from enum import Enum
 from typing import Any, Dict, Iterable, List, Optional, Set, Tuple, Union
 from urllib.parse import urlparse
 
 # These imports verify that the dependencies are available.
 import psycopg2  # noqa: F401
-import pydantic
 import sqlalchemy
 import sqlalchemy_redshift  # noqa: F401
 from pydantic.fields import Field
 from sqlalchemy import create_engine, inspect
 from sqlalchemy.engine import Connection, reflection
 from sqlalchemy.engine.reflection import Inspector
 from sqlalchemy_redshift.dialect import RedshiftDialect, RelationKey
@@ -126,26 +125,34 @@
 
 class RedshiftConfig(
     PostgresConfig,
     BaseTimeWindowConfig,
     DatasetLineageProviderConfigBase,
     DatasetS3LineageProviderConfigBase,
 ):
+    def get_identifier(self, schema: str, table: str) -> str:
+        regular = f"{schema}.{table}"
+        if self.database_alias:
+            return f"{self.database_alias}.{regular}"
+        if self.database:
+            return f"{self.database}.{regular}"
+        return regular
+
     # Although Amazon Redshift is compatible with Postgres's wire format,
     # we actually want to use the sqlalchemy-redshift package and dialect
     # because it has better caching behavior. In particular, it queries
     # the full table, column, and constraint information in a single larger
     # query, and then simply pulls out the relevant information as needed.
     # Because of this behavior, it uses dramatically fewer round trips for
     # large Redshift warehouses. As an example, see this query for the columns:
     # https://github.com/sqlalchemy-redshift/sqlalchemy-redshift/blob/60b4db04c1d26071c291aeea52f1dcb5dd8b0eb0/sqlalchemy_redshift/dialect.py#L745.
     scheme = Field(
         default="redshift+psycopg2",
         description="",
-        hidden_from_schema=True,
+        hidden_from_docs=True,
     )
 
     default_schema: str = Field(
         default="public",
         description="The default schema to use if the sql parser fails to parse the schema with `sql_based` lineage collector",
     )
 
@@ -166,18 +173,14 @@
     )
 
     table_lineage_mode: Optional[LineageMode] = Field(
         default=LineageMode.STL_SCAN_BASED,
         description="Which table lineage collector mode to use. Available modes are: [stl_scan_based, sql_based, mixed]",
     )
 
-    @pydantic.validator("platform")
-    def platform_is_always_redshift(cls, v):
-        return "redshift"
-
 
 # reflection.cache uses eval and other magic to partially rewrite the function.
 # mypy can't handle it, so we ignore it for now.
 @reflection.cache  # type: ignore
 def _get_all_table_comments(self, connection, **kw):
     COMMENT_SQL = """
         SELECT n.nspname as schema,
@@ -570,22 +573,22 @@
 
     def inspect_version(self) -> Any:
         db_engine = self.get_metadata_engine()
         logger.info("Checking current version")
         for db_row in db_engine.execute("select version()"):
             self.report.saas_version = db_row[0]
 
-    def get_workunits(self) -> Iterable[Union[MetadataWorkUnit, SqlWorkUnit]]:
+    def get_workunits_internal(self) -> Iterable[Union[MetadataWorkUnit, SqlWorkUnit]]:
         try:
             self.inspect_version()
         except Exception as e:
             self.report.report_failure("version", f"Error: {e}")
             return
 
-        for wu in super().get_workunits():
+        for wu in super().get_workunits_internal():
             yield wu
             if (
                 isinstance(wu, SqlWorkUnit)
                 and isinstance(wu.metadata, MetadataChangeEvent)
                 and isinstance(wu.metadata.proposedSnapshot, DatasetSnapshot)
             ):
                 lineage_mcp = None
@@ -692,18 +695,19 @@
                 path=f"{db_name}.{source_schema}.{source_table}",
             )
             sources.append(source)
 
         return sources
 
     def get_db_name(self, inspector: Optional[Inspector] = None) -> str:
-        db_name = getattr(self.config, "database")
-        db_alias = getattr(self.config, "database_alias")
+        db_name = self.config.database
+        db_alias = self.config.database_alias
         if db_alias:
             db_name = db_alias
+        assert db_name
         return db_name
 
     def _get_s3_path(self, path: str) -> str:
         if self.config.s3_lineage_config:
             for path_spec in self.config.s3_lineage_config.path_specs:
                 if path_spec.allowed(path):
                     table_name, table_path = path_spec.extract_table_name_and_path(path)
@@ -847,15 +851,15 @@
 
     def _populate_lineage(self) -> None:
         stl_scan_based_lineage_query: str = """
             select
                 distinct cluster,
                 target_schema,
                 target_table,
-                username,
+                username as username,
                 source_schema,
                 source_table
             from
                     (
                 select
                     distinct tbl as target_table_id,
                     sti.schema as target_schema,
@@ -869,15 +873,15 @@
                     sti.table_id = tbl
                 where starttime >= '{start_time}'
                 and starttime < '{end_time}'
                 and cluster = '{db_name}'
                     ) as target_tables
             join ( (
                 select
-                    pu.usename::varchar(40) as username,
+                    sui.usename as username,
                     ss.tbl as source_table_id,
                     sti.schema as source_schema,
                     sti.table as source_table,
                     scan_type,
                     sq.query as query
                 from
                     (
@@ -887,20 +891,20 @@
                         tbl,
                         type as scan_type
                     from
                         stl_scan
                 ) ss
                 join SVV_TABLE_INFO sti on
                     sti.table_id = ss.tbl
-                left join pg_user pu on
-                    pu.usesysid = ss.userid
                 left join stl_query sq on
                     ss.query = sq.query
+                left join svl_user_info sui on
+                    sq.userid = sui.usesysid
                 where
-                    pu.usename <> 'rdsdb')
+                    sui.usename <> 'rdsdb')
             ) as source_tables
                     using (query)
             where
                 scan_type in (1, 2, 3)
             order by cluster, target_schema, target_table, starttime asc
         """.format(
             # We need the original database name for filtering
@@ -976,29 +980,29 @@
         from
                 (
             select
                 distinct tbl as target_table_id,
                 sti.schema as target_schema,
                 sti.table as target_table,
                 sti.database as cluster,
-                usename as username,
+                sui.usename as username,
                 querytxt,
                 si.starttime as starttime
             from
                 stl_insert as si
             join SVV_TABLE_INFO sti on
                 sti.table_id = tbl
-            left join pg_user pu on
-                pu.usesysid = si.userid
+            left join svl_user_info sui on
+                si.userid = sui.usesysid
             left join stl_query sq on
                 si.query = sq.query
             left join stl_load_commits slc on
                 slc.query = si.query
             where
-                pu.usename <> 'rdsdb'
+                sui.usename <> 'rdsdb'
                 and sq.aborted = 0
                 and slc.query IS NULL
                 and cluster = '{db_name}'
                 and si.starttime >= '{start_time}'
                 and si.starttime < '{end_time}'
             ) as target_tables
             order by cluster, target_schema, target_table, starttime asc
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/sql_common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/sql_common.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 import datetime
 import logging
 import traceback
 from collections import OrderedDict
 from dataclasses import dataclass, field
-from enum import Enum
 from typing import (
     TYPE_CHECKING,
     Any,
     Callable,
     Dict,
     Iterable,
     List,
@@ -30,14 +29,18 @@
     make_dataplatform_instance_urn,
     make_dataset_urn_with_platform_instance,
     make_tag_urn,
 )
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.workunit import MetadataWorkUnit
+from datahub.ingestion.source.common.subtypes import (
+    DatasetContainerSubTypes,
+    DatasetSubTypes,
+)
 from datahub.ingestion.source.sql.sql_config import SQLAlchemyConfig
 from datahub.ingestion.source.sql.sql_utils import (
     add_table_to_schema_container,
     gen_database_container,
     gen_database_key,
     gen_schema_container,
     gen_schema_key,
@@ -71,15 +74,14 @@
     SchemaField,
     SchemaFieldDataType,
     SchemaMetadata,
     StringTypeClass,
     TimeTypeClass,
 )
 from datahub.metadata.schema_classes import (
-    ChangeTypeClass,
     DataPlatformInstanceClass,
     DatasetLineageTypeClass,
     DatasetPropertiesClass,
     GlobalTagsClass,
     SubTypesClass,
     TagAssociationClass,
     UpstreamClass,
@@ -147,19 +149,14 @@
 def get_platform_from_sqlalchemy_uri(sqlalchemy_uri: str) -> str:
     for platform, tester in PLATFORM_TO_SQLALCHEMY_URI_TESTER_MAP.items():
         if tester(sqlalchemy_uri):
             return platform
     return "external"
 
 
-class SqlContainerSubTypes(str, Enum):
-    DATABASE = "Database"
-    SCHEMA = "Schema"
-
-
 @dataclass
 class SQLSourceReport(StaleEntityRemovalSourceReport):
     tables_scanned: int = 0
     views_scanned: int = 0
     entities_profiled: int = 0
     filtered: LossyList[str] = field(default_factory=LossyList)
 
@@ -291,23 +288,23 @@
 def get_schema_metadata(
     sql_report: SQLSourceReport,
     dataset_name: str,
     platform: str,
     columns: List[dict],
     pk_constraints: Optional[dict] = None,
     foreign_keys: Optional[List[ForeignKeyConstraint]] = None,
-    canonical_schema: List[SchemaField] = [],
+    canonical_schema: Optional[List[SchemaField]] = None,
 ) -> SchemaMetadata:
     schema_metadata = SchemaMetadata(
         schemaName=dataset_name,
         platform=make_data_platform_urn(platform),
         version=0,
         hash="",
         platformSchema=MySqlDDL(tableSchema=""),
-        fields=canonical_schema,
+        fields=canonical_schema or [],
     )
     if foreign_keys is not None and foreign_keys != []:
         schema_metadata.foreignKeys = foreign_keys
 
     return schema_metadata
 
 
@@ -390,24 +387,14 @@
             return str(engine.url.database).strip('"').lower()
         else:
             raise Exception("Unable to get database name from Sqlalchemy inspector")
 
     def get_schema_names(self, inspector):
         return inspector.get_schema_names()
 
-    def get_platform_instance_id(self) -> str:
-        """
-        The source identifier such as the specific source host address required for stateful ingestion.
-        Individual subclasses need to override this method appropriately.
-        """
-        config_dict = self.config.dict()
-        host_port = config_dict.get("host_port", "no_host_port")
-        database = config_dict.get("database", "no_database")
-        return f"{self.platform}_{host_port}_{database}"
-
     def get_allowed_schemas(self, inspector: Inspector, db_name: str) -> Iterable[str]:
         # this function returns the schema names which are filtered by schema_pattern.
         for schema in self.get_schema_names(inspector):
             if not self.config.schema_pattern.allowed(schema):
                 self.report.report_dropped(f"{schema}.*")
                 continue
             else:
@@ -425,28 +412,27 @@
             platform_instance=self.config.platform_instance,
             env=self.config.env,
         )
 
         yield from gen_database_container(
             database=database,
             database_container_key=database_container_key,
-            sub_types=[SqlContainerSubTypes.DATABASE],
+            sub_types=[DatasetContainerSubTypes.DATABASE],
             domain_registry=self.domain_registry,
             domain_config=self.config.domain,
             report=self.report,
             extra_properties=extra_properties,
         )
 
     def gen_schema_containers(
         self,
         schema: str,
         database: str,
         extra_properties: Optional[Dict[str, Any]] = None,
     ) -> Iterable[MetadataWorkUnit]:
-
         database_container_key = gen_database_key(
             database,
             platform=self.platform,
             platform_instance=self.config.platform_instance,
             env=self.config.env,
         )
 
@@ -459,28 +445,27 @@
         )
 
         yield from gen_schema_container(
             database=database,
             schema=schema,
             schema_container_key=schema_container_key,
             database_container_key=database_container_key,
-            sub_types=[SqlContainerSubTypes.SCHEMA],
+            sub_types=[DatasetContainerSubTypes.SCHEMA],
             domain_registry=self.domain_registry,
             domain_config=self.config.domain,
             report=self.report,
             extra_properties=extra_properties,
         )
 
     def add_table_to_schema_container(
         self,
         dataset_urn: str,
         db_name: str,
         schema: str,
     ) -> Iterable[MetadataWorkUnit]:
-
         schema_container_key = gen_schema_key(
             db_name=db_name,
             schema=schema,
             platform=self.platform,
             platform_instance=self.config.platform_instance,
             env=self.config.env,
         )
@@ -699,25 +684,18 @@
         dataset_snapshot.aspects.append(dataset_properties)
 
         if self.config.include_table_location_lineage and location_urn:
             external_upstream_table = UpstreamClass(
                 dataset=location_urn,
                 type=DatasetLineageTypeClass.COPY,
             )
-            lineage_mcpw = MetadataChangeProposalWrapper(
-                entityType="dataset",
-                changeType=ChangeTypeClass.UPSERT,
+            lineage_wu = MetadataChangeProposalWrapper(
                 entityUrn=dataset_snapshot.urn,
-                aspectName="upstreamLineage",
                 aspect=UpstreamLineage(upstreams=[external_upstream_table]),
-            )
-            lineage_wu = MetadataWorkUnit(
-                id=f"{self.platform}-{lineage_mcpw.entityUrn}-{lineage_mcpw.aspectName}",
-                mcp=lineage_mcpw,
-            )
+            ).as_workunit()
             self.report.report_workunit(lineage_wu)
             yield lineage_wu
 
         extra_tags = self.get_extra_tags(inspector, schema, table)
         pk_constraints: dict = inspector.get_pk_constraint(table, schema)
         foreign_keys = self._get_foreign_keys(dataset_urn, inspector, schema, table)
         schema_fields = self.get_schema_fields(
@@ -744,19 +722,16 @@
         yield wu
         dpi_aspect = self.get_dataplatform_instance_aspect(dataset_urn=dataset_urn)
         if dpi_aspect:
             yield dpi_aspect
         subtypes_aspect = MetadataWorkUnit(
             id=f"{dataset_name}-subtypes",
             mcp=MetadataChangeProposalWrapper(
-                entityType="dataset",
-                changeType=ChangeTypeClass.UPSERT,
                 entityUrn=dataset_urn,
-                aspectName="subTypes",
-                aspect=SubTypesClass(typeNames=["table"]),
+                aspect=SubTypesClass(typeNames=[DatasetSubTypes.TABLE]),
             ),
         )
         self.report.report_workunit(subtypes_aspect)
         yield subtypes_aspect
 
         if self.config.domain:
             assert self.domain_registry
@@ -809,30 +784,26 @@
 
         # The "properties" field is a non-standard addition to SQLAlchemy's interface.
         properties = table_info.get("properties", {})
         return description, properties, location
 
     def get_dataplatform_instance_aspect(
         self, dataset_urn: str
-    ) -> Optional[SqlWorkUnit]:
+    ) -> Optional[MetadataWorkUnit]:
         # If we are a platform instance based source, emit the instance aspect
         if self.config.platform_instance:
-            mcp = MetadataChangeProposalWrapper(
-                entityType="dataset",
-                changeType=ChangeTypeClass.UPSERT,
+            wu = MetadataChangeProposalWrapper(
                 entityUrn=dataset_urn,
-                aspectName="dataPlatformInstance",
                 aspect=DataPlatformInstanceClass(
                     platform=make_data_platform_urn(self.platform),
                     instance=make_dataplatform_instance_urn(
                         self.platform, self.config.platform_instance
                     ),
                 ),
-            )
-            wu = SqlWorkUnit(id=f"{dataset_urn}-dataPlatformInstance", mcp=mcp)
+            ).as_workunit()
             self.report.report_workunit(wu)
             return wu
         else:
             return None
 
     def _get_columns(
         self, dataset_name: str, inspector: Inspector, schema: str, table: str
@@ -1018,24 +989,18 @@
         mce = MetadataChangeEvent(proposedSnapshot=dataset_snapshot)
         wu = SqlWorkUnit(id=dataset_name, mce=mce)
         self.report.report_workunit(wu)
         yield wu
         dpi_aspect = self.get_dataplatform_instance_aspect(dataset_urn=dataset_urn)
         if dpi_aspect:
             yield dpi_aspect
-        subtypes_aspect = MetadataWorkUnit(
-            id=f"{view}-subtypes",
-            mcp=MetadataChangeProposalWrapper(
-                entityType="dataset",
-                changeType=ChangeTypeClass.UPSERT,
-                entityUrn=dataset_urn,
-                aspectName="subTypes",
-                aspect=SubTypesClass(typeNames=["view"]),
-            ),
-        )
+        subtypes_aspect = MetadataChangeProposalWrapper(
+            entityUrn=dataset_urn,
+            aspect=SubTypesClass(typeNames=[DatasetSubTypes.VIEW]),
+        ).as_workunit()
         self.report.report_workunit(subtypes_aspect)
         yield subtypes_aspect
         if "view_definition" in properties:
             view_definition_string = properties["view_definition"]
             view_properties_aspect = ViewPropertiesClass(
                 materialized=False, viewLanguage="SQL", viewLogic=view_definition_string
             )
@@ -1217,22 +1182,18 @@
             dataset_name = request.pretty_name
             dataset_urn = make_dataset_urn_with_platform_instance(
                 self.platform,
                 dataset_name,
                 self.config.platform_instance,
                 self.config.env,
             )
-            mcp = MetadataChangeProposalWrapper(
-                entityType="dataset",
+            wu = MetadataChangeProposalWrapper(
                 entityUrn=dataset_urn,
-                changeType=ChangeTypeClass.UPSERT,
-                aspectName="datasetProfile",
                 aspect=profile,
-            )
-            wu = MetadataWorkUnit(id=f"profile-{dataset_name}", mcp=mcp)
+            ).as_workunit()
             self.report.report_workunit(wu)
 
             yield wu
 
     def prepare_profiler_args(
         self,
         inspector: Inspector,
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/sql_config.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/sql_config.py`

 * *Files 4% similar despite different names*

```diff
@@ -4,29 +4,30 @@
 from urllib.parse import quote_plus
 
 import pydantic
 from pydantic import Field
 
 from datahub.configuration.common import AllowDenyPattern
 from datahub.configuration.pydantic_field_deprecation import pydantic_field_deprecated
+from datahub.configuration.source_common import DatasetSourceConfigMixin
 from datahub.ingestion.source.ge_profiling_config import GEProfilingConfig
 from datahub.ingestion.source.state.stale_entity_removal_handler import (
     StatefulStaleMetadataRemovalConfig,
 )
 from datahub.ingestion.source.state.stateful_ingestion_base import (
     StatefulIngestionConfigBase,
 )
 
 logger: logging.Logger = logging.getLogger(__name__)
 
 
-class SQLAlchemyConfig(StatefulIngestionConfigBase):
+class SQLAlchemyConfig(StatefulIngestionConfigBase, DatasetSourceConfigMixin):
     options: dict = pydantic.Field(
         default_factory=dict,
-        description="Any options specified here will be passed to SQLAlchemy's create_engine as kwargs. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine for details.",
+        description="Any options specified here will be passed to [SQLAlchemy.create_engine](https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine) as kwargs.",
     )
     # Although the 'table_pattern' enables you to skip everything from certain schemas,
     # having another option to allow/deny on schema level is an optimization for the case when there is a large number
     # of schemas that one wants to skip and you want to avoid the time to needlessly fetch those tables only to filter
     # them out afterwards via the table_pattern.
     schema_pattern: AllowDenyPattern = Field(
         default=AllowDenyPattern.allow_all(),
@@ -93,38 +94,42 @@
 class BasicSQLAlchemyConfig(SQLAlchemyConfig):
     username: Optional[str] = Field(default=None, description="username")
     password: Optional[pydantic.SecretStr] = Field(
         default=None, exclude=True, description="password"
     )
     host_port: str = Field(description="host URL")
     database: Optional[str] = Field(default=None, description="database (catalog)")
+
     database_alias: Optional[str] = Field(
-        default=None, description="Alias to apply to database when ingesting."
+        default=None,
+        description="[Deprecated] Alias to apply to database when ingesting.",
     )
     scheme: str = Field(description="scheme")
     sqlalchemy_uri: Optional[str] = Field(
         default=None,
         description="URI of database to connect to. See https://docs.sqlalchemy.org/en/14/core/engines.html#database-urls. Takes precedence over other connection parameters.",
     )
 
     _database_alias_deprecation = pydantic_field_deprecated(
         "database_alias",
         message="database_alias is deprecated. Use platform_instance instead.",
     )
 
-    def get_sql_alchemy_url(self, uri_opts: Optional[Dict[str, Any]] = None) -> str:
+    def get_sql_alchemy_url(
+        self, uri_opts: Optional[Dict[str, Any]] = None, database: Optional[str] = None
+    ) -> str:
         if not ((self.host_port and self.scheme) or self.sqlalchemy_uri):
             raise ValueError("host_port and schema or connect_uri required.")
 
         return self.sqlalchemy_uri or make_sqlalchemy_uri(
             self.scheme,
             self.username,
             self.password.get_secret_value() if self.password is not None else None,
             self.host_port,
-            self.database,
+            self.database or database,
             uri_opts=uri_opts,
         )
 
 
 def make_sqlalchemy_uri(
     scheme: str,
     username: Optional[str],
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/sql_generic.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/sql_generic.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,10 +1,10 @@
-from dataclasses import dataclass, field
+from dataclasses import dataclass
 from datetime import datetime
-from typing import Generic, List, Optional, TypeVar
+from typing import Optional
 
 from pydantic.fields import Field
 
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.decorators import (
     SourceCapability,
     SupportStatus,
@@ -22,54 +22,51 @@
     name: str
     ordinal_position: int
     is_nullable: bool
     data_type: str
     comment: Optional[str]
 
 
-SqlTableColumn = TypeVar("SqlTableColumn", bound="BaseColumn")
-
-
 @dataclass
-class BaseTable(Generic[SqlTableColumn]):
+class BaseTable:
     name: str
     comment: Optional[str]
-    created: datetime
+    created: Optional[datetime]
     last_altered: Optional[datetime]
     size_in_bytes: Optional[int]
     rows_count: Optional[int]
-    columns: List[SqlTableColumn] = field(default_factory=list)
+    column_count: Optional[int] = None
     ddl: Optional[str] = None
 
 
 @dataclass
-class BaseView(Generic[SqlTableColumn]):
+class BaseView:
     name: str
     comment: Optional[str]
     created: Optional[datetime]
     last_altered: Optional[datetime]
     view_definition: str
     size_in_bytes: Optional[int] = None
     rows_count: Optional[int] = None
-    columns: List[SqlTableColumn] = field(default_factory=list)
+    column_count: Optional[int] = None
 
 
 class SQLAlchemyGenericConfig(SQLAlchemyConfig):
     platform: str = Field(
         description="Name of platform being ingested, used in constructing URNs."
     )
     connect_uri: str = Field(
         description="URI of database to connect to. See https://docs.sqlalchemy.org/en/14/core/engines.html#database-urls"
     )
 
     def get_sql_alchemy_url(self):
         return self.connect_uri
 
 
-@platform_name("Other SQLAlchemy databases", id="sqlalchemy")
+@platform_name("SQLAlchemy", id="sqlalchemy")
 @config_class(SQLAlchemyGenericConfig)
 @support_status(SupportStatus.CERTIFIED)
 @capability(SourceCapability.DOMAINS, "Supported via the `domain` config field")
 @capability(SourceCapability.DATA_PROFILING, "Optionally enabled via configuration")
 class SQLAlchemyGenericSource(SQLAlchemySource):
     """
     The `sqlalchemy` source is useful if we don't have a pre-built source for your chosen
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/sql_generic_profiler.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/sql_generic_profiler.py`

 * *Files 8% similar despite different names*

```diff
@@ -13,24 +13,32 @@
 )
 from datahub.ingestion.source.sql.sql_common import SQLSourceReport
 from datahub.ingestion.source.sql.sql_config import SQLAlchemyConfig
 from datahub.ingestion.source.sql.sql_generic import BaseTable, BaseView
 from datahub.ingestion.source.state.profiling_state_handler import ProfilingHandler
 from datahub.metadata.com.linkedin.pegasus2avro.dataset import DatasetProfile
 from datahub.metadata.schema_classes import DatasetProfileClass
-from datahub.utilities.stats_collections import TopKDict
+from datahub.utilities.stats_collections import TopKDict, int_top_k_dict
 
 
 @dataclass
 class DetailedProfilerReportMixin:
-    profiling_skipped_not_updated: TopKDict[str, int] = field(default_factory=TopKDict)
-    profiling_skipped_size_limit: TopKDict[str, int] = field(default_factory=TopKDict)
-
-    profiling_skipped_row_limit: TopKDict[str, int] = field(default_factory=TopKDict)
-    num_tables_not_eligible_profiling: Dict[str, int] = field(default_factory=TopKDict)
+    profiling_skipped_not_updated: TopKDict[str, int] = field(
+        default_factory=int_top_k_dict
+    )
+    profiling_skipped_size_limit: TopKDict[str, int] = field(
+        default_factory=int_top_k_dict
+    )
+
+    profiling_skipped_row_limit: TopKDict[str, int] = field(
+        default_factory=int_top_k_dict
+    )
+    num_tables_not_eligible_profiling: Dict[str, int] = field(
+        default_factory=int_top_k_dict
+    )
 
 
 class ProfilingSqlReport(DetailedProfilerReportMixin, SQLSourceReport):
     pass
 
 
 @dataclass
@@ -70,15 +78,15 @@
         ]
         table_level_profile_requests: List[TableProfilerRequest] = [
             request for request in requests if request.profile_table_level_only
         ]
         for request in table_level_profile_requests:
             profile = DatasetProfile(
                 timestampMillis=int(datetime.now().timestamp() * 1000),
-                columnCount=len(request.table.columns),
+                columnCount=request.table.column_count,
                 rowCount=request.table.rows_count,
                 sizeInBytes=request.table.size_in_bytes,
             )
             yield (request, profile)
 
         if not ge_profile_requests:
             return
@@ -159,36 +167,30 @@
         if not self.config.profile_pattern.allowed(dataset_name):
             return False
 
         schema_name = dataset_name.rsplit(".", 1)[0]
         if (threshold_time is not None) and (
             last_altered is not None and last_altered < threshold_time
         ):
-            self.report.profiling_skipped_not_updated[schema_name] = (
-                self.report.profiling_skipped_not_updated.get(schema_name, 0) + 1
-            )
+            self.report.profiling_skipped_not_updated[schema_name] += 1
             return False
 
         if self.config.profiling.profile_table_size_limit is not None and (
             size_in_bytes is None
             or size_in_bytes / (2**30)
             > self.config.profiling.profile_table_size_limit
         ):
-            self.report.profiling_skipped_size_limit[schema_name] = (
-                self.report.profiling_skipped_size_limit.get(schema_name, 0) + 1
-            )
+            self.report.profiling_skipped_size_limit[schema_name] += 1
             return False
 
         if self.config.profiling.profile_table_row_limit is not None and (
             rows_count is None
             or rows_count > self.config.profiling.profile_table_row_limit
         ):
-            self.report.profiling_skipped_row_limit[schema_name] = (
-                self.report.profiling_skipped_row_limit.get(schema_name, 0) + 1
-            )
+            self.report.profiling_skipped_row_limit[schema_name] += 1
             return False
 
         return True
 
     def get_profile_args(self) -> Dict:
         """Passed down to GE profiler"""
         return {}
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/sql_types.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/sql_types.py`

 * *Files 6% similar despite different names*

```diff
@@ -233,14 +233,24 @@
     if match:
         modified_type_base: str = match.group(1)
         return TRINO_SQL_TYPES_MAP[modified_type_base]
     else:
         return TRINO_SQL_TYPES_MAP[type_string]
 
 
+def resolve_vertica_modified_type(type_string: str) -> Any:
+    # for cases like timestamp(3), decimal(10,0)
+    match = re.match(r"([a-zA-Z ]+)\(.+\)", type_string)
+    if match:
+        modified_type_base: str = match.group(1)
+        return VERTICA_SQL_TYPES_MAP[modified_type_base]
+    else:
+        return VERTICA_SQL_TYPES_MAP[type_string]
+
+
 # see https://docs.snowflake.com/en/sql-reference/intro-summary-data-types.html
 SNOWFLAKE_TYPES_MAP: Dict[str, Any] = {
     "NUMBER": NumberType,
     "DECIMAL": NumberType,
     "NUMERIC": NumberType,
     "INT": NumberType,
     "INTEGER": NumberType,
@@ -338,7 +348,47 @@
     "date": DateType,
     "time": TimeType,
     "timestamp": TimeType,
     "row": RecordType,
     "map": MapType,
     "array": ArrayType,
 }
+
+# https://www.vertica.com/docs/11.1.x/HTML/Content/Authoring/SQLReferenceManual/DataTypes/SQLDataTypes.htm
+VERTICA_SQL_TYPES_MAP: Dict[str, Any] = {
+    "binary": BytesType,
+    "varbinary": BytesType,
+    "long varbinary": BytesType,
+    "bytea": BytesType,
+    "raw": BytesType,
+    "boolean": BooleanType,
+    "char": StringType,
+    "varchar": StringType,
+    "long varchar": StringType,
+    "date": DateType,
+    "time": TimeType,
+    "datetime": TimeType,
+    "smalldatetime": TimeType,
+    "time with timezone": TimeType,
+    "timestamp": TimeType,
+    "timestamptz": TimeType,
+    "timestamp with timezone": TimeType,
+    "interval": TimeType,
+    "interval hour to second": TimeType,
+    "double precision": NumberType,
+    "float": NumberType,
+    "float8": NumberType,
+    "real": NumberType,
+    "integer": NumberType,
+    "int": NumberType,
+    "bigint": NumberType,
+    "int8": NumberType,
+    "smallint": NumberType,
+    "tinyint": NumberType,
+    "decimal": NumberType,
+    "numeric": NumberType,
+    "number": NumberType,
+    "money": NumberType,
+    "geometry": None,
+    "geography": None,
+    "uuid": StringType,
+}
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/sql_utils.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/sql_utils.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,9 +1,8 @@
-# TODO: Remove to common
-from typing import Dict, Iterable, List, Optional
+from typing import Dict, Iterable, List, Optional, Tuple
 
 from datahub.configuration.common import AllowDenyPattern
 from datahub.emitter.mce_builder import (
     make_data_platform_urn,
     make_dataplatform_instance_urn,
     make_domain_urn,
 )
@@ -14,15 +13,17 @@
     SchemaKey,
     add_dataset_to_container,
     add_domain_to_entity_wu,
     gen_containers,
 )
 from datahub.ingestion.api.source import SourceReport
 from datahub.ingestion.api.workunit import MetadataWorkUnit
+from datahub.metadata.com.linkedin.pegasus2avro.dataset import UpstreamLineage
 from datahub.metadata.schema_classes import DataPlatformInstanceClass
+from datahub.specific.dataset import DatasetPatchBuilder
 from datahub.utilities.registries.domain_registry import DomainRegistry
 
 
 def gen_schema_key(
     db_name: str,
     schema: str,
     platform: str,
@@ -64,15 +65,14 @@
     external_url: Optional[str] = None,
     tags: Optional[List[str]] = None,
     qualified_name: Optional[str] = None,
     created: Optional[int] = None,
     last_modified: Optional[int] = None,
     extra_properties: Optional[Dict[str, str]] = None,
 ) -> Iterable[MetadataWorkUnit]:
-
     domain_urn: Optional[str] = None
     if domain_registry:
         assert domain_config
         domain_urn = gen_domain_urn(
             f"{database}.{schema}",
             domain_config=domain_config,
             domain_registry=domain_registry,
@@ -162,15 +162,14 @@
 
 
 def add_table_to_schema_container(
     dataset_urn: str,
     parent_container_key: PlatformKey,
     report: Optional[SourceReport] = None,
 ) -> Iterable[MetadataWorkUnit]:
-
     container_workunits = add_dataset_to_container(
         container_key=parent_container_key,
         dataset_urn=dataset_urn,
     )
     for wu in container_workunits:
         if report:
             report.report_workunit(wu)
@@ -178,24 +177,25 @@
 
 
 def get_domain_wu(
     dataset_name: str,
     entity_urn: str,
     domain_config: Dict[str, AllowDenyPattern],
     domain_registry: DomainRegistry,
-    report: SourceReport,
+    report: Optional[SourceReport] = None,
 ) -> Iterable[MetadataWorkUnit]:
     domain_urn = gen_domain_urn(dataset_name, domain_config, domain_registry)
     if domain_urn:
         wus = add_domain_to_entity_wu(
             entity_urn=entity_urn,
             domain_urn=domain_urn,
         )
         for wu in wus:
-            report.report_workunit(wu)
+            if report:
+                report.report_workunit(wu)
             yield wu
 
 
 def get_dataplatform_instance_aspect(
     dataset_urn: str, platform: str, platform_instance: Optional[str]
 ) -> Optional[MetadataWorkUnit]:
     # If we are a platform instance based source, emit the instance aspect
@@ -207,7 +207,40 @@
 
         return MetadataChangeProposalWrapper(
             entityUrn=dataset_urn,
             aspect=aspect,
         ).as_workunit()
     else:
         return None
+
+
+def gen_lineage(
+    dataset_urn: str,
+    lineage_info: Optional[Tuple[UpstreamLineage, Dict[str, str]]] = None,
+    incremental_lineage: bool = True,
+) -> Iterable[MetadataWorkUnit]:
+    if lineage_info is None:
+        return
+
+    upstream_lineage, upstream_column_props = lineage_info
+    if upstream_lineage is not None:
+        if incremental_lineage:
+            patch_builder: DatasetPatchBuilder = DatasetPatchBuilder(urn=dataset_urn)
+            for upstream in upstream_lineage.upstreams:
+                patch_builder.add_upstream_lineage(upstream)
+
+            lineage_workunits = [
+                MetadataWorkUnit(
+                    id=f"upstreamLineage-for-{dataset_urn}",
+                    mcp_raw=mcp,
+                )
+                for mcp in patch_builder.build()
+            ]
+        else:
+            lineage_workunits = [
+                MetadataChangeProposalWrapper(
+                    entityUrn=dataset_urn, aspect=upstream_lineage
+                ).as_workunit()
+            ]
+
+        for wu in lineage_workunits:
+            yield wu
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/trino.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/trino.py`

 * *Files 0% similar despite different names*

```diff
@@ -129,15 +129,15 @@
 TrinoDialect.get_table_names = get_table_names
 TrinoDialect.get_table_comment = get_table_comment
 TrinoDialect._get_columns = _get_columns
 
 
 class TrinoConfig(BasicSQLAlchemyConfig):
     # defaults
-    scheme = Field(default="trino", description="", hidden_from_schema=True)
+    scheme = Field(default="trino", description="", hidden_from_docs=True)
 
     def get_identifier(self: BasicSQLAlchemyConfig, schema: str, table: str) -> str:
         regular = f"{schema}.{table}"
         identifier = regular
         if self.database_alias:
             identifier = f"{self.database_alias}.{regular}"
         elif self.database:
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/two_tier_sql_source.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/two_tier_sql_source.py`

 * *Files 0% similar despite different names*

```diff
@@ -70,15 +70,14 @@
     def add_table_to_schema_container(
         self,
         dataset_urn: str,
         db_name: str,
         schema: str,
         schema_container_key: Optional[PlatformKey] = None,
     ) -> Iterable[MetadataWorkUnit]:
-
         yield from add_table_to_schema_container(
             dataset_urn=dataset_urn,
             parent_container_key=self.get_database_container_key(db_name, schema),
             report=self.report,
         )
 
     def get_allowed_schemas(
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/sql/vertica.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/sql/vertica.py`

 * *Files 1% similar despite different names*

```diff
@@ -397,15 +397,14 @@
             Iterator[Iterable[Union[SqlWorkUnit, MetadataWorkUnit]]]: [description]
         """
         projections_seen: Set[str] = set()
 
         try:
             # table_tags = self.get_extra_tags(inspector, schema, "projection")
             for projection in inspector.get_projection_names(schema):  # type: ignore
-
                 schema, projection = self.standardize_schema_table_names(
                     schema=schema, entity=projection
                 )
                 dataset_name = self.get_identifier(
                     schema=schema, entity=projection, inspector=inspector
                 )
                 dataset_name = self.normalise_dataset_name(dataset_name)
@@ -427,15 +426,14 @@
                         f"Unable to ingest {schema}.{projection} due to an exception %s",
                         ex,
                     )
                     self.report.report_warning(
                         f"{schema}.{projection}", f"Ingestion error: {ex}"
                     )
                 if sql_config.include_projection_lineage:  # type: ignore
-
                     try:
                         dataset_urn = make_dataset_urn_with_platform_instance(
                             self.platform,
                             dataset_name,
                             self.config.platform_instance,
                             self.config.env,
                         )
@@ -651,17 +649,15 @@
             Iterable[Union[SqlWorkUnit, MetadataWorkUnit]]
 
         Yields:
             Iterator[Iterable[Union[SqlWorkUnit, MetadataWorkUnit]]]:
         """
         models_seen: Set[str] = set()
         try:
-
             for models in inspector.get_models_names(schema):  # type: ignore
-
                 schema, models = self.standardize_schema_table_names(
                     schema=schema, entity=models
                 )
                 dataset_name = self.get_identifier(
                     schema="Entities", entity=models, inspector=inspector
                 )
 
@@ -839,17 +835,15 @@
             sql_config (SQLAlchemyConfig): configuration
 
         Returns:
             Iterable[Union[SqlWorkUnit, MetadataWorkUnit]]: [description]
         """
         oauth_seen: Set[str] = set()
         try:
-
             for oauth in inspector.get_Oauth_names(schema):  # type: ignore
-
                 schema, oauth = self.standardize_schema_table_names(
                     schema=schema, entity=oauth
                 )
                 dataset_name = self.get_identifier(
                     schema=schema, entity=oauth, inspector=inspector
                 )
 
@@ -985,15 +979,14 @@
         """
         description: Optional[str] = None
         properties: Dict[str, str] = {}
         # The location cannot be fetched generically, but subclasses may override
         # this method and provide a location.
         location: Optional[str] = None
         try:
-
             table_info: dict = inspector.get_oauth_comment(model, schema)  # type: ignore
         except NotImplementedError:
             return description, properties, location
         except ProgrammingError as error:
             logger.debug(
                 f"Encountered ProgrammingError. Retrying with quoted schema name for schema {schema} and oauth {model} %s",
                 error,
@@ -1021,15 +1014,14 @@
         """
         from datahub.ingestion.source.ge_data_profiler import GEProfilerRequest
 
         tables_seen: Set[str] = set()
         profile_candidates = None  # Default value if profile candidates not available.
         yield from super().loop_profiler_requests(inspector, schema, sql_config)
         for projection in inspector.get_projection_names(schema):  # type: ignore
-
             schema, projection = self.standardize_schema_table_names(
                 schema=schema, entity=projection
             )
             dataset_name = self.get_identifier(
                 schema=schema, entity=projection, inspector=inspector
             )
 
@@ -1085,15 +1077,14 @@
                     custom_sql=custom_sql,
                 ),
             )
 
     def _get_upstream_lineage_info(
         self, dataset_urn: str, view: str
     ) -> Optional[_Aspect]:
-
         dataset_key = dataset_urn_to_key(dataset_urn)
         if dataset_key is None:
             logger.warning(f"Invalid dataset urn {dataset_urn}. Could not get key!")
             return None
 
         self._populate_view_lineage(view)
         dataset_name = dataset_key.name
@@ -1116,15 +1107,14 @@
                     self.config.env,
                 ),
                 type=DatasetLineageTypeClass.TRANSFORMED,
             )
             upstream_tables.append(upstream_table)
 
         if upstream_tables:
-
             logger.debug(
                 f" lineage of '{dataset_name}': {[u.dataset for u in upstream_tables]}"
             )
 
             return UpstreamLineage(upstreams=upstream_tables)
 
         return None
@@ -1169,19 +1159,17 @@
             )
         )
         num_edges: int = 0
 
         try:
             self.view_lineage_map = defaultdict(list)
             for db_row_key in engine.execute(view_downstream_query):
-
                 downstream = f"{db_row_key['table_schema']}.{db_row_key['table_name']}"
 
                 for db_row_value in engine.execute(view_upstream_lineage_query):
-
                     upstream = f"{db_row_value['reference_table_schema']}.{db_row_value['reference_table_name']}"
 
                     view_upstream: str = upstream
                     view_name: str = downstream
                     self.view_lineage_map[view_name].append(
                         # (<upstream_table_name>, <empty_json_list_of_upstream_table_columns>, <empty_json_list_of_downstream_view_columns>)
                         (view_upstream, "[]", "[]")
@@ -1200,15 +1188,14 @@
         logger.info(
             f"A total of {num_edges} View upstream edges found found for {view}"
         )
 
     def _get_upstream_lineage_info_projection(
         self, dataset_urn: str, projection: str
     ) -> Optional[_Aspect]:
-
         dataset_key = dataset_urn_to_key(dataset_urn)
         if dataset_key is None:
             logger.warning(f"Invalid dataset urn {dataset_urn}. Could not get key!")
             return None
 
         self._populate_projection_lineage(projection)
         dataset_name = dataset_key.name
@@ -1231,15 +1218,14 @@
                     self.config.env,
                 ),
                 type=DatasetLineageTypeClass.TRANSFORMED,
             )
             upstream_tables.append(upstream_table)
 
         if upstream_tables:
-
             logger.debug(
                 f"lineage of Projection '{dataset_name}': {[u.dataset for u in upstream_tables]}"
             )
             return UpstreamLineage(upstreams=upstream_tables)
         return None
 
     def _populate_projection_lineage(self, projection: str) -> None:
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state/checkpoint.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/checkpoint.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state/entity_removal_state.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/entity_removal_state.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state/profiling_state.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/profiling_state.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state/profiling_state_handler.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/profiling_state_handler.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state/redundant_run_skip_handler.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/redundant_run_skip_handler.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state/stale_entity_removal_handler.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/stale_entity_removal_handler.py`

 * *Files 1% similar despite different names*

```diff
@@ -44,15 +44,15 @@
         description="Soft-deletes the entities present in the last successful run but missing in the current run with stateful_ingestion enabled.",
     )
     fail_safe_threshold: float = pydantic.Field(
         default=100.0,
         description="Prevents large amount of soft deletes & the state from committing from accidental changes to the source configuration if the relative change percent in entities compared to the previous state is above the 'fail_safe_threshold'.",
         le=100.0,
         ge=0.0,
-        hidden_from_schema=True,
+        hidden_from_docs=True,
     )
 
 
 @dataclass
 class StaleEntityRemovalSourceReport(StatefulIngestionReport):
     soft_deleted_stale_entities: List[str] = field(default_factory=list)
 
@@ -190,15 +190,15 @@
             return JobId(backward_comp_platform_to_job_name[platform])
 
         # Default name for everything else
         job_name_suffix = "stale_entity_removal"
         return JobId(f"{platform}_{job_name_suffix}" if platform else job_name_suffix)
 
     def _init_job_id(self) -> JobId:
-        platform: Optional[str] = getattr(self.source, "platform")
+        platform: Optional[str] = getattr(self.source, "platform", "default")
         return self.compute_job_id(platform)
 
     def _ignore_old_state(self) -> bool:
         if (
             self.stateful_ingestion_config is not None
             and self.stateful_ingestion_config.ignore_old_state
         ):
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state/stateful_ingestion_base.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/stateful_ingestion_base.py`

 * *Files 7% similar despite different names*

```diff
@@ -9,15 +9,14 @@
 
 from datahub.configuration.common import (
     ConfigModel,
     ConfigurationError,
     DynamicTypedConfig,
     LineageConfig,
 )
-from datahub.configuration.source_common import DatasetSourceConfigBase
 from datahub.configuration.time_window_config import BaseTimeWindowConfig
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.ingestion_job_checkpointing_provider_base import (
     IngestionCheckpointingProviderBase,
     JobId,
 )
 from datahub.ingestion.api.source import Source, SourceReport
@@ -58,20 +57,20 @@
     enabled: bool = Field(
         default=False,
         description="The type of the ingestion state provider registered with datahub.",
     )
     max_checkpoint_state_size: pydantic.PositiveInt = Field(
         default=2**24,  # 16 MB
         description="The maximum size of the checkpoint state in bytes. Default is 16MB",
-        hidden_from_schema=True,
+        hidden_from_docs=True,
     )
     state_provider: Optional[DynamicTypedStateProviderConfig] = Field(
         default=None,
         description="The ingestion state provider configuration.",
-        hidden_from_schema=True,
+        hidden_from_docs=True,
     )
     ignore_old_state: bool = Field(
         default=False,
         description="If set to True, ignores the previous checkpoint state.",
     )
     ignore_new_state: bool = Field(
         default=False,
@@ -87,27 +86,25 @@
                 )
         return values
 
 
 CustomConfig = TypeVar("CustomConfig", bound=StatefulIngestionConfig)
 
 
-class StatefulIngestionConfigBase(
-    DatasetSourceConfigBase, GenericModel, Generic[CustomConfig]
-):
+class StatefulIngestionConfigBase(GenericModel, Generic[CustomConfig]):
     """
     Base configuration class for stateful ingestion for source configs to inherit from.
     """
 
     stateful_ingestion: Optional[CustomConfig] = Field(
         default=None, description="Stateful Ingestion Config"
     )
 
 
-class LineageStatefulIngestionConfig(StatefulIngestionConfigBase, LineageConfig):
+class StatefulLineageConfigMixin(LineageConfig):
     store_last_lineage_extraction_timestamp: bool = Field(
         default=False,
         description="Enable checking last lineage extraction date in store.",
     )
 
     @root_validator(pre=False)
     def lineage_stateful_option_validator(cls, values: Dict) -> Dict:
@@ -118,15 +115,15 @@
                     "Stateful ingestion is disabled, disabling store_last_lineage_extraction_timestamp config option as well"
                 )
                 values["store_last_lineage_extraction_timestamp"] = False
 
         return values
 
 
-class ProfilingStatefulIngestionConfig(StatefulIngestionConfigBase):
+class StatefulProfilingConfigMixin(ConfigModel):
     store_last_profiling_timestamps: bool = Field(
         default=False,
         description="Enable storing last profile timestamp in store.",
     )
 
     @root_validator(pre=False)
     def profiling_stateful_option_validator(cls, values: Dict) -> Dict:
@@ -136,15 +133,15 @@
                 logger.warning(
                     "Stateful ingestion is disabled, disabling store_last_profiling_timestamps config option as well"
                 )
                 values["store_last_profiling_timestamps"] = False
         return values
 
 
-class UsageStatefulIngestionConfig(BaseTimeWindowConfig, StatefulIngestionConfigBase):
+class StatefulUsageConfigMixin(BaseTimeWindowConfig):
     store_last_usage_extraction_timestamp: bool = Field(
         default=True,
         description="Enable checking last usage timestamp in store.",
     )
 
     @root_validator(pre=False)
     def last_usage_extraction_stateful_option_validator(cls, values: Dict) -> Dict:
@@ -165,15 +162,17 @@
 
 class StatefulIngestionSourceBase(Source):
     """
     Defines the base class for all stateful sources.
     """
 
     def __init__(
-        self, config: StatefulIngestionConfigBase, ctx: PipelineContext
+        self,
+        config: StatefulIngestionConfigBase[StatefulIngestionConfig],
+        ctx: PipelineContext,
     ) -> None:
         super().__init__(ctx)
         self.stateful_ingestion_config = config.stateful_ingestion
         self.last_checkpoints: Dict[JobId, Optional[Checkpoint]] = {}
         self.cur_checkpoints: Dict[JobId, Optional[Checkpoint]] = {}
         self.run_summaries_to_report: Dict[JobId, DatahubIngestionRunSummaryClass] = {}
         self.report: StatefulIngestionReport = StatefulIngestionReport()
@@ -277,50 +276,31 @@
         Template base-class method that delegates the check if checkpointing is enabled for the job
         to the appropriate use-case handler registered for the job_id.
         """
         if job_id not in self._usecase_handlers:
             raise ValueError(f"No use-case handler for job_id{job_id}")
         return self._usecase_handlers[job_id].is_checkpointing_enabled()
 
-    def get_platform_instance_id(self) -> str:
-        # This method is retained for backwards compatibility, but it is not
-        # required that new sources implement it. We mainly need it for the
-        # fallback logic in _get_last_checkpoint.
-        raise NotImplementedError("no platform_instance_id configured")
-
     def _get_last_checkpoint(
         self, job_id: JobId, checkpoint_state_class: Type[StateType]
     ) -> Optional[Checkpoint]:
         """
         This is a template method implementation for querying the last checkpoint state.
         """
         last_checkpoint: Optional[Checkpoint] = None
         if self.is_stateful_ingestion_configured():
-            # TRICKY: We currently don't include the platform_instance_id in the
-            # checkpoint urn, but we previously did. As such, we need to fallback
-            # and try the old urn format if the new format doesn't return anything.
-
             # Obtain the latest checkpoint from GMS for this job.
             assert self.ctx.pipeline_name
-            last_checkpoint_aspect = self.ingestion_checkpointing_state_provider.get_latest_checkpoint(  # type: ignore
-                pipeline_name=self.ctx.pipeline_name,
-                job_name=job_id,
+            assert self.ingestion_checkpointing_state_provider
+            last_checkpoint_aspect = (
+                self.ingestion_checkpointing_state_provider.get_latest_checkpoint(
+                    pipeline_name=self.ctx.pipeline_name,
+                    job_name=job_id,
+                )
             )
-            if last_checkpoint_aspect is None:
-                # Try again with the platform_instance_id, if implemented.
-                try:
-                    platform_instance_id = self.get_platform_instance_id()
-                except NotImplementedError:
-                    pass
-                else:
-                    last_checkpoint_aspect = self.ingestion_checkpointing_state_provider.get_latest_checkpoint(  # type: ignore
-                        pipeline_name=self.ctx.pipeline_name,
-                        job_name=job_id,
-                        platform_instance_id=platform_instance_id,
-                    )
 
             # Convert it to a first-class Checkpoint object.
             last_checkpoint = Checkpoint[StateType].create_from_checkpoint_aspect(
                 job_name=job_id,
                 checkpoint_aspect=last_checkpoint_aspect,
                 state_class=checkpoint_state_class,
             )
@@ -354,14 +334,16 @@
             )
         return self.cur_checkpoints[job_id]
 
     def _prepare_checkpoint_states_for_commit(self) -> None:
         # Perform validations
         if not self.is_stateful_ingestion_configured():
             return None
+        assert self.stateful_ingestion_config
+
         if (
             self.stateful_ingestion_config
             and self.stateful_ingestion_config.ignore_new_state
         ):
             logger.info(
                 "The `ignore_new_state` config is True. Not committing current checkpoint."
             )
@@ -377,15 +359,15 @@
         job_checkpoint_aspects: Dict[JobId, DatahubIngestionCheckpointClass] = {}
         for job_name, job_checkpoint in self.cur_checkpoints.items():
             if job_checkpoint is None:
                 continue
             job_checkpoint.prepare_for_commit()
             try:
                 checkpoint_aspect = job_checkpoint.to_checkpoint_aspect(
-                    self.stateful_ingestion_config.max_checkpoint_state_size  # type: ignore
+                    self.stateful_ingestion_config.max_checkpoint_state_size
                 )
             except Exception as e:
                 logger.error(
                     f"Failed to convert checkpoint to aspect for job {job_name}. It will not be committed.",
                     e,
                 )
             else:
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state/use_case_handler.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state/use_case_handler.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/state_provider/datahub_ingestion_checkpointing_provider.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/state_provider/datahub_ingestion_checkpointing_provider.py`

 * *Files 26% similar despite different names*

```diff
@@ -7,18 +7,15 @@
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.ingestion_job_checkpointing_provider_base import (
     IngestionCheckpointingProviderBase,
     IngestionCheckpointingProviderConfig,
     JobId,
 )
 from datahub.ingestion.graph.client import DatahubClientConfig, DataHubGraph
-from datahub.metadata.schema_classes import (
-    ChangeTypeClass,
-    DatahubIngestionCheckpointClass,
-)
+from datahub.metadata.schema_classes import DatahubIngestionCheckpointClass, StatusClass
 
 logger = logging.getLogger(__name__)
 
 
 class DatahubIngestionStateProviderConfig(IngestionCheckpointingProviderConfig):
     datahub_api: Optional[DatahubClientConfig] = DatahubClientConfig()
 
@@ -60,50 +57,44 @@
             return True
         return False
 
     def get_latest_checkpoint(
         self,
         pipeline_name: str,
         job_name: JobId,
-        platform_instance_id: Optional[str] = None,
     ) -> Optional[DatahubIngestionCheckpointClass]:
         logger.debug(
             f"Querying for the latest ingestion checkpoint for pipelineName:'{pipeline_name}',"
-            f" platformInstanceId:'{platform_instance_id}', job_name:'{job_name}'"
+            f" job_name:'{job_name}'"
         )
 
-        if platform_instance_id is None:
-            data_job_urn = self.get_data_job_urn(
-                self.orchestrator_name, pipeline_name, job_name
-            )
-        else:
-            data_job_urn = self.get_data_job_legacy_urn(
-                self.orchestrator_name, pipeline_name, job_name, platform_instance_id
-            )
+        data_job_urn = self.get_data_job_urn(
+            self.orchestrator_name, pipeline_name, job_name
+        )
 
         latest_checkpoint: Optional[
             DatahubIngestionCheckpointClass
         ] = self.graph.get_latest_timeseries_value(
             entity_urn=data_job_urn,
             aspect_type=DatahubIngestionCheckpointClass,
             filter_criteria_map={
                 "pipelineName": pipeline_name,
             },
         )
         if latest_checkpoint:
             logger.debug(
                 f"The last committed ingestion checkpoint for pipelineName:'{pipeline_name}',"
-                f" platformInstanceId:'{platform_instance_id}', job_name:'{job_name}' found with start_time:"
+                f" job_name:'{job_name}' found with start_time:"
                 f" {datetime.utcfromtimestamp(latest_checkpoint.timestampMillis/1000)}"
             )
             return latest_checkpoint
         else:
             logger.debug(
                 f"No committed ingestion checkpoint for pipelineName:'{pipeline_name}',"
-                f" platformInstanceId:'{platform_instance_id}', job_name:'{job_name}' found"
+                f" job_name:'{job_name}' found"
             )
 
         return None
 
     def commit(self) -> None:
         if not self.state_to_commit:
             logger.warning(f"No state available to commit for {self.name}")
@@ -121,20 +112,26 @@
             datajob_urn = self.get_data_job_urn(
                 self.orchestrator_name,
                 checkpoint.pipelineName,
                 job_name,
             )
 
             self.graph.emit_mcp(
+                # We don't want the state payloads to show up in search. As such, we emit the
+                # dataJob aspects as soft-deleted. This doesn't affect the ability to query
+                # them using the timeseries API.
+                MetadataChangeProposalWrapper(
+                    entityUrn=datajob_urn,
+                    aspect=StatusClass(removed=True),
+                )
+            )
+            self.graph.emit_mcp(
                 MetadataChangeProposalWrapper(
-                    entityType="dataJob",
                     entityUrn=datajob_urn,
-                    aspectName="datahubIngestionCheckpoint",
                     aspect=checkpoint,
-                    changeType=ChangeTypeClass.UPSERT,
                 )
             )
 
             self.committed = True
 
             logger.debug(
                 f"Committed ingestion checkpoint for pipeline:'{checkpoint.pipelineName}', "
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/superset.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/superset.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 import json
+import logging
 from functools import lru_cache
 from typing import Dict, Iterable, Optional
 
 import dateutil.parser as dp
 import requests
 from pydantic.class_validators import root_validator, validator
 from pydantic.fields import Field
@@ -31,14 +32,16 @@
 from datahub.metadata.schema_classes import (
     ChartInfoClass,
     ChartTypeClass,
     DashboardInfoClass,
 )
 from datahub.utilities import config_clean
 
+logger = logging.getLogger(__name__)
+
 PAGE_SIZE = 25
 
 
 chart_type_from_viz_type = {
     "line": ChartTypeClass.LINE,
     "big_number": ChartTypeClass.LINE,
     "table": ChartTypeClass.TABLE,
@@ -54,15 +57,17 @@
     "box_plot": ChartTypeClass.BAR,
 }
 
 
 class SupersetConfig(ConfigModel):
     # See the Superset /security/login endpoint for details
     # https://superset.apache.org/docs/rest-api
-    connect_uri: str = Field(default="localhost:8088", description="Superset host URL.")
+    connect_uri: str = Field(
+        default="http://localhost:8088", description="Superset host URL."
+    )
     display_uri: Optional[str] = Field(
         default=None,
         description="optional URL to use in links (if `connect_uri` is only for ingestion)",
     )
     username: Optional[str] = Field(default=None, description="Superset username.")
     password: Optional[str] = Field(default=None, description="Superset password.")
     provider: str = Field(default="db", description="Superset provider.")
@@ -132,36 +137,36 @@
     def __init__(self, ctx: PipelineContext, config: SupersetConfig):
         super().__init__(ctx)
         self.config = config
         self.report = SourceReport()
 
         login_response = requests.post(
             f"{self.config.connect_uri}/api/v1/security/login",
-            None,
-            {
+            json={
                 "username": self.config.username,
                 "password": self.config.password,
                 "refresh": True,
                 "provider": self.config.provider,
             },
         )
 
         self.access_token = login_response.json()["access_token"]
+        logger.debug("Got access token from superset")
 
         self.session = requests.Session()
         self.session.headers.update(
             {
                 "Authorization": f"Bearer {self.access_token}",
                 "Content-Type": "application/json",
                 "Accept": "*/*",
             }
         )
 
         # Test the connection
-        test_response = self.session.get(f"{self.config.connect_uri}/api/v1/database")
+        test_response = self.session.get(f"{self.config.connect_uri}/api/v1/dashboard/")
         if test_response.status_code == 200:
             pass
             # TODO(Gabe): how should we message about this error?
 
     @classmethod
     def create(cls, config_dict: dict, ctx: PipelineContext) -> Source:
         config = SupersetConfig.parse_obj(config_dict)
@@ -247,23 +252,28 @@
     def emit_dashboard_mces(self) -> Iterable[MetadataWorkUnit]:
         current_dashboard_page = 0
         # we will set total dashboards to the actual number after we get the response
         total_dashboards = PAGE_SIZE
 
         while current_dashboard_page * PAGE_SIZE <= total_dashboards:
             dashboard_response = self.session.get(
-                f"{self.config.connect_uri}/api/v1/dashboard",
+                f"{self.config.connect_uri}/api/v1/dashboard/",
                 params=f"q=(page:{current_dashboard_page},page_size:{PAGE_SIZE})",
             )
+            if dashboard_response.status_code != 200:
+                logger.warning(
+                    f"Failed to get dashboard data: {dashboard_response.text}"
+                )
+            dashboard_response.raise_for_status()
+
             payload = dashboard_response.json()
             total_dashboards = payload.get("count") or 0
 
             current_dashboard_page += 1
 
-            payload = dashboard_response.json()
             for dashboard_data in payload["result"]:
                 dashboard_snapshot = self.construct_dashboard_from_api_data(
                     dashboard_data
                 )
                 mce = MetadataChangeEvent(proposedSnapshot=dashboard_snapshot)
                 wu = MetadataWorkUnit(id=dashboard_snapshot.urn, mce=mce)
                 self.report.report_workunit(wu)
@@ -348,17 +358,21 @@
     def emit_chart_mces(self) -> Iterable[MetadataWorkUnit]:
         current_chart_page = 0
         # we will set total charts to the actual number after we get the response
         total_charts = PAGE_SIZE
 
         while current_chart_page * PAGE_SIZE <= total_charts:
             chart_response = self.session.get(
-                f"{self.config.connect_uri}/api/v1/chart",
+                f"{self.config.connect_uri}/api/v1/chart/",
                 params=f"q=(page:{current_chart_page},page_size:{PAGE_SIZE})",
             )
+            if chart_response.status_code != 200:
+                logger.warning(f"Failed to get chart data: {chart_response.text}")
+            chart_response.raise_for_status()
+
             current_chart_page += 1
 
             payload = chart_response.json()
             total_charts = payload["count"]
             for chart_data in payload["result"]:
                 chart_snapshot = self.construct_chart_from_chart_data(chart_data)
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/tableau_common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/tableau_common.py`

 * *Files 4% similar despite different names*

```diff
@@ -25,27 +25,32 @@
 
 
 class TableauLineageOverrides(ConfigModel):
     platform_override_map: Optional[Dict[str, str]] = Field(
         default=None,
         description="A holder for platform -> platform mappings to generate correct dataset urns",
     )
+    database_override_map: Optional[Dict[str, str]] = Field(
+        default=None,
+        description="A holder for database -> database mappings to generate correct dataset urns",
+    )
 
 
 class MetadataQueryException(Exception):
     pass
 
 
 workbook_graphql_query = """
     {
       id
       name
       luid
       uri
       projectName
+      projectLuid
       owner {
         username
       }
       description
       uri
       createdAt
       updatedAt
@@ -66,27 +71,29 @@
 
 sheet_graphql_query = """
 {
     id
     name
     path
     luid
+    documentViewId
     createdAt
     updatedAt
     tags {
         name
     }
     containedInDashboards {
         name
         path
     }
     workbook {
         id
         name
         projectName
+        projectLuid
         owner {
           username
         }
     }
     datasourceFields {
         __typename
         id
@@ -156,14 +163,15 @@
     tags {
         name
     }
     workbook {
         id
         name
         projectName
+        projectLuid
         owner {
           username
         }
     }
 }
 """
 
@@ -239,26 +247,28 @@
         id
         name
     }
     workbook {
         id
         name
         projectName
+        projectLuid
         owner {
           username
         }
     }
 }
 """
 
 custom_sql_graphql_query = """
 {
       id
       name
       query
+      isUnsupportedCustomSql
       columns {
         id
         name
         remoteType
         description
         referencedByFields {
           datasource {
@@ -274,20 +284,22 @@
               }
               schema
               fullName
               connectionType
             }
             ... on PublishedDatasource {
               projectName
+              luid
             }
             ... on EmbeddedDatasource {
               workbook {
                 id
                 name
                 projectName
+                projectLuid
               }
             }
           }
         }
       }
       tables {
         id
@@ -301,22 +313,27 @@
         connectionType
         description
         columns {
             name
             remoteType
         }
       }
+      database{
+        name
+        connectionType
+      }
 }
 """
 
 published_datasource_graphql_query = """
 {
     __typename
     id
     name
+    luid
     hasExtracts
     extractLastRefreshTime
     extractLastIncrementalUpdateTime
     extractLastUpdateTime
     upstreamTables {
       id
       name
@@ -569,14 +586,22 @@
     if (
         lineage_overrides is not None
         and lineage_overrides.platform_override_map is not None
         and original_platform in lineage_overrides.platform_override_map.keys()
     ):
         platform = lineage_overrides.platform_override_map[original_platform]
 
+    if (
+        lineage_overrides is not None
+        and lineage_overrides.database_override_map is not None
+        and upstream_db is not None
+        and upstream_db in lineage_overrides.database_override_map.keys()
+    ):
+        upstream_db = lineage_overrides.database_override_map[upstream_db]
+
     table_name = get_fully_qualified_table_name(
         original_platform, upstream_db, schema, full_name
     )
     platform_instance = get_platform_instance(original_platform, platform_instance_map)
     return builder.make_dataset_urn_with_platform_instance(
         platform, table_name, platform_instance, env
     )
@@ -596,17 +621,19 @@
 
 def get_unique_custom_sql(custom_sql_list: List[dict]) -> List[dict]:
     unique_custom_sql = []
     for custom_sql in custom_sql_list:
         unique_csql = {
             "id": custom_sql.get("id"),
             "name": custom_sql.get("name"),
+            "isUnsupportedCustomSql": custom_sql.get("isUnsupportedCustomSql"),
             "query": custom_sql.get("query"),
             "columns": custom_sql.get("columns"),
             "tables": custom_sql.get("tables"),
+            "database": custom_sql.get("database"),
         }
         datasource_for_csql = []
         for column in custom_sql.get("columns", []):
             for field in column.get("referencedByFields", []):
                 datasource = field.get("datasource")
                 if datasource not in datasource_for_csql and datasource is not None:
                     datasource_for_csql.append(datasource)
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/unity/config.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/unity/config.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,22 +1,23 @@
 from typing import Dict, Optional
 
 import pydantic
 from pydantic import Field
 
 from datahub.configuration.common import AllowDenyPattern
+from datahub.configuration.source_common import DatasetSourceConfigMixin
 from datahub.ingestion.source.state.stale_entity_removal_handler import (
     StatefulStaleMetadataRemovalConfig,
 )
 from datahub.ingestion.source.state.stateful_ingestion_base import (
     StatefulIngestionConfigBase,
 )
 
 
-class UnityCatalogSourceConfig(StatefulIngestionConfigBase):
+class UnityCatalogSourceConfig(StatefulIngestionConfigBase, DatasetSourceConfigMixin):
     token: str = pydantic.Field(description="Databricks personal access token")
     workspace_url: str = pydantic.Field(description="Databricks workspace url")
     workspace_name: Optional[str] = pydantic.Field(
         default=None,
         description="Name of the workspace. Default to deployment name present in workspace_url",
     )
 
@@ -45,14 +46,19 @@
     )
 
     include_table_lineage: Optional[bool] = pydantic.Field(
         default=True,
         description="Option to enable/disable lineage generation.",
     )
 
+    include_table_ownership: bool = pydantic.Field(
+        default=False,
+        description="Option to enable/disable table ownership generation.",
+    )
+
     include_column_lineage: Optional[bool] = pydantic.Field(
         default=True,
         description="Option to enable/disable lineage generation. Currently we have to call a rest call per column to get column level lineage due to the Databrick api which can slow down ingestion. ",
     )
 
     stateful_ingestion: Optional[StatefulStaleMetadataRemovalConfig] = pydantic.Field(
         default=None, description="Unity Catalog Stateful Ingestion Config."
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/unity/proxy.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/unity/proxy.py`

 * *Files 2% similar despite different names*

```diff
@@ -130,14 +130,21 @@
         self._workspace_url = workspace_url
         self.report = report
 
     def check_connectivity(self) -> bool:
         self._unity_catalog_api.list_metastores()
         return True
 
+    def assigned_metastore(self) -> Optional[Metastore]:
+        response: dict = self._unity_catalog_api.get_metastore_summary()
+        if response.get("metastore_id") is None:
+            logger.info("Not found assigned metastore")
+            return None
+        return self._create_metastore(response)
+
     def metastores(self) -> Iterable[Metastore]:
         response: dict = self._unity_catalog_api.list_metastores()
         if response.get("metastores") is None:
             logger.info("Metastores not found")
             return []
         # yield here to support paginated response later
         for metastore in response["metastores"]:
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/unity/report.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/unity/report.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/unity/source.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/unity/source.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,16 +1,17 @@
 import logging
 import re
-from typing import Iterable, List, Optional
+from typing import Dict, Iterable, List, Optional
 
 from datahub.emitter.mce_builder import (
     make_data_platform_urn,
     make_dataset_urn_with_platform_instance,
     make_domain_urn,
     make_schema_field_urn,
+    make_user_urn,
 )
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.emitter.mcp_builder import (
     CatalogKey,
     MetastoreKey,
     PlatformKey,
     UnitySchemaKey,
@@ -28,14 +29,18 @@
 from datahub.ingestion.api.source import (
     CapabilityReport,
     SourceCapability,
     TestableSource,
     TestConnectionReport,
 )
 from datahub.ingestion.api.workunit import MetadataWorkUnit
+from datahub.ingestion.source.common.subtypes import (
+    DatasetContainerSubTypes,
+    DatasetSubTypes,
+)
 from datahub.ingestion.source.state.entity_removal_state import GenericCheckpointState
 from datahub.ingestion.source.state.stale_entity_removal_handler import (
     StaleEntityRemovalHandler,
 )
 from datahub.ingestion.source.state.stateful_ingestion_base import (
     StatefulIngestionSourceBase,
 )
@@ -49,14 +54,17 @@
     ViewProperties,
 )
 from datahub.metadata.schema_classes import (
     DatasetLineageTypeClass,
     DatasetPropertiesClass,
     DomainsClass,
     MySqlDDLClass,
+    OwnerClass,
+    OwnershipClass,
+    OwnershipTypeClass,
     SchemaFieldClass,
     SchemaMetadataClass,
     SubTypesClass,
     UpstreamClass,
     UpstreamLineageClass,
 )
 from datahub.utilities.hive_schema_to_avro import get_schema_fields_for_hive_column
@@ -76,14 +84,17 @@
 @capability(SourceCapability.DESCRIPTIONS, "Enabled by default")
 @capability(SourceCapability.LINEAGE_COARSE, "Enabled by default")
 @capability(SourceCapability.LINEAGE_FINE, "Enabled by default")
 @capability(SourceCapability.PLATFORM_INSTANCE, "Enabled by default")
 @capability(SourceCapability.DOMAINS, "Supported via the `domain` config field")
 @capability(SourceCapability.CONTAINERS, "Enabled by default")
 @capability(
+    SourceCapability.OWNERSHIP, "Supported via the `include_table_ownership` config"
+)
+@capability(
     SourceCapability.DELETION_DETECTION,
     "Optionally enabled via `stateful_ingestion.remove_stale_metadata`",
     supported=True,
 )
 @support_status(SupportStatus.INCUBATING)
 class UnityCatalogSource(StatefulIngestionSourceBase, TestableSource):
     """
@@ -152,31 +163,35 @@
         return test_report
 
     @classmethod
     def create(cls, config_dict, ctx):
         config = UnityCatalogSourceConfig.parse_obj(config_dict)
         return cls(ctx=ctx, config=config)
 
-    def get_platform_instance_id(self) -> str:
-        return self.config.platform_instance or self.platform
-
     def get_workunits(self) -> Iterable[MetadataWorkUnit]:
         return auto_stale_entity_removal(
             self.stale_entity_removal_handler,
             auto_workunit_reporter(
                 self.report,
                 auto_status_aspect(self.get_workunits_internal()),
             ),
         )
 
     def get_workunits_internal(self) -> Iterable[MetadataWorkUnit]:
         yield from self.process_metastores()
 
     def process_metastores(self) -> Iterable[MetadataWorkUnit]:
+        metastores: Dict[str, Metastore] = {}
+        assigned_metastore = self.unity_catalog_api_proxy.assigned_metastore()
+        if assigned_metastore:
+            metastores[assigned_metastore.metastore_id] = assigned_metastore
         for metastore in self.unity_catalog_api_proxy.metastores():
+            metastores[metastore.metastore_id] = metastore
+
+        for metastore in metastores.values():
             if not self.config.metastore_id_pattern.allowed(metastore.metastore_id):
                 self.report.metastores.dropped(metastore.metastore_id)
                 continue
 
             logger.info(f"Started to process metastore: {metastore.metastore_id}")
             yield from self.gen_metastore_containers(metastore)
             yield from self.process_catalogs(metastore)
@@ -242,14 +257,16 @@
 
         domain = self._get_domain_aspect(
             dataset_name=str(
                 f"{table.schema.catalog.name}.{table.schema.name}.{table.name}"
             )
         )
 
+        ownership = self._create_table_ownership_aspect(table)
+
         if self.config.include_column_lineage:
             self.unity_catalog_api_proxy.get_column_lineage(table)
             lineage = self._generate_column_lineage_aspect(dataset_urn, table)
         else:
             self.unity_catalog_api_proxy.table_lineage(table)
             lineage = self._generate_lineage_aspect(dataset_urn, table)
 
@@ -259,14 +276,15 @@
                 entityUrn=dataset_urn,
                 aspects=[
                     table_props,
                     view_props,
                     sub_type,
                     schema_metadata,
                     domain,
+                    ownership,
                     lineage,
                 ],
             )
         ]
 
     def _generate_column_lineage_aspect(
         self, dataset_urn: str, table: proxy.Table
@@ -336,15 +354,15 @@
     def gen_schema_containers(self, schema: Schema) -> Iterable[MetadataWorkUnit]:
         domain_urn = self._gen_domain_urn(f"{schema.catalog.name}.{schema.name}")
 
         schema_container_key = self.gen_schema_key(schema)
         yield from gen_containers(
             container_key=schema_container_key,
             name=schema.name,
-            sub_types=["Schema"],
+            sub_types=[DatasetContainerSubTypes.SCHEMA],
             parent_container_key=self.gen_catalog_key(catalog=schema.catalog),
             domain_urn=domain_urn,
             description=schema.comment,
         )
 
     def gen_metastore_containers(
         self, metastore: Metastore
@@ -352,30 +370,30 @@
         domain_urn = self._gen_domain_urn(metastore.name)
 
         metastore_container_key = self.gen_metastore_key(metastore)
 
         yield from gen_containers(
             container_key=metastore_container_key,
             name=metastore.name,
-            sub_types=["Metastore"],
+            sub_types=[DatasetContainerSubTypes.DATABRICKS_METASTORE],
             domain_urn=domain_urn,
             description=metastore.comment,
         )
 
     def gen_catalog_containers(self, catalog: Catalog) -> Iterable[MetadataWorkUnit]:
         domain_urn = self._gen_domain_urn(catalog.name)
 
         metastore_container_key = self.gen_metastore_key(catalog.metastore)
 
         catalog_container_key = self.gen_catalog_key(catalog)
 
         yield from gen_containers(
             container_key=catalog_container_key,
             name=catalog.name,
-            sub_types=["Catalog"],
+            sub_types=[DatasetContainerSubTypes.PRESTO_CATALOG],
             domain_urn=domain_urn,
             parent_container_key=metastore_container_key,
             description=catalog.comment,
         )
 
     def gen_schema_key(self, schema: Schema) -> PlatformKey:
         return UnitySchemaKey(
@@ -444,17 +462,35 @@
 
         return DatasetPropertiesClass(
             name=table.name,
             description=table.comment,
             customProperties=custom_properties,
         )
 
+    def _create_table_ownership_aspect(
+        self, table: proxy.Table
+    ) -> Optional[OwnershipClass]:
+        if self.config.include_table_ownership and table.owner:
+            return OwnershipClass(
+                owners=[
+                    OwnerClass(
+                        owner=make_user_urn(table.owner),
+                        type=OwnershipTypeClass.DATAOWNER,
+                    )
+                ]
+            )
+        return None
+
     def _create_table_sub_type_aspect(self, table: proxy.Table) -> SubTypesClass:
         return SubTypesClass(
-            typeNames=["View" if table.table_type.lower() == "view" else "Table"]
+            typeNames=[
+                DatasetSubTypes.VIEW
+                if table.table_type.lower() == "view"
+                else DatasetSubTypes.TABLE
+            ]
         )
 
     def _create_view_property_aspect(self, table: proxy.Table) -> ViewProperties:
         assert table.view_definition
         return ViewProperties(
             materialized=False, viewLanguage="SQL", viewLogic=table.view_definition
         )
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/usage/clickhouse_usage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/usage/clickhouse_usage.py`

 * *Files 1% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 from dateutil import parser
 from pydantic.fields import Field
 from pydantic.main import BaseModel
 from sqlalchemy import create_engine
 from sqlalchemy.engine import Engine
 
 import datahub.emitter.mce_builder as builder
-from datahub.configuration.source_common import EnvBasedSourceConfigBase
+from datahub.configuration.source_common import EnvConfigMixin
 from datahub.configuration.time_window_config import get_time_bucket
 from datahub.ingestion.api.decorators import (
     SourceCapability,
     SupportStatus,
     capability,
     config_class,
     platform_name,
@@ -64,17 +64,15 @@
     schema_: str = None  # type:ignore
     table: str = None  # type:ignore
     columns: List[str]
     starttime: datetime
     endtime: datetime
 
 
-class ClickHouseUsageConfig(
-    ClickHouseConfig, BaseUsageConfig, EnvBasedSourceConfigBase
-):
+class ClickHouseUsageConfig(ClickHouseConfig, BaseUsageConfig, EnvConfigMixin):
     email_domain: str = Field(description="")
     options: dict = Field(default={}, description="")
     query_log_table: str = Field(default="system.query_log", exclude=True)
 
     def get_sql_alchemy_url(self):
         return super().get_sql_alchemy_url()
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/usage/redshift_usage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/usage/redshift_usage.py`

 * *Files 1% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 
 from pydantic.fields import Field
 from pydantic.main import BaseModel
 from sqlalchemy import create_engine
 from sqlalchemy.engine import Engine
 
 import datahub.emitter.mce_builder as builder
-from datahub.configuration.source_common import EnvBasedSourceConfigBase
+from datahub.configuration.source_common import EnvConfigMixin
 from datahub.configuration.time_window_config import get_time_bucket
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.ingestion.api.common import PipelineContext
 from datahub.ingestion.api.decorators import (
     SourceCapability,
     SupportStatus,
     capability,
@@ -141,15 +141,15 @@
     schema_: str = Field(alias="schema")
     table: str
     operation_type: Optional[str] = None
     starttime: datetime
     endtime: datetime
 
 
-class RedshiftUsageConfig(RedshiftConfig, BaseUsageConfig, EnvBasedSourceConfigBase):
+class RedshiftUsageConfig(RedshiftConfig, BaseUsageConfig, EnvConfigMixin):
     email_domain: str = Field(
         description="Email domain of your organisation so users can be displayed on UI appropriately."
     )
     options: Dict = Field(
         default={},
         description="Any options specified here will be passed to SQLAlchemy's create_engine as kwargs."
         "See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine for details.",
@@ -369,26 +369,26 @@
             resource: str = f"{event.database}.{event.schema_}.{event.table}"
             # Get a reference to the bucket value(or initialize not yet in dict) and update it.
             agg_bucket: AggregatedDataset = datasets[floored_ts].setdefault(
                 resource,
                 AggregatedDataset(
                     bucket_start_time=floored_ts,
                     resource=resource,
-                    user_email_pattern=self.config.user_email_pattern,
                 ),
             )
             # current limitation in user stats UI, we need to provide email to show users
             user_email: str = f"{event.username if event.username else 'unknown'}"
             if "@" not in user_email:
                 user_email += f"@{self.config.email_domain}"
             logger.info(f"user_email: {user_email}")
             agg_bucket.add_read_entry(
                 user_email,
                 event.text,
                 [],  # TODO: not currently supported by redshift; find column level changes
+                user_email_pattern=self.config.user_email_pattern,
             )
         return datasets
 
     def _make_usage_stat(self, agg: AggregatedDataset) -> MetadataWorkUnit:
         return agg.make_usage_workunit(
             self.config.bucket_duration,
             lambda resource: builder.make_dataset_urn_with_platform_instance(
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/usage/starburst_trino_usage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/usage/starburst_trino_usage.py`

 * *Files 0% similar despite different names*

```diff
@@ -250,29 +250,29 @@
                 )
 
                 agg_bucket = datasets[floored_ts].setdefault(
                     resource,
                     AggregatedDataset(
                         bucket_start_time=floored_ts,
                         resource=resource,
-                        user_email_pattern=self.config.user_email_pattern,
                     ),
                 )
 
                 # add @unknown.com to username
                 # current limitation in user stats UI, we need to provide email to show users
                 if event.usr and "@" in parseaddr(event.usr)[1]:
                     username = event.usr
                 else:
                     username = f"{event.usr if event.usr else 'unknown'}@{self.config.email_domain}"
 
                 agg_bucket.add_read_entry(
                     username,
                     event.query,
                     metadata.columns,
+                    user_email_pattern=self.config.user_email_pattern,
                 )
         return datasets
 
     def _make_usage_stat(self, agg: AggregatedDataset) -> MetadataWorkUnit:
         return agg.make_usage_workunit(
             self.config.bucket_duration,
             lambda resource: builder.make_dataset_urn_with_platform_instance(
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source/usage/usage_common.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/usage/usage_common.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 import collections
 import dataclasses
 import logging
 from datetime import datetime
-from typing import Callable, Counter, Generic, List, Optional, TypeVar
+from typing import Callable, Counter, Generic, List, Optional, Tuple, TypeVar
 
 import pydantic
 from pydantic.fields import Field
 
 import datahub.emitter.mce_builder as builder
 from datahub.configuration.common import AllowDenyPattern
 from datahub.configuration.time_window_config import (
@@ -17,121 +17,138 @@
 from datahub.ingestion.api.workunit import MetadataWorkUnit
 from datahub.metadata.schema_classes import (
     DatasetFieldUsageCountsClass,
     DatasetUsageStatisticsClass,
     DatasetUserUsageCountsClass,
     TimeWindowSizeClass,
 )
-from datahub.utilities.sql_formatter import format_sql_query
+from datahub.utilities.sql_formatter import format_sql_query, trim_query
 
 logger = logging.getLogger(__name__)
 
 ResourceType = TypeVar("ResourceType")
 
+# The total number of characters allowed across all queries in a single workunit.
+TOTAL_BUDGET_FOR_QUERY_LIST = 24000
+
+
+def make_usage_workunit(
+    bucket_start_time: datetime,
+    resource: ResourceType,
+    query_count: int,
+    query_freq: Optional[List[Tuple[str, int]]],
+    user_freq: List[Tuple[str, int]],
+    column_freq: List[Tuple[str, int]],
+    bucket_duration: BucketDuration,
+    urn_builder: Callable[[ResourceType], str],
+    top_n_queries: int,
+    format_sql_queries: bool,
+    total_budget_for_query_list: int = TOTAL_BUDGET_FOR_QUERY_LIST,
+    query_trimmer_string: str = " ...",
+) -> MetadataWorkUnit:
+    top_sql_queries: Optional[List[str]] = None
+    if query_freq is not None:
+        budget_per_query: int = int(total_budget_for_query_list / top_n_queries)
+        top_sql_queries = [
+            trim_query(
+                format_sql_query(query, keyword_case="upper", reindent_aligned=True)
+                if format_sql_queries
+                else query,
+                budget_per_query=budget_per_query,
+                query_trimmer_string=query_trimmer_string,
+            )
+            for query, _ in query_freq
+        ]
+
+    usageStats = DatasetUsageStatisticsClass(
+        timestampMillis=int(bucket_start_time.timestamp() * 1000),
+        eventGranularity=TimeWindowSizeClass(unit=bucket_duration, multiple=1),
+        uniqueUserCount=len(user_freq),
+        totalSqlQueries=query_count,
+        topSqlQueries=top_sql_queries,
+        userCounts=[
+            DatasetUserUsageCountsClass(
+                user=builder.make_user_urn(user_email.split("@")[0]),
+                count=count,
+                userEmail=user_email,
+            )
+            for user_email, count in user_freq
+        ],
+        fieldCounts=[
+            DatasetFieldUsageCountsClass(
+                fieldPath=column,
+                count=count,
+            )
+            for column, count in column_freq
+        ],
+    )
+
+    return MetadataChangeProposalWrapper(
+        entityUrn=urn_builder(resource),
+        aspect=usageStats,
+    ).as_workunit()
+
 
 @dataclasses.dataclass
 class GenericAggregatedDataset(Generic[ResourceType]):
     bucket_start_time: datetime
     resource: ResourceType
-    user_email_pattern: AllowDenyPattern = AllowDenyPattern.allow_all()
 
     readCount: int = 0
     queryCount: int = 0
 
     queryFreq: Counter[str] = dataclasses.field(default_factory=collections.Counter)
     userFreq: Counter[str] = dataclasses.field(default_factory=collections.Counter)
     columnFreq: Counter[str] = dataclasses.field(default_factory=collections.Counter)
 
-    total_budget_for_query_list: int = 24000
-    query_trimmer_string_space: int = 10
-    query_trimmer_string: str = " ..."
-
     def add_read_entry(
         self,
         user_email: str,
         query: Optional[str],
         fields: List[str],
+        user_email_pattern: AllowDenyPattern = AllowDenyPattern.allow_all(),
     ) -> None:
-        if not self.user_email_pattern.allowed(user_email):
+        if not user_email_pattern.allowed(user_email):
             return
 
         self.readCount += 1
         self.userFreq[user_email] += 1
 
         if query:
             self.queryCount += 1
             self.queryFreq[query] += 1
         for column in fields:
             self.columnFreq[column] += 1
 
-    def trim_query(self, query: str, budget_per_query: int) -> str:
-        trimmed_query = query
-        if len(query) > budget_per_query:
-            if budget_per_query - self.query_trimmer_string_space > 0:
-                end_index = budget_per_query - self.query_trimmer_string_space
-                trimmed_query = query[:end_index] + self.query_trimmer_string
-            else:
-                raise Exception(
-                    "Budget per query is too low. Please, decrease the number of top_n_queries."
-                )
-        return trimmed_query
-
     def make_usage_workunit(
         self,
         bucket_duration: BucketDuration,
         urn_builder: Callable[[ResourceType], str],
         top_n_queries: int,
         format_sql_queries: bool,
         include_top_n_queries: bool,
+        total_budget_for_query_list: int = TOTAL_BUDGET_FOR_QUERY_LIST,
+        query_trimmer_string: str = " ...",
     ) -> MetadataWorkUnit:
-        top_sql_queries: Optional[List[str]] = None
-        if include_top_n_queries:
-            budget_per_query: int = int(
-                self.total_budget_for_query_list / top_n_queries
-            )
-            top_sql_queries = [
-                self.trim_query(
-                    format_sql_query(query, keyword_case="upper", reindent_aligned=True)
-                    if format_sql_queries
-                    else query,
-                    budget_per_query,
-                )
-                for query, _ in self.queryFreq.most_common(top_n_queries)
-            ]
-
-        usageStats = DatasetUsageStatisticsClass(
-            timestampMillis=int(self.bucket_start_time.timestamp() * 1000),
-            eventGranularity=TimeWindowSizeClass(unit=bucket_duration, multiple=1),
-            uniqueUserCount=len(self.userFreq),
-            totalSqlQueries=self.queryCount,
-            topSqlQueries=top_sql_queries,
-            userCounts=[
-                DatasetUserUsageCountsClass(
-                    user=builder.make_user_urn(user_email.split("@")[0]),
-                    count=count,
-                    userEmail=user_email,
-                )
-                for user_email, count in self.userFreq.most_common()
-            ],
-            fieldCounts=[
-                DatasetFieldUsageCountsClass(
-                    fieldPath=column,
-                    count=count,
-                )
-                for column, count in self.columnFreq.most_common()
-            ],
+        query_freq = (
+            self.queryFreq.most_common(top_n_queries) if include_top_n_queries else None
         )
-
-        mcp = MetadataChangeProposalWrapper(
-            entityUrn=urn_builder(self.resource),
-            aspect=usageStats,
-        )
-
-        return MetadataWorkUnit(
-            id=f"{self.bucket_start_time.isoformat()}-{self.resource}", mcp=mcp
+        return make_usage_workunit(
+            bucket_start_time=self.bucket_start_time,
+            resource=self.resource,
+            query_count=self.queryCount,
+            query_freq=query_freq,
+            user_freq=self.userFreq.most_common(),
+            column_freq=self.columnFreq.most_common(),
+            bucket_duration=bucket_duration,
+            urn_builder=urn_builder,
+            top_n_queries=top_n_queries,
+            format_sql_queries=format_sql_queries,
+            total_budget_for_query_list=total_budget_for_query_list,
+            query_trimmer_string=query_trimmer_string,
         )
 
 
 class BaseUsageConfig(BaseTimeWindowConfig):
     top_n_queries: pydantic.PositiveInt = Field(
         default=10, description="Number of top queries to save to each table."
     )
@@ -155,18 +172,13 @@
         default=True, description="Whether to ingest the top_n_queries."
     )
 
     @pydantic.validator("top_n_queries")
     def ensure_top_n_queries_is_not_too_big(cls, v: int) -> int:
         minimum_query_size = 20
 
-        max_queries = int(
-            GenericAggregatedDataset.total_budget_for_query_list / minimum_query_size
-        )
-        if (
-            int(GenericAggregatedDataset.total_budget_for_query_list / v)
-            < minimum_query_size
-        ):
+        max_queries = int(TOTAL_BUDGET_FOR_QUERY_LIST / minimum_query_size)
+        if v > max_queries:
             raise ValueError(
                 f"top_n_queries is set to {v} but it can be maximum {max_queries}"
             )
         return v
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_config/bigquery.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/bigquery.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_config/csv_enricher.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/csv_enricher.py`

 * *Files 12% similar despite different names*

```diff
@@ -2,15 +2,17 @@
 
 import pydantic
 
 from datahub.configuration.common import ConfigModel, ConfigurationError
 
 
 class CSVEnricherConfig(ConfigModel):
-    filename: str = pydantic.Field(description="Path to CSV file to ingest")
+    filename: str = pydantic.Field(
+        description="File path or URL of CSV file to ingest."
+    )
     write_semantics: str = pydantic.Field(
         default="PATCH",
         description='Whether the new tags, terms and owners to be added will override the existing ones added only by this source or not. Value for this config can be "PATCH" or "OVERRIDE"',
     )
     delimiter: str = pydantic.Field(
         default=",", description="Delimiter to use when parsing CSV"
     )
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_config/pulsar.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/pulsar.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,15 +1,18 @@
 import re
 from typing import Dict, List, Optional, Union
 from urllib.parse import urlparse
 
-from pydantic import Field, root_validator, validator
+from pydantic import Field, validator
 
 from datahub.configuration.common import AllowDenyPattern, ConfigurationError
-from datahub.configuration.source_common import DEFAULT_ENV, DatasetSourceConfigBase
+from datahub.configuration.source_common import (
+    EnvConfigMixin,
+    PlatformInstanceConfigMixin,
+)
 from datahub.ingestion.source.state.stale_entity_removal_handler import (
     StatefulStaleMetadataRemovalConfig,
 )
 from datahub.ingestion.source.state.stateful_ingestion_base import (
     StatefulIngestionConfigBase,
 )
 from datahub.utilities import config_clean
@@ -25,17 +28,17 @@
     # Hostnames ending on a dot are valid, if present strip exactly one
     if hostname[-1] == ".":
         hostname = hostname[:-1]
     allowed = re.compile(r"(?!-)[A-Z\d-]{1,63}(?<!-)$", re.IGNORECASE)
     return all(allowed.match(x) for x in hostname.split("."))
 
 
-class PulsarSourceConfig(StatefulIngestionConfigBase, DatasetSourceConfigBase):
-    env: str = DEFAULT_ENV
-
+class PulsarSourceConfig(
+    StatefulIngestionConfigBase, PlatformInstanceConfigMixin, EnvConfigMixin
+):
     web_service_url: str = Field(
         default="http://localhost:8080", description="The web URL for the cluster."
     )
     timeout: int = Field(
         default=5,
         description="Timout setting, how long to wait for the Pulsar rest api to send data before giving up",
     )
@@ -128,20 +131,7 @@
 
         if not _is_valid_hostname(url.hostname.__str__()):
             raise ConfigurationError(
                 f"Not a valid hostname, hostname contains invalid characters, found {url.hostname}"
             )
 
         return config_clean.remove_trailing_slashes(val)
-
-    @root_validator
-    def validate_platform_instance(cls: "PulsarSourceConfig", values: Dict) -> Dict:
-        stateful_ingestion = values.get("stateful_ingestion")
-        if (
-            stateful_ingestion
-            and stateful_ingestion.enabled
-            and not values.get("platform_instance")
-        ):
-            raise ConfigurationError(
-                "Enabling Pulsar stateful ingestion requires to specify a platform instance."
-            )
-        return values
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_config/sql/bigquery.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source/redshift/config.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,112 +1,139 @@
-import logging
-import os
-from datetime import timedelta
+from enum import Enum
 from typing import Any, Dict, List, Optional
 
-import pydantic
+from pydantic import root_validator
+from pydantic.fields import Field
 
-from datahub.configuration.common import ConfigurationError
-from datahub.configuration.time_window_config import BaseTimeWindowConfig
-from datahub.ingestion.source.sql.sql_config import SQLAlchemyConfig
-from datahub.ingestion.source_config.bigquery import BigQueryBaseConfig
-from datahub.ingestion.source_config.usage.bigquery_usage import BigQueryCredential
+from datahub.configuration import ConfigModel
+from datahub.configuration.pydantic_field_deprecation import pydantic_field_deprecated
+from datahub.configuration.source_common import DatasetLineageProviderConfigBase
+from datahub.ingestion.source.aws.path_spec import PathSpec
+from datahub.ingestion.source.sql.postgres import PostgresConfig
+from datahub.ingestion.source.state.stateful_ingestion_base import (
+    StatefulLineageConfigMixin,
+    StatefulProfilingConfigMixin,
+    StatefulUsageConfigMixin,
+)
+from datahub.ingestion.source.usage.usage_common import BaseUsageConfig
+
+
+# The lineage modes are documented in the Redshift source's docstring.
+class LineageMode(Enum):
+    SQL_BASED = "sql_based"
+    STL_SCAN_BASED = "stl_scan_based"
+    MIXED = "mixed"
+
+
+class S3LineageProviderConfig(ConfigModel):
+    """
+    Any source that produces s3 lineage from/to Datasets should inherit this class.
+    """
+
+    path_specs: List[PathSpec] = Field(
+        default=[],
+        description="List of PathSpec. See below the details about PathSpec",
+    )
+
+    strip_urls: bool = Field(
+        default=True,
+        description="Strip filename from s3 url. It only applies if path_specs are not specified.",
+    )
 
-logger = logging.getLogger(__name__)
 
+class S3DatasetLineageProviderConfigBase(ConfigModel):
+    """
+    Any source that produces s3 lineage from/to Datasets should inherit this class.
+    This is needeed to group all lineage related configs under `s3_lineage_config` config property.
+    """
 
-class BigQueryConfig(BigQueryBaseConfig, BaseTimeWindowConfig, SQLAlchemyConfig):
-    scheme: str = "bigquery"
-    project_id: Optional[str] = pydantic.Field(
-        default=None,
-        description="Project ID where you have rights to run queries and create tables. If `storage_project_id` is not specified then it is assumed this is the same project where data is stored. If not specified, will infer from environment.",
+    s3_lineage_config: S3LineageProviderConfig = Field(
+        default=S3LineageProviderConfig(),
+        description="Common config for S3 lineage generation",
     )
-    storage_project_id: Optional[str] = pydantic.Field(
+
+
+class RedshiftUsageConfig(BaseUsageConfig, StatefulUsageConfigMixin):
+    email_domain: Optional[str] = Field(
         default=None,
-        description="If your data is stored in a different project where you don't have rights to run jobs and create tables then specify this field. The same service account must have read rights in this GCP project and write rights in `project_id`.",
-    )
-    log_page_size: pydantic.PositiveInt = pydantic.Field(
-        default=1000,
-        description="The number of log item will be queried per page for lineage collection",
-    )
-    credential: Optional[BigQueryCredential] = pydantic.Field(
-        default=None, description="BigQuery credential informations"
+        description="Email domain of your organisation so users can be displayed on UI appropriately.",
     )
-    # extra_client_options, include_table_lineage and max_query_duration are relevant only when computing the lineage.
-    extra_client_options: Dict[str, Any] = pydantic.Field(
-        default={},
-        description="Additional options to pass to google.cloud.logging_v2.client.Client.",
+
+
+class RedshiftConfig(
+    PostgresConfig,
+    DatasetLineageProviderConfigBase,
+    S3DatasetLineageProviderConfigBase,
+    RedshiftUsageConfig,
+    StatefulLineageConfigMixin,
+    StatefulProfilingConfigMixin,
+):
+    database: str = Field(default="dev", description="database")
+
+    # Although Amazon Redshift is compatible with Postgres's wire format,
+    # we actually want to use the sqlalchemy-redshift package and dialect
+    # because it has better caching behavior. In particular, it queries
+    # the full table, column, and constraint information in a single larger
+    # query, and then simply pulls out the relevant information as needed.
+    # Because of this behavior, it uses dramatically fewer round trips for
+    # large Redshift warehouses. As an example, see this query for the columns:
+    # https://github.com/sqlalchemy-redshift/sqlalchemy-redshift/blob/60b4db04c1d26071c291aeea52f1dcb5dd8b0eb0/sqlalchemy_redshift/dialect.py#L745.
+    scheme = Field(
+        default="redshift+psycopg2",
+        description="",
+        hidden_from_schema=True,
     )
-    include_table_lineage: Optional[bool] = pydantic.Field(
-        default=True,
-        description="Option to enable/disable lineage generation. Is enabled by default.",
+
+    _database_alias_deprecation = pydantic_field_deprecated(
+        "database_alias",
+        message="database_alias is deprecated. Use platform_instance instead.",
     )
-    max_query_duration: timedelta = pydantic.Field(
-        default=timedelta(minutes=15),
-        description="Correction to pad start_time and end_time with. For handling the case where the read happens within our time range but the query completion event is delayed and happens after the configured end time.",
+
+    default_schema: str = Field(
+        default="public",
+        description="The default schema to use if the sql parser fails to parse the schema with `sql_based` lineage collector",
     )
 
-    bigquery_audit_metadata_datasets: Optional[List[str]] = pydantic.Field(
-        default=None,
-        description="A list of datasets that contain a table named cloudaudit_googleapis_com_data_access which contain BigQuery audit logs, specifically, those containing BigQueryAuditMetadata. It is recommended that the project of the dataset is also specified, for example, projectA.datasetB.",
+    include_table_lineage: Optional[bool] = Field(
+        default=True, description="Whether table lineage should be ingested."
     )
-    use_exported_bigquery_audit_metadata: bool = pydantic.Field(
-        default=False,
-        description="When configured, use BigQueryAuditMetadata in bigquery_audit_metadata_datasets to compute lineage information.",
+    include_copy_lineage: Optional[bool] = Field(
+        default=True,
+        description="Whether lineage should be collected from copy commands",
     )
-    use_date_sharded_audit_log_tables: bool = pydantic.Field(
+
+    include_usage_statistics: bool = Field(
         default=False,
-        description="Whether to read date sharded tables or time partitioned tables when extracting usage from exported audit logs.",
+        description="Generate usage statistic. email_domain config parameter needs to be set if enabled",
     )
-    _credentials_path: Optional[str] = pydantic.PrivateAttr(None)
-    use_v2_audit_metadata: Optional[bool] = pydantic.Field(
-        default=False, description="Whether to ingest logs using the v2 format."
+
+    include_unload_lineage: Optional[bool] = Field(
+        default=True,
+        description="Whether lineage should be collected from unload commands",
     )
-    upstream_lineage_in_report: bool = pydantic.Field(
+
+    capture_lineage_query_parser_failures: Optional[bool] = Field(
+        hide_from_schema=True,
         default=False,
-        description="Useful for debugging lineage information. Set to True to see the raw lineage created internally.",
+        description="Whether to capture lineage query parser errors with dataset properties for debugging",
     )
 
-    def __init__(self, **data: Any):
-        super().__init__(**data)
+    table_lineage_mode: Optional[LineageMode] = Field(
+        default=LineageMode.STL_SCAN_BASED,
+        description="Which table lineage collector mode to use. Available modes are: [stl_scan_based, sql_based, mixed]",
+    )
+    extra_client_options: Dict[str, Any] = {}
 
-        if self.credential:
-            self._credentials_path = self.credential.create_credential_temp_file()
-            logger.debug(
-                f"Creating temporary credential file at {self._credentials_path}"
-            )
-            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = self._credentials_path
-
-    def get_sql_alchemy_url(self, run_on_compute: bool = False) -> str:
-        if self.storage_project_id and not run_on_compute:
-            return f"{self.scheme}://{self.storage_project_id}"
-        if self.project_id:
-            return f"{self.scheme}://{self.project_id}"
-        # When project_id is not set, we will attempt to detect the project ID
-        # based on the credentials or environment variables.
-        # See https://github.com/mxmzdlv/pybigquery#authentication.
-        return f"{self.scheme}://"
-
-    @pydantic.validator("platform_instance")
-    def bigquery_doesnt_need_platform_instance(cls, v):
-        if v is not None:
-            raise ConfigurationError(
-                "BigQuery project ids are globally unique. You do not need to specify a platform instance."
-            )
-
-    @pydantic.root_validator()
-    def validate_that_bigquery_audit_metadata_datasets_is_correctly_configured(
-        cls, values: Dict[str, Any]
-    ) -> Dict[str, Any]:
-        if (
-            values.get("use_exported_bigquery_audit_metadata")
-            and not values.get("use_v2_audit_metadata")
-            and not values.get("bigquery_audit_metadata_datasets")
-        ):
-            raise ConfigurationError(
-                "bigquery_audit_metadata_datasets must be specified if using exported audit metadata. Otherwise set use_v2_audit_metadata to True."
-            )
+    @root_validator(pre=True)
+    def check_email_is_set_on_usage(cls, values):
+        if values.get("include_usage_statistics"):
+            assert (
+                "email_domain" in values and values["email_domain"]
+            ), "email_domain needs to be set if usage is enabled"
         return values
 
-    @pydantic.validator("platform")
-    def platform_is_always_bigquery(cls, v):
-        return "bigquery"
+    @root_validator()
+    def check_database_or_database_alias_set(cls, values):
+        assert values.get("database") or values.get(
+            "database_alias"
+        ), "either database or database_alias must be set"
+        return values
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_config/sql/snowflake.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/sql/snowflake.py`

 * *Files 16% similar despite different names*

```diff
@@ -8,21 +8,17 @@
 from snowflake.connector.network import (
     DEFAULT_AUTHENTICATOR,
     EXTERNAL_BROWSER_AUTHENTICATOR,
     KEY_PAIR_AUTHENTICATOR,
     OAUTH_AUTHENTICATOR,
 )
 
-from datahub.configuration.common import (
-    AllowDenyPattern,
-    ConfigModel,
-    ConfigurationError,
-    OauthConfiguration,
-)
+from datahub.configuration.common import AllowDenyPattern, OauthConfiguration
 from datahub.configuration.time_window_config import BaseTimeWindowConfig
+from datahub.configuration.validate_field_rename import pydantic_renamed_field
 from datahub.ingestion.source.snowflake.constants import (
     CLIENT_PREFETCH_THREADS,
     CLIENT_SESSION_KEEP_ALIVE,
 )
 from datahub.ingestion.source.sql.oauth_generator import OauthTokenGenerator
 from datahub.ingestion.source.sql.sql_config import (
     SQLAlchemyConfig,
@@ -44,67 +40,14 @@
     "KEY_PAIR_AUTHENTICATOR": KEY_PAIR_AUTHENTICATOR,
     "OAUTH_AUTHENTICATOR": OAUTH_AUTHENTICATOR,
 }
 
 SNOWFLAKE_HOST_SUFFIX = ".snowflakecomputing.com"
 
 
-class SnowflakeProvisionRoleConfig(ConfigModel):
-    enabled: bool = pydantic.Field(
-        default=False,
-        description="Whether provisioning of Snowflake role (used for ingestion) is enabled or not.",
-    )
-
-    # Can be used by account admin to test what sql statements will be run
-    dry_run: bool = pydantic.Field(
-        default=False,
-        description="If provision_role is enabled, whether to dry run the sql commands for system admins to see what sql grant commands would be run without actually running the grant commands.",
-    )
-
-    # Setting this to True is helpful in case you want a clean role without any extra privileges
-    # Not set to True by default because multiple parallel
-    #   snowflake ingestions can be dependent on single role
-    drop_role_if_exists: bool = pydantic.Field(
-        default=False,
-        description="Useful during testing to ensure you have a clean slate role. Not recommended for production use cases.",
-    )
-
-    # When Account admin is testing they might not want to actually do the ingestion
-    # Set this to False in case the account admin would want to
-    #   create role
-    #   grant role to user in main config
-    #   run ingestion as the user in main config
-    run_ingestion: bool = pydantic.Field(
-        default=False,
-        description="If system admins wish to skip actual ingestion of metadata during testing of the provisioning of role.",
-    )
-
-    admin_role: Optional[str] = pydantic.Field(
-        default="accountadmin",
-        description="The Snowflake role of admin user used for provisioning of the role specified by role config. System admins can audit the open source code and decide to use a different role.",
-    )
-
-    admin_username: str = pydantic.Field(
-        description="The username to be used for provisioning of role."
-    )
-
-    admin_password: Optional[pydantic.SecretStr] = pydantic.Field(
-        default=None,
-        exclude=True,
-        description="The password to be used for provisioning of role.",
-    )
-
-    @pydantic.validator("admin_username", always=True)
-    def username_not_empty(cls, v, values, **kwargs):
-        v_str: str = str(v)
-        if not v_str.strip():
-            raise ValueError("username is empty")
-        return v
-
-
 class BaseSnowflakeConfig(BaseTimeWindowConfig):
     # Note: this config model is also used by the snowflake-usage source.
 
     scheme: str = "snowflake"
     username: Optional[str] = pydantic.Field(
         default=None, description="Snowflake username."
     )
@@ -130,21 +73,17 @@
         default=None,
         description="oauth configuration - https://docs.snowflake.com/en/user-guide/python-connector-example.html#connecting-with-oauth",
     )
     authentication_type: str = pydantic.Field(
         default="DEFAULT_AUTHENTICATOR",
         description='The type of authenticator to use when connecting to Snowflake. Supports "DEFAULT_AUTHENTICATOR", "EXTERNAL_BROWSER_AUTHENTICATOR" and "KEY_PAIR_AUTHENTICATOR".',
     )
-    host_port: Optional[str] = pydantic.Field(
-        default=None, description="DEPRECATED: Snowflake account. e.g. abc48144"
-    )  # Deprecated
-    account_id: Optional[str] = pydantic.Field(
-        default=None,
+    account_id: str = pydantic.Field(
         description="Snowflake account identifier. e.g. xy12345,  xy12345.us-east-2.aws, xy12345.us-central1.gcp, xy12345.central-us.azure, xy12345.us-west-2.privatelink. Refer [Account Identifiers](https://docs.snowflake.com/en/user-guide/admin-account-identifier.html#format-2-legacy-account-locator-in-a-region) for more details.",
-    )  # Once host_port is removed this will be made mandatory
+    )
     warehouse: Optional[str] = pydantic.Field(
         default=None, description="Snowflake warehouse."
     )
     role: Optional[str] = pydantic.Field(default=None, description="Snowflake role.")
     include_table_lineage: bool = pydantic.Field(
         default=True,
         description="If enabled, populates the snowflake table-to-table and s3-to-snowflake table lineage. Requires appropriate grants given to the role and Snowflake Enterprise Edition or above.",
@@ -154,59 +93,43 @@
         description="If enabled, populates the snowflake view->table and table->view lineages (no view->view lineage yet). Requires appropriate grants given to the role, and include_table_lineage to be True. view->table lineage requires Snowflake Enterprise Edition or above.",
     )
     connect_args: Optional[Dict[str, Any]] = pydantic.Field(
         default=None,
         description="Connect args to pass to Snowflake SqlAlchemy driver",
         exclude=True,
     )
-    check_role_grants: bool = pydantic.Field(
-        default=False,
-        description="If set to True then checks role grants at the beginning of the ingestion run. To be used for debugging purposes. If you think everything is working fine then set it to False. In some cases this can take long depending on how many roles you might have.",
-    )
 
     def get_account(self) -> str:
         assert self.account_id
         return self.account_id
 
-    @pydantic.root_validator
-    def one_of_host_port_or_account_id_is_required(cls, values):
-        host_port = values.get("host_port")
-        if host_port is not None:
-            logger.warning(
-                "snowflake's `host_port` option has been deprecated; use account_id instead"
-            )
-            host_port = remove_protocol(host_port)
-            host_port = remove_trailing_slashes(host_port)
-            host_port = remove_suffix(host_port, SNOWFLAKE_HOST_SUFFIX)
-            values["host_port"] = host_port
-        account_id: Optional[str] = values.get("account_id")
-        if account_id is None:
-            if host_port is None:
-                raise ConfigurationError(
-                    "One of account_id (recommended) or host_port (deprecated) is required"
-                )
-            else:
-                values["account_id"] = host_port
-        else:
-            account_id = remove_protocol(account_id)
-            account_id = remove_trailing_slashes(account_id)
-            account_id = remove_suffix(account_id, SNOWFLAKE_HOST_SUFFIX)
-            if account_id != values["account_id"]:
-                logger.info(f"Using {account_id} as `account_id`.")
-                values["account_id"] = account_id
+    rename_host_port_to_account_id = pydantic_renamed_field("host_port", "account_id")
 
-        return values
+    @pydantic.validator("account_id")
+    def validate_account_id(cls, account_id: str) -> str:
+        account_id = remove_protocol(account_id)
+        account_id = remove_trailing_slashes(account_id)
+        account_id = remove_suffix(account_id, SNOWFLAKE_HOST_SUFFIX)
+        return account_id
 
     @pydantic.validator("authentication_type", always=True)
     def authenticator_type_is_valid(cls, v, values, field):
         if v not in VALID_AUTH_TYPES.keys():
             raise ValueError(
                 f"unsupported authenticator type '{v}' was provided,"
                 f" use one of {list(VALID_AUTH_TYPES.keys())}"
             )
+        if (
+            values.get("private_key") is not None
+            or values.get("private_key_path") is not None
+        ) and v != "KEY_PAIR_AUTHENTICATOR":
+            raise ValueError(
+                f"Either `private_key` and `private_key_path` is set but `authentication_type` is {v}. "
+                f"Should be set to 'KEY_PAIR_AUTHENTICATOR' when using key pair authentication"
+            )
         if v == "KEY_PAIR_AUTHENTICATOR":
             # If we are using key pair auth, we need the private key path and password to be set
             if (
                 values.get("private_key") is None
                 and values.get("private_key_path") is None
             ):
                 raise ValueError(
@@ -350,15 +273,14 @@
 
 
 class SnowflakeConfig(BaseSnowflakeConfig, SQLAlchemyConfig):
     database_pattern: AllowDenyPattern = AllowDenyPattern(
         deny=[r"^UTIL_DB$", r"^SNOWFLAKE$", r"^SNOWFLAKE_SAMPLE_DATA$"]
     )
 
-    provision_role: Optional[SnowflakeProvisionRoleConfig] = None
     ignore_start_time_lineage: bool = False
     upstream_lineage_in_report: bool = False
 
     def get_sql_alchemy_url(
         self,
         database: Optional[str] = None,
         username: Optional[str] = None,
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_config/usage/bigquery_usage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/usage/bigquery_usage.py`

 * *Files 4% similar despite different names*

```diff
@@ -5,15 +5,16 @@
 from datetime import timedelta
 from typing import Any, Dict, List, Optional
 
 import pydantic
 
 from datahub.configuration import ConfigModel
 from datahub.configuration.common import AllowDenyPattern, ConfigurationError
-from datahub.configuration.source_common import DatasetSourceConfigBase
+from datahub.configuration.source_common import EnvConfigMixin
+from datahub.configuration.validate_field_removal import pydantic_removed_field
 from datahub.ingestion.source.usage.usage_common import BaseUsageConfig
 from datahub.ingestion.source_config.bigquery import BigQueryBaseConfig
 
 logger = logging.getLogger(__name__)
 
 
 class BigQueryCredential(ConfigModel):
@@ -54,15 +55,15 @@
     def create_credential_temp_file(self) -> str:
         with tempfile.NamedTemporaryFile(delete=False) as fp:
             cred_json = json.dumps(self.dict(), indent=4, separators=(",", ": "))
             fp.write(cred_json.encode())
             return fp.name
 
 
-class BigQueryUsageConfig(BigQueryBaseConfig, DatasetSourceConfigBase, BaseUsageConfig):
+class BigQueryUsageConfig(BigQueryBaseConfig, EnvConfigMixin, BaseUsageConfig):
     projects: Optional[List[str]] = pydantic.Field(
         default=None,
         description="List of project ids to ingest usage from. If not specified, will infer from environment.",
     )
     project_id: Optional[str] = pydantic.Field(
         default=None,
         description="Project ID to ingest usage from. If not specified, will infer from environment. Deprecated in favour of projects ",
@@ -135,23 +136,18 @@
     def note_project_id_deprecation(cls, v, values, **kwargs):
         logger.warning(
             "bigquery-usage project_id option is deprecated; use projects instead"
         )
         values["projects"] = [v]
         return None
 
-    @pydantic.validator("platform")
-    def platform_is_always_bigquery(cls, v):
-        return "bigquery"
-
-    @pydantic.validator("platform_instance")
-    def bigquery_platform_instance_is_meaningless(cls, v):
-        raise ConfigurationError(
-            "BigQuery project-ids are globally unique. You don't need to provide a platform_instance"
-        )
+    # BigQuery project-ids are globally unique.
+    platform_instance_not_supported_for_bigquery = pydantic_removed_field(
+        "platform_instance"
+    )
 
     @pydantic.validator("use_exported_bigquery_audit_metadata")
     def use_exported_bigquery_audit_metadata_uses_v2(cls, v, values):
         if v is True and not values["use_v2_audit_metadata"]:
             raise ConfigurationError(
                 "To use exported BigQuery audit metadata, you must also use v2 audit metadata"
             )
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_config/usage/snowflake_usage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_config/usage/snowflake_usage.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_report/pulsar.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/pulsar.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_report/sql/bigquery.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/sql/bigquery.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_report/sql/snowflake.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/sql/snowflake.py`

 * *Files 10% similar despite different names*

```diff
@@ -21,21 +21,17 @@
     upstream_lineage_in_report: Optional[bool] = None
     upstream_lineage: Dict[str, List[str]] = field(default_factory=dict)
     lineage_start_time: Optional[datetime] = None
     lineage_end_time: Optional[datetime] = None
 
     cleaned_account_id: str = ""
     run_ingestion: bool = False
-    provision_role_done: bool = False
-    provision_role_success: bool = False
 
     # https://community.snowflake.com/s/topic/0TO0Z000000Unu5WAC/releases
     saas_version: Optional[str] = None
     default_warehouse: Optional[str] = None
     default_db: Optional[str] = None
     default_schema: Optional[str] = None
     role: str = ""
-    check_role_grants: Optional[bool] = None
-    role_grants: List[str] = field(default_factory=list)
 
     profile_if_updated_since: Optional[datetime] = None
     profile_candidates: Dict[str, List[str]] = field(default_factory=dict)
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_report/usage/bigquery_usage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/usage/bigquery_usage.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/source_report/usage/snowflake_usage.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/source_report/usage/snowflake_usage.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/add_dataset_browse_path.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_browse_path.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/add_dataset_ownership.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_ownership.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/add_dataset_properties.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_properties.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/add_dataset_schema_tags.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_schema_tags.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/add_dataset_schema_terms.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_schema_terms.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/add_dataset_tags.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_tags.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/add_dataset_terms.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/add_dataset_terms.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/base_transformer.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/base_transformer.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,25 +1,22 @@
 import logging
 from abc import ABCMeta, abstractmethod
-from typing import Any, Dict, Iterable, List, Optional, Type, Union
+from typing import Any, Dict, Iterable, List, Optional, Union
 
 import datahub.emitter.mce_builder as builder
 from datahub.emitter.aspect import ASPECT_MAP
 from datahub.emitter.mce_builder import Aspect
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.ingestion.api.common import ControlRecord, EndOfStream, RecordEnvelope
 from datahub.ingestion.api.transform import Transformer
 from datahub.metadata.schema_classes import (
-    DataFlowSnapshotClass,
-    DataJobSnapshotClass,
-    DatasetSnapshotClass,
     MetadataChangeEventClass,
     MetadataChangeProposalClass,
 )
-from datahub.utilities.urns.urn import Urn
+from datahub.utilities.urns.urn import Urn, guess_entity_type
 
 log = logging.getLogger(__name__)
 
 
 class LegacyMCETransformer(Transformer, metaclass=ABCMeta):
     @abstractmethod
     def transform_one(self, mce: MetadataChangeEventClass) -> MetadataChangeEventClass:
@@ -52,19 +49,14 @@
     @abstractmethod
     def entity_types(self) -> List[str]:
         """Implement this method to specify which entity types the transformer is interested in subscribing to. Defaults to ALL (encoded as "*")"""
         return ["*"]
 
     def __init__(self):
         self.entity_map: Dict[str, Dict[str, Any]] = {}
-        self.entity_type_mappings: Dict[str, Type] = {
-            "dataset": DatasetSnapshotClass,
-            "dataFlow": DataFlowSnapshotClass,
-            "dataJob": DataJobSnapshotClass,
-        }
         mixedin = False
         for mixin in [LegacyMCETransformer, SingleAspectTransformer]:
             mixedin = mixedin or isinstance(self, mixin)
         if not mixedin:
             assert (
                 "Class does not implement one of required traits {self.allowed_mixins}"
             )
@@ -79,22 +71,16 @@
             # all control events should be processed
             return True
 
         entity_types = self.entity_types()
         if "*" in entity_types:
             return True
         if isinstance(record, MetadataChangeEventClass):
-            for e in entity_types:
-                assert (
-                    e in self.entity_type_mappings
-                ), f"Do not have a class mapping for {e}. Subscription to this entity will not work for transforming MCE-s"
-                if isinstance(record.proposedSnapshot, self.entity_type_mappings[e]):
-                    return True
-            # fall through, no entity type matched
-            return False
+            entity_type = guess_entity_type(record.proposedSnapshot.urn)
+            return entity_type in entity_types
         elif isinstance(
             record, (MetadataChangeProposalWrapper, MetadataChangeProposalClass)
         ):
             return record.entityType in entity_types
 
         # default to process everything that is not caught by above checks
         return True
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/dataset_domain.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/dataset_domain.py`

 * *Files 0% similar despite different names*

```diff
@@ -151,14 +151,15 @@
         def resolve_domain(domain_urn: str) -> DomainsClass:
             domains = domain_pattern.value(domain_urn)
             return self.get_domain_class(ctx.graph, domains)
 
         generic_config = AddDatasetDomainSemanticsConfig(
             get_domains_to_add=resolve_domain,
             semantics=config.semantics,
+            replace_existing=config.replace_existing,
         )
         super().__init__(generic_config, ctx)
 
     @classmethod
     def create(
         cls, config_dict: dict, ctx: PipelineContext
     ) -> "PatternAddDatasetDomain":
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/dataset_transformer.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/dataset_transformer.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/mark_dataset_status.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/mark_dataset_status.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/ingestion/transformer/remove_dataset_ownership.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/ingestion/transformer/remove_dataset_ownership.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/integrations/great_expectations/action.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/integrations/great_expectations/action.py`

 * *Files 1% similar despite different names*

```diff
@@ -83,28 +83,30 @@
         token: Optional[str] = None,
         timeout_sec: Optional[float] = None,
         retry_status_codes: Optional[List[int]] = None,
         retry_max_times: Optional[int] = None,
         extra_headers: Optional[Dict[str, str]] = None,
         exclude_dbname: Optional[bool] = None,
         parse_table_names_from_sql: bool = False,
+        convert_urns_to_lowercase: bool = False,
     ):
         super().__init__(data_context)
         self.server_url = server_url
         self.env = env
         self.platform_alias = platform_alias
         self.platform_instance_map = platform_instance_map
         self.graceful_exceptions = graceful_exceptions
         self.token = token
         self.timeout_sec = timeout_sec
         self.retry_status_codes = retry_status_codes
         self.retry_max_times = retry_max_times
         self.extra_headers = extra_headers
         self.exclude_dbname = exclude_dbname
         self.parse_table_names_from_sql = parse_table_names_from_sql
+        self.convert_urns_to_lowercase = convert_urns_to_lowercase
 
     def _run(
         self,
         validation_result_suite: ExpectationSuiteValidationResult,
         validation_result_suite_identifier: Union[
             ValidationResultIdentifier, "GXCloudIdentifier"
         ],
@@ -589,14 +591,15 @@
                     table_name,
                     self.env,
                     self.get_platform_instance(
                         data_asset.active_batch_definition.datasource_name
                     ),
                     self.exclude_dbname,
                     self.platform_alias,
+                    self.convert_urns_to_lowercase,
                 )
                 batchSpec = BatchSpec(
                     nativeBatchId=batch_identifier,
                     customProperties=batchSpecProperties,
                 )
 
                 splitter_method = ge_batch_spec.get("splitter_method")
@@ -657,14 +660,15 @@
                         table,
                         self.env,
                         self.get_platform_instance(
                             data_asset.active_batch_definition.datasource_name
                         ),
                         self.exclude_dbname,
                         self.platform_alias,
+                        self.convert_urns_to_lowercase,
                     )
                     dataset_partitions.append(
                         {
                             "dataset_urn": dataset_urn,
                             "partitionSpec": partitionSpec,
                             "batchSpec": batchSpec,
                         }
@@ -699,14 +703,15 @@
     sqlalchemy_uri,
     schema_name,
     table_name,
     env,
     platform_instance=None,
     exclude_dbname=None,
     platform_alias=None,
+    convert_urns_to_lowercase=False,
 ):
     data_platform = get_platform_from_sqlalchemy_uri(str(sqlalchemy_uri))
     url_instance = make_url(sqlalchemy_uri)
 
     if schema_name is None and "." in table_name:
         schema_name, table_name = table_name.split(".")[-2:]
 
@@ -773,14 +778,17 @@
         warn(
             f"DataHubValidationAction failed to locate schema name for {data_platform}."
         )
         return None
 
     dataset_name = f"{schema_name}.{table_name}"
 
+    if convert_urns_to_lowercase:
+        dataset_name = dataset_name.lower()
+
     dataset_urn = builder.make_dataset_urn_with_platform_instance(
         platform=data_platform if platform_alias is None else platform_alias,
         name=dataset_name,
         platform_instance=platform_instance,
         env=env,
     )
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/lite/duckdb_lite.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/lite/duckdb_lite.py`

 * *Files 1% similar despite different names*

```diff
@@ -2,19 +2,19 @@
 import logging
 import pathlib
 import time
 from typing import Any, Dict, Iterable, List, Optional, Type, Union
 
 import duckdb
 
-from datahub.configuration.common import ConfigModel
 from datahub.emitter.aspect import ASPECT_MAP
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
 from datahub.emitter.mcp_builder import mcps_from_mce
 from datahub.emitter.serialization_helper import post_json_transform
+from datahub.lite.duckdb_lite_config import DuckDBLiteConfig
 from datahub.lite.lite_local import (
     AutoComplete,
     Browseable,
     DataHubLiteLocal,
     PathNotFoundException,
     Searchable,
     SearchFlavor,
@@ -35,20 +35,14 @@
 from datahub.utilities.urns.data_platform_urn import DataPlatformUrn
 from datahub.utilities.urns.dataset_urn import DatasetUrn
 from datahub.utilities.urns.urn import Urn
 
 logger = logging.getLogger(__name__)
 
 
-class DuckDBLiteConfig(ConfigModel):
-    file: str
-    read_only: bool = False
-    options: dict = {}
-
-
 class DuckDBLite(DataHubLiteLocal[DuckDBLiteConfig]):
     @classmethod
     def create(cls, config_dict: dict) -> "DuckDBLite":
         config: DuckDBLiteConfig = DuckDBLiteConfig.parse_obj(config_dict)
         return DuckDBLite(config)
 
     def __init__(self, config: DuckDBLiteConfig) -> None:
@@ -674,15 +668,14 @@
             self.add_edge(parent_urn, "child", type_urn)
             self.add_edge(type_urn, "child", entity_urn)
             self.add_edge(type_urn, "name", pluralize(t), remove_existing=True)
 
     def _create_edges_from_data_platform_instance(
         self, data_platform_instance_urn: Urn
     ) -> None:
-
         data_platform_urn = DataPlatformUrn.create_from_string(
             data_platform_instance_urn.get_entity_id()[0]
         )
         data_platform_instances_urn = Urn(
             entity_type="systemNode", entity_id=[str(data_platform_urn), "instances"]
         )
 
@@ -721,15 +714,14 @@
             str(data_platform_instance_urn),
             data_platform_instance_urn.get_entity_id()[-1],
         )
 
     def post_update_hook(
         self, entity_urn: str, aspect_name: str, aspect: _Aspect
     ) -> None:
-
         if isinstance(aspect, DatasetPropertiesClass):
             dp: DatasetPropertiesClass = aspect
             if dp.name:
                 specific_urn = DatasetUrn.create_from_string(entity_urn)
                 if (
                     specific_urn.get_data_platform_urn().get_entity_id_as_string()
                     == "looker"
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/lite/lite_server.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/lite/lite_server.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/lite/lite_util.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/lite/lite_util.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/assertion/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/assertion/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/chart/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/chart/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/common/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/common/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/dashboard/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dashboard/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/datahub/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/datajob/datahub/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/dataprocess/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dataprocess/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/dataset/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/dataset/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/execution/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/execution/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/identity/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/identity/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/ingestion/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/ingestion/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/key/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/key/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -37,14 +37,15 @@
 from ......schema_classes import MLFeatureTableKeyClass
 from ......schema_classes import MLModelDeploymentKeyClass
 from ......schema_classes import MLModelGroupKeyClass
 from ......schema_classes import MLModelKeyClass
 from ......schema_classes import MLPrimaryKeyKeyClass
 from ......schema_classes import NotebookKeyClass
 from ......schema_classes import PostKeyClass
+from ......schema_classes import QueryKeyClass
 from ......schema_classes import SchemaFieldKeyClass
 from ......schema_classes import TagKeyClass
 from ......schema_classes import TelemetryKeyClass
 from ......schema_classes import TestKeyClass
 
 
 AssertionKey = AssertionKeyClass
@@ -79,13 +80,14 @@
 MLFeatureTableKey = MLFeatureTableKeyClass
 MLModelDeploymentKey = MLModelDeploymentKeyClass
 MLModelGroupKey = MLModelGroupKeyClass
 MLModelKey = MLModelKeyClass
 MLPrimaryKeyKey = MLPrimaryKeyKeyClass
 NotebookKey = NotebookKeyClass
 PostKey = PostKeyClass
+QueryKey = QueryKeyClass
 SchemaFieldKey = SchemaFieldKeyClass
 TagKey = TagKeyClass
 TelemetryKey = TelemetryKeyClass
 TestKey = TestKeyClass
 
 # fmt: on
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/snapshot/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/metadata/snapshot/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/ml/metadata/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/ml/metadata/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/mxe/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/mxe/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/notebook/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/notebook/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/policy/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/policy/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/retention/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/retention/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/schema/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/schema/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/test/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/test/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/timeseries/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/timeseries/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/com/linkedin/pegasus2avro/usage/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/com/linkedin/pegasus2avro/usage/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schema.avsc` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/schema.avsc`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.47695219418016704%*

 * *Differences: {'1': "{'Aspect': {'name': 'editableDataJobProperties'}, 'name': 'EditableDataJobProperties', "*

 * *      "'namespace': 'com.linkedin.pegasus2avro.datajob', 'fields': {0: {'type': {replace: "*

 * *      "OrderedDict([('type', 'record'), ('name', 'AuditStamp'), ('namespace', "*

 * *      "'com.linkedin.pegasus2avro.common'), ('fields', [OrderedDict([('type', 'long'), ('name', "*

 * *      "'time'), ('doc', 'When did the resource/association/sub-resource move into the specific "*

 * *      "lifecycle stage represented by this AuditE []*

```diff
@@ -1,218 +1,43 @@
 [
     "null",
     {
         "Aspect": {
-            "name": "dataHubViewInfo"
+            "name": "editableDataJobProperties"
         },
-        "doc": "Information about a DataHub View. -- TODO: Understand whether an entity type filter is required.",
+        "doc": "Stores editable changes made to properties. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines",
         "fields": [
             {
-                "Searchable": {
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "The name of the View",
-                "name": "name",
-                "type": "string"
-            },
-            {
-                "default": null,
-                "doc": "Description of the view",
-                "name": "description",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "Searchable": {},
-                "doc": "The type of View",
-                "name": "type",
-                "type": {
-                    "name": "DataHubViewType",
-                    "namespace": "com.linkedin.pegasus2avro.view",
-                    "symbolDocs": {
-                        "GLOBAL": "A global view, which all users can see and use.",
-                        "PERSONAL": "A view private for a specific person."
-                    },
-                    "symbols": [
-                        "PERSONAL",
-                        "GLOBAL"
-                    ],
-                    "type": "enum"
-                }
-            },
-            {
-                "doc": "The view itself",
-                "name": "definition",
-                "type": {
-                    "doc": "A View definition.",
-                    "fields": [
-                        {
-                            "doc": "The Entity Types in the scope of the View.",
-                            "name": "entityTypes",
-                            "type": {
-                                "items": "string",
-                                "type": "array"
-                            }
-                        },
-                        {
-                            "doc": "The filter criteria, which represents the view itself",
-                            "name": "filter",
-                            "type": {
-                                "doc": "The filter for finding a record or a collection of records",
-                                "fields": [
-                                    {
-                                        "default": null,
-                                        "doc": "A list of disjunctive criterion for the filter. (or operation to combine filters)",
-                                        "name": "or",
-                                        "type": [
-                                            "null",
-                                            {
-                                                "items": {
-                                                    "doc": "A list of criterion and'd together.",
-                                                    "fields": [
-                                                        {
-                                                            "doc": "A list of and criteria the filter applies to the query",
-                                                            "name": "and",
-                                                            "type": {
-                                                                "items": {
-                                                                    "doc": "A criterion for matching a field with given value",
-                                                                    "fields": [
-                                                                        {
-                                                                            "doc": "The name of the field that the criterion refers to",
-                                                                            "name": "field",
-                                                                            "type": "string"
-                                                                        },
-                                                                        {
-                                                                            "doc": "The value of the intended field",
-                                                                            "name": "value",
-                                                                            "type": "string"
-                                                                        },
-                                                                        {
-                                                                            "default": [],
-                                                                            "doc": "Values. one of which the intended field should match\nNote, if values is set, the above \"value\" field will be ignored",
-                                                                            "name": "values",
-                                                                            "type": {
-                                                                                "items": "string",
-                                                                                "type": "array"
-                                                                            }
-                                                                        },
-                                                                        {
-                                                                            "default": "EQUAL",
-                                                                            "doc": "The condition for the criterion, e.g. EQUAL, START_WITH",
-                                                                            "name": "condition",
-                                                                            "type": {
-                                                                                "doc": "The matching condition in a filter criterion",
-                                                                                "name": "Condition",
-                                                                                "namespace": "com.linkedin.pegasus2avro.metadata.query.filter",
-                                                                                "symbolDocs": {
-                                                                                    "CONTAIN": "Represent the relation: String field contains value, e.g. name contains Profile",
-                                                                                    "END_WITH": "Represent the relation: String field ends with value, e.g. name ends with Event",
-                                                                                    "EQUAL": "Represent the relation: field = value, e.g. platform = hdfs",
-                                                                                    "GREATER_THAN": "Represent the relation greater than, e.g. ownerCount > 5",
-                                                                                    "GREATER_THAN_OR_EQUAL_TO": "Represent the relation greater than or equal to, e.g. ownerCount >= 5",
-                                                                                    "IN": "Represent the relation: String field is one of the array values to, e.g. name in [\"Profile\", \"Event\"]",
-                                                                                    "IS_NULL": "Represent the relation: field is null, e.g. platform is null",
-                                                                                    "LESS_THAN": "Represent the relation less than, e.g. ownerCount < 3",
-                                                                                    "LESS_THAN_OR_EQUAL_TO": "Represent the relation less than or equal to, e.g. ownerCount <= 3",
-                                                                                    "START_WITH": "Represent the relation: String field starts with value, e.g. name starts with PageView"
-                                                                                },
-                                                                                "symbols": [
-                                                                                    "CONTAIN",
-                                                                                    "END_WITH",
-                                                                                    "EQUAL",
-                                                                                    "IS_NULL",
-                                                                                    "GREATER_THAN",
-                                                                                    "GREATER_THAN_OR_EQUAL_TO",
-                                                                                    "IN",
-                                                                                    "LESS_THAN",
-                                                                                    "LESS_THAN_OR_EQUAL_TO",
-                                                                                    "START_WITH"
-                                                                                ],
-                                                                                "type": "enum"
-                                                                            }
-                                                                        },
-                                                                        {
-                                                                            "default": false,
-                                                                            "doc": "Whether the condition should be negated",
-                                                                            "name": "negated",
-                                                                            "type": "boolean"
-                                                                        }
-                                                                    ],
-                                                                    "name": "Criterion",
-                                                                    "namespace": "com.linkedin.pegasus2avro.metadata.query.filter",
-                                                                    "type": "record"
-                                                                },
-                                                                "type": "array"
-                                                            }
-                                                        }
-                                                    ],
-                                                    "name": "ConjunctiveCriterion",
-                                                    "namespace": "com.linkedin.pegasus2avro.metadata.query.filter",
-                                                    "type": "record"
-                                                },
-                                                "type": "array"
-                                            }
-                                        ]
-                                    },
-                                    {
-                                        "default": null,
-                                        "doc": "Deprecated! A list of conjunctive criterion for the filter. If \"or\" field is provided, then this field is ignored.",
-                                        "name": "criteria",
-                                        "type": [
-                                            "null",
-                                            {
-                                                "items": "com.linkedin.pegasus2avro.metadata.query.filter.Criterion",
-                                                "type": "array"
-                                            }
-                                        ]
-                                    }
-                                ],
-                                "name": "Filter",
-                                "namespace": "com.linkedin.pegasus2avro.metadata.query.filter",
-                                "type": "record"
-                            }
-                        }
-                    ],
-                    "name": "DataHubViewDefinition",
-                    "namespace": "com.linkedin.pegasus2avro.view",
-                    "type": "record"
-                }
-            },
-            {
-                "Searchable": {
-                    "/actor": {
-                        "fieldName": "createdBy",
-                        "fieldType": "URN"
-                    },
-                    "/time": {
-                        "fieldName": "createdAt",
-                        "fieldType": "DATETIME"
-                    }
+                "default": {
+                    "actor": "urn:li:corpuser:unknown",
+                    "impersonator": null,
+                    "message": null,
+                    "time": 0
                 },
-                "doc": "Audit stamp capturing the time and actor who created the View.",
+                "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
                 "name": "created",
                 "type": {
                     "doc": "Data captured on a resource/association/sub-resource level giving insight into when that resource/association/sub-resource moved into a particular lifecycle stage, and who acted to move it into that specific lifecycle stage.",
                     "fields": [
                         {
                             "doc": "When did the resource/association/sub-resource move into the specific lifecycle stage represented by this AuditEvent.",
                             "name": "time",
                             "type": "long"
                         },
                         {
+                            "Urn": "Urn",
                             "doc": "The entity (e.g. a member URN) which will be credited for moving the resource/association/sub-resource into the specific lifecycle stage. It is also the one used to authorize the change.",
                             "java": {
                                 "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                             },
                             "name": "actor",
                             "type": "string"
                         },
                         {
+                            "Urn": "Urn",
                             "default": null,
                             "doc": "The entity (e.g. a service URN) which performs the change on behalf of the Actor and must be authorized to act as the Actor.",
                             "java": {
                                 "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                             },
                             "name": "impersonator",
                             "type": [
@@ -232,359 +57,56 @@
                     ],
                     "name": "AuditStamp",
                     "namespace": "com.linkedin.pegasus2avro.common",
                     "type": "record"
                 }
             },
             {
-                "Searchable": {
-                    "/time": {
-                        "fieldName": "lastModifiedAt",
-                        "fieldType": "DATETIME"
-                    }
+                "default": {
+                    "actor": "urn:li:corpuser:unknown",
+                    "impersonator": null,
+                    "message": null,
+                    "time": 0
                 },
-                "doc": "Audit stamp capturing the time and actor who last modified the View.",
+                "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
                 "name": "lastModified",
                 "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-            }
-        ],
-        "name": "DataHubViewInfo",
-        "namespace": "com.linkedin.pegasus2avro.view",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "viewProperties"
-        },
-        "doc": "Details about a View. \ne.g. Gets activated when subTypes is view",
-        "fields": [
-            {
-                "Searchable": {
-                    "fieldType": "BOOLEAN",
-                    "weightsPerFieldValue": {
-                        "true": 0.5
-                    }
-                },
-                "doc": "Whether the view is materialized",
-                "name": "materialized",
-                "type": "boolean"
-            },
-            {
-                "doc": "The view logic",
-                "name": "viewLogic",
-                "type": "string"
-            },
-            {
-                "doc": "The view logic language / dialect",
-                "name": "viewLanguage",
-                "type": "string"
-            }
-        ],
-        "name": "ViewProperties",
-        "namespace": "com.linkedin.pegasus2avro.dataset",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "datasetUsageStatistics",
-            "type": "timeseries"
-        },
-        "doc": "Stats corresponding to dataset's usage.",
-        "fields": [
-            {
-                "doc": "The event timestamp field as epoch at UTC in milli seconds.",
-                "name": "timestampMillis",
-                "type": "long"
-            },
-            {
-                "default": null,
-                "doc": "Granularity of the event if applicable",
-                "name": "eventGranularity",
-                "type": [
-                    "null",
-                    {
-                        "doc": "Defines the size of a time window.",
-                        "fields": [
-                            {
-                                "doc": "Interval unit such as minute/hour/day etc.",
-                                "name": "unit",
-                                "type": {
-                                    "name": "CalendarInterval",
-                                    "namespace": "com.linkedin.pegasus2avro.timeseries",
-                                    "symbols": [
-                                        "SECOND",
-                                        "MINUTE",
-                                        "HOUR",
-                                        "DAY",
-                                        "WEEK",
-                                        "MONTH",
-                                        "QUARTER",
-                                        "YEAR"
-                                    ],
-                                    "type": "enum"
-                                }
-                            },
-                            {
-                                "default": 1,
-                                "doc": "How many units. Defaults to 1.",
-                                "name": "multiple",
-                                "type": "int"
-                            }
-                        ],
-                        "name": "TimeWindowSize",
-                        "namespace": "com.linkedin.pegasus2avro.timeseries",
-                        "type": "record"
-                    }
-                ]
-            },
-            {
-                "default": {
-                    "partition": "FULL_TABLE_SNAPSHOT",
-                    "timePartition": null,
-                    "type": "FULL_TABLE"
-                },
-                "doc": "The optional partition specification.",
-                "name": "partitionSpec",
-                "type": [
-                    {
-                        "doc": "Defines how the data is partitioned",
-                        "fields": [
-                            {
-                                "default": "PARTITION",
-                                "name": "type",
-                                "type": {
-                                    "name": "PartitionType",
-                                    "namespace": "com.linkedin.pegasus2avro.timeseries",
-                                    "symbols": [
-                                        "FULL_TABLE",
-                                        "QUERY",
-                                        "PARTITION"
-                                    ],
-                                    "type": "enum"
-                                }
-                            },
-                            {
-                                "TimeseriesField": {},
-                                "doc": "String representation of the partition",
-                                "name": "partition",
-                                "type": "string"
-                            },
-                            {
-                                "default": null,
-                                "doc": "Time window of the partition if applicable",
-                                "name": "timePartition",
-                                "type": [
-                                    "null",
-                                    {
-                                        "fields": [
-                                            {
-                                                "doc": "Start time as epoch at UTC.",
-                                                "name": "startTimeMillis",
-                                                "type": "long"
-                                            },
-                                            {
-                                                "doc": "The length of the window.",
-                                                "name": "length",
-                                                "type": "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
-                                            }
-                                        ],
-                                        "name": "TimeWindow",
-                                        "namespace": "com.linkedin.pegasus2avro.timeseries",
-                                        "type": "record"
-                                    }
-                                ]
-                            }
-                        ],
-                        "name": "PartitionSpec",
-                        "namespace": "com.linkedin.pegasus2avro.timeseries",
-                        "type": "record"
-                    },
-                    "null"
-                ]
-            },
-            {
-                "default": null,
-                "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
-                "name": "messageId",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "TimeseriesField": {},
-                "default": null,
-                "doc": "Unique user count",
-                "name": "uniqueUserCount",
-                "type": [
-                    "null",
-                    "int"
-                ]
             },
             {
-                "TimeseriesField": {},
                 "default": null,
-                "doc": "Total SQL query count",
-                "name": "totalSqlQueries",
-                "type": [
-                    "null",
-                    "int"
-                ]
-            },
-            {
-                "TimeseriesField": {},
-                "default": null,
-                "doc": "Frequent SQL queries; mostly makes sense for datasets in SQL databases",
-                "name": "topSqlQueries",
-                "type": [
-                    "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ]
-            },
-            {
-                "TimeseriesFieldCollection": {
-                    "key": "user"
-                },
-                "default": null,
-                "doc": "Users within this bucket, with frequency counts",
-                "name": "userCounts",
+                "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
+                "name": "deleted",
                 "type": [
                     "null",
-                    {
-                        "items": {
-                            "doc": "Records a single user's usage counts for a given resource",
-                            "fields": [
-                                {
-                                    "doc": "The unique id of the user.",
-                                    "java": {
-                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                    },
-                                    "name": "user",
-                                    "type": "string"
-                                },
-                                {
-                                    "TimeseriesField": {},
-                                    "doc": "Number of times the dataset has been used by the user.",
-                                    "name": "count",
-                                    "type": "int"
-                                },
-                                {
-                                    "TimeseriesField": {},
-                                    "default": null,
-                                    "doc": "If user_email is set, we attempt to resolve the user's urn upon ingest",
-                                    "name": "userEmail",
-                                    "type": [
-                                        "null",
-                                        "string"
-                                    ]
-                                }
-                            ],
-                            "name": "DatasetUserUsageCounts",
-                            "namespace": "com.linkedin.pegasus2avro.dataset",
-                            "type": "record"
-                        },
-                        "type": "array"
-                    }
+                    "com.linkedin.pegasus2avro.common.AuditStamp"
                 ]
             },
             {
-                "TimeseriesFieldCollection": {
-                    "key": "fieldPath"
-                },
-                "default": null,
-                "doc": "Field-level usage stats",
-                "name": "fieldCounts",
-                "type": [
-                    "null",
-                    {
-                        "items": {
-                            "doc": "Records field-level usage counts for a given dataset",
-                            "fields": [
-                                {
-                                    "doc": "The name of the field.",
-                                    "name": "fieldPath",
-                                    "type": "string"
-                                },
-                                {
-                                    "TimeseriesField": {},
-                                    "doc": "Number of times the field has been used.",
-                                    "name": "count",
-                                    "type": "int"
-                                }
-                            ],
-                            "name": "DatasetFieldUsageCounts",
-                            "namespace": "com.linkedin.pegasus2avro.dataset",
-                            "type": "record"
-                        },
-                        "type": "array"
-                    }
-                ]
-            }
-        ],
-        "name": "DatasetUsageStatistics",
-        "namespace": "com.linkedin.pegasus2avro.dataset",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "datasetDeprecation"
-        },
-        "Deprecated": true,
-        "doc": "Dataset deprecation status\nDeprecated! This aspect is deprecated in favor of the more-general-purpose 'Deprecation' aspect.",
-        "fields": [
-            {
                 "Searchable": {
-                    "fieldType": "BOOLEAN",
-                    "weightsPerFieldValue": {
-                        "true": 0.5
-                    }
+                    "fieldName": "editedDescription",
+                    "fieldType": "TEXT"
                 },
-                "doc": "Whether the dataset is deprecated by owner.",
-                "name": "deprecated",
-                "type": "boolean"
-            },
-            {
                 "default": null,
-                "doc": "The time user plan to decommission this dataset.",
-                "name": "decommissionTime",
-                "type": [
-                    "null",
-                    "long"
-                ]
-            },
-            {
-                "doc": "Additional information about the dataset deprecation plan, such as the wiki, doc, RB.",
-                "name": "note",
-                "type": "string"
-            },
-            {
-                "default": null,
-                "doc": "The corpuser URN which will be credited for modifying this deprecation content.",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "actor",
+                "doc": "Edited documentation of the data job ",
+                "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             }
         ],
-        "name": "DatasetDeprecation",
-        "namespace": "com.linkedin.pegasus2avro.dataset",
+        "name": "EditableDataJobProperties",
+        "namespace": "com.linkedin.pegasus2avro.datajob",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "datasetProperties"
+            "name": "dataFlowInfo"
         },
-        "doc": "Properties associated with a Dataset",
+        "doc": "Information about a Data processing flow",
         "fields": [
             {
                 "Searchable": {
                     "/*": {
                         "queryByDefault": true
                     }
                 },
@@ -611,58 +133,39 @@
             },
             {
                 "Searchable": {
                     "boostScore": 10.0,
                     "enableAutocomplete": true,
                     "fieldType": "TEXT_PARTIAL"
                 },
-                "default": null,
-                "doc": "Display name of the Dataset",
+                "doc": "Flow name",
                 "name": "name",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "Searchable": {
-                    "addToFilters": false,
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT"
-                },
-                "default": null,
-                "doc": "Fully-qualified name of the Dataset",
-                "name": "qualifiedName",
-                "type": [
-                    "null",
-                    "string"
-                ]
+                "type": "string"
             },
             {
                 "Searchable": {
                     "fieldType": "TEXT",
                     "hasValuesFieldName": "hasDescription"
                 },
                 "default": null,
-                "doc": "Documentation of the dataset",
+                "doc": "Flow description",
                 "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
-                "default": null,
-                "deprecated": "Use ExternalReference.externalUrl field instead.",
-                "doc": "The abstracted URI such as hdfs:///data/tracking/PageViewEvent, file:///dir/file_name. Uri should not include any environment specific properties. Some datasets might not have a standardized uri, which makes this field optional (i.e. kafka topic).",
-                "java": {
-                    "class": "java.net.URI"
+                "Searchable": {
+                    "fieldType": "TEXT_PARTIAL",
+                    "queryByDefault": false
                 },
-                "name": "uri",
+                "default": null,
+                "doc": "Optional project/namespace associated with the flow",
+                "name": "project",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "Searchable": {
@@ -681,14 +184,15 @@
                         "fields": [
                             {
                                 "doc": "When did the event occur",
                                 "name": "time",
                                 "type": "long"
                             },
                             {
+                                "Urn": "Urn",
                                 "default": null,
                                 "doc": "Optional: The actor urn involved in the event.",
                                 "java": {
                                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                 },
                                 "name": "actor",
                                 "type": [
@@ -713,493 +217,367 @@
                 "default": null,
                 "doc": "A timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)",
                 "name": "lastModified",
                 "type": [
                     "null",
                     "com.linkedin.pegasus2avro.common.TimeStamp"
                 ]
-            },
-            {
-                "default": [],
-                "deprecated": "Use GlobalTags aspect instead.",
-                "doc": "[Legacy] Unstructured tags for the dataset. Structured tags can be applied via the `GlobalTags` aspect.\nThis is now deprecated.",
-                "name": "tags",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                }
-            }
-        ],
-        "name": "DatasetProperties",
-        "namespace": "com.linkedin.pegasus2avro.dataset",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "datasetUpstreamLineage"
-        },
-        "deprecated": "use UpstreamLineage.fineGrainedLineages instead",
-        "doc": "Fine Grained upstream lineage for fields in a dataset",
-        "fields": [
-            {
-                "doc": "Upstream to downstream field level lineage mappings",
-                "name": "fieldMappings",
-                "type": {
-                    "items": {
-                        "deprecated": "use FineGrainedLineage instead",
-                        "doc": "Representation of mapping between fields in source dataset to the field in destination dataset",
-                        "fields": [
-                            {
-                                "doc": "Audit stamp containing who reported the field mapping and when",
-                                "name": "created",
-                                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-                            },
-                            {
-                                "doc": "Transfomration function between the fields involved",
-                                "name": "transformation",
-                                "type": [
-                                    {
-                                        "doc": "Type of the transformation involved in generating destination fields from source fields.",
-                                        "name": "TransformationType",
-                                        "namespace": "com.linkedin.pegasus2avro.common.fieldtransformer",
-                                        "symbolDocs": {
-                                            "BLACKBOX": "Field transformation expressed as unknown black box function.",
-                                            "IDENTITY": "Field transformation expressed as Identity function."
-                                        },
-                                        "symbols": [
-                                            "BLACKBOX",
-                                            "IDENTITY"
-                                        ],
-                                        "type": "enum"
-                                    },
-                                    {
-                                        "doc": "Field transformation expressed in UDF",
-                                        "fields": [
-                                            {
-                                                "doc": "A UDF mentioning how the source fields got transformed to destination field. This is the FQCN(Fully Qualified Class Name) of the udf.",
-                                                "name": "udf",
-                                                "type": "string"
-                                            }
-                                        ],
-                                        "name": "UDFTransformer",
-                                        "namespace": "com.linkedin.pegasus2avro.common.fieldtransformer",
-                                        "type": "record"
-                                    }
-                                ]
-                            },
-                            {
-                                "doc": "Source fields from which the fine grained lineage is derived",
-                                "name": "sourceFields",
-                                "type": {
-                                    "items": [
-                                        "string"
-                                    ],
-                                    "type": "array"
-                                }
-                            },
-                            {
-                                "deprecated": "use SchemaFieldPath and represent as generic Urn instead",
-                                "doc": "Destination field which is derived from source fields",
-                                "java": {
-                                    "class": "com.linkedin.pegasus2avro.common.urn.DatasetFieldUrn"
-                                },
-                                "name": "destinationField",
-                                "type": "string"
-                            }
-                        ],
-                        "name": "DatasetFieldMapping",
-                        "namespace": "com.linkedin.pegasus2avro.dataset",
-                        "type": "record"
-                    },
-                    "type": "array"
-                }
             }
         ],
-        "name": "DatasetUpstreamLineage",
-        "namespace": "com.linkedin.pegasus2avro.dataset",
+        "name": "DataFlowInfo",
+        "namespace": "com.linkedin.pegasus2avro.datajob",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "datasetProfile",
-            "type": "timeseries"
+            "name": "editableDataFlowProperties"
         },
-        "doc": "Stats corresponding to datasets",
+        "doc": "Stores editable changes made to properties. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines",
         "fields": [
             {
-                "doc": "The event timestamp field as epoch at UTC in milli seconds.",
-                "name": "timestampMillis",
-                "type": "long"
-            },
-            {
-                "default": null,
-                "doc": "Granularity of the event if applicable",
-                "name": "eventGranularity",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
-                ]
+                "default": {
+                    "actor": "urn:li:corpuser:unknown",
+                    "impersonator": null,
+                    "message": null,
+                    "time": 0
+                },
+                "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
+                "name": "created",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
             },
             {
                 "default": {
-                    "partition": "FULL_TABLE_SNAPSHOT",
-                    "timePartition": null,
-                    "type": "FULL_TABLE"
+                    "actor": "urn:li:corpuser:unknown",
+                    "impersonator": null,
+                    "message": null,
+                    "time": 0
                 },
-                "doc": "The optional partition specification.",
-                "name": "partitionSpec",
-                "type": [
-                    "com.linkedin.pegasus2avro.timeseries.PartitionSpec",
-                    "null"
-                ]
+                "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
+                "name": "lastModified",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
             },
             {
                 "default": null,
-                "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
-                "name": "messageId",
+                "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
+                "name": "deleted",
                 "type": [
                     "null",
-                    "string"
+                    "com.linkedin.pegasus2avro.common.AuditStamp"
                 ]
             },
             {
                 "Searchable": {
-                    "fieldType": "COUNT"
+                    "fieldName": "editedDescription",
+                    "fieldType": "TEXT"
                 },
                 "default": null,
-                "doc": "The total number of rows",
-                "name": "rowCount",
+                "doc": "Edited documentation of the data flow",
+                "name": "description",
                 "type": [
                     "null",
-                    "long"
+                    "string"
                 ]
-            },
+            }
+        ],
+        "name": "EditableDataFlowProperties",
+        "namespace": "com.linkedin.pegasus2avro.datajob",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "dataJobInputOutput"
+        },
+        "doc": "Information about the inputs and outputs of a Data processing job",
+        "fields": [
             {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "dataset"
+                        ],
+                        "isLineage": true,
+                        "name": "Consumes"
+                    }
+                },
                 "Searchable": {
-                    "fieldType": "COUNT"
+                    "/*": {
+                        "fieldName": "inputs",
+                        "fieldType": "URN",
+                        "numValuesFieldName": "numInputDatasets",
+                        "queryByDefault": false
+                    }
                 },
-                "default": null,
-                "doc": "The total number of columns (or schema fields)",
-                "name": "columnCount",
-                "type": [
-                    "null",
-                    "long"
-                ]
+                "Urn": "DatasetUrn",
+                "deprecated": true,
+                "doc": "Input datasets consumed by the data job during processing\nDeprecated! Use inputDatasetEdges instead.",
+                "name": "inputDatasets",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
             },
             {
+                "Relationship": {
+                    "/*/destinationUrn": {
+                        "createdActor": "inputDatasetEdges/*/created/actor",
+                        "createdOn": "inputDatasetEdges/*/created/time",
+                        "entityTypes": [
+                            "dataset"
+                        ],
+                        "isLineage": true,
+                        "name": "Consumes",
+                        "properties": "inputDatasetEdges/*/properties",
+                        "updatedActor": "inputDatasetEdges/*/lastModified/actor",
+                        "updatedOn": "inputDatasetEdges/*/lastModified/time"
+                    }
+                },
+                "Searchable": {
+                    "/*/destinationUrn": {
+                        "fieldName": "inputDatasetEdges",
+                        "fieldType": "URN",
+                        "numValuesFieldName": "numInputDatasets",
+                        "queryByDefault": false
+                    }
+                },
                 "default": null,
-                "doc": "Profiles for each column (or schema field)",
-                "name": "fieldProfiles",
+                "doc": "Input datasets consumed by the data job during processing",
+                "name": "inputDatasetEdges",
                 "type": [
                     "null",
                     {
                         "items": {
-                            "doc": "Stats corresponding to fields in a dataset",
+                            "doc": "Information about a relatonship edge.",
                             "fields": [
                                 {
-                                    "name": "fieldPath",
+                                    "Urn": "Urn",
+                                    "doc": "Urn of the source of this relationship edge.",
+                                    "java": {
+                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                    },
+                                    "name": "sourceUrn",
                                     "type": "string"
                                 },
                                 {
-                                    "default": null,
-                                    "name": "uniqueCount",
-                                    "type": [
-                                        "null",
-                                        "long"
-                                    ]
-                                },
-                                {
-                                    "default": null,
-                                    "name": "uniqueProportion",
-                                    "type": [
-                                        "null",
-                                        "float"
-                                    ]
-                                },
-                                {
-                                    "default": null,
-                                    "name": "nullCount",
-                                    "type": [
-                                        "null",
-                                        "long"
-                                    ]
-                                },
-                                {
-                                    "default": null,
-                                    "name": "nullProportion",
-                                    "type": [
-                                        "null",
-                                        "float"
-                                    ]
-                                },
-                                {
-                                    "default": null,
-                                    "name": "min",
-                                    "type": [
-                                        "null",
-                                        "string"
-                                    ]
-                                },
-                                {
-                                    "default": null,
-                                    "name": "max",
-                                    "type": [
-                                        "null",
-                                        "string"
-                                    ]
-                                },
-                                {
-                                    "default": null,
-                                    "name": "mean",
-                                    "type": [
-                                        "null",
-                                        "string"
-                                    ]
-                                },
-                                {
-                                    "default": null,
-                                    "name": "median",
-                                    "type": [
-                                        "null",
-                                        "string"
-                                    ]
-                                },
-                                {
-                                    "default": null,
-                                    "name": "stdev",
-                                    "type": [
-                                        "null",
-                                        "string"
-                                    ]
-                                },
-                                {
-                                    "default": null,
-                                    "name": "quantiles",
-                                    "type": [
-                                        "null",
-                                        {
-                                            "items": {
-                                                "fields": [
-                                                    {
-                                                        "name": "quantile",
-                                                        "type": "string"
-                                                    },
-                                                    {
-                                                        "name": "value",
-                                                        "type": "string"
-                                                    }
-                                                ],
-                                                "name": "Quantile",
-                                                "namespace": "com.linkedin.pegasus2avro.dataset",
-                                                "type": "record"
-                                            },
-                                            "type": "array"
-                                        }
-                                    ]
+                                    "Urn": "Urn",
+                                    "doc": "Urn of the destination of this relationship edge.",
+                                    "java": {
+                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                    },
+                                    "name": "destinationUrn",
+                                    "type": "string"
                                 },
                                 {
-                                    "default": null,
-                                    "name": "distinctValueFrequencies",
-                                    "type": [
-                                        "null",
-                                        {
-                                            "items": {
-                                                "fields": [
-                                                    {
-                                                        "name": "value",
-                                                        "type": "string"
-                                                    },
-                                                    {
-                                                        "name": "frequency",
-                                                        "type": "long"
-                                                    }
-                                                ],
-                                                "name": "ValueFrequency",
-                                                "namespace": "com.linkedin.pegasus2avro.dataset",
-                                                "type": "record"
-                                            },
-                                            "type": "array"
-                                        }
-                                    ]
+                                    "doc": "Audit stamp containing who created this relationship edge and when",
+                                    "name": "created",
+                                    "type": "com.linkedin.pegasus2avro.common.AuditStamp"
                                 },
                                 {
-                                    "default": null,
-                                    "name": "histogram",
-                                    "type": [
-                                        "null",
-                                        {
-                                            "fields": [
-                                                {
-                                                    "name": "boundaries",
-                                                    "type": {
-                                                        "items": "string",
-                                                        "type": "array"
-                                                    }
-                                                },
-                                                {
-                                                    "name": "heights",
-                                                    "type": {
-                                                        "items": "float",
-                                                        "type": "array"
-                                                    }
-                                                }
-                                            ],
-                                            "name": "Histogram",
-                                            "namespace": "com.linkedin.pegasus2avro.dataset",
-                                            "type": "record"
-                                        }
-                                    ]
+                                    "doc": "Audit stamp containing who last modified this relationship edge and when",
+                                    "name": "lastModified",
+                                    "type": "com.linkedin.pegasus2avro.common.AuditStamp"
                                 },
                                 {
                                     "default": null,
-                                    "name": "sampleValues",
+                                    "doc": "A generic properties bag that allows us to store specific information on this graph edge.",
+                                    "name": "properties",
                                     "type": [
                                         "null",
                                         {
-                                            "items": "string",
-                                            "type": "array"
+                                            "type": "map",
+                                            "values": "string"
                                         }
                                     ]
                                 }
                             ],
-                            "name": "DatasetFieldProfile",
-                            "namespace": "com.linkedin.pegasus2avro.dataset",
+                            "name": "Edge",
+                            "namespace": "com.linkedin.pegasus2avro.common",
                             "type": "record"
                         },
                         "type": "array"
                     }
                 ]
             },
             {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "dataset"
+                        ],
+                        "isLineage": true,
+                        "isUpstream": false,
+                        "name": "Produces"
+                    }
+                },
                 "Searchable": {
-                    "fieldType": "COUNT"
+                    "/*": {
+                        "fieldName": "outputs",
+                        "fieldType": "URN",
+                        "numValuesFieldName": "numOutputDatasets",
+                        "queryByDefault": false
+                    }
+                },
+                "Urn": "DatasetUrn",
+                "deprecated": true,
+                "doc": "Output datasets produced by the data job during processing\nDeprecated! Use outputDatasetEdges instead.",
+                "name": "outputDatasets",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
+            },
+            {
+                "Relationship": {
+                    "/*/destinationUrn": {
+                        "createdActor": "outputDatasetEdges/*/created/actor",
+                        "createdOn": "outputDatasetEdges/*/created/time",
+                        "entityTypes": [
+                            "dataset"
+                        ],
+                        "isLineage": true,
+                        "isUpstream": false,
+                        "name": "Produces",
+                        "properties": "outputDatasetEdges/*/properties",
+                        "updatedActor": "outputDatasetEdges/*/lastModified/actor",
+                        "updatedOn": "outputDatasetEdges/*/lastModified/time"
+                    }
+                },
+                "Searchable": {
+                    "/*/destinationUrn": {
+                        "fieldName": "outputDatasetEdges",
+                        "fieldType": "URN",
+                        "numValuesFieldName": "numOutputDatasets",
+                        "queryByDefault": false
+                    }
                 },
                 "default": null,
-                "doc": "Storage size in bytes",
-                "name": "sizeInBytes",
+                "doc": "Output datasets produced by the data job during processing",
+                "name": "outputDatasetEdges",
                 "type": [
                     "null",
-                    "long"
+                    {
+                        "items": "com.linkedin.pegasus2avro.common.Edge",
+                        "type": "array"
+                    }
                 ]
-            }
-        ],
-        "name": "DatasetProfile",
-        "namespace": "com.linkedin.pegasus2avro.dataset",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "upstreamLineage"
-        },
-        "doc": "Upstream lineage of a dataset",
-        "fields": [
+            },
             {
-                "doc": "List of upstream dataset lineage information",
-                "name": "upstreams",
-                "type": {
-                    "items": {
-                        "doc": "Upstream lineage information about a dataset including the source reporting the lineage",
-                        "fields": [
-                            {
-                                "default": {
-                                    "actor": "urn:li:corpuser:unknown",
-                                    "impersonator": null,
-                                    "message": null,
-                                    "time": 0
-                                },
-                                "doc": "Audit stamp containing who reported the lineage and when.",
-                                "name": "auditStamp",
-                                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-                            },
-                            {
-                                "default": null,
-                                "doc": "Audit stamp containing who created the lineage and when.",
-                                "name": "created",
-                                "type": [
-                                    "null",
-                                    "com.linkedin.pegasus2avro.common.AuditStamp"
-                                ]
-                            },
-                            {
-                                "Relationship": {
-                                    "createdActor": "upstreams/*/created/actor",
-                                    "createdOn": "upstreams/*/created/time",
-                                    "entityTypes": [
-                                        "dataset"
-                                    ],
-                                    "isLineage": true,
-                                    "name": "DownstreamOf",
-                                    "properties": "upstreams/*/properties",
-                                    "updatedActor": "upstreams/*/auditStamp/actor",
-                                    "updatedOn": "upstreams/*/auditStamp/time"
-                                },
-                                "Searchable": {
-                                    "fieldName": "upstreams",
-                                    "fieldType": "URN",
-                                    "queryByDefault": false
-                                },
-                                "doc": "The upstream dataset the lineage points to",
-                                "java": {
-                                    "class": "com.linkedin.pegasus2avro.common.urn.DatasetUrn"
-                                },
-                                "name": "dataset",
-                                "type": "string"
-                            },
-                            {
-                                "doc": "The type of the lineage",
-                                "name": "type",
-                                "type": {
-                                    "doc": "The various types of supported dataset lineage",
-                                    "name": "DatasetLineageType",
-                                    "namespace": "com.linkedin.pegasus2avro.dataset",
-                                    "symbolDocs": {
-                                        "COPY": "Direct copy without modification",
-                                        "TRANSFORMED": "Transformed data with modification (format or content change)",
-                                        "VIEW": "Represents a view defined on the sources e.g. Hive view defined on underlying hive tables or a Hive table pointing to a HDFS dataset or DALI view defined on multiple sources"
-                                    },
-                                    "symbols": [
-                                        "COPY",
-                                        "TRANSFORMED",
-                                        "VIEW"
-                                    ],
-                                    "type": "enum"
-                                }
-                            },
-                            {
-                                "default": null,
-                                "doc": "A generic properties bag that allows us to store specific information on this graph edge.",
-                                "name": "properties",
-                                "type": [
-                                    "null",
-                                    {
-                                        "type": "map",
-                                        "values": "string"
-                                    }
-                                ]
-                            }
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "dataJob"
                         ],
-                        "name": "Upstream",
-                        "namespace": "com.linkedin.pegasus2avro.dataset",
-                        "type": "record"
-                    },
-                    "type": "array"
-                }
+                        "isLineage": true,
+                        "name": "DownstreamOf"
+                    }
+                },
+                "Urn": "DataJobUrn",
+                "default": null,
+                "deprecated": true,
+                "doc": "Input datajobs that this data job depends on\nDeprecated! Use inputDatajobEdges instead.",
+                "name": "inputDatajobs",
+                "type": [
+                    "null",
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ],
+                "urn_is_array": true
             },
             {
                 "Relationship": {
-                    "/*/upstreams/*": {
+                    "/*/destinationUrn": {
+                        "createdActor": "inputDatajobEdges/*/created/actor",
+                        "createdOn": "inputDatajobEdges/*/created/time",
+                        "entityTypes": [
+                            "dataJob"
+                        ],
+                        "isLineage": true,
+                        "name": "DownstreamOf",
+                        "properties": "inputDatajobEdges/*/properties",
+                        "updatedActor": "inputDatajobEdges/*/lastModified/actor",
+                        "updatedOn": "inputDatajobEdges/*/lastModified/time"
+                    }
+                },
+                "default": null,
+                "doc": "Input datajobs that this data job depends on",
+                "name": "inputDatajobEdges",
+                "type": [
+                    "null",
+                    {
+                        "items": "com.linkedin.pegasus2avro.common.Edge",
+                        "type": "array"
+                    }
+                ]
+            },
+            {
+                "Relationship": {
+                    "/*": {
                         "entityTypes": [
-                            "dataset",
                             "schemaField"
                         ],
-                        "name": "DownstreamOf"
+                        "name": "Consumes"
+                    }
+                },
+                "Searchable": {
+                    "/*": {
+                        "fieldName": "inputFields",
+                        "fieldType": "URN",
+                        "numValuesFieldName": "numInputFields",
+                        "queryByDefault": false
                     }
                 },
+                "Urn": "Urn",
                 "default": null,
-                "doc": " List of fine-grained lineage information, including field-level lineage",
+                "doc": "Fields of the input datasets used by this job",
+                "name": "inputDatasetFields",
+                "type": [
+                    "null",
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ],
+                "urn_is_array": true
+            },
+            {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "schemaField"
+                        ],
+                        "name": "Produces"
+                    }
+                },
+                "Searchable": {
+                    "/*": {
+                        "fieldName": "outputFields",
+                        "fieldType": "URN",
+                        "numValuesFieldName": "numOutputFields",
+                        "queryByDefault": false
+                    }
+                },
+                "Urn": "Urn",
+                "default": null,
+                "doc": "Fields of the output datasets this job writes to",
+                "name": "outputDatasetFields",
+                "type": [
+                    "null",
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ],
+                "urn_is_array": true
+            },
+            {
+                "default": null,
+                "doc": "Fine-grained column-level lineages",
                 "name": "fineGrainedLineages",
                 "type": [
                     "null",
                     {
                         "items": {
                             "doc": "A fine-grained lineage from upstream fields/datasets to downstream field(s)",
                             "fields": [
@@ -1220,24 +598,26 @@
                                             "DATASET",
                                             "NONE"
                                         ],
                                         "type": "enum"
                                     }
                                 },
                                 {
+                                    "Urn": "Urn",
                                     "default": null,
                                     "doc": "Upstream entities in the lineage",
                                     "name": "upstreams",
                                     "type": [
                                         "null",
                                         {
                                             "items": "string",
                                             "type": "array"
                                         }
-                                    ]
+                                    ],
+                                    "urn_is_array": true
                                 },
                                 {
                                     "doc": "The type of downstream field(s)",
                                     "name": "downstreamType",
                                     "type": {
                                         "doc": "The type of downstream field(s) in a fine-grained lineage",
                                         "name": "FineGrainedLineageDownstreamType",
@@ -1250,24 +630,26 @@
                                             "FIELD",
                                             "FIELD_SET"
                                         ],
                                         "type": "enum"
                                     }
                                 },
                                 {
+                                    "Urn": "Urn",
                                     "default": null,
                                     "doc": "Downstream fields in the lineage",
                                     "name": "downstreams",
                                     "type": [
                                         "null",
                                         {
                                             "items": "string",
                                             "type": "array"
                                         }
-                                    ]
+                                    ],
+                                    "urn_is_array": true
                                 },
                                 {
                                     "default": null,
                                     "doc": "The transform operation applied to the upstream entities to produce the downstream field(s)",
                                     "name": "transformOperation",
                                     "type": [
                                         "null",
@@ -1286,1577 +668,2058 @@
                             "type": "record"
                         },
                         "type": "array"
                     }
                 ]
             }
         ],
-        "name": "UpstreamLineage",
-        "namespace": "com.linkedin.pegasus2avro.dataset",
+        "name": "DataJobInputOutput",
+        "namespace": "com.linkedin.pegasus2avro.datajob",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "editableDatasetProperties"
+            "name": "dataJobInfo"
         },
-        "doc": "EditableDatasetProperties stores editable changes made to dataset properties. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines",
+        "doc": "Information about a Data processing job",
         "fields": [
             {
-                "default": {
-                    "actor": "urn:li:corpuser:unknown",
-                    "impersonator": null,
-                    "message": null,
-                    "time": 0
-                },
-                "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
-                "name": "created",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-            },
-            {
-                "default": {
-                    "actor": "urn:li:corpuser:unknown",
-                    "impersonator": null,
-                    "message": null,
-                    "time": 0
+                "Searchable": {
+                    "/*": {
+                        "queryByDefault": true
+                    }
                 },
-                "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
-                "name": "lastModified",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+                "default": {},
+                "doc": "Custom property bag.",
+                "name": "customProperties",
+                "type": {
+                    "type": "map",
+                    "values": "string"
+                }
             },
             {
                 "default": null,
-                "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
-                "name": "deleted",
+                "doc": "URL where the reference exist",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "externalUrl",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.common.AuditStamp"
+                    "string"
                 ]
             },
             {
                 "Searchable": {
-                    "fieldName": "editedDescription",
-                    "fieldType": "TEXT"
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL"
+                },
+                "doc": "Job name",
+                "name": "name",
+                "type": "string"
+            },
+            {
+                "Searchable": {
+                    "fieldType": "TEXT",
+                    "hasValuesFieldName": "hasDescription"
                 },
                 "default": null,
-                "doc": "Documentation of the dataset",
+                "doc": "Job description",
                 "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
-            }
-        ],
-        "name": "EditableDatasetProperties",
-        "namespace": "com.linkedin.pegasus2avro.dataset",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "dataHubStepStateProperties"
-        },
-        "doc": "The properties associated with a DataHub step state",
-        "fields": [
-            {
-                "default": {},
-                "doc": "Description of the secret",
-                "name": "properties",
-                "type": {
-                    "type": "map",
-                    "values": "string"
-                }
             },
             {
-                "doc": "Audit stamp describing the last person to update it.",
-                "name": "lastModified",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-            }
-        ],
-        "name": "DataHubStepStateProperties",
-        "namespace": "com.linkedin.pegasus2avro.step",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "dataHubSecretValue"
-        },
-        "doc": "The value of a DataHub Secret",
-        "fields": [
-            {
-                "Searchable": {
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "The display name for the secret",
-                "name": "name",
-                "type": "string"
-            },
-            {
-                "doc": "The AES-encrypted value of the DataHub secret.",
-                "name": "value",
-                "type": "string"
+                "doc": "Datajob type\n*NOTE**: AzkabanJobType is deprecated. Please use strings instead.",
+                "name": "type",
+                "type": [
+                    {
+                        "doc": "The various types of support azkaban jobs",
+                        "name": "AzkabanJobType",
+                        "namespace": "com.linkedin.pegasus2avro.datajob.azkaban",
+                        "symbolDocs": {
+                            "COMMAND": "The command job type is one of the basic built-in types. It runs multiple UNIX commands using java processbuilder.\nUpon execution, Azkaban spawns off a process to run the command.",
+                            "GLUE": "Glue type is for running AWS Glue job transforms.",
+                            "HADOOP_JAVA": "Runs a java program with ability to access Hadoop cluster.\nhttps://azkaban.readthedocs.io/en/latest/jobTypes.html#java-job-type",
+                            "HADOOP_SHELL": "In large part, this is the same Command type. The difference is its ability to talk to a Hadoop cluster\nsecurely, via Hadoop tokens.",
+                            "HIVE": "Hive type is for running Hive jobs.",
+                            "PIG": "Pig type is for running Pig jobs.",
+                            "SQL": "SQL is for running Presto, mysql queries etc"
+                        },
+                        "symbols": [
+                            "COMMAND",
+                            "HADOOP_JAVA",
+                            "HADOOP_SHELL",
+                            "HIVE",
+                            "PIG",
+                            "SQL",
+                            "GLUE"
+                        ],
+                        "type": "enum"
+                    },
+                    "string"
+                ]
             },
             {
+                "Urn": "DataFlowUrn",
                 "default": null,
-                "doc": "Description of the secret",
-                "name": "description",
+                "doc": "DataFlow urn that this job is part of",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.DataFlowUrn"
+                },
+                "name": "flowUrn",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "Searchable": {
                     "/time": {
-                        "fieldName": "createdTime",
+                        "fieldName": "createdAt",
                         "fieldType": "DATETIME"
                     }
                 },
                 "default": null,
-                "doc": "Created Audit stamp",
+                "doc": "A timestamp documenting when the asset was created in the source Data Platform (not on DataHub)",
                 "name": "created",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.common.AuditStamp"
+                    "com.linkedin.pegasus2avro.common.TimeStamp"
                 ]
-            }
-        ],
-        "name": "DataHubSecretValue",
-        "namespace": "com.linkedin.pegasus2avro.secret",
-        "type": "record"
-    },
-    "com.linkedin.pegasus2avro.metadata.query.filter.Filter",
-    {
-        "Aspect": {
-            "entityAspects": [
-                "domains",
-                "container",
-                "deprecation",
-                "dashboardUsageStatistics",
-                "inputFields",
-                "subTypes",
-                "embed"
-            ],
-            "entityCategory": "_unset_",
-            "keyForEntity": "dashboard",
-            "name": "dashboardKey"
-        },
-        "doc": "Key for a Dashboard",
-        "fields": [
+            },
             {
                 "Searchable": {
-                    "boostScore": 4.0,
-                    "fieldName": "tool",
-                    "fieldType": "TEXT_PARTIAL"
+                    "/time": {
+                        "fieldName": "lastModifiedAt",
+                        "fieldType": "DATETIME"
+                    }
                 },
-                "doc": "The name of the dashboard tool such as looker, redash etc.",
-                "name": "dashboardTool",
-                "type": "string"
+                "default": null,
+                "doc": "A timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)",
+                "name": "lastModified",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.common.TimeStamp"
+                ]
             },
             {
-                "doc": "Unique id for the dashboard. This id should be globally unique for a dashboarding tool even when there are multiple deployments of it. As an example, dashboard URL could be used here for Looker such as 'looker.linkedin.com/dashboards/1234'",
-                "name": "dashboardId",
-                "type": "string"
+                "default": null,
+                "deprecated": "Use Data Process Instance model, instead",
+                "doc": "Status of the job - Deprecated for Data Process Instance model.",
+                "name": "status",
+                "type": [
+                    "null",
+                    {
+                        "doc": "Job statuses",
+                        "name": "JobStatus",
+                        "namespace": "com.linkedin.pegasus2avro.datajob",
+                        "symbolDocs": {
+                            "COMPLETED": "Jobs with successful completion.",
+                            "FAILED": "Jobs that have failed.",
+                            "IN_PROGRESS": "Jobs currently running.",
+                            "SKIPPED": "Jobs that have been skipped.",
+                            "STARTING": "Jobs being initialized.",
+                            "STOPPED": "Jobs that have stopped.",
+                            "STOPPING": "Jobs being stopped.",
+                            "UNKNOWN": "Jobs with unknown status (either unmappable or unavailable)"
+                        },
+                        "symbols": [
+                            "STARTING",
+                            "IN_PROGRESS",
+                            "STOPPING",
+                            "STOPPED",
+                            "COMPLETED",
+                            "FAILED",
+                            "UNKNOWN",
+                            "SKIPPED"
+                        ],
+                        "type": "enum"
+                    }
+                ]
             }
         ],
-        "name": "DashboardKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "name": "DataJobInfo",
+        "namespace": "com.linkedin.pegasus2avro.datajob",
         "type": "record"
     },
     {
         "Aspect": {
-            "entityAspects": [
-                "glossaryTerms",
-                "editableMlFeatureTableProperties",
-                "domains"
-            ],
-            "entityCategory": "core",
-            "keyForEntity": "mlFeatureTable",
-            "name": "mlFeatureTableKey"
+            "name": "versionInfo"
         },
-        "doc": "Key for an MLFeatureTable",
+        "doc": "Information about a Data processing job",
         "fields": [
             {
-                "Relationship": {
-                    "entityTypes": [
-                        "dataPlatform"
-                    ],
-                    "name": "SourcePlatform"
+                "Searchable": {
+                    "/*": {
+                        "queryByDefault": true
+                    }
                 },
-                "doc": "Data platform urn associated with the feature table",
+                "default": {},
+                "doc": "Custom property bag.",
+                "name": "customProperties",
+                "type": {
+                    "type": "map",
+                    "values": "string"
+                }
+            },
+            {
+                "default": null,
+                "doc": "URL where the reference exist",
                 "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
                 },
-                "name": "platform",
-                "type": "string"
+                "name": "externalUrl",
+                "type": [
+                    "null",
+                    "string"
+                ]
             },
             {
-                "Searchable": {
-                    "boostScore": 8.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "Name of the feature table",
-                "name": "name",
+                "doc": "The version which can indentify a job version like a commit hash or md5 hash",
+                "name": "version",
                 "type": "string"
-            }
-        ],
-        "name": "MLFeatureTableKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "entityAspects": [
-                "dataHubAccessTokenInfo"
-            ],
-            "entityCategory": "internal",
-            "keyForEntity": "dataHubAccessToken",
-            "name": "dataHubAccessTokenKey"
-        },
-        "doc": "Key for a DataHub Access Token",
-        "fields": [
+            },
             {
-                "doc": "Access token's SHA-256 hashed JWT signature",
-                "name": "id",
+                "doc": "The type of the version like git hash or md5 hash",
+                "name": "versionType",
                 "type": "string"
             }
         ],
-        "name": "DataHubAccessTokenKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "name": "VersionInfo",
+        "namespace": "com.linkedin.pegasus2avro.datajob",
         "type": "record"
     },
     {
         "Aspect": {
-            "entityAspects": [
-                "dataHubIngestionSourceInfo"
-            ],
-            "entityCategory": "internal",
-            "keyForEntity": "dataHubIngestionSource",
-            "name": "dataHubIngestionSourceKey"
+            "name": "datahubIngestionRunSummary",
+            "type": "timeseries"
         },
-        "doc": "Key for a DataHub ingestion source",
+        "doc": "Summary of a datahub ingestion run for a given platform.",
         "fields": [
             {
-                "doc": "A unique id for the Ingestion Source, either generated or provided",
-                "name": "id",
-                "type": "string"
-            }
-        ],
-        "name": "DataHubIngestionSourceKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "entityAspects": [
-                "assertionInfo",
-                "dataPlatformInstance",
-                "assertionRunEvent",
-                "status"
-            ],
-            "entityCategory": "core",
-            "entityDoc": "Assertion represents a data quality rule applied on one or more dataset.",
-            "keyForEntity": "assertion",
-            "name": "assertionKey"
-        },
-        "doc": "Key for a Assertion",
-        "fields": [
+                "doc": "The event timestamp field as epoch at UTC in milli seconds.",
+                "name": "timestampMillis",
+                "type": "long"
+            },
             {
-                "doc": "Unique id for the assertion.",
-                "name": "assertionId",
-                "type": "string"
-            }
-        ],
-        "name": "AssertionKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "entityAspects": [
-                "glossaryTerms",
-                "editableMlPrimaryKeyProperties",
-                "domains"
-            ],
-            "entityCategory": "core",
-            "keyForEntity": "mlPrimaryKey",
-            "name": "mlPrimaryKeyKey"
-        },
-        "doc": "Key for an MLPrimaryKey",
-        "fields": [
+                "default": null,
+                "doc": "Granularity of the event if applicable",
+                "name": "eventGranularity",
+                "type": [
+                    "null",
+                    {
+                        "doc": "Defines the size of a time window.",
+                        "fields": [
+                            {
+                                "doc": "Interval unit such as minute/hour/day etc.",
+                                "name": "unit",
+                                "type": {
+                                    "name": "CalendarInterval",
+                                    "namespace": "com.linkedin.pegasus2avro.timeseries",
+                                    "symbols": [
+                                        "SECOND",
+                                        "MINUTE",
+                                        "HOUR",
+                                        "DAY",
+                                        "WEEK",
+                                        "MONTH",
+                                        "QUARTER",
+                                        "YEAR"
+                                    ],
+                                    "type": "enum"
+                                }
+                            },
+                            {
+                                "default": 1,
+                                "doc": "How many units. Defaults to 1.",
+                                "name": "multiple",
+                                "type": "int"
+                            }
+                        ],
+                        "name": "TimeWindowSize",
+                        "namespace": "com.linkedin.pegasus2avro.timeseries",
+                        "type": "record"
+                    }
+                ]
+            },
             {
-                "Searchable": {
-                    "fieldType": "TEXT_PARTIAL"
+                "default": {
+                    "partition": "FULL_TABLE_SNAPSHOT",
+                    "timePartition": null,
+                    "type": "FULL_TABLE"
                 },
-                "doc": "Namespace for the primary key",
-                "name": "featureNamespace",
-                "type": "string"
+                "doc": "The optional partition specification.",
+                "name": "partitionSpec",
+                "type": [
+                    {
+                        "doc": "Defines how the data is partitioned",
+                        "fields": [
+                            {
+                                "default": "PARTITION",
+                                "name": "type",
+                                "type": {
+                                    "name": "PartitionType",
+                                    "namespace": "com.linkedin.pegasus2avro.timeseries",
+                                    "symbols": [
+                                        "FULL_TABLE",
+                                        "QUERY",
+                                        "PARTITION"
+                                    ],
+                                    "type": "enum"
+                                }
+                            },
+                            {
+                                "TimeseriesField": {},
+                                "doc": "String representation of the partition",
+                                "name": "partition",
+                                "type": "string"
+                            },
+                            {
+                                "default": null,
+                                "doc": "Time window of the partition if applicable",
+                                "name": "timePartition",
+                                "type": [
+                                    "null",
+                                    {
+                                        "fields": [
+                                            {
+                                                "doc": "Start time as epoch at UTC.",
+                                                "name": "startTimeMillis",
+                                                "type": "long"
+                                            },
+                                            {
+                                                "doc": "The length of the window.",
+                                                "name": "length",
+                                                "type": "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
+                                            }
+                                        ],
+                                        "name": "TimeWindow",
+                                        "namespace": "com.linkedin.pegasus2avro.timeseries",
+                                        "type": "record"
+                                    }
+                                ]
+                            }
+                        ],
+                        "name": "PartitionSpec",
+                        "namespace": "com.linkedin.pegasus2avro.timeseries",
+                        "type": "record"
+                    },
+                    "null"
+                ]
             },
             {
-                "Searchable": {
-                    "boostScore": 8.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "Name of the primary key",
-                "name": "name",
-                "type": "string"
-            }
-        ],
-        "name": "MLPrimaryKeyKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "entityAspects": [
-                "dataHubUpgradeRequest",
-                "dataHubUpgradeResult"
-            ],
-            "entityCategory": "internal",
-            "keyForEntity": "dataHubUpgrade",
-            "name": "dataHubUpgradeKey"
-        },
-        "doc": "Key for a DataHubUpgrade",
-        "fields": [
+                "default": null,
+                "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
+                "name": "messageId",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
             {
-                "name": "id",
+                "TimeseriesField": {},
+                "doc": "The name of the pipeline that ran ingestion, a stable unique user provided identifier.\n e.g. my_snowflake1-to-datahub.",
+                "name": "pipelineName",
                 "type": "string"
-            }
-        ],
-        "name": "DataHubUpgradeKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "entityAspects": [
-                "telemetryClientId"
-            ],
-            "entityCategory": "internal",
-            "keyForEntity": "telemetry",
-            "name": "telemetryKey"
-        },
-        "doc": "Key for the telemetry client ID, only one should ever exist",
-        "fields": [
+            },
             {
-                "doc": "The telemetry entity name, which serves as a unique id",
-                "name": "name",
+                "TimeseriesField": {},
+                "doc": "The id of the instance against which the ingestion pipeline ran.\ne.g.: Bigquery project ids, MySQL hostnames etc.",
+                "name": "platformInstanceId",
                 "type": "string"
-            }
-        ],
-        "name": "TelemetryKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "entityAspects": [
-                "dataHubSecretValue"
-            ],
-            "entityCategory": "internal",
-            "keyForEntity": "dataHubSecret",
-            "name": "dataHubSecretKey"
-        },
-        "doc": "Key for a DataHub Secret",
-        "fields": [
+            },
             {
-                "doc": "A unique id for the Secret",
-                "name": "id",
+                "TimeseriesField": {},
+                "doc": "The runId for this pipeline instance.",
+                "name": "runId",
                 "type": "string"
-            }
-        ],
-        "name": "DataHubSecretKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "entityAspects": [
-                "dataHubRetentionConfig"
-            ],
-            "entityCategory": "internal",
-            "keyForEntity": "dataHubRetention",
-            "name": "dataHubRetentionKey"
-        },
-        "doc": "Key for a DataHub Retention",
-        "fields": [
+            },
             {
-                "doc": "Entity name to apply retention to. * (or empty) for applying defaults.",
-                "name": "entityName",
-                "type": "string"
+                "TimeseriesField": {},
+                "doc": "Run Status - Succeeded/Skipped/Failed etc.",
+                "name": "runStatus",
+                "type": "com.linkedin.pegasus2avro.datajob.JobStatus"
             },
             {
-                "doc": "Aspect name to apply retention to. * (or empty) for applying defaults.",
-                "name": "aspectName",
-                "type": "string"
-            }
-        ],
-        "name": "DataHubRetentionKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "entityAspects": [
-                "dataHubRoleInfo"
-            ],
-            "entityCategory": "core",
-            "keyForEntity": "dataHubRole",
-            "name": "dataHubRoleKey"
-        },
-        "doc": "Key for a DataHub Role",
-        "fields": [
+                "default": null,
+                "doc": "The number of workunits written to sink.",
+                "name": "numWorkUnitsCommitted",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            },
             {
-                "doc": "A unique id for the DataHub role record. Generated on the server side at role creation time.",
-                "name": "id",
-                "type": "string"
+                "default": null,
+                "doc": "The number of workunits that are produced.",
+                "name": "numWorkUnitsCreated",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "The number of events produced (MCE + MCP).",
+                "name": "numEvents",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "The total number of entities produced (unique entity urns).",
+                "name": "numEntities",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "The total number of aspects produced across all entities.",
+                "name": "numAspects",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Total number of source API calls.",
+                "name": "numSourceAPICalls",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Total latency across all source API calls.",
+                "name": "totalLatencySourceAPICalls",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Total number of sink API calls.",
+                "name": "numSinkAPICalls",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Total latency across all sink API calls.",
+                "name": "totalLatencySinkAPICalls",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Number of warnings generated.",
+                "name": "numWarnings",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Number of errors generated.",
+                "name": "numErrors",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Number of entities skipped.",
+                "name": "numEntitiesSkipped",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "The non-sensitive key-value pairs of the yaml config used as json string.",
+                "name": "config",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Custom value.",
+                "name": "custom_summary",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "TimeseriesField": {},
+                "default": null,
+                "doc": "The software version of this ingestion.",
+                "name": "softwareVersion",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "The hostname the ingestion pipeline ran on.",
+                "name": "systemHostName",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "TimeseriesField": {},
+                "default": null,
+                "doc": "The os the ingestion pipeline ran on.",
+                "name": "operatingSystemName",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "The number of processors on the host the ingestion pipeline ran on.",
+                "name": "numProcessors",
+                "type": [
+                    "null",
+                    "int"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "The total amount of memory on the host the ingestion pipeline ran on.",
+                "name": "totalMemory",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "The available memory on the host the ingestion pipeline ran on.",
+                "name": "availableMemory",
+                "type": [
+                    "null",
+                    "long"
+                ]
             }
         ],
-        "name": "DataHubRoleKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "name": "DatahubIngestionRunSummary",
+        "namespace": "com.linkedin.pegasus2avro.datajob.datahub",
         "type": "record"
     },
     {
         "Aspect": {
-            "entityAspects": [
-                "dataPlatformInstanceProperties",
-                "ownership",
-                "globalTags",
-                "institutionalMemory",
-                "deprecation",
-                "status"
-            ],
-            "entityCategory": "internal",
-            "keyForEntity": "dataPlatformInstance",
-            "name": "dataPlatformInstanceKey"
+            "name": "datahubIngestionCheckpoint",
+            "type": "timeseries"
         },
-        "doc": "Key for a Dataset",
+        "doc": "Checkpoint of a datahub ingestion run for a given job.",
         "fields": [
             {
-                "doc": "Data platform urn associated with the instance",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                "doc": "The event timestamp field as epoch at UTC in milli seconds.",
+                "name": "timestampMillis",
+                "type": "long"
+            },
+            {
+                "default": null,
+                "doc": "Granularity of the event if applicable",
+                "name": "eventGranularity",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
+                ]
+            },
+            {
+                "default": {
+                    "partition": "FULL_TABLE_SNAPSHOT",
+                    "timePartition": null,
+                    "type": "FULL_TABLE"
                 },
-                "name": "platform",
+                "doc": "The optional partition specification.",
+                "name": "partitionSpec",
+                "type": [
+                    "com.linkedin.pegasus2avro.timeseries.PartitionSpec",
+                    "null"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
+                "name": "messageId",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "TimeseriesField": {},
+                "doc": "The name of the pipeline that ran ingestion, a stable unique user provided identifier.\n e.g. my_snowflake1-to-datahub.",
+                "name": "pipelineName",
                 "type": "string"
             },
             {
-                "doc": "Unique instance id",
-                "name": "instance",
+                "TimeseriesField": {},
+                "doc": "The id of the instance against which the ingestion pipeline ran.\ne.g.: Bigquery project ids, MySQL hostnames etc.",
+                "name": "platformInstanceId",
                 "type": "string"
-            }
-        ],
-        "name": "DataPlatformInstanceKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "entityAspects": [
-                "dataHubStepStateProperties"
-            ],
-            "entityCategory": "core",
-            "keyForEntity": "dataHubStepState",
-            "name": "dataHubStepStateKey"
-        },
-        "doc": "Key for a DataHub Step State",
-        "fields": [
+            },
             {
-                "doc": "A unique id for the state",
-                "name": "id",
+                "doc": "Json-encoded string representation of the non-secret members of the config .",
+                "name": "config",
+                "type": "string"
+            },
+            {
+                "doc": "Opaque blob of the state representation.",
+                "name": "state",
+                "type": {
+                    "doc": "The checkpoint state object of a datahub ingestion run for a given job.",
+                    "fields": [
+                        {
+                            "doc": "The version of the state format.",
+                            "name": "formatVersion",
+                            "type": "string"
+                        },
+                        {
+                            "doc": "The serialization/deserialization protocol.",
+                            "name": "serde",
+                            "type": "string"
+                        },
+                        {
+                            "default": null,
+                            "doc": "Opaque blob of the state representation.",
+                            "name": "payload",
+                            "type": [
+                                "null",
+                                "bytes"
+                            ]
+                        }
+                    ],
+                    "name": "IngestionCheckpointState",
+                    "namespace": "com.linkedin.pegasus2avro.datajob.datahub",
+                    "type": "record"
+                }
+            },
+            {
+                "TimeseriesField": {},
+                "doc": "The run identifier of this job.",
+                "name": "runId",
                 "type": "string"
             }
         ],
-        "name": "DataHubStepStateKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "name": "DatahubIngestionCheckpoint",
+        "namespace": "com.linkedin.pegasus2avro.datajob.datahub",
         "type": "record"
     },
     {
         "Aspect": {
-            "entityAspects": [
-                "dataProcessInstanceInput",
-                "dataProcessInstanceOutput",
-                "dataProcessInstanceProperties",
-                "dataProcessInstanceRelationships",
-                "dataProcessInstanceRunEvent"
-            ],
-            "entityCategory": "_unset_",
-            "entityDoc": "DataProcessInstance represents an instance of a datajob/jobflow run",
-            "keyForEntity": "dataProcessInstance",
-            "name": "dataProcessInstanceKey"
+            "name": "embed"
         },
-        "doc": "Key for an Asset DataProcessInstance",
+        "doc": "Information regarding rendering an embed for an asset.",
         "fields": [
             {
-                "doc": "A unique id for the DataProcessInstance . Should be separate from the name used for displaying a DataProcessInstance.",
-                "name": "id",
-                "type": "string"
+                "default": null,
+                "doc": "An embed URL to be rendered inside of an iframe.",
+                "name": "renderUrl",
+                "type": [
+                    "null",
+                    "string"
+                ]
             }
         ],
-        "name": "DataProcessInstanceKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "name": "Embed",
+        "namespace": "com.linkedin.pegasus2avro.common",
         "type": "record"
     },
     {
         "Aspect": {
-            "entityAspects": [
-                "postInfo"
-            ],
-            "entityCategory": "core",
-            "keyForEntity": "post",
-            "name": "postKey"
+            "name": "inputFields"
         },
-        "doc": "Key for a Post.",
+        "doc": "Information about the fields a chart or dashboard references",
         "fields": [
             {
-                "doc": "A unique id for the DataHub Post record. Generated on the server side at Post creation time.",
-                "name": "id",
-                "type": "string"
+                "doc": "List of fields being referenced",
+                "name": "fields",
+                "type": {
+                    "items": {
+                        "doc": "Information about a field a chart or dashboard references",
+                        "fields": [
+                            {
+                                "Relationship": {
+                                    "entityTypes": [
+                                        "schemaField"
+                                    ],
+                                    "name": "consumesField"
+                                },
+                                "Urn": "Urn",
+                                "doc": "Urn of the schema being referenced for lineage purposes",
+                                "java": {
+                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                },
+                                "name": "schemaFieldUrn",
+                                "type": "string"
+                            },
+                            {
+                                "default": null,
+                                "doc": "Copied version of the referenced schema field object for indexing purposes",
+                                "name": "schemaField",
+                                "type": [
+                                    "null",
+                                    {
+                                        "doc": "SchemaField to describe metadata related to dataset schema.",
+                                        "fields": [
+                                            {
+                                                "Searchable": {
+                                                    "boostScore": 5.0,
+                                                    "fieldName": "fieldPaths",
+                                                    "fieldType": "TEXT"
+                                                },
+                                                "doc": "Flattened name of the field. Field is computed from jsonPath field.",
+                                                "name": "fieldPath",
+                                                "type": "string"
+                                            },
+                                            {
+                                                "Deprecated": true,
+                                                "default": null,
+                                                "doc": "Flattened name of a field in JSON Path notation.",
+                                                "name": "jsonPath",
+                                                "type": [
+                                                    "null",
+                                                    "string"
+                                                ]
+                                            },
+                                            {
+                                                "default": false,
+                                                "doc": "Indicates if this field is optional or nullable",
+                                                "name": "nullable",
+                                                "type": "boolean"
+                                            },
+                                            {
+                                                "Searchable": {
+                                                    "boostScore": 0.1,
+                                                    "fieldName": "fieldDescriptions",
+                                                    "fieldType": "TEXT"
+                                                },
+                                                "default": null,
+                                                "doc": "Description",
+                                                "name": "description",
+                                                "type": [
+                                                    "null",
+                                                    "string"
+                                                ]
+                                            },
+                                            {
+                                                "Searchable": {
+                                                    "boostScore": 0.2,
+                                                    "fieldName": "fieldLabels",
+                                                    "fieldType": "TEXT"
+                                                },
+                                                "default": null,
+                                                "doc": "Label of the field. Provides a more human-readable name for the field than field path. Some sources will\nprovide this metadata but not all sources have the concept of a label. If just one string is associated with\na field in a source, that is most likely a description.",
+                                                "name": "label",
+                                                "type": [
+                                                    "null",
+                                                    "string"
+                                                ]
+                                            },
+                                            {
+                                                "default": null,
+                                                "doc": "An AuditStamp corresponding to the creation of this schema field.",
+                                                "name": "created",
+                                                "type": [
+                                                    "null",
+                                                    "com.linkedin.pegasus2avro.common.AuditStamp"
+                                                ]
+                                            },
+                                            {
+                                                "default": null,
+                                                "doc": "An AuditStamp corresponding to the last modification of this schema field.",
+                                                "name": "lastModified",
+                                                "type": [
+                                                    "null",
+                                                    "com.linkedin.pegasus2avro.common.AuditStamp"
+                                                ]
+                                            },
+                                            {
+                                                "doc": "Platform independent field type of the field.",
+                                                "name": "type",
+                                                "type": {
+                                                    "doc": "Schema field data types",
+                                                    "fields": [
+                                                        {
+                                                            "doc": "Data platform specific types",
+                                                            "name": "type",
+                                                            "type": [
+                                                                {
+                                                                    "doc": "Boolean field type.",
+                                                                    "fields": [],
+                                                                    "name": "BooleanType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                },
+                                                                {
+                                                                    "doc": "Fixed field type.",
+                                                                    "fields": [],
+                                                                    "name": "FixedType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                },
+                                                                {
+                                                                    "doc": "String field type.",
+                                                                    "fields": [],
+                                                                    "name": "StringType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                },
+                                                                {
+                                                                    "doc": "Bytes field type.",
+                                                                    "fields": [],
+                                                                    "name": "BytesType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                },
+                                                                {
+                                                                    "doc": "Number data type: long, integer, short, etc..",
+                                                                    "fields": [],
+                                                                    "name": "NumberType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                },
+                                                                {
+                                                                    "doc": "Date field type.",
+                                                                    "fields": [],
+                                                                    "name": "DateType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                },
+                                                                {
+                                                                    "doc": "Time field type. This should also be used for datetimes.",
+                                                                    "fields": [],
+                                                                    "name": "TimeType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                },
+                                                                {
+                                                                    "doc": "Enum field type.",
+                                                                    "fields": [],
+                                                                    "name": "EnumType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                },
+                                                                {
+                                                                    "doc": "Null field type.",
+                                                                    "fields": [],
+                                                                    "name": "NullType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                },
+                                                                {
+                                                                    "doc": "Map field type.",
+                                                                    "fields": [
+                                                                        {
+                                                                            "default": null,
+                                                                            "doc": "Key type in a map",
+                                                                            "name": "keyType",
+                                                                            "type": [
+                                                                                "null",
+                                                                                "string"
+                                                                            ]
+                                                                        },
+                                                                        {
+                                                                            "default": null,
+                                                                            "doc": "Type of the value in a map",
+                                                                            "name": "valueType",
+                                                                            "type": [
+                                                                                "null",
+                                                                                "string"
+                                                                            ]
+                                                                        }
+                                                                    ],
+                                                                    "name": "MapType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                },
+                                                                {
+                                                                    "doc": "Array field type.",
+                                                                    "fields": [
+                                                                        {
+                                                                            "default": null,
+                                                                            "doc": "List of types this array holds.",
+                                                                            "name": "nestedType",
+                                                                            "type": [
+                                                                                "null",
+                                                                                {
+                                                                                    "items": "string",
+                                                                                    "type": "array"
+                                                                                }
+                                                                            ]
+                                                                        }
+                                                                    ],
+                                                                    "name": "ArrayType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                },
+                                                                {
+                                                                    "doc": "Union field type.",
+                                                                    "fields": [
+                                                                        {
+                                                                            "default": null,
+                                                                            "doc": "List of types in union type.",
+                                                                            "name": "nestedTypes",
+                                                                            "type": [
+                                                                                "null",
+                                                                                {
+                                                                                    "items": "string",
+                                                                                    "type": "array"
+                                                                                }
+                                                                            ]
+                                                                        }
+                                                                    ],
+                                                                    "name": "UnionType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                },
+                                                                {
+                                                                    "doc": "Record field type.",
+                                                                    "fields": [],
+                                                                    "name": "RecordType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                                    "type": "record"
+                                                                }
+                                                            ]
+                                                        }
+                                                    ],
+                                                    "name": "SchemaFieldDataType",
+                                                    "namespace": "com.linkedin.pegasus2avro.schema",
+                                                    "type": "record"
+                                                }
+                                            },
+                                            {
+                                                "doc": "The native type of the field in the dataset's platform as declared by platform schema.",
+                                                "name": "nativeDataType",
+                                                "type": "string"
+                                            },
+                                            {
+                                                "default": false,
+                                                "doc": "There are use cases when a field in type B references type A. A field in A references field of type B. In such cases, we will mark the first field as recursive.",
+                                                "name": "recursive",
+                                                "type": "boolean"
+                                            },
+                                            {
+                                                "Relationship": {
+                                                    "/tags/*/tag": {
+                                                        "entityTypes": [
+                                                            "tag"
+                                                        ],
+                                                        "name": "SchemaFieldTaggedWith"
+                                                    }
+                                                },
+                                                "Searchable": {
+                                                    "/tags/*/tag": {
+                                                        "boostScore": 0.5,
+                                                        "fieldName": "fieldTags",
+                                                        "fieldType": "URN"
+                                                    }
+                                                },
+                                                "default": null,
+                                                "doc": "Tags associated with the field",
+                                                "name": "globalTags",
+                                                "type": [
+                                                    "null",
+                                                    {
+                                                        "Aspect": {
+                                                            "name": "globalTags"
+                                                        },
+                                                        "doc": "Tag aspect used for applying tags to an entity",
+                                                        "fields": [
+                                                            {
+                                                                "Relationship": {
+                                                                    "/*/tag": {
+                                                                        "entityTypes": [
+                                                                            "tag"
+                                                                        ],
+                                                                        "name": "TaggedWith"
+                                                                    }
+                                                                },
+                                                                "Searchable": {
+                                                                    "/*/tag": {
+                                                                        "addToFilters": true,
+                                                                        "boostScore": 0.5,
+                                                                        "fieldName": "tags",
+                                                                        "fieldType": "URN",
+                                                                        "filterNameOverride": "Tag",
+                                                                        "hasValuesFieldName": "hasTags",
+                                                                        "queryByDefault": true
+                                                                    }
+                                                                },
+                                                                "doc": "Tags associated with a given entity",
+                                                                "name": "tags",
+                                                                "type": {
+                                                                    "items": {
+                                                                        "doc": "Properties of an applied tag. For now, just an Urn. In the future we can extend this with other properties, e.g.\npropagation parameters.",
+                                                                        "fields": [
+                                                                            {
+                                                                                "Urn": "TagUrn",
+                                                                                "doc": "Urn of the applied tag",
+                                                                                "java": {
+                                                                                    "class": "com.linkedin.pegasus2avro.common.urn.TagUrn"
+                                                                                },
+                                                                                "name": "tag",
+                                                                                "type": "string"
+                                                                            },
+                                                                            {
+                                                                                "default": null,
+                                                                                "doc": "Additional context about the association",
+                                                                                "name": "context",
+                                                                                "type": [
+                                                                                    "null",
+                                                                                    "string"
+                                                                                ]
+                                                                            }
+                                                                        ],
+                                                                        "name": "TagAssociation",
+                                                                        "namespace": "com.linkedin.pegasus2avro.common",
+                                                                        "type": "record"
+                                                                    },
+                                                                    "type": "array"
+                                                                }
+                                                            }
+                                                        ],
+                                                        "name": "GlobalTags",
+                                                        "namespace": "com.linkedin.pegasus2avro.common",
+                                                        "type": "record"
+                                                    }
+                                                ]
+                                            },
+                                            {
+                                                "Relationship": {
+                                                    "/terms/*/urn": {
+                                                        "entityTypes": [
+                                                            "glossaryTerm"
+                                                        ],
+                                                        "name": "SchemaFieldWithGlossaryTerm"
+                                                    }
+                                                },
+                                                "Searchable": {
+                                                    "/terms/*/urn": {
+                                                        "boostScore": 0.5,
+                                                        "fieldName": "fieldGlossaryTerms",
+                                                        "fieldType": "URN"
+                                                    }
+                                                },
+                                                "default": null,
+                                                "doc": "Glossary terms associated with the field",
+                                                "name": "glossaryTerms",
+                                                "type": [
+                                                    "null",
+                                                    {
+                                                        "Aspect": {
+                                                            "name": "glossaryTerms"
+                                                        },
+                                                        "doc": "Related business terms information",
+                                                        "fields": [
+                                                            {
+                                                                "doc": "The related business terms",
+                                                                "name": "terms",
+                                                                "type": {
+                                                                    "items": {
+                                                                        "doc": "Properties of an applied glossary term.",
+                                                                        "fields": [
+                                                                            {
+                                                                                "Relationship": {
+                                                                                    "entityTypes": [
+                                                                                        "glossaryTerm"
+                                                                                    ],
+                                                                                    "name": "TermedWith"
+                                                                                },
+                                                                                "Searchable": {
+                                                                                    "addToFilters": true,
+                                                                                    "fieldName": "glossaryTerms",
+                                                                                    "fieldType": "URN",
+                                                                                    "filterNameOverride": "Glossary Term",
+                                                                                    "hasValuesFieldName": "hasGlossaryTerms"
+                                                                                },
+                                                                                "Urn": "GlossaryTermUrn",
+                                                                                "doc": "Urn of the applied glossary term",
+                                                                                "java": {
+                                                                                    "class": "com.linkedin.pegasus2avro.common.urn.GlossaryTermUrn"
+                                                                                },
+                                                                                "name": "urn",
+                                                                                "type": "string"
+                                                                            },
+                                                                            {
+                                                                                "default": null,
+                                                                                "doc": "Additional context about the association",
+                                                                                "name": "context",
+                                                                                "type": [
+                                                                                    "null",
+                                                                                    "string"
+                                                                                ]
+                                                                            }
+                                                                        ],
+                                                                        "name": "GlossaryTermAssociation",
+                                                                        "namespace": "com.linkedin.pegasus2avro.common",
+                                                                        "type": "record"
+                                                                    },
+                                                                    "type": "array"
+                                                                }
+                                                            },
+                                                            {
+                                                                "doc": "Audit stamp containing who reported the related business term",
+                                                                "name": "auditStamp",
+                                                                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+                                                            }
+                                                        ],
+                                                        "name": "GlossaryTerms",
+                                                        "namespace": "com.linkedin.pegasus2avro.common",
+                                                        "type": "record"
+                                                    }
+                                                ]
+                                            },
+                                            {
+                                                "default": false,
+                                                "doc": "For schema fields that are part of complex keys, set this field to true\nWe do this to easily distinguish between value and key fields",
+                                                "name": "isPartOfKey",
+                                                "type": "boolean"
+                                            },
+                                            {
+                                                "default": null,
+                                                "doc": "For Datasets which are partitioned, this determines the partitioning key.",
+                                                "name": "isPartitioningKey",
+                                                "type": [
+                                                    "null",
+                                                    "boolean"
+                                                ]
+                                            },
+                                            {
+                                                "default": null,
+                                                "doc": "For schema fields that have other properties that are not modeled explicitly,\nuse this field to serialize those properties into a JSON string",
+                                                "name": "jsonProps",
+                                                "type": [
+                                                    "null",
+                                                    "string"
+                                                ]
+                                            }
+                                        ],
+                                        "name": "SchemaField",
+                                        "namespace": "com.linkedin.pegasus2avro.schema",
+                                        "type": "record"
+                                    }
+                                ]
+                            }
+                        ],
+                        "name": "InputField",
+                        "namespace": "com.linkedin.pegasus2avro.common",
+                        "type": "record"
+                    },
+                    "type": "array"
+                }
             }
         ],
-        "name": "PostKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "name": "InputFields",
+        "namespace": "com.linkedin.pegasus2avro.common",
         "type": "record"
     },
     {
         "Aspect": {
-            "entityAspects": [
-                "globalSettingsInfo"
-            ],
-            "entityCategory": "internal",
-            "entityDoc": "Global settings for an the platform",
-            "keyForEntity": "globalSettings",
-            "name": "globalSettingsKey"
+            "name": "cost"
         },
-        "doc": "Key for a Global Settings",
         "fields": [
             {
-                "doc": "Id for the settings. There should be only 1 global settings urn: urn:li:globalSettings:0",
-                "name": "id",
-                "type": "string"
+                "name": "costType",
+                "type": {
+                    "doc": "Type of Cost Code",
+                    "name": "CostType",
+                    "namespace": "com.linkedin.pegasus2avro.common",
+                    "symbolDocs": {
+                        "ORG_COST_TYPE": "Org Cost Type to which the Cost of this entity should be attributed to"
+                    },
+                    "symbols": [
+                        "ORG_COST_TYPE"
+                    ],
+                    "type": "enum"
+                }
+            },
+            {
+                "name": "cost",
+                "type": {
+                    "fields": [
+                        {
+                            "default": null,
+                            "name": "costId",
+                            "type": [
+                                "null",
+                                "double"
+                            ]
+                        },
+                        {
+                            "default": null,
+                            "name": "costCode",
+                            "type": [
+                                "null",
+                                "string"
+                            ]
+                        },
+                        {
+                            "doc": "Contains the name of the field that has its value set.",
+                            "name": "fieldDiscriminator",
+                            "type": {
+                                "name": "CostCostDiscriminator",
+                                "namespace": "com.linkedin.pegasus2avro.common",
+                                "symbols": [
+                                    "costId",
+                                    "costCode"
+                                ],
+                                "type": "enum"
+                            }
+                        }
+                    ],
+                    "name": "CostCost",
+                    "namespace": "com.linkedin.pegasus2avro.common",
+                    "type": "record"
+                }
             }
         ],
-        "name": "GlobalSettingsKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "name": "Cost",
+        "namespace": "com.linkedin.pegasus2avro.common",
         "type": "record"
     },
     {
         "Aspect": {
-            "entityAspects": [
-                "domains",
-                "container",
-                "deprecation",
-                "inputFields",
-                "chartUsageStatistics",
-                "embed"
-            ],
-            "entityCategory": "_unset_",
-            "keyForEntity": "chart",
-            "name": "chartKey"
+            "name": "subTypes"
         },
-        "doc": "Key for a Chart",
+        "doc": "Sub Types. Use this aspect to specialize a generic Entity\ne.g. Making a Dataset also be a View or also be a LookerExplore",
         "fields": [
             {
                 "Searchable": {
-                    "boostScore": 4.0,
-                    "fieldName": "tool",
-                    "fieldType": "TEXT_PARTIAL"
+                    "/*": {
+                        "addToFilters": true,
+                        "fieldType": "KEYWORD",
+                        "filterNameOverride": "Sub Type",
+                        "queryByDefault": true
+                    }
                 },
-                "doc": "The name of the dashboard tool such as looker, redash etc.",
-                "name": "dashboardTool",
-                "type": "string"
-            },
-            {
-                "doc": "Unique id for the chart. This id should be globally unique for a dashboarding tool even when there are multiple deployments of it. As an example, chart URL could be used here for Looker such as 'looker.linkedin.com/looks/1234'",
-                "name": "chartId",
-                "type": "string"
+                "doc": "The names of the specific types.",
+                "name": "typeNames",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                }
             }
         ],
-        "name": "ChartKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "name": "SubTypes",
+        "namespace": "com.linkedin.pegasus2avro.common",
         "type": "record"
     },
+    "com.linkedin.pegasus2avro.common.GlobalTags",
     {
         "Aspect": {
-            "entityAspects": [
-                "datahubIngestionRunSummary",
-                "datahubIngestionCheckpoint",
-                "domains",
-                "deprecation",
-                "versionInfo"
-            ],
-            "entityCategory": "_unset_",
-            "keyForEntity": "dataJob",
-            "name": "dataJobKey"
+            "name": "siblings"
         },
-        "doc": "Key for a Data Job",
+        "doc": "Siblings information of an entity.",
         "fields": [
             {
                 "Relationship": {
-                    "entityTypes": [
-                        "dataFlow"
-                    ],
-                    "name": "IsPartOf"
+                    "/*": {
+                        "entityTypes": [
+                            "dataset"
+                        ],
+                        "name": "SiblingOf"
+                    }
                 },
                 "Searchable": {
-                    "fieldName": "dataFlow",
-                    "fieldType": "URN_PARTIAL",
-                    "queryByDefault": false
+                    "/*": {
+                        "fieldName": "siblings",
+                        "fieldType": "URN",
+                        "queryByDefault": false
+                    }
                 },
-                "doc": "Standardized data processing flow urn representing the flow for the job",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                "Urn": "Urn",
+                "doc": "List of sibling entities",
+                "name": "siblings",
+                "type": {
+                    "items": "string",
+                    "type": "array"
                 },
-                "name": "flow",
-                "type": "string"
+                "urn_is_array": true
             },
             {
-                "Searchable": {
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "Unique Identifier of the data job",
-                "name": "jobId",
-                "type": "string"
+                "doc": "If this is the leader entity of the set of siblings",
+                "name": "primary",
+                "type": "boolean"
             }
         ],
-        "name": "DataJobKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "name": "Siblings",
+        "namespace": "com.linkedin.pegasus2avro.common",
         "type": "record"
     },
     {
         "Aspect": {
-            "entityAspects": [
-                "domainProperties",
-                "institutionalMemory",
-                "ownership"
-            ],
-            "entityCategory": "_unset_",
-            "entityDoc": "A data domain within an organization.",
-            "keyForEntity": "domain",
-            "name": "domainKey"
+            "name": "operation",
+            "type": "timeseries"
         },
-        "doc": "Key for an Asset Domain",
+        "doc": "Operational info for an entity.",
         "fields": [
             {
-                "doc": "A unique id for the domain. Should be separate from the name used for displaying a Domain.",
-                "name": "id",
-                "type": "string"
-            }
-        ],
-        "name": "DomainKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "mlModelDeploymentKey"
-        },
-        "doc": "Key for an ML model deployment",
-        "fields": [
+                "doc": "The event timestamp field as epoch at UTC in milli seconds.",
+                "name": "timestampMillis",
+                "type": "long"
+            },
             {
-                "doc": "Standardized platform urn for the model Deployment",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "platform",
-                "type": "string"
+                "default": null,
+                "doc": "Granularity of the event if applicable",
+                "name": "eventGranularity",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
+                ]
             },
             {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
+                "default": {
+                    "partition": "FULL_TABLE_SNAPSHOT",
+                    "timePartition": null,
+                    "type": "FULL_TABLE"
                 },
-                "doc": "Name of the MLModelDeployment",
-                "name": "name",
-                "type": "string"
+                "doc": "The optional partition specification.",
+                "name": "partitionSpec",
+                "type": [
+                    "com.linkedin.pegasus2avro.timeseries.PartitionSpec",
+                    "null"
+                ]
             },
             {
-                "Searchable": {
-                    "addToFilters": true,
-                    "fieldType": "TEXT_PARTIAL",
-                    "filterNameOverride": "Environment",
-                    "queryByDefault": false
+                "default": null,
+                "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
+                "name": "messageId",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "TimeseriesField": {},
+                "Urn": "Urn",
+                "default": null,
+                "doc": "Actor who issued this operation.",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                 },
-                "doc": "Fabric type where model Deployment belongs to or where it was generated",
-                "name": "origin",
+                "name": "actor",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "TimeseriesField": {},
+                "doc": "Operation type of change.",
+                "name": "operationType",
                 "type": {
-                    "doc": "Fabric group type",
-                    "name": "FabricType",
+                    "doc": "Enum to define the operation type when an entity changes.",
+                    "name": "OperationType",
                     "namespace": "com.linkedin.pegasus2avro.common",
                     "symbolDocs": {
-                        "CORP": "Designates corporation fabrics",
-                        "DEV": "Designates development fabrics",
-                        "EI": "Designates early-integration fabrics",
-                        "NON_PROD": "Designates non-production fabrics",
-                        "PRE": "Designates pre-production fabrics",
-                        "PROD": "Designates production fabrics",
-                        "QA": "Designates quality assurance fabrics",
-                        "STG": "Designates staging fabrics",
-                        "TEST": "Designates testing fabrics",
-                        "UAT": "Designates user acceptance testing fabrics"
+                        "ALTER": "Asset was altered",
+                        "CREATE": "Asset was created",
+                        "CUSTOM": "Custom asset operation",
+                        "DELETE": "Rows were deleted",
+                        "DROP": "Asset was dropped",
+                        "INSERT": "Rows were inserted",
+                        "UPDATE": "Rows were updated"
                     },
                     "symbols": [
-                        "DEV",
-                        "TEST",
-                        "QA",
-                        "UAT",
-                        "EI",
-                        "PRE",
-                        "STG",
-                        "NON_PROD",
-                        "PROD",
-                        "CORP"
+                        "INSERT",
+                        "UPDATE",
+                        "DELETE",
+                        "CREATE",
+                        "ALTER",
+                        "DROP",
+                        "CUSTOM",
+                        "UNKNOWN"
                     ],
                     "type": "enum"
                 }
-            }
-        ],
-        "name": "MLModelDeploymentKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "dataProcessKey"
-        },
-        "doc": "Key for a Data Process",
-        "fields": [
-            {
-                "Searchable": {
-                    "boostScore": 4.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "Process name i.e. an ETL job name",
-                "name": "name",
-                "type": "string"
             },
             {
-                "Searchable": {
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "Standardized Orchestrator where data process is defined.\nTODO: Migrate towards something that can be validated like DataPlatform urn",
-                "name": "orchestrator",
-                "type": "string"
+                "TimeseriesField": {},
+                "default": null,
+                "doc": "A custom type of operation. Required if operationType is CUSTOM.",
+                "name": "customOperationType",
+                "type": [
+                    "null",
+                    "string"
+                ]
             },
             {
-                "Searchable": {
-                    "fieldType": "TEXT_PARTIAL",
-                    "queryByDefault": false
-                },
-                "doc": "Fabric type where dataset belongs to or where it was generated.",
-                "name": "origin",
-                "type": "com.linkedin.pegasus2avro.common.FabricType"
-            }
-        ],
-        "name": "DataProcessKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "entityAspects": [
-                "glossaryTerms",
-                "editableMlModelProperties",
-                "domains"
-            ],
-            "entityCategory": "core",
-            "keyForEntity": "mlModel",
-            "name": "mlModelKey"
-        },
-        "doc": "Key for an ML model",
-        "fields": [
-            {
-                "doc": "Standardized platform urn for the model",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "platform",
-                "type": "string"
+                "TimeseriesField": {},
+                "default": null,
+                "doc": "How many rows were affected by this operation.",
+                "name": "numAffectedRows",
+                "type": [
+                    "null",
+                    "long"
+                ]
             },
             {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
+                "TimeseriesFieldCollection": {
+                    "key": "datasetName"
                 },
-                "doc": "Name of the MLModel",
-                "name": "name",
-                "type": "string"
+                "Urn": "Urn",
+                "default": null,
+                "doc": "Which other datasets were affected by this operation.",
+                "name": "affectedDatasets",
+                "type": [
+                    "null",
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ],
+                "urn_is_array": true
             },
             {
-                "Searchable": {
-                    "addToFilters": true,
-                    "fieldType": "TEXT_PARTIAL",
-                    "filterNameOverride": "Environment",
-                    "queryByDefault": false
-                },
-                "doc": "Fabric type where model belongs to or where it was generated",
-                "name": "origin",
-                "type": "com.linkedin.pegasus2avro.common.FabricType"
-            }
-        ],
-        "name": "MLModelKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "entityAspects": [
-                "dataPlatformInfo"
-            ],
-            "entityCategory": "core",
-            "keyForEntity": "dataPlatform",
-            "name": "dataPlatformKey"
-        },
-        "doc": "Key for a Data Platform",
-        "fields": [
-            {
-                "doc": "Data platform name i.e. hdfs, oracle, espresso",
-                "name": "platformName",
-                "type": "string"
-            }
-        ],
-        "name": "DataPlatformKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "entityAspects": [
-                "glossaryTerms",
-                "editableMlModelGroupProperties",
-                "domains"
-            ],
-            "entityCategory": "core",
-            "keyForEntity": "mlModelGroup",
-            "name": "mlModelGroupKey"
-        },
-        "doc": "Key for an ML model group",
-        "fields": [
-            {
-                "doc": "Standardized platform urn for the model group",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "platform",
-                "type": "string"
+                "TimeseriesField": {},
+                "default": null,
+                "doc": "Source Type",
+                "name": "sourceType",
+                "type": [
+                    "null",
+                    {
+                        "doc": "The source of an operation",
+                        "name": "OperationSourceType",
+                        "namespace": "com.linkedin.pegasus2avro.common",
+                        "symbolDocs": {
+                            "DATA_PLATFORM": "Rows were updated",
+                            "DATA_PROCESS": "Provided by a Data Process"
+                        },
+                        "symbols": [
+                            "DATA_PROCESS",
+                            "DATA_PLATFORM"
+                        ],
+                        "type": "enum"
+                    }
+                ]
             },
             {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "Name of the MLModelGroup",
-                "name": "name",
-                "type": "string"
+                "default": null,
+                "doc": "Custom properties",
+                "name": "customProperties",
+                "type": [
+                    "null",
+                    {
+                        "type": "map",
+                        "values": "string"
+                    }
+                ]
             },
             {
                 "Searchable": {
-                    "fieldType": "TEXT_PARTIAL",
-                    "queryByDefault": false
+                    "fieldName": "lastOperationTime",
+                    "fieldType": "DATETIME"
                 },
-                "doc": "Fabric type where model group belongs to or where it was generated",
-                "name": "origin",
-                "type": "com.linkedin.pegasus2avro.common.FabricType"
+                "TimeseriesField": {},
+                "doc": "The time at which the operation occurred. Would be better named 'operationTime'",
+                "name": "lastUpdatedTimestamp",
+                "type": "long"
             }
         ],
-        "name": "MLModelGroupKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "name": "Operation",
+        "namespace": "com.linkedin.pegasus2avro.common",
         "type": "record"
     },
     {
         "Aspect": {
-            "entityAspects": [
-                "glossaryTerms",
-                "editableMlFeatureProperties",
-                "domains"
-            ],
-            "entityCategory": "core",
-            "keyForEntity": "mlFeature",
-            "name": "mlFeatureKey"
+            "name": "browsePaths"
         },
-        "doc": "Key for an MLFeature",
+        "doc": "Shared aspect containing Browse Paths to be indexed for an entity.",
         "fields": [
             {
                 "Searchable": {
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "Namespace for the feature",
-                "name": "featureNamespace",
-                "type": "string"
-            },
-            {
-                "Searchable": {
-                    "boostScore": 8.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
+                    "/*": {
+                        "fieldName": "browsePaths",
+                        "fieldType": "BROWSE_PATH"
+                    }
                 },
-                "doc": "Name of the feature",
-                "name": "name",
-                "type": "string"
+                "doc": "A list of valid browse paths for the entity.\n\nBrowse paths are expected to be forward slash-separated strings. For example: 'prod/snowflake/datasetName'",
+                "name": "paths",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                }
             }
         ],
-        "name": "MLFeatureKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "name": "BrowsePaths",
+        "namespace": "com.linkedin.pegasus2avro.common",
         "type": "record"
     },
+    "com.linkedin.pegasus2avro.common.GlossaryTerms",
     {
         "Aspect": {
-            "entityAspects": [
-                "glossaryNodeInfo",
-                "institutionalMemory",
-                "ownership",
-                "status"
-            ],
-            "entityCategory": "_unset_",
-            "keyForEntity": "glossaryNode",
-            "name": "glossaryNodeKey"
+            "name": "origin"
         },
-        "doc": "Key for a GlossaryNode",
+        "doc": "Carries information about where an entity originated from.",
         "fields": [
             {
-                "Searchable": {
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "name": "name",
-                "type": "string"
-            }
-        ],
-        "name": "GlossaryNodeKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "entityAspects": [
-                "glossaryTermInfo",
-                "institutionalMemory",
-                "schemaMetadata",
-                "ownership",
-                "deprecation",
-                "domains"
-            ],
-            "entityCategory": "_unset_",
-            "keyForEntity": "glossaryTerm",
-            "name": "glossaryTermKey"
-        },
-        "doc": "Key for a GlossaryTerm",
-        "fields": [
+                "doc": "Where an entity originated from. Either NATIVE or EXTERNAL.",
+                "name": "type",
+                "type": {
+                    "doc": "Enum to define where an entity originated from.",
+                    "name": "OriginType",
+                    "namespace": "com.linkedin.pegasus2avro.common",
+                    "symbolDocs": {
+                        "EXTERNAL": "The entity is external to DataHub.",
+                        "NATIVE": "The entity is native to DataHub."
+                    },
+                    "symbols": [
+                        "NATIVE",
+                        "EXTERNAL"
+                    ],
+                    "type": "enum"
+                }
+            },
             {
-                "Searchable": {
-                    "enableAutocomplete": true,
-                    "fieldName": "id",
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "The term name, which serves as a unique id",
-                "name": "name",
-                "type": "string"
+                "default": null,
+                "doc": "Only populated if type is EXTERNAL. The externalType of the entity, such as the name of the identity provider.",
+                "name": "externalType",
+                "type": [
+                    "null",
+                    "string"
+                ]
             }
         ],
-        "name": "GlossaryTermKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "name": "Origin",
+        "namespace": "com.linkedin.pegasus2avro.common",
         "type": "record"
     },
     {
         "Aspect": {
-            "entityAspects": [
-                "viewProperties",
-                "subTypes",
-                "datasetProfile",
-                "datasetUsageStatistics",
-                "operation",
-                "domains",
-                "schemaMetadata",
-                "status",
-                "container",
-                "deprecation",
-                "testResults",
-                "siblings",
-                "embed"
-            ],
-            "entityCategory": "core",
-            "entityDoc": "Datasets represent logical or physical data assets stored or represented in various data platforms. Tables, Views, Streams are all instances of datasets.",
-            "keyForEntity": "dataset",
-            "name": "datasetKey"
+            "name": "dataPlatformInstance"
         },
-        "doc": "Key for a Dataset",
+        "doc": "The specific instance of the data platform that this entity belongs to",
         "fields": [
             {
                 "Searchable": {
-                    "enableAutocomplete": true,
-                    "fieldType": "URN"
+                    "addToFilters": true,
+                    "fieldType": "URN",
+                    "filterNameOverride": "Platform"
                 },
-                "doc": "Data platform urn associated with the dataset",
+                "Urn": "Urn",
+                "doc": "Data Platform",
                 "java": {
                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                 },
                 "name": "platform",
                 "type": "string"
             },
             {
                 "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldName": "id",
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "Unique guid for dataset",
-                "name": "name",
-                "type": "string"
-            },
-            {
-                "Searchable": {
                     "addToFilters": true,
-                    "fieldType": "TEXT_PARTIAL",
-                    "filterNameOverride": "Environment",
-                    "queryByDefault": false
+                    "fieldName": "platformInstance",
+                    "fieldType": "URN",
+                    "filterNameOverride": "Platform Instance"
                 },
-                "doc": "Fabric type where dataset belongs to or where it was generated.",
-                "name": "origin",
-                "type": "com.linkedin.pegasus2avro.common.FabricType"
-            }
-        ],
-        "name": "DatasetKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "entityAspects": [
-                "dataHubViewInfo"
-            ],
-            "entityCategory": "core",
-            "keyForEntity": "dataHubView",
-            "name": "dataHubViewKey"
-        },
-        "doc": "Key for a DataHub View",
-        "fields": [
-            {
-                "doc": "A unique id for the View",
-                "name": "id",
-                "type": "string"
-            }
-        ],
-        "name": "DataHubViewKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "entityAspects": [
-                "dataHubExecutionRequestInput",
-                "dataHubExecutionRequestSignal",
-                "dataHubExecutionRequestResult"
-            ],
-            "entityCategory": "internal",
-            "keyForEntity": "dataHubExecutionRequest",
-            "name": "dataHubExecutionRequestKey"
-        },
-        "doc": "Key for an DataHub Execution Request",
-        "fields": [
-            {
-                "doc": "A unique id for the DataHub execution request.",
-                "name": "id",
-                "type": "string"
+                "Urn": "Urn",
+                "default": null,
+                "doc": "Instance of the data platform (e.g. db instance)",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                },
+                "name": "instance",
+                "type": [
+                    "null",
+                    "string"
+                ]
             }
         ],
-        "name": "ExecutionRequestKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "name": "DataPlatformInstance",
+        "namespace": "com.linkedin.pegasus2avro.common",
         "type": "record"
     },
     {
         "Aspect": {
-            "entityAspects": [
-                "testInfo"
-            ],
-            "entityCategory": "core",
-            "entityDoc": "A DataHub test",
-            "keyForEntity": "test",
-            "name": "testKey"
+            "name": "institutionalMemory"
         },
-        "doc": "Key for a Test",
+        "doc": "Institutional memory of an entity. This is a way to link to relevant documentation and provide description of the documentation. Institutional or tribal knowledge is very important for users to leverage the entity.",
         "fields": [
             {
-                "doc": "Unique id for the test",
-                "name": "id",
-                "type": "string"
+                "doc": "List of records that represent institutional memory of an entity. Each record consists of a link, description, creator and timestamps associated with that record.",
+                "name": "elements",
+                "type": {
+                    "items": {
+                        "doc": "Metadata corresponding to a record of institutional memory.",
+                        "fields": [
+                            {
+                                "doc": "Link to an engineering design document or a wiki page.",
+                                "java": {
+                                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                                },
+                                "name": "url",
+                                "type": "string"
+                            },
+                            {
+                                "doc": "Description of the link.",
+                                "name": "description",
+                                "type": "string"
+                            },
+                            {
+                                "doc": "Audit stamp associated with creation of this record",
+                                "name": "createStamp",
+                                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+                            }
+                        ],
+                        "name": "InstitutionalMemoryMetadata",
+                        "namespace": "com.linkedin.pegasus2avro.common",
+                        "type": "record"
+                    },
+                    "type": "array"
+                }
             }
         ],
-        "name": "TestKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "name": "InstitutionalMemory",
+        "namespace": "com.linkedin.pegasus2avro.common",
         "type": "record"
     },
     {
         "Aspect": {
-            "entityAspects": [
-                "domains",
-                "deprecation",
-                "versionInfo"
-            ],
-            "entityCategory": "core",
-            "keyForEntity": "dataFlow",
-            "name": "dataFlowKey"
+            "name": "deprecation"
         },
-        "doc": "Key for a Data Flow",
+        "doc": "Deprecation status of an entity",
         "fields": [
             {
                 "Searchable": {
-                    "fieldType": "TEXT_PARTIAL"
+                    "fieldType": "BOOLEAN",
+                    "weightsPerFieldValue": {
+                        "true": 0.5
+                    }
                 },
-                "doc": "Workflow manager like azkaban, airflow which orchestrates the flow",
-                "name": "orchestrator",
-                "type": "string"
+                "doc": "Whether the entity is deprecated.",
+                "name": "deprecated",
+                "type": "boolean"
             },
             {
-                "Searchable": {
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "Unique Identifier of the data flow",
-                "name": "flowId",
+                "default": null,
+                "doc": "The time user plan to decommission this entity.",
+                "name": "decommissionTime",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            },
+            {
+                "doc": "Additional information about the entity deprecation plan, such as the wiki, doc, RB.",
+                "name": "note",
                 "type": "string"
             },
             {
-                "Searchable": {
-                    "fieldType": "TEXT_PARTIAL"
+                "Urn": "Urn",
+                "doc": "The user URN which will be credited for modifying this deprecation content.",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                 },
-                "doc": "Cluster where the flow is executed",
-                "name": "cluster",
+                "name": "actor",
                 "type": "string"
             }
         ],
-        "name": "DataFlowKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "name": "Deprecation",
+        "namespace": "com.linkedin.pegasus2avro.common",
         "type": "record"
     },
     {
         "Aspect": {
-            "entityAspects": [
-                "dataHubPolicyInfo"
-            ],
-            "entityCategory": "internal",
-            "entityDoc": "DataHub Policies represent access policies granted to users or groups on metadata operations like edit, view etc.",
-            "keyForEntity": "dataHubPolicy",
-            "name": "dataHubPolicyKey"
+            "name": "ownership"
         },
-        "doc": "Key for a DataHub Policy",
+        "doc": "Ownership information of an entity.",
         "fields": [
             {
-                "doc": "A unique id for the DataHub access policy record. Generated on the server side at policy creation time.",
-                "name": "id",
-                "type": "string"
-            }
-        ],
-        "name": "DataHubPolicyKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "entityAspects": [
-                "containerProperties",
-                "editableContainerProperties",
-                "dataPlatformInstance",
-                "subTypes",
-                "ownership",
-                "container",
-                "globalTags",
-                "glossaryTerms",
-                "institutionalMemory",
-                "browsePaths",
-                "status",
-                "domains"
-            ],
-            "entityCategory": "_unset_",
-            "entityDoc": "A container of related data assets.",
-            "keyForEntity": "container",
-            "name": "containerKey"
-        },
-        "doc": "Key for an Asset Container",
-        "fields": [
+                "doc": "List of owners of the entity.",
+                "name": "owners",
+                "type": {
+                    "items": {
+                        "doc": "Ownership information",
+                        "fields": [
+                            {
+                                "Relationship": {
+                                    "entityTypes": [
+                                        "corpuser",
+                                        "corpGroup"
+                                    ],
+                                    "name": "OwnedBy"
+                                },
+                                "Searchable": {
+                                    "addToFilters": true,
+                                    "fieldName": "owners",
+                                    "fieldType": "URN",
+                                    "filterNameOverride": "Owned By",
+                                    "hasValuesFieldName": "hasOwners",
+                                    "queryByDefault": false
+                                },
+                                "Urn": "Urn",
+                                "doc": "Owner URN, e.g. urn:li:corpuser:ldap, urn:li:corpGroup:group_name, and urn:li:multiProduct:mp_name\n(Caveat: only corpuser is currently supported in the frontend.)",
+                                "java": {
+                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                },
+                                "name": "owner",
+                                "type": "string"
+                            },
+                            {
+                                "doc": "The type of the ownership",
+                                "name": "type",
+                                "type": {
+                                    "deprecatedSymbols": {
+                                        "CONSUMER": true,
+                                        "DATAOWNER": true,
+                                        "DELEGATE": true,
+                                        "DEVELOPER": true,
+                                        "PRODUCER": true,
+                                        "STAKEHOLDER": true
+                                    },
+                                    "doc": "Asset owner types",
+                                    "name": "OwnershipType",
+                                    "namespace": "com.linkedin.pegasus2avro.common",
+                                    "symbolDocs": {
+                                        "BUSINESS_OWNER": "A person or group who is responsible for logical, or business related, aspects of the asset.",
+                                        "CONSUMER": "A person, group, or service that consumes the data\nDeprecated! Use TECHNICAL_OWNER or BUSINESS_OWNER instead.",
+                                        "DATAOWNER": "A person or group that is owning the data\nDeprecated! Use TECHNICAL_OWNER instead.",
+                                        "DATA_STEWARD": "A steward, expert, or delegate responsible for the asset.",
+                                        "DELEGATE": "A person or a group that overseas the operation, e.g. a DBA or SRE.\nDeprecated! Use TECHNICAL_OWNER instead.",
+                                        "DEVELOPER": "A person or group that is in charge of developing the code\nDeprecated! Use TECHNICAL_OWNER instead.",
+                                        "NONE": "No specific type associated to the owner.",
+                                        "PRODUCER": "A person, group, or service that produces/generates the data\nDeprecated! Use TECHNICAL_OWNER instead.",
+                                        "STAKEHOLDER": "A person or a group that has direct business interest\nDeprecated! Use TECHNICAL_OWNER, BUSINESS_OWNER, or STEWARD instead.",
+                                        "TECHNICAL_OWNER": "person or group who is responsible for technical aspects of the asset."
+                                    },
+                                    "symbols": [
+                                        "TECHNICAL_OWNER",
+                                        "BUSINESS_OWNER",
+                                        "DATA_STEWARD",
+                                        "NONE",
+                                        "DEVELOPER",
+                                        "DATAOWNER",
+                                        "DELEGATE",
+                                        "PRODUCER",
+                                        "CONSUMER",
+                                        "STAKEHOLDER"
+                                    ],
+                                    "type": "enum"
+                                }
+                            },
+                            {
+                                "default": null,
+                                "doc": "Source information for the ownership",
+                                "name": "source",
+                                "type": [
+                                    "null",
+                                    {
+                                        "doc": "Source/provider of the ownership information",
+                                        "fields": [
+                                            {
+                                                "doc": "The type of the source",
+                                                "name": "type",
+                                                "type": {
+                                                    "name": "OwnershipSourceType",
+                                                    "namespace": "com.linkedin.pegasus2avro.common",
+                                                    "symbolDocs": {
+                                                        "AUDIT": "Auditing system or audit logs",
+                                                        "DATABASE": "Database, e.g. GRANTS table",
+                                                        "FILE_SYSTEM": "File system, e.g. file/directory owner",
+                                                        "ISSUE_TRACKING_SYSTEM": "Issue tracking system, e.g. Jira",
+                                                        "MANUAL": "Manually provided by a user",
+                                                        "OTHER": "Other sources",
+                                                        "SERVICE": "Other ownership-like service, e.g. Nuage, ACL service etc",
+                                                        "SOURCE_CONTROL": "SCM system, e.g. GIT, SVN"
+                                                    },
+                                                    "symbols": [
+                                                        "AUDIT",
+                                                        "DATABASE",
+                                                        "FILE_SYSTEM",
+                                                        "ISSUE_TRACKING_SYSTEM",
+                                                        "MANUAL",
+                                                        "SERVICE",
+                                                        "SOURCE_CONTROL",
+                                                        "OTHER"
+                                                    ],
+                                                    "type": "enum"
+                                                }
+                                            },
+                                            {
+                                                "default": null,
+                                                "doc": "A reference URL for the source",
+                                                "name": "url",
+                                                "type": [
+                                                    "null",
+                                                    "string"
+                                                ]
+                                            }
+                                        ],
+                                        "name": "OwnershipSource",
+                                        "namespace": "com.linkedin.pegasus2avro.common",
+                                        "type": "record"
+                                    }
+                                ]
+                            }
+                        ],
+                        "name": "Owner",
+                        "namespace": "com.linkedin.pegasus2avro.common",
+                        "type": "record"
+                    },
+                    "type": "array"
+                }
+            },
             {
-                "default": null,
-                "doc": "Unique guid for container",
-                "name": "guid",
-                "type": [
-                    "null",
-                    "string"
-                ]
+                "default": {
+                    "actor": "urn:li:corpuser:unknown",
+                    "impersonator": null,
+                    "message": null,
+                    "time": 0
+                },
+                "doc": "Audit stamp containing who last modified the record and when. A value of 0 in the time field indicates missing data.",
+                "name": "lastModified",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
             }
         ],
-        "name": "ContainerKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "name": "Ownership",
+        "namespace": "com.linkedin.pegasus2avro.common",
         "type": "record"
     },
     {
         "Aspect": {
-            "entityAspects": [
-                "corpGroupInfo",
-                "corpGroupEditableInfo",
-                "globalTags",
-                "ownership",
-                "status",
-                "origin"
-            ],
-            "entityCategory": "_unset_",
-            "entityDoc": "CorpGroup represents an identity of a group of users in the enterprise.",
-            "keyForEntity": "corpGroup",
-            "name": "corpGroupKey"
+            "name": "status"
         },
-        "doc": "Key for a CorpGroup",
+        "doc": "The lifecycle status metadata of an entity, e.g. dataset, metric, feature, etc.\nThis aspect is used to represent soft deletes conventionally.",
         "fields": [
             {
                 "Searchable": {
-                    "fieldType": "TEXT_PARTIAL"
+                    "fieldType": "BOOLEAN"
                 },
-                "doc": "The URL-encoded name of the AD/LDAP group. Serves as a globally unique identifier within DataHub.",
-                "name": "name",
-                "type": "string"
-            }
-        ],
-        "name": "CorpGroupKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "entityAspects": [
-                "inviteToken"
-            ],
-            "entityCategory": "core",
-            "keyForEntity": "inviteToken",
-            "name": "inviteTokenKey"
-        },
-        "doc": "Key for an InviteToken.",
-        "fields": [
-            {
-                "doc": "A unique id for the invite token.",
-                "name": "id",
-                "type": "string"
-            }
-        ],
-        "name": "InviteTokenKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "entityAspects": [
-                "notebookInfo",
-                "notebookContent",
-                "editableNotebookProperties",
-                "ownership",
-                "status",
-                "globalTags",
-                "glossaryTerms",
-                "browsePaths",
-                "institutionalMemory",
-                "domains",
-                "subTypes",
-                "dataPlatformInstance"
-            ],
-            "entityCategory": "_unset_",
-            "entityDoc": "Notebook represents a combination of query, text, chart and etc. This is in BETA version",
-            "keyForEntity": "notebook",
-            "name": "notebookKey"
-        },
-        "doc": "Key for a Notebook",
-        "fields": [
-            {
-                "doc": "The name of the Notebook tool such as QueryBook, etc.",
-                "name": "notebookTool",
-                "type": "string"
-            },
-            {
-                "doc": "Unique id for the Notebook. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773'",
-                "name": "notebookId",
-                "type": "string"
+                "default": false,
+                "doc": "Whether the entity has been removed (soft-deleted).",
+                "name": "removed",
+                "type": "boolean"
             }
         ],
-        "name": "NotebookKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "name": "Status",
+        "namespace": "com.linkedin.pegasus2avro.common",
         "type": "record"
     },
     {
         "Aspect": {
-            "entityAspects": [],
-            "entityCategory": "core",
-            "keyForEntity": "schemaField",
-            "name": "schemaFieldKey"
+            "name": "sourceCode"
         },
-        "doc": "Key for a SchemaField",
+        "doc": "Source Code",
         "fields": [
             {
-                "Searchable": {
-                    "fieldType": "URN"
-                },
-                "doc": "Parent associated with the schema field",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "parent",
-                "type": "string"
-            },
-            {
-                "Searchable": {
-                    "fieldType": "KEYWORD"
-                },
-                "doc": "fieldPath identifying the schema field",
-                "name": "fieldPath",
-                "type": "string"
+                "doc": "Source Code along with types",
+                "name": "sourceCode",
+                "type": {
+                    "items": {
+                        "doc": "Source Code Url Entity",
+                        "fields": [
+                            {
+                                "doc": "Source Code Url Types",
+                                "name": "type",
+                                "type": {
+                                    "name": "SourceCodeUrlType",
+                                    "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+                                    "symbols": [
+                                        "ML_MODEL_SOURCE_CODE",
+                                        "TRAINING_PIPELINE_SOURCE_CODE",
+                                        "EVALUATION_PIPELINE_SOURCE_CODE"
+                                    ],
+                                    "type": "enum"
+                                }
+                            },
+                            {
+                                "doc": "Source Code Url",
+                                "java": {
+                                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                                },
+                                "name": "sourceCodeUrl",
+                                "type": "string"
+                            }
+                        ],
+                        "name": "SourceCodeUrl",
+                        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+                        "type": "record"
+                    },
+                    "type": "array"
+                }
             }
         ],
-        "name": "SchemaFieldKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "name": "SourceCode",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "entityAspects": [
-                "tagProperties",
-                "ownership",
-                "deprecation"
-            ],
-            "entityCategory": "_unset_",
-            "keyForEntity": "tag",
-            "name": "tagKey"
+            "name": "editableMlModelProperties"
         },
-        "doc": "Key for a Tag",
+        "doc": "Properties associated with a ML Model editable from the UI",
         "fields": [
             {
                 "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldName": "id",
-                    "fieldType": "TEXT_PARTIAL"
+                    "fieldName": "editedDescription",
+                    "fieldType": "TEXT"
                 },
-                "doc": "The tag name, which serves as a unique id",
-                "name": "name",
-                "type": "string"
+                "default": null,
+                "doc": "Documentation of the ml model",
+                "name": "description",
+                "type": [
+                    "null",
+                    "string"
+                ]
             }
         ],
-        "name": "TagKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "name": "EditableMLModelProperties",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "entityAspects": [
-                "corpUserInfo",
-                "corpUserEditableInfo",
-                "corpUserStatus",
-                "groupMembership",
-                "globalTags",
-                "status",
-                "corpUserCredentials",
-                "nativeGroupMembership",
-                "corpUserSettings",
-                "origin",
-                "roleMembership"
-            ],
-            "entityCategory": "_unset_",
-            "entityDoc": "CorpUser represents an identity of a person (or an account) in the enterprise.",
-            "keyForEntity": "corpuser",
-            "name": "corpUserKey"
+            "name": "editableMlPrimaryKeyProperties"
         },
-        "doc": "Key for a CorpUser",
+        "doc": "Properties associated with a MLPrimaryKey editable from the UI",
         "fields": [
             {
-                "Searchable": {
-                    "boostScore": 2.0,
-                    "enableAutocomplete": true,
-                    "fieldName": "ldap",
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "The name of the AD/LDAP user.",
-                "name": "username",
-                "type": "string"
+                "default": null,
+                "doc": "Documentation of the MLPrimaryKey",
+                "name": "description",
+                "type": [
+                    "null",
+                    "string"
+                ]
             }
         ],
-        "name": "CorpUserKey",
-        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "name": "EditableMLPrimaryKeyProperties",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "mlModelCaveatsAndRecommendations"
+            "name": "mlModelEvaluationData"
         },
-        "doc": "This section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset? Are there additional recommendations for model use?",
+        "doc": "All referenced datasets would ideally point to any set of documents that provide visibility into the source and composition of the dataset.",
         "fields": [
             {
-                "default": null,
-                "doc": "This section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?",
-                "name": "caveats",
-                "type": [
-                    "null",
-                    {
-                        "doc": "This section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset? Are there additional recommendations for model use?",
+                "doc": "Details on the dataset(s) used for the quantitative analyses in the MLModel",
+                "name": "evaluationData",
+                "type": {
+                    "items": {
+                        "doc": "BaseData record",
                         "fields": [
                             {
-                                "default": null,
-                                "doc": "Did the results suggest any further testing?",
-                                "name": "needsFurtherTesting",
-                                "type": [
-                                    "null",
-                                    "boolean"
-                                ]
+                                "Urn": "DatasetUrn",
+                                "doc": "What dataset were used in the MLModel?",
+                                "java": {
+                                    "class": "com.linkedin.pegasus2avro.common.urn.DatasetUrn"
+                                },
+                                "name": "dataset",
+                                "type": "string"
                             },
                             {
                                 "default": null,
-                                "doc": "Caveat Description\nFor ex: Given gender classes are binary (male/not male), which we include as male/female. Further work needed to evaluate across a spectrum of genders.",
-                                "name": "caveatDescription",
+                                "doc": "Why was this dataset chosen?",
+                                "name": "motivation",
                                 "type": [
                                     "null",
                                     "string"
                                 ]
                             },
                             {
                                 "default": null,
-                                "doc": "Relevant groups that were not represented in the evaluation dataset?",
-                                "name": "groupsNotRepresented",
+                                "doc": "How was the data preprocessed (e.g., tokenization of sentences, cropping of images, any filtering such as dropping images without faces)?",
+                                "name": "preProcessing",
                                 "type": [
                                     "null",
                                     {
                                         "items": "string",
                                         "type": "array"
                                     }
                                 ]
                             }
                         ],
-                        "name": "CaveatDetails",
+                        "name": "BaseData",
                         "namespace": "com.linkedin.pegasus2avro.ml.metadata",
                         "type": "record"
-                    }
-                ]
-            },
-            {
-                "default": null,
-                "doc": "Recommendations on where this MLModel should be used.",
-                "name": "recommendations",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "default": null,
-                "doc": "Ideal characteristics of an evaluation dataset for this MLModel",
-                "name": "idealDatasetCharacteristics",
-                "type": [
-                    "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ]
+                    },
+                    "type": "array"
+                }
             }
         ],
-        "name": "CaveatsAndRecommendations",
+        "name": "EvaluationData",
         "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "mlModelEthicalConsiderations"
+            "name": "editableMlFeatureTableProperties"
         },
-        "doc": "This section is intended to demonstrate the ethical considerations that went into MLModel development, surfacing ethical challenges and solutions to stakeholders.",
+        "doc": "Properties associated with a MLFeatureTable editable from the ui",
         "fields": [
             {
+                "Searchable": {
+                    "fieldName": "editedDescription",
+                    "fieldType": "TEXT"
+                },
                 "default": null,
-                "doc": "Does the MLModel use any sensitive data (e.g., protected classes)?",
-                "name": "data",
-                "type": [
-                    "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ]
-            },
-            {
-                "default": null,
-                "doc": " Is the MLModel intended to inform decisions about matters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?",
-                "name": "humanLife",
-                "type": [
-                    "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ]
-            },
-            {
-                "default": null,
-                "doc": "What risk mitigation strategies were used during MLModel development?",
-                "name": "mitigations",
-                "type": [
-                    "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ]
-            },
-            {
-                "default": null,
-                "doc": "What risks may be present in MLModel usage? Try to identify the potential recipients, likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown.",
-                "name": "risksAndHarms",
-                "type": [
-                    "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ]
-            },
-            {
-                "default": null,
-                "doc": "Are there any known MLModel use cases that are especially fraught? This may connect directly to the intended use section",
-                "name": "useCases",
+                "doc": "Documentation of the MLFeatureTable",
+                "name": "description",
                 "type": [
                     "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
+                    "string"
                 ]
             }
         ],
-        "name": "EthicalConsiderations",
+        "name": "EditableMLFeatureTableProperties",
         "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "mlFeatureProperties"
+            "name": "mlPrimaryKeyProperties"
         },
-        "doc": "Properties associated with a MLFeature",
+        "doc": "Properties associated with a MLPrimaryKey",
         "fields": [
             {
-                "Searchable": {
-                    "fieldType": "TEXT",
-                    "hasValuesFieldName": "hasDescription"
-                },
                 "default": null,
-                "doc": "Documentation of the MLFeature",
+                "doc": "Documentation of the MLPrimaryKey",
                 "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "Data Type of the MLFeature",
+                "doc": "Data Type of the MLPrimaryKey",
                 "name": "dataType",
                 "type": [
                     "null",
                     {
                         "doc": "MLFeature Data Type",
                         "name": "MLFeatureDataType",
                         "namespace": "com.linkedin.pegasus2avro.common",
@@ -2900,15 +2763,15 @@
                         ],
                         "type": "enum"
                     }
                 ]
             },
             {
                 "default": null,
-                "doc": "Version of the MLFeature",
+                "doc": "Version of the MLPrimaryKey",
                 "name": "version",
                 "type": [
                     "null",
                     {
                         "doc": "A resource-defined string representing the resource state for the purpose of concurrency control",
                         "fields": [
                             {
@@ -2932,347 +2795,311 @@
                         "entityTypes": [
                             "dataset"
                         ],
                         "isLineage": true,
                         "name": "DerivedFrom"
                     }
                 },
-                "default": null,
-                "doc": "Source of the MLFeature",
+                "Urn": "Urn",
+                "doc": "Source of the MLPrimaryKey",
                 "name": "sources",
-                "type": [
-                    "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ]
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
             }
         ],
-        "name": "MLFeatureProperties",
+        "name": "MLPrimaryKeyProperties",
         "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "mlMetric"
+            "name": "mlHyperParam"
         },
-        "doc": "Properties associated with an ML Metric",
+        "doc": "Properties associated with an ML Hyper Param",
         "fields": [
             {
-                "doc": "Name of the mlMetric",
+                "doc": "Name of the MLHyperParam",
                 "name": "name",
                 "type": "string"
             },
             {
                 "default": null,
-                "doc": "Documentation of the mlMetric",
+                "doc": "Documentation of the MLHyperParam",
                 "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "The value of the mlMetric",
+                "doc": "The value of the MLHyperParam",
                 "name": "value",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "Date when the mlMetric was developed",
+                "doc": "Date when the MLHyperParam was developed",
                 "name": "createdAt",
                 "type": [
                     "null",
                     "long"
                 ]
             }
         ],
-        "name": "MLMetric",
+        "name": "MLHyperParam",
         "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "mlPrimaryKeyProperties"
+            "name": "mlModelDeploymentProperties"
         },
-        "doc": "Properties associated with a MLPrimaryKey",
+        "doc": "Properties associated with an ML Model Deployment",
         "fields": [
             {
+                "Searchable": {
+                    "/*": {
+                        "queryByDefault": true
+                    }
+                },
+                "default": {},
+                "doc": "Custom property bag.",
+                "name": "customProperties",
+                "type": {
+                    "type": "map",
+                    "values": "string"
+                }
+            },
+            {
                 "default": null,
-                "doc": "Documentation of the MLPrimaryKey",
-                "name": "description",
+                "doc": "URL where the reference exist",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "externalUrl",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
+                "Searchable": {
+                    "fieldType": "TEXT",
+                    "hasValuesFieldName": "hasDescription"
+                },
                 "default": null,
-                "doc": "Data Type of the MLPrimaryKey",
-                "name": "dataType",
+                "doc": "Documentation of the MLModelDeployment",
+                "name": "description",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.common.MLFeatureDataType"
+                    "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "Version of the MLPrimaryKey",
-                "name": "version",
+                "doc": "Date when the MLModelDeployment was developed",
+                "name": "createdAt",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.common.VersionTag"
+                    "long"
                 ]
             },
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "dataset"
-                        ],
-                        "isLineage": true,
-                        "name": "DerivedFrom"
-                    }
-                },
-                "doc": "Source of the MLPrimaryKey",
-                "name": "sources",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                }
-            }
-        ],
-        "name": "MLPrimaryKeyProperties",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "mlModelQuantitativeAnalyses"
-        },
-        "doc": "Quantitative analyses should be disaggregated, that is, broken down by the chosen factors. Quantitative analyses should provide the results of evaluating the MLModel according to the chosen metrics, providing confidence interval values when possible.",
-        "fields": [
-            {
                 "default": null,
-                "doc": "Link to a dashboard with results showing how the MLModel performed with respect to each factor",
-                "name": "unitaryResults",
+                "doc": "Version of the MLModelDeployment",
+                "name": "version",
                 "type": [
                     "null",
-                    "string"
+                    "com.linkedin.pegasus2avro.common.VersionTag"
                 ]
             },
             {
                 "default": null,
-                "doc": "Link to a dashboard with results showing how the MLModel performed with respect to the intersection of evaluated factors?",
-                "name": "intersectionalResults",
+                "doc": "Status of the deployment",
+                "name": "status",
                 "type": [
                     "null",
-                    "string"
+                    {
+                        "doc": "Model endpoint statuses",
+                        "name": "DeploymentStatus",
+                        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+                        "symbolDocs": {
+                            "CREATING": "Deployments being created.",
+                            "DELETING": "Deployments being deleted.",
+                            "FAILED": "Deployments with an error state.",
+                            "IN_SERVICE": "Deployments that are active.",
+                            "OUT_OF_SERVICE": "Deployments out of service.",
+                            "ROLLING_BACK": "Deployments being reverted to a previous version.",
+                            "UNKNOWN": "Deployments with unknown/unmappable state.",
+                            "UPDATING": "Deployments being updated."
+                        },
+                        "symbols": [
+                            "OUT_OF_SERVICE",
+                            "CREATING",
+                            "UPDATING",
+                            "ROLLING_BACK",
+                            "IN_SERVICE",
+                            "DELETING",
+                            "FAILED",
+                            "UNKNOWN"
+                        ],
+                        "type": "enum"
+                    }
                 ]
             }
         ],
-        "name": "QuantitativeAnalyses",
+        "name": "MLModelDeploymentProperties",
         "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "mlFeatureTableProperties"
+            "name": "mlModelCaveatsAndRecommendations"
         },
-        "doc": "Properties associated with a MLFeatureTable",
+        "doc": "This section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset? Are there additional recommendations for model use?",
         "fields": [
             {
-                "Searchable": {
-                    "/*": {
-                        "queryByDefault": true
-                    }
-                },
-                "default": {},
-                "doc": "Custom property bag.",
-                "name": "customProperties",
-                "type": {
-                    "type": "map",
-                    "values": "string"
-                }
-            },
-            {
-                "Searchable": {
-                    "fieldType": "TEXT",
-                    "hasValuesFieldName": "hasDescription"
-                },
                 "default": null,
-                "doc": "Documentation of the MLFeatureTable",
-                "name": "description",
+                "doc": "This section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?",
+                "name": "caveats",
                 "type": [
                     "null",
-                    "string"
+                    {
+                        "doc": "This section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset? Are there additional recommendations for model use?",
+                        "fields": [
+                            {
+                                "default": null,
+                                "doc": "Did the results suggest any further testing?",
+                                "name": "needsFurtherTesting",
+                                "type": [
+                                    "null",
+                                    "boolean"
+                                ]
+                            },
+                            {
+                                "default": null,
+                                "doc": "Caveat Description\nFor ex: Given gender classes are binary (male/not male), which we include as male/female. Further work needed to evaluate across a spectrum of genders.",
+                                "name": "caveatDescription",
+                                "type": [
+                                    "null",
+                                    "string"
+                                ]
+                            },
+                            {
+                                "default": null,
+                                "doc": "Relevant groups that were not represented in the evaluation dataset?",
+                                "name": "groupsNotRepresented",
+                                "type": [
+                                    "null",
+                                    {
+                                        "items": "string",
+                                        "type": "array"
+                                    }
+                                ]
+                            }
+                        ],
+                        "name": "CaveatDetails",
+                        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+                        "type": "record"
+                    }
                 ]
             },
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "mlFeature"
-                        ],
-                        "name": "Contains"
-                    }
-                },
-                "Searchable": {
-                    "/*": {
-                        "fieldName": "features",
-                        "fieldType": "URN"
-                    }
-                },
                 "default": null,
-                "doc": "List of features contained in the feature table",
-                "name": "mlFeatures",
+                "doc": "Recommendations on where this MLModel should be used.",
+                "name": "recommendations",
                 "type": [
                     "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
+                    "string"
                 ]
             },
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "mlPrimaryKey"
-                        ],
-                        "name": "KeyedBy"
-                    }
-                },
-                "Searchable": {
-                    "/*": {
-                        "fieldName": "primaryKeys",
-                        "fieldType": "URN"
-                    }
-                },
                 "default": null,
-                "doc": "List of primary keys in the feature table (if multiple, assumed to act as a composite key)",
-                "name": "mlPrimaryKeys",
+                "doc": "Ideal characteristics of an evaluation dataset for this MLModel",
+                "name": "idealDatasetCharacteristics",
                 "type": [
                     "null",
                     {
                         "items": "string",
                         "type": "array"
                     }
                 ]
             }
         ],
-        "name": "MLFeatureTableProperties",
+        "name": "CaveatsAndRecommendations",
         "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "editableMlPrimaryKeyProperties"
+            "name": "mlMetric"
         },
-        "doc": "Properties associated with a MLPrimaryKey editable from the UI",
+        "doc": "Properties associated with an ML Metric",
         "fields": [
             {
+                "doc": "Name of the mlMetric",
+                "name": "name",
+                "type": "string"
+            },
+            {
                 "default": null,
-                "doc": "Documentation of the MLPrimaryKey",
+                "doc": "Documentation of the mlMetric",
                 "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
-            }
-        ],
-        "name": "EditableMLPrimaryKeyProperties",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "editableMlModelProperties"
-        },
-        "doc": "Properties associated with a ML Model editable from the UI",
-        "fields": [
+            },
             {
-                "Searchable": {
-                    "fieldName": "editedDescription",
-                    "fieldType": "TEXT"
-                },
                 "default": null,
-                "doc": "Documentation of the ml model",
-                "name": "description",
+                "doc": "The value of the mlMetric",
+                "name": "value",
                 "type": [
                     "null",
                     "string"
                 ]
-            }
-        ],
-        "name": "EditableMLModelProperties",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "editableMlModelGroupProperties"
-        },
-        "doc": "Properties associated with an ML Model Group editable from the UI",
-        "fields": [
+            },
             {
-                "Searchable": {
-                    "fieldName": "editedDescription",
-                    "fieldType": "TEXT"
-                },
                 "default": null,
-                "doc": "Documentation of the ml model group",
-                "name": "description",
+                "doc": "Date when the mlMetric was developed",
+                "name": "createdAt",
                 "type": [
                     "null",
-                    "string"
+                    "long"
                 ]
             }
         ],
-        "name": "EditableMLModelGroupProperties",
+        "name": "MLMetric",
         "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "mlModelMetrics"
+            "name": "mlModelTrainingData"
         },
-        "doc": "Metrics to be featured for the MLModel.",
+        "doc": "Ideally, the MLModel card would contain as much information about the training data as the evaluation data. However, there might be cases where it is not feasible to provide this level of detailed information about the training data. For example, the data may be proprietary, or require a non-disclosure agreement. In these cases, we advocate for basic details about the distributions over groups in the data, as well as any other details that could inform stakeholders on the kinds of biases the model may have encoded.",
         "fields": [
             {
-                "default": null,
-                "doc": "Measures of MLModel performance",
-                "name": "performanceMeasures",
-                "type": [
-                    "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ]
-            },
-            {
-                "default": null,
-                "doc": "Decision Thresholds used (if any)?",
-                "name": "decisionThreshold",
-                "type": [
-                    "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ]
+                "doc": "Details on the dataset(s) used for training the MLModel",
+                "name": "trainingData",
+                "type": {
+                    "items": "com.linkedin.pegasus2avro.ml.metadata.BaseData",
+                    "type": "array"
+                }
             }
         ],
-        "name": "Metrics",
+        "name": "TrainingData",
         "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
             "name": "mlModelGroupProperties"
         },
@@ -3326,65 +3153,44 @@
         ],
         "name": "MLModelGroupProperties",
         "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "intendedUse"
+            "name": "mlModelMetrics"
         },
-        "doc": "Intended Use for the ML Model",
+        "doc": "Metrics to be featured for the MLModel.",
         "fields": [
             {
                 "default": null,
-                "doc": "Primary Use cases for the MLModel.",
-                "name": "primaryUses",
+                "doc": "Measures of MLModel performance",
+                "name": "performanceMeasures",
                 "type": [
                     "null",
                     {
                         "items": "string",
                         "type": "array"
                     }
                 ]
             },
             {
                 "default": null,
-                "doc": "Primary Intended Users - For example, was the MLModel developed for entertainment purposes, for hobbyists, or enterprise solutions?",
-                "name": "primaryUsers",
-                "type": [
-                    "null",
-                    {
-                        "items": {
-                            "name": "IntendedUserType",
-                            "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-                            "symbols": [
-                                "ENTERPRISE",
-                                "HOBBY",
-                                "ENTERTAINMENT"
-                            ],
-                            "type": "enum"
-                        },
-                        "type": "array"
-                    }
-                ]
-            },
-            {
-                "default": null,
-                "doc": "Highlight technology that the MLModel might easily be confused with, or related contexts that users could try to apply the MLModel to.",
-                "name": "outOfScopeUses",
+                "doc": "Decision Thresholds used (if any)?",
+                "name": "decisionThreshold",
                 "type": [
                     "null",
                     {
                         "items": "string",
                         "type": "array"
                     }
                 ]
             }
         ],
-        "name": "IntendedUse",
+        "name": "Metrics",
         "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
             "name": "mlModelFactorPrompts"
         },
@@ -3460,259 +3266,145 @@
         ],
         "name": "MLModelFactorPrompts",
         "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "editableMlFeatureTableProperties"
+            "name": "mlModelEthicalConsiderations"
         },
-        "doc": "Properties associated with a MLFeatureTable editable from the ui",
+        "doc": "This section is intended to demonstrate the ethical considerations that went into MLModel development, surfacing ethical challenges and solutions to stakeholders.",
         "fields": [
             {
-                "Searchable": {
-                    "fieldName": "editedDescription",
-                    "fieldType": "TEXT"
-                },
                 "default": null,
-                "doc": "Documentation of the MLFeatureTable",
-                "name": "description",
+                "doc": "Does the MLModel use any sensitive data (e.g., protected classes)?",
+                "name": "data",
                 "type": [
                     "null",
-                    "string"
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
                 ]
-            }
-        ],
-        "name": "EditableMLFeatureTableProperties",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "mlModelDeploymentProperties"
-        },
-        "doc": "Properties associated with an ML Model Deployment",
-        "fields": [
+            },
             {
-                "Searchable": {
-                    "/*": {
-                        "queryByDefault": true
+                "default": null,
+                "doc": " Is the MLModel intended to inform decisions about matters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?",
+                "name": "humanLife",
+                "type": [
+                    "null",
+                    {
+                        "items": "string",
+                        "type": "array"
                     }
-                },
-                "default": {},
-                "doc": "Custom property bag.",
-                "name": "customProperties",
-                "type": {
-                    "type": "map",
-                    "values": "string"
-                }
+                ]
             },
             {
                 "default": null,
-                "doc": "URL where the reference exist",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                },
-                "name": "externalUrl",
+                "doc": "What risk mitigation strategies were used during MLModel development?",
+                "name": "mitigations",
                 "type": [
                     "null",
-                    "string"
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ]
+            },
+            {
+                "default": null,
+                "doc": "What risks may be present in MLModel usage? Try to identify the potential recipients, likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown.",
+                "name": "risksAndHarms",
+                "type": [
+                    "null",
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
                 ]
             },
             {
+                "default": null,
+                "doc": "Are there any known MLModel use cases that are especially fraught? This may connect directly to the intended use section",
+                "name": "useCases",
+                "type": [
+                    "null",
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ]
+            }
+        ],
+        "name": "EthicalConsiderations",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "mlFeatureProperties"
+        },
+        "doc": "Properties associated with a MLFeature",
+        "fields": [
+            {
                 "Searchable": {
                     "fieldType": "TEXT",
                     "hasValuesFieldName": "hasDescription"
                 },
                 "default": null,
-                "doc": "Documentation of the MLModelDeployment",
+                "doc": "Documentation of the MLFeature",
                 "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "Date when the MLModelDeployment was developed",
-                "name": "createdAt",
+                "doc": "Data Type of the MLFeature",
+                "name": "dataType",
                 "type": [
                     "null",
-                    "long"
+                    "com.linkedin.pegasus2avro.common.MLFeatureDataType"
                 ]
             },
             {
                 "default": null,
-                "doc": "Version of the MLModelDeployment",
+                "doc": "Version of the MLFeature",
                 "name": "version",
                 "type": [
                     "null",
                     "com.linkedin.pegasus2avro.common.VersionTag"
                 ]
             },
             {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "dataset"
+                        ],
+                        "isLineage": true,
+                        "name": "DerivedFrom"
+                    }
+                },
+                "Urn": "Urn",
                 "default": null,
-                "doc": "Status of the deployment",
-                "name": "status",
+                "doc": "Source of the MLFeature",
+                "name": "sources",
                 "type": [
                     "null",
                     {
-                        "doc": "Model endpoint statuses",
-                        "name": "DeploymentStatus",
-                        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-                        "symbolDocs": {
-                            "CREATING": "Deployments being created.",
-                            "DELETING": "Deployments being deleted.",
-                            "FAILED": "Deployments with an error state.",
-                            "IN_SERVICE": "Deployments that are active.",
-                            "OUT_OF_SERVICE": "Deployments out of service.",
-                            "ROLLING_BACK": "Deployments being reverted to a previous version.",
-                            "UNKNOWN": "Deployments with unknown/unmappable state.",
-                            "UPDATING": "Deployments being updated."
-                        },
-                        "symbols": [
-                            "OUT_OF_SERVICE",
-                            "CREATING",
-                            "UPDATING",
-                            "ROLLING_BACK",
-                            "IN_SERVICE",
-                            "DELETING",
-                            "FAILED",
-                            "UNKNOWN"
-                        ],
-                        "type": "enum"
+                        "items": "string",
+                        "type": "array"
                     }
-                ]
-            }
-        ],
-        "name": "MLModelDeploymentProperties",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "sourceCode"
-        },
-        "doc": "Source Code",
-        "fields": [
-            {
-                "doc": "Source Code along with types",
-                "name": "sourceCode",
-                "type": {
-                    "items": {
-                        "doc": "Source Code Url Entity",
-                        "fields": [
-                            {
-                                "doc": "Source Code Url Types",
-                                "name": "type",
-                                "type": {
-                                    "name": "SourceCodeUrlType",
-                                    "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-                                    "symbols": [
-                                        "ML_MODEL_SOURCE_CODE",
-                                        "TRAINING_PIPELINE_SOURCE_CODE",
-                                        "EVALUATION_PIPELINE_SOURCE_CODE"
-                                    ],
-                                    "type": "enum"
-                                }
-                            },
-                            {
-                                "doc": "Source Code Url",
-                                "java": {
-                                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                                },
-                                "name": "sourceCodeUrl",
-                                "type": "string"
-                            }
-                        ],
-                        "name": "SourceCodeUrl",
-                        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-                        "type": "record"
-                    },
-                    "type": "array"
-                }
-            }
-        ],
-        "name": "SourceCode",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "mlModelEvaluationData"
-        },
-        "doc": "All referenced datasets would ideally point to any set of documents that provide visibility into the source and composition of the dataset.",
-        "fields": [
-            {
-                "doc": "Details on the dataset(s) used for the quantitative analyses in the MLModel",
-                "name": "evaluationData",
-                "type": {
-                    "items": {
-                        "doc": "BaseData record",
-                        "fields": [
-                            {
-                                "doc": "What dataset were used in the MLModel?",
-                                "java": {
-                                    "class": "com.linkedin.pegasus2avro.common.urn.DatasetUrn"
-                                },
-                                "name": "dataset",
-                                "type": "string"
-                            },
-                            {
-                                "default": null,
-                                "doc": "Why was this dataset chosen?",
-                                "name": "motivation",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "How was the data preprocessed (e.g., tokenization of sentences, cropping of images, any filtering such as dropping images without faces)?",
-                                "name": "preProcessing",
-                                "type": [
-                                    "null",
-                                    {
-                                        "items": "string",
-                                        "type": "array"
-                                    }
-                                ]
-                            }
-                        ],
-                        "name": "BaseData",
-                        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-                        "type": "record"
-                    },
-                    "type": "array"
-                }
-            }
-        ],
-        "name": "EvaluationData",
-        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "mlModelTrainingData"
-        },
-        "doc": "Ideally, the MLModel card would contain as much information about the training data as the evaluation data. However, there might be cases where it is not feasible to provide this level of detailed information about the training data. For example, the data may be proprietary, or require a non-disclosure agreement. In these cases, we advocate for basic details about the distributions over groups in the data, as well as any other details that could inform stakeholders on the kinds of biases the model may have encoded.",
-        "fields": [
-            {
-                "doc": "Details on the dataset(s) used for training the MLModel",
-                "name": "trainingData",
-                "type": {
-                    "items": "com.linkedin.pegasus2avro.ml.metadata.BaseData",
-                    "type": "array"
-                }
+                ],
+                "urn_is_array": true
             }
         ],
-        "name": "TrainingData",
+        "name": "MLFeatureProperties",
         "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
             "name": "mlModelProperties"
         },
@@ -3809,57 +3501,15 @@
             {
                 "default": null,
                 "doc": "Hyperparameters of the MLModel",
                 "name": "hyperParams",
                 "type": [
                     "null",
                     {
-                        "items": {
-                            "Aspect": {
-                                "name": "mlHyperParam"
-                            },
-                            "doc": "Properties associated with an ML Hyper Param",
-                            "fields": [
-                                {
-                                    "doc": "Name of the MLHyperParam",
-                                    "name": "name",
-                                    "type": "string"
-                                },
-                                {
-                                    "default": null,
-                                    "doc": "Documentation of the MLHyperParam",
-                                    "name": "description",
-                                    "type": [
-                                        "null",
-                                        "string"
-                                    ]
-                                },
-                                {
-                                    "default": null,
-                                    "doc": "The value of the MLHyperParam",
-                                    "name": "value",
-                                    "type": [
-                                        "null",
-                                        "string"
-                                    ]
-                                },
-                                {
-                                    "default": null,
-                                    "doc": "Date when the MLHyperParam was developed",
-                                    "name": "createdAt",
-                                    "type": [
-                                        "null",
-                                        "long"
-                                    ]
-                                }
-                            ],
-                            "name": "MLHyperParam",
-                            "namespace": "com.linkedin.pegasus2avro.ml.metadata",
-                            "type": "record"
-                        },
+                        "items": "com.linkedin.pegasus2avro.ml.metadata.MLHyperParam",
                         "type": "array"
                     }
                 ]
             },
             {
                 "default": null,
                 "doc": "Metrics of the MLModel used in training",
@@ -3890,24 +3540,26 @@
                         "entityTypes": [
                             "mlFeature"
                         ],
                         "isLineage": true,
                         "name": "Consumes"
                     }
                 },
+                "Urn": "MLFeatureUrn",
                 "default": null,
                 "doc": "List of features used for MLModel training",
                 "name": "mlFeatures",
                 "type": [
                     "null",
                     {
                         "items": "string",
                         "type": "array"
                     }
-                ]
+                ],
+                "urn_is_array": true
             },
             {
                 "default": [],
                 "doc": "Tags for the MLModel",
                 "name": "tags",
                 "type": {
                     "items": "string",
@@ -3919,89 +3571,97 @@
                     "/*": {
                         "entityTypes": [
                             "mlModelDeployment"
                         ],
                         "name": "DeployedTo"
                     }
                 },
+                "Urn": "Urn",
                 "default": null,
                 "doc": "Deployments for the MLModel",
                 "name": "deployments",
                 "type": [
                     "null",
                     {
                         "items": "string",
                         "type": "array"
                     }
-                ]
+                ],
+                "urn_is_array": true
             },
             {
                 "Relationship": {
                     "/*": {
                         "entityTypes": [
                             "dataJob"
                         ],
                         "isLineage": true,
                         "name": "TrainedBy"
                     }
                 },
+                "Urn": "Urn",
                 "default": null,
                 "doc": "List of jobs (if any) used to train the model",
                 "name": "trainingJobs",
                 "type": [
                     "null",
                     {
                         "items": "string",
                         "type": "array"
                     }
-                ]
+                ],
+                "urn_is_array": true
             },
             {
                 "Relationship": {
                     "/*": {
                         "entityTypes": [
                             "dataJob"
                         ],
                         "isLineage": true,
                         "isUpstream": false,
                         "name": "UsedBy"
                     }
                 },
+                "Urn": "Urn",
                 "default": null,
                 "doc": "List of jobs (if any) that use the model",
                 "name": "downstreamJobs",
                 "type": [
                     "null",
                     {
                         "items": "string",
                         "type": "array"
                     }
-                ]
+                ],
+                "urn_is_array": true
             },
             {
                 "Relationship": {
                     "/*": {
                         "entityTypes": [
                             "mlModelGroup"
                         ],
                         "isLineage": true,
                         "isUpstream": false,
                         "name": "MemberOf"
                     }
                 },
+                "Urn": "Urn",
                 "default": null,
                 "doc": "Groups the model belongs to",
                 "name": "groups",
                 "type": [
                     "null",
                     {
                         "items": "string",
                         "type": "array"
                     }
-                ]
+                ],
+                "urn_is_array": true
             }
         ],
         "name": "MLModelProperties",
         "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
@@ -4024,3821 +3684,845 @@
                 ]
             }
         ],
         "name": "EditableMLFeatureProperties",
         "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
-    "com.linkedin.pegasus2avro.ml.metadata.MLHyperParam",
     {
         "Aspect": {
-            "name": "schemaMetadata"
-        },
-        "doc": "SchemaMetadata to describe metadata related to store schema",
-        "fields": [
-            {
-                "doc": "Schema name e.g. PageViewEvent, identity.Profile, ams.account_management_tracking",
-                "name": "schemaName",
-                "type": "string",
-                "validate": {
-                    "strlen": {
-                        "max": 500,
-                        "min": 1
-                    }
-                }
-            },
-            {
-                "doc": "Standardized platform urn where schema is defined. The data platform Urn (urn:li:platform:{platform_name})",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.DataPlatformUrn"
-                },
-                "name": "platform",
-                "type": "string"
-            },
-            {
-                "doc": "Every change to SchemaMetadata in the resource results in a new version. Version is server assigned. This version is differ from platform native schema version.",
-                "name": "version",
-                "type": "long"
-            },
-            {
-                "default": {
-                    "actor": "urn:li:corpuser:unknown",
-                    "impersonator": null,
-                    "message": null,
-                    "time": 0
-                },
-                "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
-                "name": "created",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-            },
-            {
-                "default": {
-                    "actor": "urn:li:corpuser:unknown",
-                    "impersonator": null,
-                    "message": null,
-                    "time": 0
-                },
-                "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
-                "name": "lastModified",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-            },
-            {
-                "default": null,
-                "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
-                "name": "deleted",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.common.AuditStamp"
-                ]
-            },
-            {
-                "default": null,
-                "doc": "Dataset this schema metadata is associated with.",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.DatasetUrn"
-                },
-                "name": "dataset",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "default": null,
-                "doc": "The cluster this schema metadata resides from",
-                "name": "cluster",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "doc": "the SHA1 hash of the schema content",
-                "name": "hash",
-                "type": "string"
-            },
-            {
-                "doc": "The native schema in the dataset's platform.",
-                "name": "platformSchema",
-                "type": [
-                    {
-                        "doc": "Schema text of an espresso table schema.",
-                        "fields": [
-                            {
-                                "doc": "The native espresso document schema.",
-                                "name": "documentSchema",
-                                "type": "string"
-                            },
-                            {
-                                "doc": "The espresso table schema definition.",
-                                "name": "tableSchema",
-                                "type": "string"
-                            }
-                        ],
-                        "name": "EspressoSchema",
-                        "namespace": "com.linkedin.pegasus2avro.schema",
-                        "type": "record"
-                    },
-                    {
-                        "doc": "Schema holder for oracle data definition language that describes an oracle table.",
-                        "fields": [
-                            {
-                                "doc": "The native schema in the dataset's platform. This is a human readable (json blob) table schema.",
-                                "name": "tableSchema",
-                                "type": "string"
-                            }
-                        ],
-                        "name": "OracleDDL",
-                        "namespace": "com.linkedin.pegasus2avro.schema",
-                        "type": "record"
-                    },
-                    {
-                        "doc": "Schema holder for MySql data definition language that describes an MySql table.",
-                        "fields": [
-                            {
-                                "doc": "The native schema in the dataset's platform. This is a human readable (json blob) table schema.",
-                                "name": "tableSchema",
-                                "type": "string"
-                            }
-                        ],
-                        "name": "MySqlDDL",
-                        "namespace": "com.linkedin.pegasus2avro.schema",
-                        "type": "record"
-                    },
-                    {
-                        "doc": "Schema holder for presto data definition language that describes a presto view.",
-                        "fields": [
-                            {
-                                "doc": "The raw schema in the dataset's platform. This includes the DDL and the columns extracted from DDL.",
-                                "name": "rawSchema",
-                                "type": "string"
-                            }
-                        ],
-                        "name": "PrestoDDL",
-                        "namespace": "com.linkedin.pegasus2avro.schema",
-                        "type": "record"
-                    },
-                    {
-                        "doc": "Schema holder for kafka schema.",
-                        "fields": [
-                            {
-                                "doc": "The native kafka document schema. This is a human readable avro document schema.",
-                                "name": "documentSchema",
-                                "type": "string"
-                            },
-                            {
-                                "default": null,
-                                "doc": "The native kafka key schema as retrieved from Schema Registry",
-                                "name": "keySchema",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            }
-                        ],
-                        "name": "KafkaSchema",
-                        "namespace": "com.linkedin.pegasus2avro.schema",
-                        "type": "record"
-                    },
-                    {
-                        "doc": "Schema text of binary JSON schema.",
-                        "fields": [
-                            {
-                                "doc": "The native schema text for binary JSON file format.",
-                                "name": "schema",
-                                "type": "string"
-                            }
-                        ],
-                        "name": "BinaryJsonSchema",
-                        "namespace": "com.linkedin.pegasus2avro.schema",
-                        "type": "record"
-                    },
-                    {
-                        "doc": "Schema text of an ORC schema.",
-                        "fields": [
-                            {
-                                "doc": "The native schema for ORC file format.",
-                                "name": "schema",
-                                "type": "string"
-                            }
-                        ],
-                        "name": "OrcSchema",
-                        "namespace": "com.linkedin.pegasus2avro.schema",
-                        "type": "record"
-                    },
-                    {
-                        "doc": "The dataset has no specific schema associated with it",
-                        "fields": [],
-                        "name": "Schemaless",
-                        "namespace": "com.linkedin.pegasus2avro.schema",
-                        "type": "record"
-                    },
-                    {
-                        "doc": "Schema text of a key-value store schema.",
-                        "fields": [
-                            {
-                                "doc": "The raw schema for the key in the key-value store.",
-                                "name": "keySchema",
-                                "type": "string"
-                            },
-                            {
-                                "doc": "The raw schema for the value in the key-value store.",
-                                "name": "valueSchema",
-                                "type": "string"
-                            }
-                        ],
-                        "name": "KeyValueSchema",
-                        "namespace": "com.linkedin.pegasus2avro.schema",
-                        "type": "record"
-                    },
-                    {
-                        "doc": "Schema holder for undefined schema types.",
-                        "fields": [
-                            {
-                                "doc": "The native schema in the dataset's platform.",
-                                "name": "rawSchema",
-                                "type": "string"
-                            }
-                        ],
-                        "name": "OtherSchema",
-                        "namespace": "com.linkedin.pegasus2avro.schema",
-                        "type": "record"
-                    }
-                ]
-            },
-            {
-                "doc": "Client provided a list of fields from document schema.",
-                "name": "fields",
-                "type": {
-                    "items": {
-                        "doc": "SchemaField to describe metadata related to dataset schema.",
-                        "fields": [
-                            {
-                                "Searchable": {
-                                    "fieldName": "fieldPaths",
-                                    "fieldType": "TEXT"
-                                },
-                                "doc": "Flattened name of the field. Field is computed from jsonPath field.",
-                                "name": "fieldPath",
-                                "type": "string"
-                            },
-                            {
-                                "Deprecated": true,
-                                "default": null,
-                                "doc": "Flattened name of a field in JSON Path notation.",
-                                "name": "jsonPath",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            },
-                            {
-                                "default": false,
-                                "doc": "Indicates if this field is optional or nullable",
-                                "name": "nullable",
-                                "type": "boolean"
-                            },
-                            {
-                                "Searchable": {
-                                    "boostScore": 0.1,
-                                    "fieldName": "fieldDescriptions",
-                                    "fieldType": "TEXT"
-                                },
-                                "default": null,
-                                "doc": "Description",
-                                "name": "description",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            },
-                            {
-                                "Searchable": {
-                                    "boostScore": 0.2,
-                                    "fieldName": "fieldLabels",
-                                    "fieldType": "TEXT"
-                                },
-                                "default": null,
-                                "doc": "Label of the field. Provides a more human-readable name for the field than field path. Some sources will\nprovide this metadata but not all sources have the concept of a label. If just one string is associated with\na field in a source, that is most likely a description.",
-                                "name": "label",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "An AuditStamp corresponding to the creation of this schema field.",
-                                "name": "created",
-                                "type": [
-                                    "null",
-                                    "com.linkedin.pegasus2avro.common.AuditStamp"
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "An AuditStamp corresponding to the last modification of this schema field.",
-                                "name": "lastModified",
-                                "type": [
-                                    "null",
-                                    "com.linkedin.pegasus2avro.common.AuditStamp"
-                                ]
-                            },
-                            {
-                                "doc": "Platform independent field type of the field.",
-                                "name": "type",
-                                "type": {
-                                    "doc": "Schema field data types",
-                                    "fields": [
-                                        {
-                                            "doc": "Data platform specific types",
-                                            "name": "type",
-                                            "type": [
-                                                {
-                                                    "doc": "Boolean field type.",
-                                                    "fields": [],
-                                                    "name": "BooleanType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                },
-                                                {
-                                                    "doc": "Fixed field type.",
-                                                    "fields": [],
-                                                    "name": "FixedType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                },
-                                                {
-                                                    "doc": "String field type.",
-                                                    "fields": [],
-                                                    "name": "StringType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                },
-                                                {
-                                                    "doc": "Bytes field type.",
-                                                    "fields": [],
-                                                    "name": "BytesType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                },
-                                                {
-                                                    "doc": "Number data type: long, integer, short, etc..",
-                                                    "fields": [],
-                                                    "name": "NumberType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                },
-                                                {
-                                                    "doc": "Date field type.",
-                                                    "fields": [],
-                                                    "name": "DateType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                },
-                                                {
-                                                    "doc": "Time field type. This should also be used for datetimes.",
-                                                    "fields": [],
-                                                    "name": "TimeType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                },
-                                                {
-                                                    "doc": "Enum field type.",
-                                                    "fields": [],
-                                                    "name": "EnumType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                },
-                                                {
-                                                    "doc": "Null field type.",
-                                                    "fields": [],
-                                                    "name": "NullType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                },
-                                                {
-                                                    "doc": "Map field type.",
-                                                    "fields": [
-                                                        {
-                                                            "default": null,
-                                                            "doc": "Key type in a map",
-                                                            "name": "keyType",
-                                                            "type": [
-                                                                "null",
-                                                                "string"
-                                                            ]
-                                                        },
-                                                        {
-                                                            "default": null,
-                                                            "doc": "Type of the value in a map",
-                                                            "name": "valueType",
-                                                            "type": [
-                                                                "null",
-                                                                "string"
-                                                            ]
-                                                        }
-                                                    ],
-                                                    "name": "MapType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                },
-                                                {
-                                                    "doc": "Array field type.",
-                                                    "fields": [
-                                                        {
-                                                            "default": null,
-                                                            "doc": "List of types this array holds.",
-                                                            "name": "nestedType",
-                                                            "type": [
-                                                                "null",
-                                                                {
-                                                                    "items": "string",
-                                                                    "type": "array"
-                                                                }
-                                                            ]
-                                                        }
-                                                    ],
-                                                    "name": "ArrayType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                },
-                                                {
-                                                    "doc": "Union field type.",
-                                                    "fields": [
-                                                        {
-                                                            "default": null,
-                                                            "doc": "List of types in union type.",
-                                                            "name": "nestedTypes",
-                                                            "type": [
-                                                                "null",
-                                                                {
-                                                                    "items": "string",
-                                                                    "type": "array"
-                                                                }
-                                                            ]
-                                                        }
-                                                    ],
-                                                    "name": "UnionType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                },
-                                                {
-                                                    "doc": "Record field type.",
-                                                    "fields": [],
-                                                    "name": "RecordType",
-                                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                                    "type": "record"
-                                                }
-                                            ]
-                                        }
-                                    ],
-                                    "name": "SchemaFieldDataType",
-                                    "namespace": "com.linkedin.pegasus2avro.schema",
-                                    "type": "record"
-                                }
-                            },
-                            {
-                                "doc": "The native type of the field in the dataset's platform as declared by platform schema.",
-                                "name": "nativeDataType",
-                                "type": "string"
-                            },
-                            {
-                                "default": false,
-                                "doc": "There are use cases when a field in type B references type A. A field in A references field of type B. In such cases, we will mark the first field as recursive.",
-                                "name": "recursive",
-                                "type": "boolean"
-                            },
-                            {
-                                "Relationship": {
-                                    "/tags/*/tag": {
-                                        "entityTypes": [
-                                            "tag"
-                                        ],
-                                        "name": "SchemaFieldTaggedWith"
-                                    }
-                                },
-                                "Searchable": {
-                                    "/tags/*/tag": {
-                                        "boostScore": 0.5,
-                                        "fieldName": "fieldTags",
-                                        "fieldType": "URN"
-                                    }
-                                },
-                                "default": null,
-                                "doc": "Tags associated with the field",
-                                "name": "globalTags",
-                                "type": [
-                                    "null",
-                                    {
-                                        "Aspect": {
-                                            "name": "globalTags"
-                                        },
-                                        "doc": "Tag aspect used for applying tags to an entity",
-                                        "fields": [
-                                            {
-                                                "Searchable": {
-                                                    "/*/tag": {
-                                                        "addToFilters": true,
-                                                        "boostScore": 0.5,
-                                                        "fieldName": "tags",
-                                                        "fieldType": "URN",
-                                                        "queryByDefault": true
-                                                    }
-                                                },
-                                                "doc": "Tags associated with a given entity",
-                                                "name": "tags",
-                                                "type": {
-                                                    "items": {
-                                                        "doc": "Properties of an applied tag. For now, just an Urn. In the future we can extend this with other properties, e.g.\npropagation parameters.",
-                                                        "fields": [
-                                                            {
-                                                                "Relationship": {
-                                                                    "entityTypes": [
-                                                                        "tag"
-                                                                    ],
-                                                                    "name": "TaggedWith"
-                                                                },
-                                                                "Searchable": {
-                                                                    "addToFilters": true,
-                                                                    "fieldName": "tags",
-                                                                    "fieldType": "URN",
-                                                                    "filterNameOverride": "Tag",
-                                                                    "hasValuesFieldName": "hasTags"
-                                                                },
-                                                                "doc": "Urn of the applied tag",
-                                                                "java": {
-                                                                    "class": "com.linkedin.pegasus2avro.common.urn.TagUrn"
-                                                                },
-                                                                "name": "tag",
-                                                                "type": "string"
-                                                            },
-                                                            {
-                                                                "default": null,
-                                                                "doc": "Additional context about the association",
-                                                                "name": "context",
-                                                                "type": [
-                                                                    "null",
-                                                                    "string"
-                                                                ]
-                                                            }
-                                                        ],
-                                                        "name": "TagAssociation",
-                                                        "namespace": "com.linkedin.pegasus2avro.common",
-                                                        "type": "record"
-                                                    },
-                                                    "type": "array"
-                                                }
-                                            }
-                                        ],
-                                        "name": "GlobalTags",
-                                        "namespace": "com.linkedin.pegasus2avro.common",
-                                        "type": "record"
-                                    }
-                                ]
-                            },
-                            {
-                                "Relationship": {
-                                    "/terms/*/urn": {
-                                        "entityTypes": [
-                                            "glossaryTerm"
-                                        ],
-                                        "name": "SchemaFieldWithGlossaryTerm"
-                                    }
-                                },
-                                "Searchable": {
-                                    "/terms/*/urn": {
-                                        "boostScore": 0.5,
-                                        "fieldName": "fieldGlossaryTerms",
-                                        "fieldType": "URN"
-                                    }
-                                },
-                                "default": null,
-                                "doc": "Glossary terms associated with the field",
-                                "name": "glossaryTerms",
-                                "type": [
-                                    "null",
-                                    {
-                                        "Aspect": {
-                                            "name": "glossaryTerms"
-                                        },
-                                        "doc": "Related business terms information",
-                                        "fields": [
-                                            {
-                                                "doc": "The related business terms",
-                                                "name": "terms",
-                                                "type": {
-                                                    "items": {
-                                                        "doc": "Properties of an applied glossary term.",
-                                                        "fields": [
-                                                            {
-                                                                "Relationship": {
-                                                                    "entityTypes": [
-                                                                        "glossaryTerm"
-                                                                    ],
-                                                                    "name": "TermedWith"
-                                                                },
-                                                                "Searchable": {
-                                                                    "addToFilters": true,
-                                                                    "fieldName": "glossaryTerms",
-                                                                    "fieldType": "URN",
-                                                                    "filterNameOverride": "Glossary Term",
-                                                                    "hasValuesFieldName": "hasGlossaryTerms"
-                                                                },
-                                                                "doc": "Urn of the applied glossary term",
-                                                                "java": {
-                                                                    "class": "com.linkedin.pegasus2avro.common.urn.GlossaryTermUrn"
-                                                                },
-                                                                "name": "urn",
-                                                                "type": "string"
-                                                            },
-                                                            {
-                                                                "default": null,
-                                                                "doc": "Additional context about the association",
-                                                                "name": "context",
-                                                                "type": [
-                                                                    "null",
-                                                                    "string"
-                                                                ]
-                                                            }
-                                                        ],
-                                                        "name": "GlossaryTermAssociation",
-                                                        "namespace": "com.linkedin.pegasus2avro.common",
-                                                        "type": "record"
-                                                    },
-                                                    "type": "array"
-                                                }
-                                            },
-                                            {
-                                                "doc": "Audit stamp containing who reported the related business term",
-                                                "name": "auditStamp",
-                                                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-                                            }
-                                        ],
-                                        "name": "GlossaryTerms",
-                                        "namespace": "com.linkedin.pegasus2avro.common",
-                                        "type": "record"
-                                    }
-                                ]
-                            },
-                            {
-                                "default": false,
-                                "doc": "For schema fields that are part of complex keys, set this field to true\nWe do this to easily distinguish between value and key fields",
-                                "name": "isPartOfKey",
-                                "type": "boolean"
-                            },
-                            {
-                                "default": null,
-                                "doc": "For Datasets which are partitioned, this determines the partitioning key.",
-                                "name": "isPartitioningKey",
-                                "type": [
-                                    "null",
-                                    "boolean"
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "For schema fields that have other properties that are not modeled explicitly,\nuse this field to serialize those properties into a JSON string",
-                                "name": "jsonProps",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            }
-                        ],
-                        "name": "SchemaField",
-                        "namespace": "com.linkedin.pegasus2avro.schema",
-                        "type": "record"
-                    },
-                    "type": "array"
-                }
-            },
-            {
-                "default": null,
-                "doc": "Client provided list of fields that define primary keys to access record. Field order defines hierarchical espresso keys. Empty lists indicates absence of primary key access patter. Value is a SchemaField@fieldPath.",
-                "name": "primaryKeys",
-                "type": [
-                    "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ]
-            },
-            {
-                "default": null,
-                "deprecated": "Use foreignKeys instead.",
-                "doc": "Map captures all the references schema makes to external datasets. Map key is ForeignKeySpecName typeref.",
-                "name": "foreignKeysSpecs",
-                "type": [
-                    "null",
-                    {
-                        "type": "map",
-                        "values": {
-                            "doc": "Description of a foreign key in a schema.",
-                            "fields": [
-                                {
-                                    "doc": "Foreign key definition in metadata schema.",
-                                    "name": "foreignKey",
-                                    "type": [
-                                        {
-                                            "doc": "For non-urn based foregin keys.",
-                                            "fields": [
-                                                {
-                                                    "doc": "dataset that stores the resource.",
-                                                    "java": {
-                                                        "class": "com.linkedin.pegasus2avro.common.urn.DatasetUrn"
-                                                    },
-                                                    "name": "parentDataset",
-                                                    "type": "string"
-                                                },
-                                                {
-                                                    "doc": "List of fields in hosting(current) SchemaMetadata that conform a foreign key. List can contain a single entry or multiple entries if several entries in hosting schema conform a foreign key in a single parent dataset.",
-                                                    "name": "currentFieldPaths",
-                                                    "type": {
-                                                        "items": "string",
-                                                        "type": "array"
-                                                    }
-                                                },
-                                                {
-                                                    "doc": "SchemaField@fieldPath that uniquely identify field in parent dataset that this field references.",
-                                                    "name": "parentField",
-                                                    "type": "string"
-                                                }
-                                            ],
-                                            "name": "DatasetFieldForeignKey",
-                                            "namespace": "com.linkedin.pegasus2avro.schema",
-                                            "type": "record"
-                                        },
-                                        {
-                                            "doc": "If SchemaMetadata fields make any external references and references are of type com.linkedin.pegasus2avro.common.Urn or any children, this models can be used to mark it.",
-                                            "fields": [
-                                                {
-                                                    "doc": "Field in hosting(current) SchemaMetadata.",
-                                                    "name": "currentFieldPath",
-                                                    "type": "string"
-                                                }
-                                            ],
-                                            "name": "UrnForeignKey",
-                                            "namespace": "com.linkedin.pegasus2avro.schema",
-                                            "type": "record"
-                                        }
-                                    ]
-                                }
-                            ],
-                            "name": "ForeignKeySpec",
-                            "namespace": "com.linkedin.pegasus2avro.schema",
-                            "type": "record"
-                        }
-                    }
-                ]
-            },
-            {
-                "default": null,
-                "doc": "List of foreign key constraints for the schema",
-                "name": "foreignKeys",
-                "type": [
-                    "null",
-                    {
-                        "items": {
-                            "doc": "Description of a foreign key constraint in a schema.",
-                            "fields": [
-                                {
-                                    "doc": "Name of the constraint, likely provided from the source",
-                                    "name": "name",
-                                    "type": "string"
-                                },
-                                {
-                                    "Relationship": {
-                                        "/*": {
-                                            "entityTypes": [
-                                                "schemaField"
-                                            ],
-                                            "name": "ForeignKeyTo"
-                                        }
-                                    },
-                                    "doc": "Fields the constraint maps to on the foreign dataset",
-                                    "name": "foreignFields",
-                                    "type": {
-                                        "items": "string",
-                                        "type": "array"
-                                    }
-                                },
-                                {
-                                    "doc": "Fields the constraint maps to on the source dataset",
-                                    "name": "sourceFields",
-                                    "type": {
-                                        "items": "string",
-                                        "type": "array"
-                                    }
-                                },
-                                {
-                                    "Relationship": {
-                                        "entityTypes": [
-                                            "dataset"
-                                        ],
-                                        "name": "ForeignKeyToDataset"
-                                    },
-                                    "doc": "Reference to the foreign dataset for ease of lookup",
-                                    "java": {
-                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                    },
-                                    "name": "foreignDataset",
-                                    "type": "string"
-                                }
-                            ],
-                            "name": "ForeignKeyConstraint",
-                            "namespace": "com.linkedin.pegasus2avro.schema",
-                            "type": "record"
-                        },
-                        "type": "array"
-                    }
-                ]
-            }
-        ],
-        "name": "SchemaMetadata",
-        "namespace": "com.linkedin.pegasus2avro.schema",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "editableSchemaMetadata"
-        },
-        "doc": "EditableSchemaMetadata stores editable changes made to schema metadata. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines.",
-        "fields": [
-            {
-                "default": {
-                    "actor": "urn:li:corpuser:unknown",
-                    "impersonator": null,
-                    "message": null,
-                    "time": 0
-                },
-                "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
-                "name": "created",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-            },
-            {
-                "default": {
-                    "actor": "urn:li:corpuser:unknown",
-                    "impersonator": null,
-                    "message": null,
-                    "time": 0
-                },
-                "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
-                "name": "lastModified",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-            },
-            {
-                "default": null,
-                "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
-                "name": "deleted",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.common.AuditStamp"
-                ]
-            },
-            {
-                "doc": "Client provided a list of fields from document schema.",
-                "name": "editableSchemaFieldInfo",
-                "type": {
-                    "items": {
-                        "doc": "SchemaField to describe metadata related to dataset schema.",
-                        "fields": [
-                            {
-                                "doc": "FieldPath uniquely identifying the SchemaField this metadata is associated with",
-                                "name": "fieldPath",
-                                "type": "string"
-                            },
-                            {
-                                "Searchable": {
-                                    "boostScore": 0.1,
-                                    "fieldName": "editedFieldDescriptions",
-                                    "fieldType": "TEXT"
-                                },
-                                "default": null,
-                                "doc": "Description",
-                                "name": "description",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            },
-                            {
-                                "Relationship": {
-                                    "/tags/*/tag": {
-                                        "entityTypes": [
-                                            "tag"
-                                        ],
-                                        "name": "EditableSchemaFieldTaggedWith"
-                                    }
-                                },
-                                "Searchable": {
-                                    "/tags/*/tag": {
-                                        "boostScore": 0.5,
-                                        "fieldName": "editedFieldTags",
-                                        "fieldType": "URN"
-                                    }
-                                },
-                                "default": null,
-                                "doc": "Tags associated with the field",
-                                "name": "globalTags",
-                                "type": [
-                                    "null",
-                                    "com.linkedin.pegasus2avro.common.GlobalTags"
-                                ]
-                            },
-                            {
-                                "Relationship": {
-                                    "/terms/*/urn": {
-                                        "entityTypes": [
-                                            "glossaryTerm"
-                                        ],
-                                        "name": "EditableSchemaFieldWithGlossaryTerm"
-                                    }
-                                },
-                                "Searchable": {
-                                    "/terms/*/urn": {
-                                        "boostScore": 0.5,
-                                        "fieldName": "editedFieldGlossaryTerms",
-                                        "fieldType": "URN"
-                                    }
-                                },
-                                "default": null,
-                                "doc": "Glossary terms associated with the field",
-                                "name": "glossaryTerms",
-                                "type": [
-                                    "null",
-                                    "com.linkedin.pegasus2avro.common.GlossaryTerms"
-                                ]
-                            }
-                        ],
-                        "name": "EditableSchemaFieldInfo",
-                        "namespace": "com.linkedin.pegasus2avro.schema",
-                        "type": "record"
-                    },
-                    "type": "array"
-                }
-            }
-        ],
-        "name": "EditableSchemaMetadata",
-        "namespace": "com.linkedin.pegasus2avro.schema",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "chartInfo"
-        },
-        "doc": "Information about a chart",
-        "fields": [
-            {
-                "Searchable": {
-                    "/*": {
-                        "queryByDefault": true
-                    }
-                },
-                "default": {},
-                "doc": "Custom property bag.",
-                "name": "customProperties",
-                "type": {
-                    "type": "map",
-                    "values": "string"
-                }
-            },
-            {
-                "default": null,
-                "doc": "URL where the reference exist",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                },
-                "name": "externalUrl",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "Searchable": {
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "Title of the chart",
-                "name": "title",
-                "type": "string"
-            },
-            {
-                "Searchable": {},
-                "doc": "Detailed description about the chart",
-                "name": "description",
-                "type": "string"
-            },
-            {
-                "doc": "Captures information about who created/last modified/deleted this chart and when",
-                "name": "lastModified",
-                "type": {
-                    "doc": "Data captured on a resource/association/sub-resource level giving insight into when that resource/association/sub-resource moved into various lifecycle stages, and who acted to move it into those lifecycle stages. The recommended best practice is to include this record in your record schema, and annotate its fields as @readOnly in your resource. See https://github.com/linkedin/rest.li/wiki/Validation-in-Rest.li#restli-validation-annotations",
-                    "fields": [
-                        {
-                            "default": {
-                                "actor": "urn:li:corpuser:unknown",
-                                "impersonator": null,
-                                "message": null,
-                                "time": 0
-                            },
-                            "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
-                            "name": "created",
-                            "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-                        },
-                        {
-                            "default": {
-                                "actor": "urn:li:corpuser:unknown",
-                                "impersonator": null,
-                                "message": null,
-                                "time": 0
-                            },
-                            "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
-                            "name": "lastModified",
-                            "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-                        },
-                        {
-                            "default": null,
-                            "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
-                            "name": "deleted",
-                            "type": [
-                                "null",
-                                "com.linkedin.pegasus2avro.common.AuditStamp"
-                            ]
-                        }
-                    ],
-                    "name": "ChangeAuditStamps",
-                    "namespace": "com.linkedin.pegasus2avro.common",
-                    "type": "record"
-                }
-            },
-            {
-                "default": null,
-                "doc": "URL for the chart. This could be used as an external link on DataHub to allow users access/view the chart",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                },
-                "name": "chartUrl",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "Relationship": {
-                    "/*/string": {
-                        "entityTypes": [
-                            "dataset"
-                        ],
-                        "isLineage": true,
-                        "name": "Consumes"
-                    }
-                },
-                "default": null,
-                "deprecated": true,
-                "doc": "Data sources for the chart\nDeprecated! Use inputEdges instead.",
-                "name": "inputs",
-                "type": [
-                    "null",
-                    {
-                        "items": [
-                            "string"
-                        ],
-                        "type": "array"
-                    }
-                ]
-            },
-            {
-                "Relationship": {
-                    "/*/destinationUrn": {
-                        "createdActor": "inputEdges/*/created/actor",
-                        "createdOn": "inputEdges/*/created/time",
-                        "entityTypes": [
-                            "dataset"
-                        ],
-                        "isLineage": true,
-                        "name": "Consumes",
-                        "properties": "inputEdges/*/properties",
-                        "updatedActor": "inputEdges/*/lastModified/actor",
-                        "updatedOn": "inputEdges/*/lastModified/time"
-                    }
-                },
-                "default": null,
-                "doc": "Data sources for the chart",
-                "name": "inputEdges",
-                "type": [
-                    "null",
-                    {
-                        "items": {
-                            "doc": "Information about a relatonship edge.",
-                            "fields": [
-                                {
-                                    "doc": "Urn of the source of this relationship edge.",
-                                    "java": {
-                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                    },
-                                    "name": "sourceUrn",
-                                    "type": "string"
-                                },
-                                {
-                                    "doc": "Urn of the destination of this relationship edge.",
-                                    "java": {
-                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                    },
-                                    "name": "destinationUrn",
-                                    "type": "string"
-                                },
-                                {
-                                    "doc": "Audit stamp containing who created this relationship edge and when",
-                                    "name": "created",
-                                    "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-                                },
-                                {
-                                    "doc": "Audit stamp containing who last modified this relationship edge and when",
-                                    "name": "lastModified",
-                                    "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-                                },
-                                {
-                                    "default": null,
-                                    "doc": "A generic properties bag that allows us to store specific information on this graph edge.",
-                                    "name": "properties",
-                                    "type": [
-                                        "null",
-                                        {
-                                            "type": "map",
-                                            "values": "string"
-                                        }
-                                    ]
-                                }
-                            ],
-                            "name": "Edge",
-                            "namespace": "com.linkedin.pegasus2avro.common",
-                            "type": "record"
-                        },
-                        "type": "array"
-                    }
-                ]
-            },
-            {
-                "Searchable": {
-                    "addToFilters": true,
-                    "fieldType": "KEYWORD",
-                    "filterNameOverride": "Chart Type"
-                },
-                "default": null,
-                "doc": "Type of the chart",
-                "name": "type",
-                "type": [
-                    "null",
-                    {
-                        "doc": "The various types of charts",
-                        "name": "ChartType",
-                        "namespace": "com.linkedin.pegasus2avro.chart",
-                        "symbolDocs": {
-                            "BAR": "Chart showing a Bar chart",
-                            "PIE": "Chart showing a Pie chart",
-                            "SCATTER": "Chart showing a Scatter plot",
-                            "TABLE": "Chart showing a table",
-                            "TEXT": "Chart showing Markdown formatted text"
-                        },
-                        "symbols": [
-                            "BAR",
-                            "PIE",
-                            "SCATTER",
-                            "TABLE",
-                            "TEXT",
-                            "LINE",
-                            "AREA",
-                            "HISTOGRAM",
-                            "BOX_PLOT",
-                            "WORD_CLOUD",
-                            "COHORT"
-                        ],
-                        "type": "enum"
-                    }
-                ]
-            },
-            {
-                "Searchable": {
-                    "addToFilters": true,
-                    "fieldType": "KEYWORD",
-                    "filterNameOverride": "Access Level"
-                },
-                "default": null,
-                "doc": "Access level for the chart",
-                "name": "access",
-                "type": [
-                    "null",
-                    {
-                        "doc": "The various access levels",
-                        "name": "AccessLevel",
-                        "namespace": "com.linkedin.pegasus2avro.common",
-                        "symbolDocs": {
-                            "PRIVATE": "Private availability to certain set of users",
-                            "PUBLIC": "Publicly available access level"
-                        },
-                        "symbols": [
-                            "PUBLIC",
-                            "PRIVATE"
-                        ],
-                        "type": "enum"
-                    }
-                ]
-            },
-            {
-                "default": null,
-                "doc": "The time when this chart last refreshed",
-                "name": "lastRefreshed",
-                "type": [
-                    "null",
-                    "long"
-                ]
-            }
-        ],
-        "name": "ChartInfo",
-        "namespace": "com.linkedin.pegasus2avro.chart",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "editableChartProperties"
-        },
-        "doc": "Stores editable changes made to properties. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines",
-        "fields": [
-            {
-                "default": {
-                    "actor": "urn:li:corpuser:unknown",
-                    "impersonator": null,
-                    "message": null,
-                    "time": 0
-                },
-                "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
-                "name": "created",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-            },
-            {
-                "default": {
-                    "actor": "urn:li:corpuser:unknown",
-                    "impersonator": null,
-                    "message": null,
-                    "time": 0
-                },
-                "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
-                "name": "lastModified",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-            },
-            {
-                "default": null,
-                "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
-                "name": "deleted",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.common.AuditStamp"
-                ]
-            },
-            {
-                "Searchable": {
-                    "fieldName": "editedDescription",
-                    "fieldType": "TEXT"
-                },
-                "default": null,
-                "doc": "Edited documentation of the chart ",
-                "name": "description",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            }
-        ],
-        "name": "EditableChartProperties",
-        "namespace": "com.linkedin.pegasus2avro.chart",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "chartQuery"
-        },
-        "doc": "Information for chart query which is used for getting data of the chart",
-        "fields": [
-            {
-                "doc": "Raw query to build a chart from input datasets",
-                "name": "rawQuery",
-                "type": "string"
-            },
-            {
-                "Searchable": {
-                    "addToFilters": true,
-                    "fieldName": "queryType",
-                    "fieldType": "KEYWORD",
-                    "filterNameOverride": "Query Type"
-                },
-                "doc": "Chart query type",
-                "name": "type",
-                "type": {
-                    "name": "ChartQueryType",
-                    "namespace": "com.linkedin.pegasus2avro.chart",
-                    "symbolDocs": {
-                        "LOOKML": "LookML queries",
-                        "SQL": "SQL type queries"
-                    },
-                    "symbols": [
-                        "LOOKML",
-                        "SQL"
-                    ],
-                    "type": "enum"
-                }
-            }
-        ],
-        "name": "ChartQuery",
-        "namespace": "com.linkedin.pegasus2avro.chart",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "chartUsageStatistics",
-            "type": "timeseries"
-        },
-        "doc": "Experimental (Subject to breaking change) -- Stats corresponding to chart's usage.\n\nIf this aspect represents the latest snapshot of the statistics about a Chart, the eventGranularity field should be null.\nIf this aspect represents a bucketed window of usage statistics (e.g. over a day), then the eventGranularity field should be set accordingly.",
-        "fields": [
-            {
-                "doc": "The event timestamp field as epoch at UTC in milli seconds.",
-                "name": "timestampMillis",
-                "type": "long"
-            },
-            {
-                "default": null,
-                "doc": "Granularity of the event if applicable",
-                "name": "eventGranularity",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
-                ]
-            },
-            {
-                "default": {
-                    "partition": "FULL_TABLE_SNAPSHOT",
-                    "timePartition": null,
-                    "type": "FULL_TABLE"
-                },
-                "doc": "The optional partition specification.",
-                "name": "partitionSpec",
-                "type": [
-                    "com.linkedin.pegasus2avro.timeseries.PartitionSpec",
-                    "null"
-                ]
-            },
-            {
-                "default": null,
-                "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
-                "name": "messageId",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "TimeseriesField": {},
-                "default": null,
-                "doc": "The total number of times chart has been viewed",
-                "name": "viewsCount",
-                "type": [
-                    "null",
-                    "int"
-                ]
-            },
-            {
-                "TimeseriesField": {},
-                "default": null,
-                "doc": "Unique user count",
-                "name": "uniqueUserCount",
-                "type": [
-                    "null",
-                    "int"
-                ]
-            },
-            {
-                "TimeseriesFieldCollection": {
-                    "key": "user"
-                },
-                "default": null,
-                "doc": "Users within this bucket, with frequency counts",
-                "name": "userCounts",
-                "type": [
-                    "null",
-                    {
-                        "items": {
-                            "doc": "Records a single user's usage counts for a given resource",
-                            "fields": [
-                                {
-                                    "doc": "The unique id of the user.",
-                                    "java": {
-                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                    },
-                                    "name": "user",
-                                    "type": "string"
-                                },
-                                {
-                                    "TimeseriesField": {},
-                                    "default": null,
-                                    "doc": "The number of times the user has viewed the chart",
-                                    "name": "viewsCount",
-                                    "type": [
-                                        "null",
-                                        "int"
-                                    ]
-                                }
-                            ],
-                            "name": "ChartUserUsageCounts",
-                            "namespace": "com.linkedin.pegasus2avro.chart",
-                            "type": "record"
-                        },
-                        "type": "array"
-                    }
-                ]
-            }
-        ],
-        "name": "ChartUsageStatistics",
-        "namespace": "com.linkedin.pegasus2avro.chart",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "container"
-        },
-        "doc": "Link from an asset to its parent container",
-        "fields": [
-            {
-                "Relationship": {
-                    "entityTypes": [
-                        "container"
-                    ],
-                    "name": "IsPartOf"
-                },
-                "Searchable": {
-                    "addToFilters": true,
-                    "fieldName": "container",
-                    "fieldType": "URN",
-                    "filterNameOverride": "Container",
-                    "hasValuesFieldName": "hasContainer"
-                },
-                "doc": "The parent container of an asset",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "container",
-                "type": "string"
-            }
-        ],
-        "name": "Container",
-        "namespace": "com.linkedin.pegasus2avro.container",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "containerProperties"
-        },
-        "doc": "Information about a Asset Container as received from a 3rd party source system",
-        "fields": [
-            {
-                "Searchable": {
-                    "/*": {
-                        "queryByDefault": true
-                    }
-                },
-                "default": {},
-                "doc": "Custom property bag.",
-                "name": "customProperties",
-                "type": {
-                    "type": "map",
-                    "values": "string"
-                }
-            },
-            {
-                "default": null,
-                "doc": "URL where the reference exist",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                },
-                "name": "externalUrl",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "Display name of the Asset Container",
-                "name": "name",
-                "type": "string"
-            },
-            {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "default": null,
-                "doc": "Fully-qualified name of the Container",
-                "name": "qualifiedName",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "Searchable": {
-                    "fieldType": "TEXT",
-                    "hasValuesFieldName": "hasDescription"
-                },
-                "default": null,
-                "doc": "Description of the Asset Container as it exists inside a source system",
-                "name": "description",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "Searchable": {
-                    "/time": {
-                        "fieldName": "createdAt",
-                        "fieldType": "DATETIME"
-                    }
-                },
-                "default": null,
-                "doc": "A timestamp documenting when the asset was created in the source Data Platform (not on DataHub)",
-                "name": "created",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.common.TimeStamp"
-                ]
-            },
-            {
-                "Searchable": {
-                    "/time": {
-                        "fieldName": "lastModifiedAt",
-                        "fieldType": "DATETIME"
-                    }
-                },
-                "default": null,
-                "doc": "A timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)",
-                "name": "lastModified",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.common.TimeStamp"
-                ]
-            }
-        ],
-        "name": "ContainerProperties",
-        "namespace": "com.linkedin.pegasus2avro.container",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "editableContainerProperties"
+            "name": "editableMlModelGroupProperties"
         },
-        "doc": "Editable information about an Asset Container as defined on the DataHub Platform",
+        "doc": "Properties associated with an ML Model Group editable from the UI",
         "fields": [
             {
                 "Searchable": {
                     "fieldName": "editedDescription",
                     "fieldType": "TEXT"
                 },
                 "default": null,
-                "doc": "Description of the Asset Container as its received on the DataHub Platform",
-                "name": "description",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            }
-        ],
-        "name": "EditableContainerProperties",
-        "namespace": "com.linkedin.pegasus2avro.container",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "postInfo"
-        },
-        "doc": "Information about a DataHub Post.",
-        "fields": [
-            {
-                "doc": "Type of the Post.",
-                "name": "type",
-                "type": {
-                    "doc": "Enum defining types of Posts.",
-                    "name": "PostType",
-                    "namespace": "com.linkedin.pegasus2avro.post",
-                    "symbolDocs": {
-                        "HOME_PAGE_ANNOUNCEMENT": "The Post is an Home Page announcement."
-                    },
-                    "symbols": [
-                        "HOME_PAGE_ANNOUNCEMENT"
-                    ],
-                    "type": "enum"
-                }
-            },
-            {
-                "doc": "Content stored in the post.",
-                "name": "content",
-                "type": {
-                    "doc": "Content stored inside a Post.",
-                    "fields": [
-                        {
-                            "Searchable": {
-                                "fieldType": "TEXT_PARTIAL"
-                            },
-                            "doc": "Title of the post.",
-                            "name": "title",
-                            "type": "string"
-                        },
-                        {
-                            "doc": "Type of content held in the post.",
-                            "name": "type",
-                            "type": {
-                                "doc": "Enum defining the type of content held in a Post.",
-                                "name": "PostContentType",
-                                "namespace": "com.linkedin.pegasus2avro.post",
-                                "symbolDocs": {
-                                    "LINK": "Link content",
-                                    "TEXT": "Text content"
-                                },
-                                "symbols": [
-                                    "TEXT",
-                                    "LINK"
-                                ],
-                                "type": "enum"
-                            }
-                        },
-                        {
-                            "default": null,
-                            "doc": "Optional description of the post.",
-                            "name": "description",
-                            "type": [
-                                "null",
-                                "string"
-                            ]
-                        },
-                        {
-                            "default": null,
-                            "doc": "Optional link that the post is associated with.",
-                            "java": {
-                                "class": "com.linkedin.pegasus2avro.common.url.Url",
-                                "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                            },
-                            "name": "link",
-                            "type": [
-                                "null",
-                                "string"
-                            ]
-                        },
-                        {
-                            "default": null,
-                            "doc": "Optional media that the post is storing",
-                            "name": "media",
-                            "type": [
-                                "null",
-                                {
-                                    "doc": "Carries information about which roles a user is assigned to.",
-                                    "fields": [
-                                        {
-                                            "doc": "Type of content the Media is storing, e.g. image, video, etc.",
-                                            "name": "type",
-                                            "type": {
-                                                "doc": "Enum defining the type of content a Media object holds.",
-                                                "name": "MediaType",
-                                                "namespace": "com.linkedin.pegasus2avro.common",
-                                                "symbolDocs": {
-                                                    "IMAGE": "The Media holds an image."
-                                                },
-                                                "symbols": [
-                                                    "IMAGE"
-                                                ],
-                                                "type": "enum"
-                                            }
-                                        },
-                                        {
-                                            "doc": "Where the media content is stored.",
-                                            "java": {
-                                                "class": "com.linkedin.pegasus2avro.common.url.Url",
-                                                "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                                            },
-                                            "name": "location",
-                                            "type": "string"
-                                        }
-                                    ],
-                                    "name": "Media",
-                                    "namespace": "com.linkedin.pegasus2avro.common",
-                                    "type": "record"
-                                }
-                            ]
-                        }
-                    ],
-                    "name": "PostContent",
-                    "namespace": "com.linkedin.pegasus2avro.post",
-                    "type": "record"
-                }
-            },
-            {
-                "Searchable": {
-                    "fieldType": "COUNT"
-                },
-                "doc": "The time at which the post was initially created",
-                "name": "created",
-                "type": "long"
-            },
-            {
-                "Searchable": {
-                    "fieldType": "COUNT"
-                },
-                "doc": "The time at which the post was last modified",
-                "name": "lastModified",
-                "type": "long"
-            }
-        ],
-        "name": "PostInfo",
-        "namespace": "com.linkedin.pegasus2avro.post",
-        "type": "record"
-    },
-    {
-        "Event": {
-            "name": "entityChangeEvent"
-        },
-        "doc": "Shared fields for all entity change events.",
-        "fields": [
-            {
-                "doc": "The type of the entity affected. Corresponds to the entity registry, e.g. 'dataset', 'chart', 'dashboard', etc.",
-                "name": "entityType",
-                "type": "string"
-            },
-            {
-                "doc": "The urn of the entity which was affected.",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "entityUrn",
-                "type": "string"
-            },
-            {
-                "doc": "The category type (TAG, GLOSSARY_TERM, OWNERSHIP, TECHNICAL_SCHEMA, etc). This is used to determine what the rest of the schema will look like.",
-                "name": "category",
-                "type": "string"
-            },
-            {
-                "doc": "The operation type. This is used to determine what the rest of the schema will look like.",
-                "name": "operation",
-                "type": "string"
-            },
-            {
-                "default": null,
-                "doc": "The urn of the entity which was affected.",
-                "name": "modifier",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "default": null,
-                "doc": "Arbitrary key-value parameters corresponding to the event.",
-                "name": "parameters",
-                "type": [
-                    "null",
-                    {
-                        "doc": "Arbitrary key-value parameters for an Entity Change Event. (any record).",
-                        "fields": [],
-                        "name": "Parameters",
-                        "namespace": "com.linkedin.pegasus2avro.platform.event.v1",
-                        "type": "record"
-                    }
-                ]
-            },
-            {
-                "doc": "Audit stamp of the operation",
-                "name": "auditStamp",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-            },
-            {
-                "doc": "The version of the event type, incremented in integers.",
-                "name": "version",
-                "type": "int"
-            }
-        ],
-        "name": "EntityChangeEvent",
-        "namespace": "com.linkedin.pegasus2avro.platform.event.v1",
-        "type": "record"
-    },
-    {
-        "doc": "Usage data for a given resource, rolled up into a bucket.",
-        "fields": [
-            {
-                "doc": " Bucket start time in milliseconds ",
-                "name": "bucket",
-                "type": "long"
-            },
-            {
-                "doc": " Bucket duration ",
-                "name": "duration",
-                "type": {
-                    "doc": "Enum to define the length of a bucket when doing aggregations",
-                    "name": "WindowDuration",
-                    "namespace": "com.linkedin.pegasus2avro.common",
-                    "symbols": [
-                        "YEAR",
-                        "MONTH",
-                        "WEEK",
-                        "DAY",
-                        "HOUR"
-                    ],
-                    "type": "enum"
-                }
-            },
-            {
-                "doc": " Resource associated with these usage stats ",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "resource",
-                "type": "string"
-            },
-            {
-                "doc": " Metrics associated with this bucket ",
-                "name": "metrics",
-                "type": {
-                    "doc": "Metrics for usage data for a given resource and bucket. Not all fields\nmake sense for all buckets, so every field is optional.",
-                    "fields": [
-                        {
-                            "default": null,
-                            "doc": " Unique user count ",
-                            "name": "uniqueUserCount",
-                            "type": [
-                                "null",
-                                "int"
-                            ]
-                        },
-                        {
-                            "default": null,
-                            "doc": " Users within this bucket, with frequency counts ",
-                            "name": "users",
-                            "type": [
-                                "null",
-                                {
-                                    "items": {
-                                        "doc": " Records a single user's usage counts for a given resource ",
-                                        "fields": [
-                                            {
-                                                "default": null,
-                                                "java": {
-                                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                                },
-                                                "name": "user",
-                                                "type": [
-                                                    "null",
-                                                    "string"
-                                                ]
-                                            },
-                                            {
-                                                "name": "count",
-                                                "type": "int"
-                                            },
-                                            {
-                                                "default": null,
-                                                "doc": " If user_email is set, we attempt to resolve the user's urn upon ingest ",
-                                                "name": "userEmail",
-                                                "type": [
-                                                    "null",
-                                                    "string"
-                                                ]
-                                            }
-                                        ],
-                                        "name": "UserUsageCounts",
-                                        "namespace": "com.linkedin.pegasus2avro.usage",
-                                        "type": "record"
-                                    },
-                                    "type": "array"
-                                }
-                            ]
-                        },
-                        {
-                            "default": null,
-                            "doc": " Total SQL query count ",
-                            "name": "totalSqlQueries",
-                            "type": [
-                                "null",
-                                "int"
-                            ]
-                        },
-                        {
-                            "default": null,
-                            "doc": " Frequent SQL queries; mostly makes sense for datasets in SQL databases ",
-                            "name": "topSqlQueries",
-                            "type": [
-                                "null",
-                                {
-                                    "items": "string",
-                                    "type": "array"
-                                }
-                            ]
-                        },
-                        {
-                            "default": null,
-                            "doc": " Field-level usage stats ",
-                            "name": "fields",
-                            "type": [
-                                "null",
-                                {
-                                    "items": {
-                                        "doc": " Records field-level usage counts for a given resource ",
-                                        "fields": [
-                                            {
-                                                "name": "fieldName",
-                                                "type": "string"
-                                            },
-                                            {
-                                                "name": "count",
-                                                "type": "int"
-                                            }
-                                        ],
-                                        "name": "FieldUsageCounts",
-                                        "namespace": "com.linkedin.pegasus2avro.usage",
-                                        "type": "record"
-                                    },
-                                    "type": "array"
-                                }
-                            ]
-                        }
-                    ],
-                    "name": "UsageAggregationMetrics",
-                    "namespace": "com.linkedin.pegasus2avro.usage",
-                    "type": "record"
-                }
-            }
-        ],
-        "name": "UsageAggregation",
-        "namespace": "com.linkedin.pegasus2avro.usage",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "institutionalMemory"
-        },
-        "doc": "Institutional memory of an entity. This is a way to link to relevant documentation and provide description of the documentation. Institutional or tribal knowledge is very important for users to leverage the entity.",
-        "fields": [
-            {
-                "doc": "List of records that represent institutional memory of an entity. Each record consists of a link, description, creator and timestamps associated with that record.",
-                "name": "elements",
-                "type": {
-                    "items": {
-                        "doc": "Metadata corresponding to a record of institutional memory.",
-                        "fields": [
-                            {
-                                "doc": "Link to an engineering design document or a wiki page.",
-                                "java": {
-                                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                                },
-                                "name": "url",
-                                "type": "string"
-                            },
-                            {
-                                "doc": "Description of the link.",
-                                "name": "description",
-                                "type": "string"
-                            },
-                            {
-                                "doc": "Audit stamp associated with creation of this record",
-                                "name": "createStamp",
-                                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-                            }
-                        ],
-                        "name": "InstitutionalMemoryMetadata",
-                        "namespace": "com.linkedin.pegasus2avro.common",
-                        "type": "record"
-                    },
-                    "type": "array"
-                }
-            }
-        ],
-        "name": "InstitutionalMemory",
-        "namespace": "com.linkedin.pegasus2avro.common",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "dataPlatformInstance"
-        },
-        "doc": "The specific instance of the data platform that this entity belongs to",
-        "fields": [
-            {
-                "Searchable": {
-                    "addToFilters": true,
-                    "fieldType": "URN",
-                    "filterNameOverride": "Platform"
-                },
-                "doc": "Data Platform",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "platform",
-                "type": "string"
-            },
-            {
-                "Searchable": {
-                    "addToFilters": true,
-                    "fieldName": "platformInstance",
-                    "fieldType": "URN",
-                    "filterNameOverride": "Platform Instance"
-                },
-                "default": null,
-                "doc": "Instance of the data platform (e.g. db instance)",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "instance",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            }
-        ],
-        "name": "DataPlatformInstance",
-        "namespace": "com.linkedin.pegasus2avro.common",
-        "type": "record"
-    },
-    "com.linkedin.pegasus2avro.common.GlobalTags",
-    {
-        "Aspect": {
-            "name": "embed"
-        },
-        "doc": "Information regarding rendering an embed for an asset.",
-        "fields": [
-            {
-                "default": null,
-                "doc": "An embed URL to be rendered inside of an iframe.",
-                "name": "renderUrl",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            }
-        ],
-        "name": "Embed",
-        "namespace": "com.linkedin.pegasus2avro.common",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "ownership"
-        },
-        "doc": "Ownership information of an entity.",
-        "fields": [
-            {
-                "doc": "List of owners of the entity.",
-                "name": "owners",
-                "type": {
-                    "items": {
-                        "doc": "Ownership information",
-                        "fields": [
-                            {
-                                "Relationship": {
-                                    "entityTypes": [
-                                        "corpuser",
-                                        "corpGroup"
-                                    ],
-                                    "name": "OwnedBy"
-                                },
-                                "Searchable": {
-                                    "addToFilters": true,
-                                    "fieldName": "owners",
-                                    "fieldType": "URN",
-                                    "filterNameOverride": "Owned By",
-                                    "hasValuesFieldName": "hasOwners",
-                                    "queryByDefault": false
-                                },
-                                "doc": "Owner URN, e.g. urn:li:corpuser:ldap, urn:li:corpGroup:group_name, and urn:li:multiProduct:mp_name\n(Caveat: only corpuser is currently supported in the frontend.)",
-                                "java": {
-                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                },
-                                "name": "owner",
-                                "type": "string"
-                            },
-                            {
-                                "doc": "The type of the ownership",
-                                "name": "type",
-                                "type": {
-                                    "deprecatedSymbols": {
-                                        "CONSUMER": true,
-                                        "DATAOWNER": true,
-                                        "DELEGATE": true,
-                                        "DEVELOPER": true,
-                                        "PRODUCER": true,
-                                        "STAKEHOLDER": true
-                                    },
-                                    "doc": "Asset owner types",
-                                    "name": "OwnershipType",
-                                    "namespace": "com.linkedin.pegasus2avro.common",
-                                    "symbolDocs": {
-                                        "BUSINESS_OWNER": "A person or group who is responsible for logical, or business related, aspects of the asset.",
-                                        "CONSUMER": "A person, group, or service that consumes the data\nDeprecated! Use TECHNICAL_OWNER or BUSINESS_OWNER instead.",
-                                        "DATAOWNER": "A person or group that is owning the data\nDeprecated! Use TECHNICAL_OWNER instead.",
-                                        "DATA_STEWARD": "A steward, expert, or delegate responsible for the asset.",
-                                        "DELEGATE": "A person or a group that overseas the operation, e.g. a DBA or SRE.\nDeprecated! Use TECHNICAL_OWNER instead.",
-                                        "DEVELOPER": "A person or group that is in charge of developing the code\nDeprecated! Use TECHNICAL_OWNER instead.",
-                                        "NONE": "No specific type associated to the owner.",
-                                        "PRODUCER": "A person, group, or service that produces/generates the data\nDeprecated! Use TECHNICAL_OWNER instead.",
-                                        "STAKEHOLDER": "A person or a group that has direct business interest\nDeprecated! Use TECHNICAL_OWNER, BUSINESS_OWNER, or STEWARD instead.",
-                                        "TECHNICAL_OWNER": "person or group who is responsible for technical aspects of the asset."
-                                    },
-                                    "symbols": [
-                                        "TECHNICAL_OWNER",
-                                        "BUSINESS_OWNER",
-                                        "DATA_STEWARD",
-                                        "NONE",
-                                        "DEVELOPER",
-                                        "DATAOWNER",
-                                        "DELEGATE",
-                                        "PRODUCER",
-                                        "CONSUMER",
-                                        "STAKEHOLDER"
-                                    ],
-                                    "type": "enum"
-                                }
-                            },
-                            {
-                                "default": null,
-                                "doc": "Source information for the ownership",
-                                "name": "source",
-                                "type": [
-                                    "null",
-                                    {
-                                        "doc": "Source/provider of the ownership information",
-                                        "fields": [
-                                            {
-                                                "doc": "The type of the source",
-                                                "name": "type",
-                                                "type": {
-                                                    "name": "OwnershipSourceType",
-                                                    "namespace": "com.linkedin.pegasus2avro.common",
-                                                    "symbolDocs": {
-                                                        "AUDIT": "Auditing system or audit logs",
-                                                        "DATABASE": "Database, e.g. GRANTS table",
-                                                        "FILE_SYSTEM": "File system, e.g. file/directory owner",
-                                                        "ISSUE_TRACKING_SYSTEM": "Issue tracking system, e.g. Jira",
-                                                        "MANUAL": "Manually provided by a user",
-                                                        "OTHER": "Other sources",
-                                                        "SERVICE": "Other ownership-like service, e.g. Nuage, ACL service etc",
-                                                        "SOURCE_CONTROL": "SCM system, e.g. GIT, SVN"
-                                                    },
-                                                    "symbols": [
-                                                        "AUDIT",
-                                                        "DATABASE",
-                                                        "FILE_SYSTEM",
-                                                        "ISSUE_TRACKING_SYSTEM",
-                                                        "MANUAL",
-                                                        "SERVICE",
-                                                        "SOURCE_CONTROL",
-                                                        "OTHER"
-                                                    ],
-                                                    "type": "enum"
-                                                }
-                                            },
-                                            {
-                                                "default": null,
-                                                "doc": "A reference URL for the source",
-                                                "name": "url",
-                                                "type": [
-                                                    "null",
-                                                    "string"
-                                                ]
-                                            }
-                                        ],
-                                        "name": "OwnershipSource",
-                                        "namespace": "com.linkedin.pegasus2avro.common",
-                                        "type": "record"
-                                    }
-                                ]
-                            }
-                        ],
-                        "name": "Owner",
-                        "namespace": "com.linkedin.pegasus2avro.common",
-                        "type": "record"
-                    },
-                    "type": "array"
-                }
-            },
-            {
-                "default": {
-                    "actor": "urn:li:corpuser:unknown",
-                    "impersonator": null,
-                    "message": null,
-                    "time": 0
-                },
-                "doc": "Audit stamp containing who last modified the record and when. A value of 0 in the time field indicates missing data.",
-                "name": "lastModified",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-            }
-        ],
-        "name": "Ownership",
-        "namespace": "com.linkedin.pegasus2avro.common",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "cost"
-        },
-        "fields": [
-            {
-                "name": "costType",
-                "type": {
-                    "doc": "Type of Cost Code",
-                    "name": "CostType",
-                    "namespace": "com.linkedin.pegasus2avro.common",
-                    "symbolDocs": {
-                        "ORG_COST_TYPE": "Org Cost Type to which the Cost of this entity should be attributed to"
-                    },
-                    "symbols": [
-                        "ORG_COST_TYPE"
-                    ],
-                    "type": "enum"
-                }
-            },
-            {
-                "name": "cost",
-                "type": {
-                    "fields": [
-                        {
-                            "default": null,
-                            "name": "costId",
-                            "type": [
-                                "null",
-                                "double"
-                            ]
-                        },
-                        {
-                            "default": null,
-                            "name": "costCode",
-                            "type": [
-                                "null",
-                                "string"
-                            ]
-                        },
-                        {
-                            "doc": "Contains the name of the field that has its value set.",
-                            "name": "fieldDiscriminator",
-                            "type": {
-                                "name": "CostCostDiscriminator",
-                                "namespace": "com.linkedin.pegasus2avro.common",
-                                "symbols": [
-                                    "costId",
-                                    "costCode"
-                                ],
-                                "type": "enum"
-                            }
-                        }
-                    ],
-                    "name": "CostCost",
-                    "namespace": "com.linkedin.pegasus2avro.common",
-                    "type": "record"
-                }
-            }
-        ],
-        "name": "Cost",
-        "namespace": "com.linkedin.pegasus2avro.common",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "status"
-        },
-        "doc": "The lifecycle status metadata of an entity, e.g. dataset, metric, feature, etc.\nThis aspect is used to represent soft deletes conventionally.",
-        "fields": [
-            {
-                "Searchable": {
-                    "fieldType": "BOOLEAN"
-                },
-                "default": false,
-                "doc": "Whether the entity has been removed (soft-deleted).",
-                "name": "removed",
-                "type": "boolean"
-            }
-        ],
-        "name": "Status",
-        "namespace": "com.linkedin.pegasus2avro.common",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "operation",
-            "type": "timeseries"
-        },
-        "doc": "Operational info for an entity.",
-        "fields": [
-            {
-                "doc": "The event timestamp field as epoch at UTC in milli seconds.",
-                "name": "timestampMillis",
-                "type": "long"
-            },
-            {
-                "default": null,
-                "doc": "Granularity of the event if applicable",
-                "name": "eventGranularity",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
-                ]
-            },
-            {
-                "default": {
-                    "partition": "FULL_TABLE_SNAPSHOT",
-                    "timePartition": null,
-                    "type": "FULL_TABLE"
-                },
-                "doc": "The optional partition specification.",
-                "name": "partitionSpec",
-                "type": [
-                    "com.linkedin.pegasus2avro.timeseries.PartitionSpec",
-                    "null"
-                ]
-            },
-            {
-                "default": null,
-                "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
-                "name": "messageId",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "TimeseriesField": {},
-                "default": null,
-                "doc": "Actor who issued this operation.",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "actor",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "TimeseriesField": {},
-                "doc": "Operation type of change.",
-                "name": "operationType",
-                "type": {
-                    "doc": "Enum to define the operation type when an entity changes.",
-                    "name": "OperationType",
-                    "namespace": "com.linkedin.pegasus2avro.common",
-                    "symbolDocs": {
-                        "ALTER": "Asset was altered",
-                        "CREATE": "Asset was created",
-                        "CUSTOM": "Custom asset operation",
-                        "DELETE": "Rows were deleted",
-                        "DROP": "Asset was dropped",
-                        "INSERT": "Rows were inserted",
-                        "UPDATE": "Rows were updated"
-                    },
-                    "symbols": [
-                        "INSERT",
-                        "UPDATE",
-                        "DELETE",
-                        "CREATE",
-                        "ALTER",
-                        "DROP",
-                        "CUSTOM",
-                        "UNKNOWN"
-                    ],
-                    "type": "enum"
-                }
-            },
-            {
-                "TimeseriesField": {},
-                "default": null,
-                "doc": "A custom type of operation. Required if operationType is CUSTOM.",
-                "name": "customOperationType",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "TimeseriesField": {},
-                "default": null,
-                "doc": "How many rows were affected by this operation.",
-                "name": "numAffectedRows",
-                "type": [
-                    "null",
-                    "long"
-                ]
-            },
-            {
-                "TimeseriesFieldCollection": {
-                    "key": "datasetName"
-                },
-                "default": null,
-                "doc": "Which other datasets were affected by this operation.",
-                "name": "affectedDatasets",
-                "type": [
-                    "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
-                ]
-            },
-            {
-                "TimeseriesField": {},
-                "default": null,
-                "doc": "Source Type",
-                "name": "sourceType",
-                "type": [
-                    "null",
-                    {
-                        "doc": "The source of an operation",
-                        "name": "OperationSourceType",
-                        "namespace": "com.linkedin.pegasus2avro.common",
-                        "symbolDocs": {
-                            "DATA_PLATFORM": "Rows were updated",
-                            "DATA_PROCESS": "Provided by a Data Process"
-                        },
-                        "symbols": [
-                            "DATA_PROCESS",
-                            "DATA_PLATFORM"
-                        ],
-                        "type": "enum"
-                    }
-                ]
-            },
-            {
-                "default": null,
-                "doc": "Custom properties",
-                "name": "customProperties",
-                "type": [
-                    "null",
-                    {
-                        "type": "map",
-                        "values": "string"
-                    }
-                ]
-            },
-            {
-                "Searchable": {
-                    "fieldName": "lastOperationTime",
-                    "fieldType": "DATETIME"
-                },
-                "TimeseriesField": {},
-                "doc": "The time at which the operation occurred. Would be better named 'operationTime'",
-                "name": "lastUpdatedTimestamp",
-                "type": "long"
-            }
-        ],
-        "name": "Operation",
-        "namespace": "com.linkedin.pegasus2avro.common",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "deprecation"
-        },
-        "doc": "Deprecation status of an entity",
-        "fields": [
-            {
-                "Searchable": {
-                    "fieldType": "BOOLEAN",
-                    "weightsPerFieldValue": {
-                        "true": 0.5
-                    }
-                },
-                "doc": "Whether the entity is deprecated.",
-                "name": "deprecated",
-                "type": "boolean"
-            },
-            {
-                "default": null,
-                "doc": "The time user plan to decommission this entity.",
-                "name": "decommissionTime",
-                "type": [
-                    "null",
-                    "long"
-                ]
-            },
-            {
-                "doc": "Additional information about the entity deprecation plan, such as the wiki, doc, RB.",
-                "name": "note",
-                "type": "string"
-            },
-            {
-                "doc": "The user URN which will be credited for modifying this deprecation content.",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "actor",
-                "type": "string"
-            }
-        ],
-        "name": "Deprecation",
-        "namespace": "com.linkedin.pegasus2avro.common",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "inputFields"
-        },
-        "doc": "Information about the fields a chart or dashboard references",
-        "fields": [
-            {
-                "doc": "List of fields being referenced",
-                "name": "fields",
-                "type": {
-                    "items": {
-                        "doc": "Information about a field a chart or dashboard references",
-                        "fields": [
-                            {
-                                "Relationship": {
-                                    "entityTypes": [
-                                        "schemaField"
-                                    ],
-                                    "name": "consumesField"
-                                },
-                                "doc": "Urn of the schema being referenced for lineage purposes",
-                                "java": {
-                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                },
-                                "name": "schemaFieldUrn",
-                                "type": "string"
-                            },
-                            {
-                                "default": null,
-                                "doc": "Copied version of the referenced schema field object for indexing purposes",
-                                "name": "schemaField",
-                                "type": [
-                                    "null",
-                                    "com.linkedin.pegasus2avro.schema.SchemaField"
-                                ]
-                            }
-                        ],
-                        "name": "InputField",
-                        "namespace": "com.linkedin.pegasus2avro.common",
-                        "type": "record"
-                    },
-                    "type": "array"
-                }
-            }
-        ],
-        "name": "InputFields",
-        "namespace": "com.linkedin.pegasus2avro.common",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "browsePaths"
-        },
-        "doc": "Shared aspect containing Browse Paths to be indexed for an entity.",
-        "fields": [
-            {
-                "Searchable": {
-                    "/*": {
-                        "fieldName": "browsePaths",
-                        "fieldType": "BROWSE_PATH"
-                    }
-                },
-                "doc": "A list of valid browse paths for the entity.\n\nBrowse paths are expected to be forward slash-separated strings. For example: 'prod/snowflake/datasetName'",
-                "name": "paths",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                }
-            }
-        ],
-        "name": "BrowsePaths",
-        "namespace": "com.linkedin.pegasus2avro.common",
-        "type": "record"
-    },
-    "com.linkedin.pegasus2avro.common.GlossaryTerms",
-    {
-        "Aspect": {
-            "name": "siblings"
-        },
-        "doc": "Siblings information of an entity.",
-        "fields": [
-            {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "dataset"
-                        ],
-                        "name": "SiblingOf"
-                    }
-                },
-                "Searchable": {
-                    "/*": {
-                        "fieldName": "siblings",
-                        "fieldType": "URN",
-                        "queryByDefault": false
-                    }
-                },
-                "doc": "List of sibling entities",
-                "name": "siblings",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                }
-            },
-            {
-                "doc": "If this is the leader entity of the set of siblings",
-                "name": "primary",
-                "type": "boolean"
-            }
-        ],
-        "name": "Siblings",
-        "namespace": "com.linkedin.pegasus2avro.common",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "subTypes"
-        },
-        "doc": "Sub Types. Use this aspect to specialize a generic Entity\ne.g. Making a Dataset also be a View or also be a LookerExplore",
-        "fields": [
-            {
-                "Searchable": {
-                    "/*": {
-                        "addToFilters": true,
-                        "fieldType": "KEYWORD",
-                        "filterNameOverride": "Sub Type",
-                        "queryByDefault": true
-                    }
-                },
-                "doc": "The names of the specific types.",
-                "name": "typeNames",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                }
-            }
-        ],
-        "name": "SubTypes",
-        "namespace": "com.linkedin.pegasus2avro.common",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "origin"
-        },
-        "doc": "Carries information about where an entity originated from.",
-        "fields": [
-            {
-                "doc": "Where an entity originated from. Either NATIVE or EXTERNAL.",
-                "name": "type",
-                "type": {
-                    "doc": "Enum to define where an entity originated from.",
-                    "name": "OriginType",
-                    "namespace": "com.linkedin.pegasus2avro.common",
-                    "symbolDocs": {
-                        "EXTERNAL": "The entity is external to DataHub.",
-                        "NATIVE": "The entity is native to DataHub."
-                    },
-                    "symbols": [
-                        "NATIVE",
-                        "EXTERNAL"
-                    ],
-                    "type": "enum"
-                }
-            },
-            {
-                "default": null,
-                "doc": "Only populated if type is EXTERNAL. The externalType of the entity, such as the name of the identity provider.",
-                "name": "externalType",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            }
-        ],
-        "name": "Origin",
-        "namespace": "com.linkedin.pegasus2avro.common",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "tagProperties"
-        },
-        "doc": "Properties associated with a Tag",
-        "fields": [
-            {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "Display name of the tag",
-                "name": "name",
-                "type": "string"
-            },
-            {
-                "Searchable": {},
-                "default": null,
-                "doc": "Documentation of the tag",
+                "doc": "Documentation of the ml model group",
                 "name": "description",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "default": null,
-                "doc": "The color associated with the Tag in Hex. For example #FFFFFF.",
-                "name": "colorHex",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            }
-        ],
-        "name": "TagProperties",
-        "namespace": "com.linkedin.pegasus2avro.tag",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "dataProcessInstanceOutput"
-        },
-        "doc": "Information about the outputs of a Data process",
-        "fields": [
-            {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "dataset"
-                        ],
-                        "name": "Produces"
-                    }
-                },
-                "Searchable": {
-                    "/*": {
-                        "addToFilters": true,
-                        "fieldName": "outputs",
-                        "fieldType": "URN",
-                        "numValuesFieldName": "numOutputs",
-                        "queryByDefault": false
-                    }
-                },
-                "doc": "Output datasets to be produced",
-                "name": "outputs",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                }
-            }
-        ],
-        "name": "DataProcessInstanceOutput",
-        "namespace": "com.linkedin.pegasus2avro.dataprocess",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "dataProcessInstanceProperties"
-        },
-        "doc": "The inputs and outputs of this data process",
-        "fields": [
-            {
-                "Searchable": {
-                    "/*": {
-                        "queryByDefault": true
-                    }
-                },
-                "default": {},
-                "doc": "Custom property bag.",
-                "name": "customProperties",
-                "type": {
-                    "type": "map",
-                    "values": "string"
-                }
-            },
-            {
-                "default": null,
-                "doc": "URL where the reference exist",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                },
-                "name": "externalUrl",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "Process name",
-                "name": "name",
-                "type": "string"
-            },
-            {
-                "Searchable": {
-                    "addToFilters": true,
-                    "fieldType": "KEYWORD",
-                    "filterNameOverride": "Process Type"
-                },
-                "default": null,
-                "doc": "Process type",
-                "name": "type",
-                "type": [
-                    "null",
-                    {
-                        "name": "DataProcessType",
-                        "namespace": "com.linkedin.pegasus2avro.dataprocess",
-                        "symbols": [
-                            "BATCH_SCHEDULED",
-                            "BATCH_AD_HOC",
-                            "STREAMING"
-                        ],
-                        "type": "enum"
-                    }
-                ]
-            },
-            {
-                "Searchable": {
-                    "/time": {
-                        "fieldName": "created",
-                        "fieldType": "COUNT",
-                        "queryByDefault": false
-                    }
-                },
-                "doc": "Audit stamp containing who reported the lineage and when",
-                "name": "created",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-            }
-        ],
-        "name": "DataProcessInstanceProperties",
-        "namespace": "com.linkedin.pegasus2avro.dataprocess",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "dataProcessInstanceInput"
-        },
-        "doc": "Information about the inputs datasets of a Data process",
-        "fields": [
-            {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "dataset"
-                        ],
-                        "name": "Consumes"
-                    }
-                },
-                "Searchable": {
-                    "/*": {
-                        "addToFilters": true,
-                        "fieldName": "inputs",
-                        "fieldType": "URN",
-                        "numValuesFieldName": "numInputs",
-                        "queryByDefault": false
-                    }
-                },
-                "doc": "Input datasets to be consumed",
-                "name": "inputs",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                }
-            }
-        ],
-        "name": "DataProcessInstanceInput",
-        "namespace": "com.linkedin.pegasus2avro.dataprocess",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "dataProcessInstanceRunEvent",
-            "type": "timeseries"
-        },
-        "doc": "An event representing the current status of data process run.\nDataProcessRunEvent should be used for reporting the status of a dataProcess' run.",
-        "fields": [
-            {
-                "doc": "The event timestamp field as epoch at UTC in milli seconds.",
-                "name": "timestampMillis",
-                "type": "long"
-            },
-            {
-                "default": null,
-                "doc": "Granularity of the event if applicable",
-                "name": "eventGranularity",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
-                ]
-            },
-            {
-                "default": {
-                    "partition": "FULL_TABLE_SNAPSHOT",
-                    "timePartition": null,
-                    "type": "FULL_TABLE"
-                },
-                "doc": "The optional partition specification.",
-                "name": "partitionSpec",
-                "type": [
-                    "com.linkedin.pegasus2avro.timeseries.PartitionSpec",
-                    "null"
-                ]
-            },
-            {
-                "default": null,
-                "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
-                "name": "messageId",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "default": null,
-                "doc": "URL where the reference exist",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                },
-                "name": "externalUrl",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "TimeseriesField": {},
-                "name": "status",
-                "type": {
-                    "name": "DataProcessRunStatus",
-                    "namespace": "com.linkedin.pegasus2avro.dataprocess",
-                    "symbolDocs": {
-                        "STARTED": "The status where the Data processing run is in."
-                    },
-                    "symbols": [
-                        "STARTED",
-                        "COMPLETE"
-                    ],
-                    "type": "enum"
-                }
-            },
-            {
-                "default": null,
-                "doc": "Return the try number that this Instance Run is in",
-                "name": "attempt",
-                "type": [
-                    "null",
-                    "int"
-                ]
-            },
-            {
-                "TimeseriesField": {},
-                "default": null,
-                "doc": "The final result of the Data Processing run.",
-                "name": "result",
-                "type": [
-                    "null",
-                    {
-                        "fields": [
-                            {
-                                "doc": " The final result, e.g. SUCCESS, FAILURE, SKIPPED, or UP_FOR_RETRY.",
-                                "name": "type",
-                                "type": {
-                                    "name": "RunResultType",
-                                    "namespace": "com.linkedin.pegasus2avro.dataprocess",
-                                    "symbolDocs": {
-                                        "FAILURE": " The Run Failed",
-                                        "SKIPPED": " The Run Skipped",
-                                        "SUCCESS": " The Run Succeeded",
-                                        "UP_FOR_RETRY": " The Run Failed and will Retry"
-                                    },
-                                    "symbols": [
-                                        "SUCCESS",
-                                        "FAILURE",
-                                        "SKIPPED",
-                                        "UP_FOR_RETRY"
-                                    ],
-                                    "type": "enum"
-                                }
-                            },
-                            {
-                                "doc": "It identifies the system where the native result comes from like Airflow, Azkaban, etc..",
-                                "name": "nativeResultType",
-                                "type": "string"
-                            }
-                        ],
-                        "name": "DataProcessInstanceRunResult",
-                        "namespace": "com.linkedin.pegasus2avro.dataprocess",
-                        "type": "record"
-                    }
+                "type": [
+                    "null",
+                    "string"
                 ]
             }
         ],
-        "name": "DataProcessInstanceRunEvent",
-        "namespace": "com.linkedin.pegasus2avro.dataprocess",
+        "name": "EditableMLModelGroupProperties",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dataProcessInfo"
+            "name": "mlFeatureTableProperties"
         },
-        "doc": "The inputs and outputs of this data process",
+        "doc": "Properties associated with a MLFeatureTable",
         "fields": [
             {
+                "Searchable": {
+                    "/*": {
+                        "queryByDefault": true
+                    }
+                },
+                "default": {},
+                "doc": "Custom property bag.",
+                "name": "customProperties",
+                "type": {
+                    "type": "map",
+                    "values": "string"
+                }
+            },
+            {
+                "Searchable": {
+                    "fieldType": "TEXT",
+                    "hasValuesFieldName": "hasDescription"
+                },
+                "default": null,
+                "doc": "Documentation of the MLFeatureTable",
+                "name": "description",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
                 "Relationship": {
                     "/*": {
                         "entityTypes": [
-                            "dataset"
+                            "mlFeature"
                         ],
-                        "isLineage": true,
-                        "name": "Consumes"
+                        "name": "Contains"
                     }
                 },
                 "Searchable": {
                     "/*": {
-                        "fieldName": "inputs",
-                        "fieldType": "URN",
-                        "numValuesFieldName": "numInputDatasets",
-                        "queryByDefault": false
+                        "fieldName": "features",
+                        "fieldType": "URN"
                     }
                 },
+                "Urn": "Urn",
                 "default": null,
-                "doc": "the inputs of the data process",
-                "name": "inputs",
+                "doc": "List of features contained in the feature table",
+                "name": "mlFeatures",
                 "type": [
                     "null",
                     {
                         "items": "string",
                         "type": "array"
                     }
-                ]
+                ],
+                "urn_is_array": true
             },
             {
                 "Relationship": {
                     "/*": {
                         "entityTypes": [
-                            "dataset"
+                            "mlPrimaryKey"
                         ],
-                        "isLineage": true,
-                        "name": "Consumes"
+                        "name": "KeyedBy"
                     }
                 },
                 "Searchable": {
                     "/*": {
-                        "fieldName": "outputs",
-                        "fieldType": "URN",
-                        "numValuesFieldName": "numOutputDatasets",
-                        "queryByDefault": false
+                        "fieldName": "primaryKeys",
+                        "fieldType": "URN"
                     }
                 },
+                "Urn": "Urn",
                 "default": null,
-                "doc": "the outputs of the data process",
-                "name": "outputs",
+                "doc": "List of primary keys in the feature table (if multiple, assumed to act as a composite key)",
+                "name": "mlPrimaryKeys",
                 "type": [
                     "null",
                     {
                         "items": "string",
                         "type": "array"
                     }
-                ]
+                ],
+                "urn_is_array": true
             }
         ],
-        "name": "DataProcessInfo",
-        "namespace": "com.linkedin.pegasus2avro.dataprocess",
+        "name": "MLFeatureTableProperties",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dataProcessInstanceRelationships"
+            "name": "mlModelQuantitativeAnalyses"
         },
-        "doc": "Information about Data process relationships",
+        "doc": "Quantitative analyses should be disaggregated, that is, broken down by the chosen factors. Quantitative analyses should provide the results of evaluating the MLModel according to the chosen metrics, providing confidence interval values when possible.",
         "fields": [
             {
-                "Relationship": {
-                    "entityTypes": [
-                        "dataJob",
-                        "dataFlow"
-                    ],
-                    "name": "InstanceOf"
-                },
-                "Searchable": {
-                    "/*": {
-                        "fieldName": "parentTemplate",
-                        "fieldType": "URN",
-                        "queryByDefault": false
-                    }
-                },
                 "default": null,
-                "doc": "The parent entity whose run instance it is",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "parentTemplate",
+                "doc": "Link to a dashboard with results showing how the MLModel performed with respect to each factor",
+                "name": "unitaryResults",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
-                "Relationship": {
-                    "entityTypes": [
-                        "dataProcessInstance"
-                    ],
-                    "name": "ChildOf"
-                },
-                "Searchable": {
-                    "/*": {
-                        "fieldName": "parentInstance",
-                        "fieldType": "URN",
-                        "queryByDefault": false
-                    }
-                },
                 "default": null,
-                "doc": "The parent DataProcessInstance where it belongs to.\nIf it is a Airflow Task then it should belong to an Airflow Dag run as well\nwhich will be another DataProcessInstance",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "parentInstance",
+                "doc": "Link to a dashboard with results showing how the MLModel performed with respect to the intersection of evaluated factors?",
+                "name": "intersectionalResults",
                 "type": [
                     "null",
                     "string"
                 ]
+            }
+        ],
+        "name": "QuantitativeAnalyses",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "intendedUse"
+        },
+        "doc": "Intended Use for the ML Model",
+        "fields": [
+            {
+                "default": null,
+                "doc": "Primary Use cases for the MLModel.",
+                "name": "primaryUses",
+                "type": [
+                    "null",
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ]
             },
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "dataProcessInstance"
-                        ],
-                        "name": "UpstreamOf"
+                "default": null,
+                "doc": "Primary Intended Users - For example, was the MLModel developed for entertainment purposes, for hobbyists, or enterprise solutions?",
+                "name": "primaryUsers",
+                "type": [
+                    "null",
+                    {
+                        "items": {
+                            "name": "IntendedUserType",
+                            "namespace": "com.linkedin.pegasus2avro.ml.metadata",
+                            "symbols": [
+                                "ENTERPRISE",
+                                "HOBBY",
+                                "ENTERTAINMENT"
+                            ],
+                            "type": "enum"
+                        },
+                        "type": "array"
                     }
-                },
-                "Searchable": {
-                    "/*": {
-                        "fieldName": "upstream",
-                        "fieldType": "URN",
-                        "numValuesFieldName": "numUpstreams",
-                        "queryByDefault": false
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Highlight technology that the MLModel might easily be confused with, or related contexts that users could try to apply the MLModel to.",
+                "name": "outOfScopeUses",
+                "type": [
+                    "null",
+                    {
+                        "items": "string",
+                        "type": "array"
                     }
-                },
-                "doc": "Input DataProcessInstance which triggered this dataprocess instance",
-                "name": "upstreamInstances",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                }
+                ]
             }
         ],
-        "name": "DataProcessInstanceRelationships",
-        "namespace": "com.linkedin.pegasus2avro.dataprocess",
+        "name": "IntendedUse",
+        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dataHubRetentionConfig"
+            "name": "dataHubPolicyInfo"
         },
+        "doc": "Information about a DataHub (UI) access policy.",
         "fields": [
             {
-                "name": "retention",
+                "Searchable": {
+                    "fieldType": "TEXT_PARTIAL"
+                },
+                "doc": "Display name of the Policy",
+                "name": "displayName",
+                "type": "string"
+            },
+            {
+                "Searchable": {
+                    "fieldType": "TEXT"
+                },
+                "doc": "Description of the Policy",
+                "name": "description",
+                "type": "string"
+            },
+            {
+                "doc": "The type of policy",
+                "name": "type",
+                "type": "string"
+            },
+            {
+                "doc": "The state of policy, ACTIVE or INACTIVE",
+                "name": "state",
+                "type": "string"
+            },
+            {
+                "default": null,
+                "doc": "The resource that the policy applies to. Not required for some 'Platform' privileges.",
+                "name": "resources",
+                "type": [
+                    "null",
+                    {
+                        "doc": "Information used to filter DataHub resource.",
+                        "fields": [
+                            {
+                                "default": null,
+                                "deprecated": true,
+                                "doc": "The type of resource that the policy applies to. This will most often be a data asset entity name, for\nexample 'dataset'. It is not strictly required because in the future we will want to support filtering a resource\nby domain, as well.",
+                                "name": "type",
+                                "type": [
+                                    "null",
+                                    "string"
+                                ]
+                            },
+                            {
+                                "default": null,
+                                "deprecated": true,
+                                "doc": "A specific set of resources to apply the policy to, e.g. asset urns",
+                                "name": "resources",
+                                "type": [
+                                    "null",
+                                    {
+                                        "items": "string",
+                                        "type": "array"
+                                    }
+                                ]
+                            },
+                            {
+                                "default": false,
+                                "deprecated": true,
+                                "doc": "Whether the policy should be applied to all assets matching the filter.",
+                                "name": "allResources",
+                                "type": "boolean"
+                            },
+                            {
+                                "default": null,
+                                "doc": "Filter to apply privileges to",
+                                "name": "filter",
+                                "type": [
+                                    "null",
+                                    {
+                                        "doc": "The filter for specifying the resource or actor to apply privileges to",
+                                        "fields": [
+                                            {
+                                                "doc": "A list of criteria to apply conjunctively (so all criteria must pass)",
+                                                "name": "criteria",
+                                                "type": {
+                                                    "items": {
+                                                        "doc": "A criterion for matching a field with given value",
+                                                        "fields": [
+                                                            {
+                                                                "doc": "The name of the field that the criterion refers to",
+                                                                "name": "field",
+                                                                "type": "string"
+                                                            },
+                                                            {
+                                                                "doc": "Values. Matches criterion if any one of the values matches condition (OR-relationship)",
+                                                                "name": "values",
+                                                                "type": {
+                                                                    "items": "string",
+                                                                    "type": "array"
+                                                                }
+                                                            },
+                                                            {
+                                                                "default": "EQUALS",
+                                                                "doc": "The condition for the criterion",
+                                                                "name": "condition",
+                                                                "type": {
+                                                                    "doc": "The matching condition in a filter criterion",
+                                                                    "name": "PolicyMatchCondition",
+                                                                    "namespace": "com.linkedin.pegasus2avro.policy",
+                                                                    "symbolDocs": {
+                                                                        "EQUALS": "Whether the field matches the value"
+                                                                    },
+                                                                    "symbols": [
+                                                                        "EQUALS"
+                                                                    ],
+                                                                    "type": "enum"
+                                                                }
+                                                            }
+                                                        ],
+                                                        "name": "PolicyMatchCriterion",
+                                                        "namespace": "com.linkedin.pegasus2avro.policy",
+                                                        "type": "record"
+                                                    },
+                                                    "type": "array"
+                                                }
+                                            }
+                                        ],
+                                        "name": "PolicyMatchFilter",
+                                        "namespace": "com.linkedin.pegasus2avro.policy",
+                                        "type": "record"
+                                    }
+                                ]
+                            }
+                        ],
+                        "name": "DataHubResourceFilter",
+                        "namespace": "com.linkedin.pegasus2avro.policy",
+                        "type": "record"
+                    }
+                ]
+            },
+            {
+                "doc": "The privileges that the policy grants.",
+                "name": "privileges",
                 "type": {
-                    "doc": "Base class that encapsulates different retention policies.\nOnly one of the fields should be set",
+                    "items": "string",
+                    "type": "array"
+                }
+            },
+            {
+                "doc": "The actors that the policy applies to.",
+                "name": "actors",
+                "type": {
+                    "doc": "Information used to filter DataHub actors.",
                     "fields": [
                         {
+                            "Urn": "Urn",
                             "default": null,
-                            "name": "version",
+                            "doc": "A specific set of users to apply the policy to (disjunctive)",
+                            "name": "users",
                             "type": [
                                 "null",
                                 {
-                                    "doc": "Keep max N latest records",
-                                    "fields": [
-                                        {
-                                            "name": "maxVersions",
-                                            "type": "int"
-                                        }
-                                    ],
-                                    "name": "VersionBasedRetention",
-                                    "namespace": "com.linkedin.pegasus2avro.retention",
-                                    "type": "record"
+                                    "items": "string",
+                                    "type": "array"
                                 }
-                            ]
+                            ],
+                            "urn_is_array": true
                         },
                         {
+                            "Urn": "Urn",
                             "default": null,
-                            "name": "time",
+                            "doc": "A specific set of groups to apply the policy to (disjunctive)",
+                            "name": "groups",
                             "type": [
                                 "null",
                                 {
-                                    "doc": "Keep records that are less than X seconds old",
-                                    "fields": [
-                                        {
-                                            "name": "maxAgeInSeconds",
-                                            "type": "int"
-                                        }
+                                    "items": "string",
+                                    "type": "array"
+                                }
+                            ],
+                            "urn_is_array": true
+                        },
+                        {
+                            "default": false,
+                            "doc": "Whether the filter should return true for owners of a particular resource.\nOnly applies to policies of type 'Metadata', which have a resource associated with them.",
+                            "name": "resourceOwners",
+                            "type": "boolean"
+                        },
+                        {
+                            "default": false,
+                            "doc": "Whether the filter should apply to all users.",
+                            "name": "allUsers",
+                            "type": "boolean"
+                        },
+                        {
+                            "default": false,
+                            "doc": "Whether the filter should apply to all groups.",
+                            "name": "allGroups",
+                            "type": "boolean"
+                        },
+                        {
+                            "Relationship": {
+                                "/*": {
+                                    "entityTypes": [
+                                        "dataHubRole"
                                     ],
-                                    "name": "TimeBasedRetention",
-                                    "namespace": "com.linkedin.pegasus2avro.retention",
-                                    "type": "record"
+                                    "name": "IsAssociatedWithRole"
                                 }
-                            ]
+                            },
+                            "Urn": "Urn",
+                            "default": null,
+                            "doc": "A specific set of roles to apply the policy to (disjunctive).",
+                            "name": "roles",
+                            "type": [
+                                "null",
+                                {
+                                    "items": "string",
+                                    "type": "array"
+                                }
+                            ],
+                            "urn_is_array": true
                         }
                     ],
-                    "name": "Retention",
-                    "namespace": "com.linkedin.pegasus2avro.retention",
+                    "name": "DataHubActorFilter",
+                    "namespace": "com.linkedin.pegasus2avro.policy",
                     "type": "record"
                 }
-            }
-        ],
-        "name": "DataHubRetentionConfig",
-        "namespace": "com.linkedin.pegasus2avro.retention",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "telemetryClientId"
-        },
-        "doc": "A simple wrapper around a String to persist the client ID for telemetry in DataHub's backend DB",
-        "fields": [
+            },
             {
-                "doc": "A string representing the telemetry client ID",
-                "name": "clientId",
-                "type": "string"
+                "default": true,
+                "doc": "Whether the policy should be editable via the UI",
+                "name": "editable",
+                "type": "boolean"
+            },
+            {
+                "Searchable": {
+                    "fieldType": "DATETIME"
+                },
+                "default": null,
+                "doc": "Timestamp when the policy was last updated",
+                "name": "lastUpdatedTimestamp",
+                "type": [
+                    "null",
+                    "long"
+                ]
             }
         ],
-        "name": "TelemetryClientId",
-        "namespace": "com.linkedin.pegasus2avro.telemetry",
+        "name": "DataHubPolicyInfo",
+        "namespace": "com.linkedin.pegasus2avro.policy",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "domainProperties"
+            "name": "dataHubRoleInfo"
         },
-        "doc": "Information about a Domain",
+        "doc": "Information about a DataHub Role.",
         "fields": [
             {
                 "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
                     "fieldType": "TEXT_PARTIAL"
                 },
-                "doc": "Display name of the Domain",
+                "doc": "Name of the Role",
                 "name": "name",
                 "type": "string"
             },
             {
-                "default": null,
-                "doc": "Description of the Domain",
+                "Searchable": {
+                    "fieldType": "TEXT"
+                },
+                "doc": "Description of the Role",
                 "name": "description",
-                "type": [
-                    "null",
-                    "string"
-                ]
+                "type": "string"
             },
             {
-                "Searchable": {
-                    "/time": {
-                        "fieldName": "createdTime",
-                        "fieldType": "DATETIME"
-                    }
-                },
-                "default": null,
-                "doc": "Created Audit stamp",
-                "name": "created",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.common.AuditStamp"
-                ]
+                "default": false,
+                "doc": "Whether the role should be editable via the UI",
+                "name": "editable",
+                "type": "boolean"
             }
         ],
-        "name": "DomainProperties",
-        "namespace": "com.linkedin.pegasus2avro.domain",
+        "name": "DataHubRoleInfo",
+        "namespace": "com.linkedin.pegasus2avro.policy",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "domains"
+            "name": "dataHubStepStateProperties"
         },
-        "doc": "Links from an Asset to its Domains",
+        "doc": "The properties associated with a DataHub step state",
         "fields": [
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "domain"
-                        ],
-                        "name": "AssociatedWith"
-                    }
-                },
-                "Searchable": {
-                    "/*": {
-                        "addToFilters": true,
-                        "fieldName": "domains",
-                        "fieldType": "URN",
-                        "filterNameOverride": "Domain",
-                        "hasValuesFieldName": "hasDomain"
-                    }
-                },
-                "doc": "The Domains attached to an Asset",
-                "name": "domains",
+                "default": {},
+                "doc": "Description of the secret",
+                "name": "properties",
                 "type": {
-                    "items": "string",
-                    "type": "array"
+                    "type": "map",
+                    "values": "string"
                 }
+            },
+            {
+                "doc": "Audit stamp describing the last person to update it.",
+                "name": "lastModified",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
             }
         ],
-        "name": "Domains",
-        "namespace": "com.linkedin.pegasus2avro.domain",
+        "name": "DataHubStepStateProperties",
+        "namespace": "com.linkedin.pegasus2avro.step",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dataHubAccessTokenInfo"
+            "name": "dataPlatformInfo"
         },
-        "doc": "Information about a DataHub Access Token",
+        "doc": "Information about a data platform",
         "fields": [
             {
                 "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": false,
                     "fieldType": "TEXT_PARTIAL"
                 },
-                "doc": "User defined name for the access token if defined.",
+                "doc": "Name of the data platform",
                 "name": "name",
-                "type": "string"
-            },
-            {
-                "Searchable": {
-                    "fieldType": "URN"
-                },
-                "doc": "Urn of the actor to which this access token belongs to.",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "actorUrn",
-                "type": "string"
-            },
-            {
-                "Searchable": {
-                    "fieldType": "URN"
-                },
-                "doc": "Urn of the actor which created this access token.",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "ownerUrn",
-                "type": "string"
-            },
-            {
-                "Searchable": {
-                    "fieldType": "COUNT",
-                    "queryByDefault": false
-                },
-                "doc": "When the token was created.",
-                "name": "createdAt",
-                "type": "long"
+                "type": "string",
+                "validate": {
+                    "strlen": {
+                        "max": 15
+                    }
+                }
             },
             {
                 "Searchable": {
-                    "fieldType": "COUNT",
-                    "queryByDefault": false
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL"
                 },
                 "default": null,
-                "doc": "When the token expires.",
-                "name": "expiresAt",
+                "doc": "The name that will be used for displaying a platform type.",
+                "name": "displayName",
                 "type": [
                     "null",
-                    "long"
+                    "string"
                 ]
             },
             {
+                "doc": "Platform type this data platform describes",
+                "name": "type",
+                "type": {
+                    "doc": "Platform types available at LinkedIn",
+                    "name": "PlatformType",
+                    "namespace": "com.linkedin.pegasus2avro.dataplatform",
+                    "symbolDocs": {
+                        "FILE_SYSTEM": "Value for a file system, e.g. hdfs",
+                        "KEY_VALUE_STORE": "Value for a key value store, e.g. espresso, voldemort",
+                        "MESSAGE_BROKER": "Value for a message broker, e.g. kafka",
+                        "OBJECT_STORE": "Value for an object store, e.g. ambry",
+                        "OLAP_DATASTORE": "Value for an OLAP datastore, e.g. pinot",
+                        "OTHERS": "Value for other platforms, e.g salesforce, dovetail",
+                        "QUERY_ENGINE": "Value for a query engine, e.g. presto",
+                        "RELATIONAL_DB": "Value for a relational database, e.g. oracle, mysql",
+                        "SEARCH_ENGINE": "Value for a search engine, e.g seas"
+                    },
+                    "symbols": [
+                        "FILE_SYSTEM",
+                        "KEY_VALUE_STORE",
+                        "MESSAGE_BROKER",
+                        "OBJECT_STORE",
+                        "OLAP_DATASTORE",
+                        "OTHERS",
+                        "QUERY_ENGINE",
+                        "RELATIONAL_DB",
+                        "SEARCH_ENGINE"
+                    ],
+                    "type": "enum"
+                }
+            },
+            {
+                "doc": "The delimiter in the dataset names on the data platform, e.g. '/' for HDFS and '.' for Oracle",
+                "name": "datasetNameDelimiter",
+                "type": "string"
+            },
+            {
                 "default": null,
-                "doc": "Description of the token if defined.",
-                "name": "description",
+                "doc": "The URL for a logo associated with the platform",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "logoUrl",
                 "type": [
                     "null",
                     "string"
                 ]
             }
         ],
-        "name": "DataHubAccessTokenInfo",
-        "namespace": "com.linkedin.pegasus2avro.access.token",
+        "name": "DataPlatformInfo",
+        "namespace": "com.linkedin.pegasus2avro.dataplatform",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dataHubIngestionSourceInfo"
+            "name": "dataHubExecutionRequestResult"
         },
-        "doc": "Info about a DataHub ingestion source",
+        "doc": "The result of an execution request",
         "fields": [
             {
-                "Searchable": {
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "The display name of the ingestion source",
-                "name": "name",
-                "type": "string"
-            },
-            {
-                "doc": "The type of the source itself, e.g. mysql, bigquery, bigquery-usage. Should match the recipe.",
-                "name": "type",
+                "doc": "The status of the execution request",
+                "name": "status",
                 "type": "string"
             },
             {
                 "default": null,
-                "doc": "Data Platform URN associated with the source",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "platform",
+                "doc": "The pretty-printed execution report.",
+                "name": "report",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "The schedule on which the ingestion source is executed",
-                "name": "schedule",
+                "doc": "A structured report if available.",
+                "name": "structuredReport",
                 "type": [
                     "null",
                     {
-                        "doc": "The schedule associated with an ingestion source.",
+                        "doc": "A flexible carrier for structured results of an execution request.\nThe goal is to allow for free flow of structured responses from execution tasks to the orchestrator or observer.\nThe full spectrum of different execution report types is not intended to be modeled by this object.",
                         "fields": [
                             {
-                                "doc": "A cron-formatted execution interval, as a cron string, e.g. * * * * *",
-                                "name": "interval",
+                                "doc": "The type of the structured report. (e.g. INGESTION_REPORT, TEST_CONNECTION_REPORT, etc.)",
+                                "name": "type",
                                 "type": "string"
                             },
                             {
-                                "doc": "Timezone in which the cron interval applies, e.g. America/Los Angeles",
-                                "name": "timezone",
+                                "doc": "The serialized value of the structured report",
+                                "name": "serializedValue",
+                                "type": "string"
+                            },
+                            {
+                                "doc": "The content-type of the serialized value (e.g. application/json, application/json;gzip etc.)",
+                                "name": "contentType",
                                 "type": "string"
                             }
                         ],
-                        "name": "DataHubIngestionSourceSchedule",
-                        "namespace": "com.linkedin.pegasus2avro.ingestion",
+                        "name": "StructuredExecutionReport",
+                        "namespace": "com.linkedin.pegasus2avro.execution",
                         "type": "record"
                     }
                 ]
             },
             {
-                "doc": "Parameters associated with the Ingestion Source",
-                "name": "config",
-                "type": {
-                    "fields": [
-                        {
-                            "doc": "The JSON recipe to use for ingestion",
-                            "name": "recipe",
-                            "type": "string"
-                        },
-                        {
-                            "default": null,
-                            "doc": "The PyPI version of the datahub CLI to use when executing a recipe",
-                            "name": "version",
-                            "type": [
-                                "null",
-                                "string"
-                            ]
-                        },
-                        {
-                            "default": null,
-                            "doc": "The id of the executor to use to execute the ingestion run",
-                            "name": "executorId",
-                            "type": [
-                                "null",
-                                "string"
-                            ]
-                        },
-                        {
-                            "default": null,
-                            "doc": "Whether or not to run this ingestion source in debug mode",
-                            "name": "debugMode",
-                            "type": [
-                                "null",
-                                "boolean"
-                            ]
-                        }
-                    ],
-                    "name": "DataHubIngestionSourceConfig",
-                    "namespace": "com.linkedin.pegasus2avro.ingestion",
-                    "type": "record"
-                }
+                "Searchable": {
+                    "fieldName": "startTimeMs",
+                    "fieldType": "COUNT",
+                    "queryByDefault": false
+                },
+                "default": null,
+                "doc": "Time at which the request was created",
+                "name": "startTimeMs",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Duration in milliseconds",
+                "name": "durationMs",
+                "type": [
+                    "null",
+                    "long"
+                ]
             }
         ],
-        "name": "DataHubIngestionSourceInfo",
-        "namespace": "com.linkedin.pegasus2avro.ingestion",
+        "name": "ExecutionRequestResult",
+        "namespace": "com.linkedin.pegasus2avro.execution",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "notebookInfo"
+            "name": "dataHubExecutionRequestInput"
         },
-        "doc": "Information about a Notebook\nNote: This is IN BETA version",
+        "doc": "An request to execution some remote logic or action.\nTODO: Determine who is responsible for emitting execution request success or failure. Executor?",
         "fields": [
             {
-                "Searchable": {
-                    "/*": {
-                        "queryByDefault": true
-                    }
-                },
-                "default": {},
-                "doc": "Custom property bag.",
-                "name": "customProperties",
+                "doc": "The name of the task to execute, for example RUN_INGEST",
+                "name": "task",
+                "type": "string"
+            },
+            {
+                "doc": "Arguments provided to the task",
+                "name": "args",
                 "type": {
                     "type": "map",
                     "values": "string"
                 }
             },
             {
-                "default": null,
-                "doc": "URL where the reference exist",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                },
-                "name": "externalUrl",
-                "type": [
-                    "null",
-                    "string"
-                ]
+                "doc": "Advanced: specify a specific executor to route the request to. If none is provided, a \"default\" executor is used.",
+                "name": "executorId",
+                "type": "string"
             },
             {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "Title of the Notebook",
-                "name": "title",
-                "type": "string"
+                "doc": "Source which created the execution request",
+                "name": "source",
+                "type": {
+                    "fields": [
+                        {
+                            "doc": "The type of the execution request source, e.g. INGESTION_SOURCE",
+                            "name": "type",
+                            "type": "string"
+                        },
+                        {
+                            "Relationship": {
+                                "entityTypes": [
+                                    "dataHubIngestionSource"
+                                ],
+                                "name": "ingestionSource"
+                            },
+                            "Searchable": {
+                                "fieldName": "ingestionSource",
+                                "fieldType": "KEYWORD",
+                                "queryByDefault": false
+                            },
+                            "Urn": "Urn",
+                            "default": null,
+                            "doc": "The urn of the ingestion source associated with the ingestion request. Present if type is INGESTION_SOURCE",
+                            "java": {
+                                "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                            },
+                            "name": "ingestionSource",
+                            "type": [
+                                "null",
+                                "string"
+                            ]
+                        }
+                    ],
+                    "name": "ExecutionRequestSource",
+                    "namespace": "com.linkedin.pegasus2avro.execution",
+                    "type": "record"
+                }
             },
             {
                 "Searchable": {
-                    "fieldType": "TEXT",
-                    "hasValuesFieldName": "hasDescription"
+                    "fieldName": "requestTimeMs",
+                    "fieldType": "COUNT",
+                    "queryByDefault": false
                 },
-                "default": null,
-                "doc": "Detailed description about the Notebook",
-                "name": "description",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "doc": "Captures information about who created/last modified/deleted this Notebook and when",
-                "name": "changeAuditStamps",
-                "type": "com.linkedin.pegasus2avro.common.ChangeAuditStamps"
+                "doc": "Time at which the execution request input was created",
+                "name": "requestedAt",
+                "type": "long"
             }
         ],
-        "name": "NotebookInfo",
-        "namespace": "com.linkedin.pegasus2avro.notebook",
+        "name": "ExecutionRequestInput",
+        "namespace": "com.linkedin.pegasus2avro.execution",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "editableNotebookProperties"
+            "name": "dataHubExecutionRequestSignal"
         },
-        "doc": "Stores editable changes made to properties. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines\nNote: This is IN BETA version",
+        "doc": "An signal sent to a running execution request",
         "fields": [
             {
-                "default": {
-                    "actor": "urn:li:corpuser:unknown",
-                    "impersonator": null,
-                    "message": null,
-                    "time": 0
-                },
-                "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
-                "name": "created",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
-            },
-            {
-                "default": {
-                    "actor": "urn:li:corpuser:unknown",
-                    "impersonator": null,
-                    "message": null,
-                    "time": 0
-                },
-                "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
-                "name": "lastModified",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+                "doc": "The signal to issue, e.g. KILL",
+                "name": "signal",
+                "type": "string"
             },
             {
                 "default": null,
-                "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
-                "name": "deleted",
+                "doc": "Advanced: specify a specific executor to route the request to. If none is provided, a \"default\" executor is used.",
+                "name": "executorId",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.common.AuditStamp"
+                    "string"
                 ]
             },
             {
-                "Searchable": {
-                    "fieldName": "editedDescription",
-                    "fieldType": "TEXT"
-                },
-                "default": null,
-                "doc": "Edited documentation of the Notebook",
-                "name": "description",
-                "type": [
-                    "null",
-                    "string"
-                ]
+                "doc": "Audit Stamp",
+                "name": "createdAt",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
             }
         ],
-        "name": "EditableNotebookProperties",
-        "namespace": "com.linkedin.pegasus2avro.notebook",
+        "name": "ExecutionRequestSignal",
+        "namespace": "com.linkedin.pegasus2avro.execution",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "notebookContent"
+            "name": "chartQuery"
         },
-        "doc": "Content in a Notebook\nNote: This is IN BETA version",
+        "doc": "Information for chart query which is used for getting data of the chart",
         "fields": [
             {
-                "default": [],
-                "doc": "The content of a Notebook which is composed by a list of NotebookCell",
-                "name": "cells",
+                "doc": "Raw query to build a chart from input datasets",
+                "name": "rawQuery",
+                "type": "string"
+            },
+            {
+                "Searchable": {
+                    "addToFilters": true,
+                    "fieldName": "queryType",
+                    "fieldType": "KEYWORD",
+                    "filterNameOverride": "Query Type"
+                },
+                "doc": "Chart query type",
+                "name": "type",
                 "type": {
-                    "items": {
-                        "doc": "A record of all supported cells for a Notebook. Only one type of cell will be non-null.",
-                        "fields": [
-                            {
-                                "default": null,
-                                "doc": "The text cell content. The will be non-null only when all other cell field is null.",
-                                "name": "textCell",
-                                "type": [
-                                    "null",
-                                    {
-                                        "doc": "Text cell in a Notebook, which will present content in text format",
-                                        "fields": [
-                                            {
-                                                "default": null,
-                                                "doc": "Title of the cell",
-                                                "name": "cellTitle",
-                                                "type": [
-                                                    "null",
-                                                    "string"
-                                                ]
-                                            },
-                                            {
-                                                "doc": "Unique id for the cell. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773/?cellId=1234'",
-                                                "name": "cellId",
-                                                "type": "string"
-                                            },
-                                            {
-                                                "doc": "Captures information about who created/last modified/deleted this Notebook cell and when",
-                                                "name": "changeAuditStamps",
-                                                "type": "com.linkedin.pegasus2avro.common.ChangeAuditStamps"
-                                            },
-                                            {
-                                                "doc": "The actual text in a TextCell in a Notebook",
-                                                "name": "text",
-                                                "type": "string"
-                                            }
-                                        ],
-                                        "name": "TextCell",
-                                        "namespace": "com.linkedin.pegasus2avro.notebook",
-                                        "type": "record"
-                                    }
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "The query cell content. The will be non-null only when all other cell field is null.",
-                                "name": "queryCell",
-                                "type": [
-                                    "null",
-                                    {
-                                        "doc": "Query cell in a Notebook, which will present content in query format",
-                                        "fields": [
-                                            {
-                                                "default": null,
-                                                "doc": "Title of the cell",
-                                                "name": "cellTitle",
-                                                "type": [
-                                                    "null",
-                                                    "string"
-                                                ]
-                                            },
-                                            {
-                                                "doc": "Unique id for the cell. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773/?cellId=1234'",
-                                                "name": "cellId",
-                                                "type": "string"
-                                            },
-                                            {
-                                                "doc": "Captures information about who created/last modified/deleted this Notebook cell and when",
-                                                "name": "changeAuditStamps",
-                                                "type": "com.linkedin.pegasus2avro.common.ChangeAuditStamps"
-                                            },
-                                            {
-                                                "doc": "Raw query to explain some specific logic in a Notebook",
-                                                "name": "rawQuery",
-                                                "type": "string"
-                                            },
-                                            {
-                                                "default": null,
-                                                "doc": "Captures information about who last executed this query cell and when",
-                                                "name": "lastExecuted",
-                                                "type": [
-                                                    "null",
-                                                    "com.linkedin.pegasus2avro.common.AuditStamp"
-                                                ]
-                                            }
-                                        ],
-                                        "name": "QueryCell",
-                                        "namespace": "com.linkedin.pegasus2avro.notebook",
-                                        "type": "record"
-                                    }
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "The chart cell content. The will be non-null only when all other cell field is null.",
-                                "name": "chartCell",
-                                "type": [
-                                    "null",
-                                    {
-                                        "doc": "Chart cell in a notebook, which will present content in chart format",
-                                        "fields": [
-                                            {
-                                                "default": null,
-                                                "doc": "Title of the cell",
-                                                "name": "cellTitle",
-                                                "type": [
-                                                    "null",
-                                                    "string"
-                                                ]
-                                            },
-                                            {
-                                                "doc": "Unique id for the cell. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773/?cellId=1234'",
-                                                "name": "cellId",
-                                                "type": "string"
-                                            },
-                                            {
-                                                "doc": "Captures information about who created/last modified/deleted this Notebook cell and when",
-                                                "name": "changeAuditStamps",
-                                                "type": "com.linkedin.pegasus2avro.common.ChangeAuditStamps"
-                                            }
-                                        ],
-                                        "name": "ChartCell",
-                                        "namespace": "com.linkedin.pegasus2avro.notebook",
-                                        "type": "record"
-                                    }
-                                ]
-                            },
-                            {
-                                "doc": "The type of this Notebook cell",
-                                "name": "type",
-                                "type": {
-                                    "doc": "Type of Notebook Cell",
-                                    "name": "NotebookCellType",
-                                    "namespace": "com.linkedin.pegasus2avro.notebook",
-                                    "symbolDocs": {
-                                        "CHART_CELL": "CHART Notebook cell type. The cell content is chart only.",
-                                        "QUERY_CELL": "QUERY Notebook cell type. The cell context is query only.",
-                                        "TEXT_CELL": "TEXT Notebook cell type. The cell context is text only."
-                                    },
-                                    "symbols": [
-                                        "TEXT_CELL",
-                                        "QUERY_CELL",
-                                        "CHART_CELL"
-                                    ],
-                                    "type": "enum"
-                                }
-                            }
-                        ],
-                        "name": "NotebookCell",
-                        "namespace": "com.linkedin.pegasus2avro.notebook",
-                        "type": "record"
+                    "name": "ChartQueryType",
+                    "namespace": "com.linkedin.pegasus2avro.chart",
+                    "symbolDocs": {
+                        "LOOKML": "LookML queries",
+                        "SQL": "SQL type queries"
                     },
-                    "type": "array"
+                    "symbols": [
+                        "LOOKML",
+                        "SQL"
+                    ],
+                    "type": "enum"
                 }
             }
         ],
-        "name": "NotebookContent",
-        "namespace": "com.linkedin.pegasus2avro.notebook",
+        "name": "ChartQuery",
+        "namespace": "com.linkedin.pegasus2avro.chart",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "editableDashboardProperties"
+            "name": "editableChartProperties"
         },
         "doc": "Stores editable changes made to properties. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines",
         "fields": [
             {
                 "default": {
                     "actor": "urn:li:corpuser:unknown",
                     "impersonator": null,
@@ -7871,32 +4555,32 @@
             },
             {
                 "Searchable": {
                     "fieldName": "editedDescription",
                     "fieldType": "TEXT"
                 },
                 "default": null,
-                "doc": "Edited documentation of the dashboard",
+                "doc": "Edited documentation of the chart ",
                 "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             }
         ],
-        "name": "EditableDashboardProperties",
-        "namespace": "com.linkedin.pegasus2avro.dashboard",
+        "name": "EditableChartProperties",
+        "namespace": "com.linkedin.pegasus2avro.chart",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dashboardUsageStatistics",
+            "name": "chartUsageStatistics",
             "type": "timeseries"
         },
-        "doc": "Experimental (Subject to breaking change) -- Stats corresponding to dashboard's usage.\n\nIf this aspect represents the latest snapshot of the statistics about a Dashboard, the eventGranularity field should be null. \nIf this aspect represents a bucketed window of usage statistics (e.g. over a day), then the eventGranularity field should be set accordingly. ",
+        "doc": "Experimental (Subject to breaking change) -- Stats corresponding to chart's usage.\n\nIf this aspect represents the latest snapshot of the statistics about a Chart, the eventGranularity field should be null.\nIf this aspect represents a bucketed window of usage statistics (e.g. over a day), then the eventGranularity field should be set accordingly.",
         "fields": [
             {
                 "doc": "The event timestamp field as epoch at UTC in milli seconds.",
                 "name": "timestampMillis",
                 "type": "long"
             },
             {
@@ -7929,34 +4613,24 @@
                     "null",
                     "string"
                 ]
             },
             {
                 "TimeseriesField": {},
                 "default": null,
-                "doc": "The total number of times dashboard has been viewed",
+                "doc": "The total number of times chart has been viewed",
                 "name": "viewsCount",
                 "type": [
                     "null",
                     "int"
                 ]
             },
             {
                 "TimeseriesField": {},
                 "default": null,
-                "doc": "The total number of dashboard executions (refreshes / syncs) ",
-                "name": "executionsCount",
-                "type": [
-                    "null",
-                    "int"
-                ]
-            },
-            {
-                "TimeseriesField": {},
-                "default": null,
                 "doc": "Unique user count",
                 "name": "uniqueUserCount",
                 "type": [
                     "null",
                     "int"
                 ]
             },
@@ -7970,100 +4644,51 @@
                 "type": [
                     "null",
                     {
                         "items": {
                             "doc": "Records a single user's usage counts for a given resource",
                             "fields": [
                                 {
+                                    "Urn": "Urn",
                                     "doc": "The unique id of the user.",
                                     "java": {
                                         "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                     },
                                     "name": "user",
                                     "type": "string"
                                 },
                                 {
                                     "TimeseriesField": {},
                                     "default": null,
-                                    "doc": "The number of times the user has viewed the dashboard",
+                                    "doc": "The number of times the user has viewed the chart",
                                     "name": "viewsCount",
                                     "type": [
                                         "null",
                                         "int"
                                     ]
-                                },
-                                {
-                                    "TimeseriesField": {},
-                                    "default": null,
-                                    "doc": "The number of times the user has executed (refreshed) the dashboard",
-                                    "name": "executionsCount",
-                                    "type": [
-                                        "null",
-                                        "int"
-                                    ]
-                                },
-                                {
-                                    "TimeseriesField": {},
-                                    "default": null,
-                                    "doc": "Normalized numeric metric representing user's dashboard usage -- the number of times the user executed or viewed the dashboard. ",
-                                    "name": "usageCount",
-                                    "type": [
-                                        "null",
-                                        "int"
-                                    ]
-                                },
-                                {
-                                    "TimeseriesField": {},
-                                    "default": null,
-                                    "doc": "If user_email is set, we attempt to resolve the user's urn upon ingest",
-                                    "name": "userEmail",
-                                    "type": [
-                                        "null",
-                                        "string"
-                                    ]
                                 }
                             ],
-                            "name": "DashboardUserUsageCounts",
-                            "namespace": "com.linkedin.pegasus2avro.dashboard",
+                            "name": "ChartUserUsageCounts",
+                            "namespace": "com.linkedin.pegasus2avro.chart",
                             "type": "record"
                         },
                         "type": "array"
                     }
                 ]
-            },
-            {
-                "TimeseriesField": {},
-                "default": null,
-                "doc": "The total number of times that the dashboard has been favorited ",
-                "name": "favoritesCount",
-                "type": [
-                    "null",
-                    "int"
-                ]
-            },
-            {
-                "TimeseriesField": {},
-                "default": null,
-                "doc": "Last viewed at\n\nThis should not be set in cases where statistics are windowed. ",
-                "name": "lastViewedAt",
-                "type": [
-                    "null",
-                    "long"
-                ]
             }
         ],
-        "name": "DashboardUsageStatistics",
-        "namespace": "com.linkedin.pegasus2avro.dashboard",
+        "name": "ChartUsageStatistics",
+        "namespace": "com.linkedin.pegasus2avro.chart",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dashboardInfo"
+            "name": "chartInfo"
         },
-        "doc": "Information about a dashboard",
+        "doc": "Information about a chart",
         "fields": [
             {
                 "Searchable": {
                     "/*": {
                         "queryByDefault": true
                     }
                 },
@@ -8086,1133 +4711,1953 @@
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "Searchable": {
-                    "boostScore": 10.0,
                     "enableAutocomplete": true,
                     "fieldType": "TEXT_PARTIAL"
                 },
-                "doc": "Title of the dashboard",
+                "doc": "Title of the chart",
                 "name": "title",
                 "type": "string"
             },
             {
-                "Searchable": {
-                    "fieldType": "TEXT",
-                    "hasValuesFieldName": "hasDescription"
-                },
-                "doc": "Detailed description about the dashboard",
+                "Searchable": {},
+                "doc": "Detailed description about the chart",
                 "name": "description",
                 "type": "string"
             },
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "chart"
-                        ],
-                        "isLineage": true,
-                        "name": "Contains"
-                    }
-                },
-                "default": [],
-                "deprecated": true,
-                "doc": "Charts in a dashboard\nDeprecated! Use chartEdges instead.",
-                "name": "charts",
+                "doc": "Captures information about who created/last modified/deleted this chart and when",
+                "name": "lastModified",
                 "type": {
-                    "items": "string",
-                    "type": "array"
+                    "doc": "Data captured on a resource/association/sub-resource level giving insight into when that resource/association/sub-resource moved into various lifecycle stages, and who acted to move it into those lifecycle stages. The recommended best practice is to include this record in your record schema, and annotate its fields as @readOnly in your resource. See https://github.com/linkedin/rest.li/wiki/Validation-in-Rest.li#restli-validation-annotations",
+                    "fields": [
+                        {
+                            "default": {
+                                "actor": "urn:li:corpuser:unknown",
+                                "impersonator": null,
+                                "message": null,
+                                "time": 0
+                            },
+                            "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
+                            "name": "created",
+                            "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+                        },
+                        {
+                            "default": {
+                                "actor": "urn:li:corpuser:unknown",
+                                "impersonator": null,
+                                "message": null,
+                                "time": 0
+                            },
+                            "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
+                            "name": "lastModified",
+                            "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+                        },
+                        {
+                            "default": null,
+                            "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
+                            "name": "deleted",
+                            "type": [
+                                "null",
+                                "com.linkedin.pegasus2avro.common.AuditStamp"
+                            ]
+                        }
+                    ],
+                    "name": "ChangeAuditStamps",
+                    "namespace": "com.linkedin.pegasus2avro.common",
+                    "type": "record"
                 }
             },
             {
-                "Relationship": {
-                    "/*/destinationUrn": {
-                        "createdActor": "chartEdges/*/created/actor",
-                        "createdOn": "chartEdges/*/created/time",
-                        "entityTypes": [
-                            "chart"
-                        ],
-                        "isLineage": true,
-                        "name": "Contains",
-                        "properties": "chartEdges/*/properties",
-                        "updatedActor": "chartEdges/*/lastModified/actor",
-                        "updatedOn": "chartEdges/*/lastModified/time"
-                    }
-                },
                 "default": null,
-                "doc": "Charts in a dashboard",
-                "name": "chartEdges",
+                "doc": "URL for the chart. This could be used as an external link on DataHub to allow users access/view the chart",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "chartUrl",
                 "type": [
                     "null",
-                    {
-                        "items": "com.linkedin.pegasus2avro.common.Edge",
-                        "type": "array"
-                    }
+                    "string"
                 ]
             },
             {
                 "Relationship": {
-                    "/*": {
+                    "/*/string": {
                         "entityTypes": [
                             "dataset"
                         ],
                         "isLineage": true,
                         "name": "Consumes"
                     }
                 },
-                "default": [],
+                "default": null,
                 "deprecated": true,
-                "doc": "Datasets consumed by a dashboard\nDeprecated! Use datasetEdges instead.",
-                "name": "datasets",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                }
+                "doc": "Data sources for the chart\nDeprecated! Use inputEdges instead.",
+                "name": "inputs",
+                "type": [
+                    "null",
+                    {
+                        "items": [
+                            "string"
+                        ],
+                        "type": "array"
+                    }
+                ]
             },
             {
                 "Relationship": {
                     "/*/destinationUrn": {
-                        "createdActor": "datasetEdges/*/created/actor",
-                        "createdOn": "datasetEdges/*/created/time",
+                        "createdActor": "inputEdges/*/created/actor",
+                        "createdOn": "inputEdges/*/created/time",
                         "entityTypes": [
                             "dataset"
                         ],
                         "isLineage": true,
                         "name": "Consumes",
-                        "properties": "datasetEdges/*/properties",
-                        "updatedActor": "datasetEdges/*/lastModified/actor",
-                        "updatedOn": "datasetEdges/*/lastModified/time"
+                        "properties": "inputEdges/*/properties",
+                        "updatedActor": "inputEdges/*/lastModified/actor",
+                        "updatedOn": "inputEdges/*/lastModified/time"
                     }
                 },
                 "default": null,
-                "doc": "Datasets consumed by a dashboard",
-                "name": "datasetEdges",
+                "doc": "Data sources for the chart",
+                "name": "inputEdges",
                 "type": [
                     "null",
                     {
                         "items": "com.linkedin.pegasus2avro.common.Edge",
                         "type": "array"
                     }
                 ]
             },
             {
-                "doc": "Captures information about who created/last modified/deleted this dashboard and when",
-                "name": "lastModified",
-                "type": "com.linkedin.pegasus2avro.common.ChangeAuditStamps"
-            },
-            {
-                "default": null,
-                "doc": "URL for the dashboard. This could be used as an external link on DataHub to allow users access/view the dashboard",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                "Searchable": {
+                    "addToFilters": true,
+                    "fieldType": "KEYWORD",
+                    "filterNameOverride": "Chart Type"
                 },
-                "name": "dashboardUrl",
+                "default": null,
+                "doc": "Type of the chart",
+                "name": "type",
                 "type": [
                     "null",
-                    "string"
+                    {
+                        "doc": "The various types of charts",
+                        "name": "ChartType",
+                        "namespace": "com.linkedin.pegasus2avro.chart",
+                        "symbolDocs": {
+                            "BAR": "Chart showing a Bar chart",
+                            "PIE": "Chart showing a Pie chart",
+                            "SCATTER": "Chart showing a Scatter plot",
+                            "TABLE": "Chart showing a table",
+                            "TEXT": "Chart showing Markdown formatted text"
+                        },
+                        "symbols": [
+                            "BAR",
+                            "PIE",
+                            "SCATTER",
+                            "TABLE",
+                            "TEXT",
+                            "LINE",
+                            "AREA",
+                            "HISTOGRAM",
+                            "BOX_PLOT",
+                            "WORD_CLOUD",
+                            "COHORT"
+                        ],
+                        "type": "enum"
+                    }
                 ]
             },
             {
                 "Searchable": {
                     "addToFilters": true,
                     "fieldType": "KEYWORD",
                     "filterNameOverride": "Access Level"
                 },
                 "default": null,
-                "doc": "Access level for the dashboard",
+                "doc": "Access level for the chart",
                 "name": "access",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.common.AccessLevel"
+                    {
+                        "doc": "The various access levels",
+                        "name": "AccessLevel",
+                        "namespace": "com.linkedin.pegasus2avro.common",
+                        "symbolDocs": {
+                            "PRIVATE": "Private availability to certain set of users",
+                            "PUBLIC": "Publicly available access level"
+                        },
+                        "symbols": [
+                            "PUBLIC",
+                            "PRIVATE"
+                        ],
+                        "type": "enum"
+                    }
                 ]
             },
             {
                 "default": null,
-                "doc": "The time when this dashboard last refreshed",
+                "doc": "The time when this chart last refreshed",
                 "name": "lastRefreshed",
                 "type": [
                     "null",
                     "long"
                 ]
             }
         ],
-        "name": "DashboardInfo",
-        "namespace": "com.linkedin.pegasus2avro.dashboard",
+        "name": "ChartInfo",
+        "namespace": "com.linkedin.pegasus2avro.chart",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "roleMembership"
+            "name": "editableContainerProperties"
         },
-        "doc": "Carries information about which roles a user is assigned to.",
+        "doc": "Editable information about an Asset Container as defined on the DataHub Platform",
         "fields": [
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "dataHubRole"
-                        ],
-                        "name": "IsMemberOfRole"
-                    }
+                "Searchable": {
+                    "fieldName": "editedDescription",
+                    "fieldType": "TEXT"
                 },
-                "name": "roles",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                }
+                "default": null,
+                "doc": "Description of the Asset Container as its received on the DataHub Platform",
+                "name": "description",
+                "type": [
+                    "null",
+                    "string"
+                ]
             }
         ],
-        "name": "RoleMembership",
-        "namespace": "com.linkedin.pegasus2avro.identity",
+        "name": "EditableContainerProperties",
+        "namespace": "com.linkedin.pegasus2avro.container",
         "type": "record"
     },
     {
         "Aspect": {
-            "EntityUrns": [
-                "com.linkedin.pegasus2avro.common.CorpGroupUrn"
-            ],
-            "name": "corpGroupInfo"
+            "name": "containerProperties"
         },
-        "doc": "Information about a Corp Group ingested from a third party source",
+        "doc": "Information about a Asset Container as received from a 3rd party source system",
         "fields": [
             {
                 "Searchable": {
-                    "boostScore": 10.0,
-                    "fieldType": "TEXT_PARTIAL",
-                    "queryByDefault": true
+                    "/*": {
+                        "queryByDefault": true
+                    }
                 },
-                "default": null,
-                "doc": "The name of the group.",
-                "name": "displayName",
-                "type": [
-                    "null",
-                    "string"
-                ]
+                "default": {},
+                "doc": "Custom property bag.",
+                "name": "customProperties",
+                "type": {
+                    "type": "map",
+                    "values": "string"
+                }
             },
             {
                 "default": null,
-                "doc": "email of this group",
-                "name": "email",
+                "doc": "URL where the reference exist",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "externalUrl",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "corpuser"
-                        ],
-                        "name": "OwnedBy"
-                    }
-                },
-                "deprecated": true,
-                "doc": "owners of this group\nDeprecated! Replaced by Ownership aspect.",
-                "name": "admins",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                }
-            },
-            {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "corpuser"
-                        ],
-                        "name": "IsPartOf"
-                    }
+                "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL"
                 },
-                "deprecated": true,
-                "doc": "List of ldap urn in this group.\nDeprecated! Replaced by GroupMembership aspect.",
-                "name": "members",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                }
+                "doc": "Display name of the Asset Container",
+                "name": "name",
+                "type": "string"
             },
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "corpGroup"
-                        ],
-                        "name": "IsPartOf"
-                    }
+                "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL"
                 },
-                "deprecated": true,
-                "doc": "List of groups in this group.\nDeprecated! This field is unused.",
-                "name": "groups",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                }
+                "default": null,
+                "doc": "Fully-qualified name of the Container",
+                "name": "qualifiedName",
+                "type": [
+                    "null",
+                    "string"
+                ]
             },
             {
                 "Searchable": {
-                    "fieldType": "TEXT_PARTIAL"
+                    "fieldType": "TEXT",
+                    "hasValuesFieldName": "hasDescription"
                 },
                 "default": null,
-                "doc": "A description of the group.",
+                "doc": "Description of the Asset Container as it exists inside a source system",
                 "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
+                "Searchable": {
+                    "/time": {
+                        "fieldName": "createdAt",
+                        "fieldType": "DATETIME"
+                    }
+                },
                 "default": null,
-                "doc": "Slack channel for the group",
-                "name": "slack",
+                "doc": "A timestamp documenting when the asset was created in the source Data Platform (not on DataHub)",
+                "name": "created",
                 "type": [
                     "null",
-                    "string"
+                    "com.linkedin.pegasus2avro.common.TimeStamp"
                 ]
             },
             {
                 "Searchable": {
                     "/time": {
-                        "fieldName": "createdTime",
+                        "fieldName": "lastModifiedAt",
                         "fieldType": "DATETIME"
                     }
                 },
                 "default": null,
-                "doc": "Created Audit stamp",
-                "name": "created",
+                "doc": "A timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)",
+                "name": "lastModified",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.common.AuditStamp"
+                    "com.linkedin.pegasus2avro.common.TimeStamp"
                 ]
             }
         ],
-        "name": "CorpGroupInfo",
-        "namespace": "com.linkedin.pegasus2avro.identity",
+        "name": "ContainerProperties",
+        "namespace": "com.linkedin.pegasus2avro.container",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "nativeGroupMembership"
+            "name": "container"
         },
-        "doc": "Carries information about the native CorpGroups a user is in.",
+        "doc": "Link from an asset to its parent container",
         "fields": [
             {
                 "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "corpGroup"
-                        ],
-                        "name": "IsMemberOfNativeGroup"
-                    }
+                    "entityTypes": [
+                        "container"
+                    ],
+                    "name": "IsPartOf"
                 },
-                "name": "nativeGroups",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                }
+                "Searchable": {
+                    "addToFilters": true,
+                    "fieldName": "container",
+                    "fieldType": "URN",
+                    "filterNameOverride": "Container",
+                    "hasValuesFieldName": "hasContainer"
+                },
+                "Urn": "Urn",
+                "doc": "The parent container of an asset",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                },
+                "name": "container",
+                "type": "string"
             }
         ],
-        "name": "NativeGroupMembership",
-        "namespace": "com.linkedin.pegasus2avro.identity",
+        "name": "Container",
+        "namespace": "com.linkedin.pegasus2avro.container",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "corpUserSettings"
+            "name": "glossaryNodeInfo"
         },
-        "doc": "Settings that a user can customize through the datahub ui",
+        "doc": "Properties associated with a GlossaryNode",
         "fields": [
             {
-                "doc": "Settings for a user around the appearance of their DataHub U",
-                "name": "appearance",
-                "type": {
-                    "doc": "Settings for a user around the appearance of their DataHub UI",
-                    "fields": [
-                        {
-                            "default": null,
-                            "doc": "Flag whether the user should see a homepage with only datasets, charts and dashboards. Intended for users\nwho have less operational use cases for the datahub tool.",
-                            "name": "showSimplifiedHomepage",
-                            "type": [
-                                "null",
-                                "boolean"
-                            ]
-                        }
-                    ],
-                    "name": "CorpUserAppearanceSettings",
-                    "namespace": "com.linkedin.pegasus2avro.identity",
-                    "type": "record"
-                }
+                "Searchable": {},
+                "doc": "Definition of business node",
+                "name": "definition",
+                "type": "string"
             },
             {
+                "Relationship": {
+                    "entityTypes": [
+                        "glossaryNode"
+                    ],
+                    "name": "IsPartOf"
+                },
+                "Searchable": {
+                    "fieldName": "parentNode",
+                    "fieldType": "URN",
+                    "hasValuesFieldName": "hasParentNode"
+                },
+                "Urn": "GlossaryNodeUrn",
                 "default": null,
-                "doc": "User preferences for the Views feature.",
-                "name": "views",
+                "doc": "Parent node of the glossary term",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.GlossaryNodeUrn"
+                },
+                "name": "parentNode",
                 "type": [
                     "null",
-                    {
-                        "doc": "Settings related to the 'Views' feature.",
-                        "fields": [
-                            {
-                                "default": null,
-                                "doc": "The default View which is selected for the user.\nIf none is chosen, then this value will be left blank.",
-                                "java": {
-                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                },
-                                "name": "defaultView",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            }
-                        ],
-                        "name": "CorpUserViewsSettings",
-                        "namespace": "com.linkedin.pegasus2avro.identity",
-                        "type": "record"
-                    }
+                    "string"
                 ]
-            }
-        ],
-        "name": "CorpUserSettings",
-        "namespace": "com.linkedin.pegasus2avro.identity",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "corpUserStatus"
-        },
-        "doc": "The status of the user, e.g. provisioned, active, suspended, etc.",
-        "fields": [
+            },
             {
                 "Searchable": {
-                    "fieldType": "KEYWORD"
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldName": "displayName",
+                    "fieldType": "TEXT_PARTIAL"
                 },
-                "doc": "Status of the user, e.g. PROVISIONED / ACTIVE / SUSPENDED",
-                "name": "status",
-                "type": "string"
+                "default": null,
+                "doc": "Display name of the node",
+                "name": "name",
+                "type": [
+                    "null",
+                    "string"
+                ]
             },
             {
-                "doc": "Audit stamp containing who last modified the status and when.",
-                "name": "lastModified",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+                "Searchable": {
+                    "fieldType": "TEXT_PARTIAL"
+                },
+                "default": null,
+                "doc": "Optional id for the GlossaryNode",
+                "name": "id",
+                "type": [
+                    "null",
+                    "string"
+                ]
             }
         ],
-        "name": "CorpUserStatus",
-        "namespace": "com.linkedin.pegasus2avro.identity",
+        "name": "GlossaryNodeInfo",
+        "namespace": "com.linkedin.pegasus2avro.glossary",
         "type": "record"
     },
     {
         "Aspect": {
-            "EntityUrns": [
-                "com.linkedin.pegasus2avro.common.CorpuserUrn"
-            ],
-            "name": "corpUserEditableInfo"
+            "name": "glossaryTermInfo"
         },
-        "doc": "Linkedin corp user information that can be edited from UI",
+        "doc": "Properties associated with a GlossaryTerm",
         "fields": [
             {
-                "default": null,
-                "doc": "About me section of the user",
-                "name": "aboutMe",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "Searchable": {
-                    "/*": {
-                        "fieldType": "TEXT"
-                    }
-                },
-                "default": [],
-                "doc": "Teams that the user belongs to e.g. Metadata",
-                "name": "teams",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                }
-            },
-            {
                 "Searchable": {
                     "/*": {
-                        "fieldType": "TEXT"
+                        "queryByDefault": true
                     }
                 },
-                "default": [],
-                "doc": "Skills that the user possesses e.g. Machine Learning",
-                "name": "skills",
+                "default": {},
+                "doc": "Custom property bag.",
+                "name": "customProperties",
                 "type": {
-                    "items": "string",
-                    "type": "array"
+                    "type": "map",
+                    "values": "string"
                 }
             },
             {
-                "default": "https://raw.githubusercontent.com/datahub-project/datahub/master/datahub-web-react/src/images/default_avatar.png",
-                "doc": "A URL which points to a picture which user wants to set as a profile photo",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                },
-                "name": "pictureLink",
-                "type": "string"
-            },
-            {
                 "Searchable": {
-                    "boostScore": 10.0,
-                    "fieldType": "TEXT_PARTIAL",
-                    "queryByDefault": true
+                    "fieldType": "TEXT_PARTIAL"
                 },
                 "default": null,
-                "doc": "DataHub-native display name",
-                "name": "displayName",
+                "doc": "Optional id for the term",
+                "name": "id",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
+                "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL"
+                },
                 "default": null,
-                "doc": "DataHub-native Title, e.g. 'Software Engineer'",
-                "name": "title",
+                "doc": "Display name of the term",
+                "name": "name",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
+                "Searchable": {},
+                "doc": "Definition of business term.",
+                "name": "definition",
+                "type": "string"
+            },
+            {
+                "Relationship": {
+                    "entityTypes": [
+                        "glossaryNode"
+                    ],
+                    "name": "IsPartOf"
+                },
+                "Searchable": {
+                    "fieldName": "parentNode",
+                    "fieldType": "URN",
+                    "hasValuesFieldName": "hasParentNode"
+                },
+                "Urn": "GlossaryNodeUrn",
                 "default": null,
-                "doc": "Slack handle for the user",
-                "name": "slack",
+                "doc": "Parent node of the glossary term",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.GlossaryNodeUrn"
+                },
+                "name": "parentNode",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
+                "Searchable": {
+                    "fieldType": "KEYWORD"
+                },
+                "doc": "Source of the Business Term (INTERNAL or EXTERNAL) with default value as INTERNAL",
+                "name": "termSource",
+                "type": "string"
+            },
+            {
+                "Searchable": {
+                    "fieldType": "KEYWORD"
+                },
                 "default": null,
-                "doc": "Phone number to contact the user",
-                "name": "phone",
+                "doc": "External Reference to the business-term",
+                "name": "sourceRef",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "Email address to contact the user",
-                "name": "email",
+                "doc": "The abstracted URL such as https://spec.edmcouncil.org/fibo/ontology/FBC/FinancialInstruments/FinancialInstruments/CashInstrument.",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "sourceUrl",
                 "type": [
                     "null",
                     "string"
                 ]
-            }
-        ],
-        "name": "CorpUserEditableInfo",
-        "namespace": "com.linkedin.pegasus2avro.identity",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "inviteToken"
-        },
-        "doc": "Aspect used to store invite tokens.",
-        "fields": [
-            {
-                "doc": "The encrypted invite token.",
-                "name": "token",
-                "type": "string"
             },
             {
-                "Searchable": {
-                    "fieldName": "role",
-                    "fieldType": "KEYWORD",
-                    "hasValuesFieldName": "hasRole"
-                },
                 "default": null,
-                "doc": "The role that this invite token may be associated with",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "role",
+                "deprecated": true,
+                "doc": "Schema definition of the glossary term",
+                "name": "rawSchema",
                 "type": [
                     "null",
                     "string"
                 ]
             }
         ],
-        "name": "InviteToken",
-        "namespace": "com.linkedin.pegasus2avro.identity",
+        "name": "GlossaryTermInfo",
+        "namespace": "com.linkedin.pegasus2avro.glossary",
         "type": "record"
     },
     {
         "Aspect": {
-            "EntityUrns": [
-                "com.linkedin.pegasus2avro.common.CorpuserUrn"
-            ],
-            "name": "corpUserInfo"
+            "name": "glossaryRelatedTerms"
         },
-        "doc": "Linkedin corp user information",
+        "doc": "Has A / Is A lineage information about a glossary Term reporting the lineage",
         "fields": [
             {
-                "Searchable": {
+                "Relationship": {
                     "/*": {
-                        "queryByDefault": true
+                        "entityTypes": [
+                            "glossaryTerm"
+                        ],
+                        "name": "IsA"
                     }
                 },
-                "default": {},
-                "doc": "Custom property bag.",
-                "name": "customProperties",
-                "type": {
-                    "type": "map",
-                    "values": "string"
-                }
-            },
-            {
                 "Searchable": {
-                    "fieldType": "BOOLEAN",
-                    "weightsPerFieldValue": {
-                        "true": 2.0
+                    "/*": {
+                        "boostScore": 2.0,
+                        "fieldName": "isRelatedTerms",
+                        "fieldType": "URN"
                     }
                 },
-                "doc": "Deprecated! Use CorpUserStatus instead. Whether the corpUser is active, ref: https://iwww.corp.linkedin.com/wiki/cf/display/GTSD/Accessing+Active+Directory+via+LDAP+tools",
-                "name": "active",
-                "type": "boolean"
-            },
-            {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "fieldType": "TEXT_PARTIAL",
-                    "queryByDefault": true
-                },
+                "Urn": "GlossaryTermUrn",
                 "default": null,
-                "doc": "displayName of this user ,  e.g.  Hang Zhang(DataHQ)",
-                "name": "displayName",
+                "doc": "The relationship Is A with glossary term",
+                "name": "isRelatedTerms",
                 "type": [
                     "null",
-                    "string"
-                ]
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ],
+                "urn_is_array": true
             },
             {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "glossaryTerm"
+                        ],
+                        "name": "HasA"
+                    }
+                },
                 "Searchable": {
-                    "fieldType": "KEYWORD",
-                    "queryByDefault": true
+                    "/*": {
+                        "boostScore": 2.0,
+                        "fieldName": "hasRelatedTerms",
+                        "fieldType": "URN"
+                    }
                 },
+                "Urn": "GlossaryTermUrn",
                 "default": null,
-                "doc": "email address of this user",
-                "name": "email",
+                "doc": "The relationship Has A with glossary term",
+                "name": "hasRelatedTerms",
                 "type": [
                     "null",
-                    "string"
-                ]
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ],
+                "urn_is_array": true
             },
             {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "glossaryTerm"
+                        ],
+                        "name": "HasValue"
+                    }
+                },
                 "Searchable": {
-                    "fieldType": "KEYWORD",
-                    "queryByDefault": true
+                    "/*": {
+                        "fieldName": "values",
+                        "fieldType": "URN"
+                    }
                 },
+                "Urn": "GlossaryTermUrn",
                 "default": null,
-                "doc": "title of this user",
-                "name": "title",
+                "doc": "The relationship Has Value with glossary term.\nThese are fixed value a term has. For example a ColorEnum where RED, GREEN and YELLOW are fixed values.",
+                "name": "values",
                 "type": [
                     "null",
-                    "string"
-                ]
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ],
+                "urn_is_array": true
             },
             {
                 "Relationship": {
-                    "entityTypes": [
-                        "corpuser"
-                    ],
-                    "name": "ReportsTo"
+                    "/*": {
+                        "entityTypes": [
+                            "glossaryTerm"
+                        ],
+                        "name": "IsRelatedTo"
+                    }
                 },
                 "Searchable": {
-                    "fieldName": "managerLdap",
-                    "fieldType": "URN",
-                    "queryByDefault": true
+                    "/*": {
+                        "fieldName": "relatedTerms",
+                        "fieldType": "URN"
+                    }
                 },
+                "Urn": "GlossaryTermUrn",
                 "default": null,
-                "doc": "direct manager of this user",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.CorpuserUrn"
-                },
-                "name": "managerUrn",
+                "doc": "The relationship isRelatedTo with glossary term",
+                "name": "relatedTerms",
                 "type": [
                     "null",
-                    "string"
-                ]
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ],
+                "urn_is_array": true
+            }
+        ],
+        "name": "GlossaryRelatedTerms",
+        "namespace": "com.linkedin.pegasus2avro.glossary",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "tagProperties"
+        },
+        "doc": "Properties associated with a Tag",
+        "fields": [
+            {
+                "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL"
+                },
+                "doc": "Display name of the tag",
+                "name": "name",
+                "type": "string"
             },
             {
+                "Searchable": {},
                 "default": null,
-                "doc": "department id this user belong to",
-                "name": "departmentId",
+                "doc": "Documentation of the tag",
+                "name": "description",
                 "type": [
                     "null",
-                    "long"
+                    "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "department name this user belong to",
-                "name": "departmentName",
+                "doc": "The color associated with the Tag in Hex. For example #FFFFFF.",
+                "name": "colorHex",
                 "type": [
                     "null",
                     "string"
                 ]
+            }
+        ],
+        "name": "TagProperties",
+        "namespace": "com.linkedin.pegasus2avro.tag",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "querySubjects"
+        },
+        "doc": "Information about the subjects of a particular Query, i.e. the assets\nbeing queried.",
+        "fields": [
+            {
+                "doc": "One or more subjects of the query.\n\nIn single-asset queries (e.g. table select), this will contain the Table reference\nand optionally schema field references.\n\nIn multi-asset queries (e.g. table joins), this may contain multiple Table references\nand optionally schema field references.",
+                "name": "subjects",
+                "type": {
+                    "items": {
+                        "doc": "A single subject of a particular query.\nIn the future, we may evolve this model to include richer details\nabout the Query Subject in relation to the query.",
+                        "fields": [
+                            {
+                                "Relationship": {
+                                    "entityTypes": [
+                                        "dataset",
+                                        "schemaField"
+                                    ],
+                                    "name": "IsAssociatedWith"
+                                },
+                                "Searchable": {
+                                    "fieldName": "entities",
+                                    "fieldType": "URN"
+                                },
+                                "Urn": "Urn",
+                                "doc": "An entity which is the subject of a query.",
+                                "java": {
+                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                },
+                                "name": "entity",
+                                "type": "string"
+                            }
+                        ],
+                        "name": "QuerySubject",
+                        "namespace": "com.linkedin.pegasus2avro.query",
+                        "type": "record"
+                    },
+                    "type": "array"
+                }
+            }
+        ],
+        "name": "QuerySubjects",
+        "namespace": "com.linkedin.pegasus2avro.query",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "queryProperties"
+        },
+        "doc": "Information about a Query against one or more data assets (e.g. Tables or Views).",
+        "fields": [
+            {
+                "doc": "The Query Statement.",
+                "name": "statement",
+                "type": {
+                    "doc": "A query statement against one or more data assets.",
+                    "fields": [
+                        {
+                            "doc": "The query text",
+                            "name": "value",
+                            "type": "string"
+                        },
+                        {
+                            "default": "SQL",
+                            "doc": "The language of the Query, e.g. SQL.",
+                            "name": "language",
+                            "type": {
+                                "name": "QueryLanguage",
+                                "namespace": "com.linkedin.pegasus2avro.query",
+                                "symbolDocs": {
+                                    "SQL": "A SQL Query"
+                                },
+                                "symbols": [
+                                    "SQL"
+                                ],
+                                "type": "enum"
+                            }
+                        }
+                    ],
+                    "name": "QueryStatement",
+                    "namespace": "com.linkedin.pegasus2avro.query",
+                    "type": "record"
+                }
+            },
+            {
+                "Searchable": {},
+                "doc": "The source of the Query",
+                "name": "source",
+                "type": {
+                    "name": "QuerySource",
+                    "namespace": "com.linkedin.pegasus2avro.query",
+                    "symbolDocs": {
+                        "MANUAL": "The query was entered manually by a user (via the UI)."
+                    },
+                    "symbols": [
+                        "MANUAL"
+                    ],
+                    "type": "enum"
+                }
             },
             {
+                "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL"
+                },
                 "default": null,
-                "doc": "first name of this user",
-                "name": "firstName",
+                "doc": "Optional display name to identify the query.",
+                "name": "name",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "last name of this user",
-                "name": "lastName",
+                "doc": "The Query description.",
+                "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "Searchable": {
+                    "/actor": {
+                        "fieldName": "createdBy",
+                        "fieldType": "URN"
+                    },
+                    "/time": {
+                        "fieldName": "createdAt",
+                        "fieldType": "DATETIME"
+                    }
+                },
+                "doc": "Audit stamp capturing the time and actor who created the Query.",
+                "name": "created",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+            },
+            {
+                "Searchable": {
+                    "/actor": {
+                        "fieldName": "lastModifiedBy",
+                        "fieldType": "URN"
+                    },
+                    "/time": {
+                        "fieldName": "lastModifiedAt",
+                        "fieldType": "DATETIME"
+                    }
+                },
+                "doc": "Audit stamp capturing the time and actor who last modified the Query.",
+                "name": "lastModified",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+            }
+        ],
+        "name": "QueryProperties",
+        "namespace": "com.linkedin.pegasus2avro.query",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "telemetryClientId"
+        },
+        "doc": "A simple wrapper around a String to persist the client ID for telemetry in DataHub's backend DB",
+        "fields": [
+            {
+                "doc": "A string representing the telemetry client ID",
+                "name": "clientId",
+                "type": "string"
+            }
+        ],
+        "name": "TelemetryClientId",
+        "namespace": "com.linkedin.pegasus2avro.telemetry",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "domainProperties"
+        },
+        "doc": "Information about a Domain",
+        "fields": [
+            {
+                "Searchable": {
                     "boostScore": 10.0,
-                    "fieldType": "TEXT_PARTIAL",
-                    "queryByDefault": true
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL"
                 },
+                "doc": "Display name of the Domain",
+                "name": "name",
+                "type": "string"
+            },
+            {
                 "default": null,
-                "doc": "Common name of this user, format is firstName + lastName (split by a whitespace)",
-                "name": "fullName",
+                "doc": "Description of the Domain",
+                "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
+                "Searchable": {
+                    "/time": {
+                        "fieldName": "createdTime",
+                        "fieldType": "DATETIME"
+                    }
+                },
                 "default": null,
-                "doc": "two uppercase letters country code. e.g.  US",
-                "name": "countryCode",
+                "doc": "Created Audit stamp",
+                "name": "created",
                 "type": [
                     "null",
-                    "string"
+                    "com.linkedin.pegasus2avro.common.AuditStamp"
                 ]
             }
         ],
-        "name": "CorpUserInfo",
-        "namespace": "com.linkedin.pegasus2avro.identity",
+        "name": "DomainProperties",
+        "namespace": "com.linkedin.pegasus2avro.domain",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "groupMembership"
+            "name": "domains"
         },
-        "doc": "Carries information about the CorpGroups a user is in.",
+        "doc": "Links from an Asset to its Domains",
         "fields": [
             {
                 "Relationship": {
                     "/*": {
                         "entityTypes": [
-                            "corpGroup"
+                            "domain"
                         ],
-                        "name": "IsMemberOfGroup"
+                        "name": "AssociatedWith"
                     }
                 },
-                "name": "groups",
+                "Searchable": {
+                    "/*": {
+                        "addToFilters": true,
+                        "fieldName": "domains",
+                        "fieldType": "URN",
+                        "filterNameOverride": "Domain",
+                        "hasValuesFieldName": "hasDomain"
+                    }
+                },
+                "Urn": "Urn",
+                "doc": "The Domains attached to an Asset",
+                "name": "domains",
                 "type": {
                     "items": "string",
                     "type": "array"
-                }
+                },
+                "urn_is_array": true
             }
         ],
-        "name": "GroupMembership",
-        "namespace": "com.linkedin.pegasus2avro.identity",
+        "name": "Domains",
+        "namespace": "com.linkedin.pegasus2avro.domain",
         "type": "record"
     },
     {
         "Aspect": {
-            "EntityUrns": [
-                "com.linkedin.pegasus2avro.common.CorpuserUrn"
-            ],
-            "name": "corpUserCredentials"
+            "name": "editableSchemaMetadata"
         },
-        "doc": "Corp user credentials",
+        "doc": "EditableSchemaMetadata stores editable changes made to schema metadata. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines.",
         "fields": [
             {
-                "doc": "Salt used to hash password",
-                "name": "salt",
-                "type": "string"
+                "default": {
+                    "actor": "urn:li:corpuser:unknown",
+                    "impersonator": null,
+                    "message": null,
+                    "time": 0
+                },
+                "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
+                "name": "created",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
             },
             {
-                "doc": "Hashed password generated by concatenating salt and password, then hashing",
-                "name": "hashedPassword",
-                "type": "string"
+                "default": {
+                    "actor": "urn:li:corpuser:unknown",
+                    "impersonator": null,
+                    "message": null,
+                    "time": 0
+                },
+                "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
+                "name": "lastModified",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
             },
             {
                 "default": null,
-                "doc": "Optional token needed to reset a user's password. Can only be set by the admin.",
-                "name": "passwordResetToken",
+                "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
+                "name": "deleted",
                 "type": [
                     "null",
-                    "string"
+                    "com.linkedin.pegasus2avro.common.AuditStamp"
                 ]
             },
             {
-                "default": null,
-                "doc": "When the password reset token expires.",
-                "name": "passwordResetTokenExpirationTimeMillis",
-                "type": [
-                    "null",
-                    "long"
-                ]
+                "doc": "Client provided a list of fields from document schema.",
+                "name": "editableSchemaFieldInfo",
+                "type": {
+                    "items": {
+                        "doc": "SchemaField to describe metadata related to dataset schema.",
+                        "fields": [
+                            {
+                                "doc": "FieldPath uniquely identifying the SchemaField this metadata is associated with",
+                                "name": "fieldPath",
+                                "type": "string"
+                            },
+                            {
+                                "Searchable": {
+                                    "boostScore": 0.1,
+                                    "fieldName": "editedFieldDescriptions",
+                                    "fieldType": "TEXT"
+                                },
+                                "default": null,
+                                "doc": "Description",
+                                "name": "description",
+                                "type": [
+                                    "null",
+                                    "string"
+                                ]
+                            },
+                            {
+                                "Relationship": {
+                                    "/tags/*/tag": {
+                                        "entityTypes": [
+                                            "tag"
+                                        ],
+                                        "name": "EditableSchemaFieldTaggedWith"
+                                    }
+                                },
+                                "Searchable": {
+                                    "/tags/*/tag": {
+                                        "boostScore": 0.5,
+                                        "fieldName": "editedFieldTags",
+                                        "fieldType": "URN"
+                                    }
+                                },
+                                "default": null,
+                                "doc": "Tags associated with the field",
+                                "name": "globalTags",
+                                "type": [
+                                    "null",
+                                    "com.linkedin.pegasus2avro.common.GlobalTags"
+                                ]
+                            },
+                            {
+                                "Relationship": {
+                                    "/terms/*/urn": {
+                                        "entityTypes": [
+                                            "glossaryTerm"
+                                        ],
+                                        "name": "EditableSchemaFieldWithGlossaryTerm"
+                                    }
+                                },
+                                "Searchable": {
+                                    "/terms/*/urn": {
+                                        "boostScore": 0.5,
+                                        "fieldName": "editedFieldGlossaryTerms",
+                                        "fieldType": "URN"
+                                    }
+                                },
+                                "default": null,
+                                "doc": "Glossary terms associated with the field",
+                                "name": "glossaryTerms",
+                                "type": [
+                                    "null",
+                                    "com.linkedin.pegasus2avro.common.GlossaryTerms"
+                                ]
+                            }
+                        ],
+                        "name": "EditableSchemaFieldInfo",
+                        "namespace": "com.linkedin.pegasus2avro.schema",
+                        "type": "record"
+                    },
+                    "type": "array"
+                }
             }
         ],
-        "name": "CorpUserCredentials",
-        "namespace": "com.linkedin.pegasus2avro.identity",
+        "name": "EditableSchemaMetadata",
+        "namespace": "com.linkedin.pegasus2avro.schema",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "corpGroupEditableInfo"
+            "name": "schemaMetadata"
         },
-        "doc": "Group information that can be edited from UI",
+        "doc": "SchemaMetadata to describe metadata related to store schema",
         "fields": [
             {
-                "Searchable": {
-                    "fieldName": "editedDescription",
-                    "fieldType": "TEXT"
+                "doc": "Schema name e.g. PageViewEvent, identity.Profile, ams.account_management_tracking",
+                "name": "schemaName",
+                "type": "string",
+                "validate": {
+                    "strlen": {
+                        "max": 500,
+                        "min": 1
+                    }
+                }
+            },
+            {
+                "Urn": "DataPlatformUrn",
+                "doc": "Standardized platform urn where schema is defined. The data platform Urn (urn:li:platform:{platform_name})",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.DataPlatformUrn"
+                },
+                "name": "platform",
+                "type": "string"
+            },
+            {
+                "doc": "Every change to SchemaMetadata in the resource results in a new version. Version is server assigned. This version is differ from platform native schema version.",
+                "name": "version",
+                "type": "long"
+            },
+            {
+                "default": {
+                    "actor": "urn:li:corpuser:unknown",
+                    "impersonator": null,
+                    "message": null,
+                    "time": 0
+                },
+                "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
+                "name": "created",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+            },
+            {
+                "default": {
+                    "actor": "urn:li:corpuser:unknown",
+                    "impersonator": null,
+                    "message": null,
+                    "time": 0
                 },
+                "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
+                "name": "lastModified",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+            },
+            {
                 "default": null,
-                "doc": "A description of the group",
-                "name": "description",
+                "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
+                "name": "deleted",
                 "type": [
                     "null",
-                    "string"
+                    "com.linkedin.pegasus2avro.common.AuditStamp"
                 ]
             },
             {
-                "default": "https://raw.githubusercontent.com/datahub-project/datahub/master/datahub-web-react/src/images/default_avatar.png",
-                "doc": "A URL which points to a picture which user wants to set as the photo for the group",
+                "Urn": "DatasetUrn",
+                "default": null,
+                "doc": "Dataset this schema metadata is associated with.",
                 "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                    "class": "com.linkedin.pegasus2avro.common.urn.DatasetUrn"
                 },
-                "name": "pictureLink",
-                "type": "string"
-            },
-            {
-                "default": null,
-                "doc": "Slack channel for the group",
-                "name": "slack",
+                "name": "dataset",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "Email address to contact the group",
-                "name": "email",
+                "doc": "The cluster this schema metadata resides from",
+                "name": "cluster",
                 "type": [
                     "null",
                     "string"
                 ]
-            }
-        ],
-        "name": "CorpGroupEditableInfo",
-        "namespace": "com.linkedin.pegasus2avro.identity",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "dataPlatformInstanceProperties"
-        },
-        "doc": "Properties associated with a Data Platform Instance",
-        "fields": [
+            },
             {
-                "Searchable": {
-                    "/*": {
-                        "queryByDefault": true
+                "doc": "the SHA1 hash of the schema content",
+                "name": "hash",
+                "type": "string"
+            },
+            {
+                "doc": "The native schema in the dataset's platform.",
+                "name": "platformSchema",
+                "type": [
+                    {
+                        "doc": "Schema text of an espresso table schema.",
+                        "fields": [
+                            {
+                                "doc": "The native espresso document schema.",
+                                "name": "documentSchema",
+                                "type": "string"
+                            },
+                            {
+                                "doc": "The espresso table schema definition.",
+                                "name": "tableSchema",
+                                "type": "string"
+                            }
+                        ],
+                        "name": "EspressoSchema",
+                        "namespace": "com.linkedin.pegasus2avro.schema",
+                        "type": "record"
+                    },
+                    {
+                        "doc": "Schema holder for oracle data definition language that describes an oracle table.",
+                        "fields": [
+                            {
+                                "doc": "The native schema in the dataset's platform. This is a human readable (json blob) table schema.",
+                                "name": "tableSchema",
+                                "type": "string"
+                            }
+                        ],
+                        "name": "OracleDDL",
+                        "namespace": "com.linkedin.pegasus2avro.schema",
+                        "type": "record"
+                    },
+                    {
+                        "doc": "Schema holder for MySql data definition language that describes an MySql table.",
+                        "fields": [
+                            {
+                                "doc": "The native schema in the dataset's platform. This is a human readable (json blob) table schema.",
+                                "name": "tableSchema",
+                                "type": "string"
+                            }
+                        ],
+                        "name": "MySqlDDL",
+                        "namespace": "com.linkedin.pegasus2avro.schema",
+                        "type": "record"
+                    },
+                    {
+                        "doc": "Schema holder for presto data definition language that describes a presto view.",
+                        "fields": [
+                            {
+                                "doc": "The raw schema in the dataset's platform. This includes the DDL and the columns extracted from DDL.",
+                                "name": "rawSchema",
+                                "type": "string"
+                            }
+                        ],
+                        "name": "PrestoDDL",
+                        "namespace": "com.linkedin.pegasus2avro.schema",
+                        "type": "record"
+                    },
+                    {
+                        "doc": "Schema holder for kafka schema.",
+                        "fields": [
+                            {
+                                "doc": "The native kafka document schema. This is a human readable avro document schema.",
+                                "name": "documentSchema",
+                                "type": "string"
+                            },
+                            {
+                                "default": null,
+                                "doc": "The native kafka key schema as retrieved from Schema Registry",
+                                "name": "keySchema",
+                                "type": [
+                                    "null",
+                                    "string"
+                                ]
+                            }
+                        ],
+                        "name": "KafkaSchema",
+                        "namespace": "com.linkedin.pegasus2avro.schema",
+                        "type": "record"
+                    },
+                    {
+                        "doc": "Schema text of binary JSON schema.",
+                        "fields": [
+                            {
+                                "doc": "The native schema text for binary JSON file format.",
+                                "name": "schema",
+                                "type": "string"
+                            }
+                        ],
+                        "name": "BinaryJsonSchema",
+                        "namespace": "com.linkedin.pegasus2avro.schema",
+                        "type": "record"
+                    },
+                    {
+                        "doc": "Schema text of an ORC schema.",
+                        "fields": [
+                            {
+                                "doc": "The native schema for ORC file format.",
+                                "name": "schema",
+                                "type": "string"
+                            }
+                        ],
+                        "name": "OrcSchema",
+                        "namespace": "com.linkedin.pegasus2avro.schema",
+                        "type": "record"
+                    },
+                    {
+                        "doc": "The dataset has no specific schema associated with it",
+                        "fields": [],
+                        "name": "Schemaless",
+                        "namespace": "com.linkedin.pegasus2avro.schema",
+                        "type": "record"
+                    },
+                    {
+                        "doc": "Schema text of a key-value store schema.",
+                        "fields": [
+                            {
+                                "doc": "The raw schema for the key in the key-value store.",
+                                "name": "keySchema",
+                                "type": "string"
+                            },
+                            {
+                                "doc": "The raw schema for the value in the key-value store.",
+                                "name": "valueSchema",
+                                "type": "string"
+                            }
+                        ],
+                        "name": "KeyValueSchema",
+                        "namespace": "com.linkedin.pegasus2avro.schema",
+                        "type": "record"
+                    },
+                    {
+                        "doc": "Schema holder for undefined schema types.",
+                        "fields": [
+                            {
+                                "doc": "The native schema in the dataset's platform.",
+                                "name": "rawSchema",
+                                "type": "string"
+                            }
+                        ],
+                        "name": "OtherSchema",
+                        "namespace": "com.linkedin.pegasus2avro.schema",
+                        "type": "record"
                     }
-                },
-                "default": {},
-                "doc": "Custom property bag.",
-                "name": "customProperties",
+                ]
+            },
+            {
+                "doc": "Client provided a list of fields from document schema.",
+                "name": "fields",
                 "type": {
-                    "type": "map",
-                    "values": "string"
+                    "items": "com.linkedin.pegasus2avro.schema.SchemaField",
+                    "type": "array"
                 }
             },
             {
                 "default": null,
-                "doc": "URL where the reference exist",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                },
-                "name": "externalUrl",
+                "doc": "Client provided list of fields that define primary keys to access record. Field order defines hierarchical espresso keys. Empty lists indicates absence of primary key access patter. Value is a SchemaField@fieldPath.",
+                "name": "primaryKeys",
                 "type": [
                     "null",
-                    "string"
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
                 ]
             },
             {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
-                    "fieldType": "TEXT_PARTIAL"
-                },
                 "default": null,
-                "doc": "Display name of the Data Platform Instance",
-                "name": "name",
+                "deprecated": "Use foreignKeys instead.",
+                "doc": "Map captures all the references schema makes to external datasets. Map key is ForeignKeySpecName typeref.",
+                "name": "foreignKeysSpecs",
                 "type": [
                     "null",
-                    "string"
+                    {
+                        "type": "map",
+                        "values": {
+                            "doc": "Description of a foreign key in a schema.",
+                            "fields": [
+                                {
+                                    "doc": "Foreign key definition in metadata schema.",
+                                    "name": "foreignKey",
+                                    "type": [
+                                        {
+                                            "doc": "For non-urn based foregin keys.",
+                                            "fields": [
+                                                {
+                                                    "Urn": "DatasetUrn",
+                                                    "doc": "dataset that stores the resource.",
+                                                    "java": {
+                                                        "class": "com.linkedin.pegasus2avro.common.urn.DatasetUrn"
+                                                    },
+                                                    "name": "parentDataset",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "doc": "List of fields in hosting(current) SchemaMetadata that conform a foreign key. List can contain a single entry or multiple entries if several entries in hosting schema conform a foreign key in a single parent dataset.",
+                                                    "name": "currentFieldPaths",
+                                                    "type": {
+                                                        "items": "string",
+                                                        "type": "array"
+                                                    }
+                                                },
+                                                {
+                                                    "doc": "SchemaField@fieldPath that uniquely identify field in parent dataset that this field references.",
+                                                    "name": "parentField",
+                                                    "type": "string"
+                                                }
+                                            ],
+                                            "name": "DatasetFieldForeignKey",
+                                            "namespace": "com.linkedin.pegasus2avro.schema",
+                                            "type": "record"
+                                        },
+                                        {
+                                            "doc": "If SchemaMetadata fields make any external references and references are of type com.linkedin.pegasus2avro.common.Urn or any children, this models can be used to mark it.",
+                                            "fields": [
+                                                {
+                                                    "doc": "Field in hosting(current) SchemaMetadata.",
+                                                    "name": "currentFieldPath",
+                                                    "type": "string"
+                                                }
+                                            ],
+                                            "name": "UrnForeignKey",
+                                            "namespace": "com.linkedin.pegasus2avro.schema",
+                                            "type": "record"
+                                        }
+                                    ]
+                                }
+                            ],
+                            "name": "ForeignKeySpec",
+                            "namespace": "com.linkedin.pegasus2avro.schema",
+                            "type": "record"
+                        }
+                    }
                 ]
             },
             {
-                "Searchable": {
-                    "fieldType": "TEXT",
-                    "hasValuesFieldName": "hasDescription"
-                },
                 "default": null,
-                "doc": "Documentation of the Data Platform Instance",
-                "name": "description",
+                "doc": "List of foreign key constraints for the schema",
+                "name": "foreignKeys",
                 "type": [
                     "null",
-                    "string"
+                    {
+                        "items": {
+                            "doc": "Description of a foreign key constraint in a schema.",
+                            "fields": [
+                                {
+                                    "doc": "Name of the constraint, likely provided from the source",
+                                    "name": "name",
+                                    "type": "string"
+                                },
+                                {
+                                    "Relationship": {
+                                        "/*": {
+                                            "entityTypes": [
+                                                "schemaField"
+                                            ],
+                                            "name": "ForeignKeyTo"
+                                        }
+                                    },
+                                    "Urn": "Urn",
+                                    "doc": "Fields the constraint maps to on the foreign dataset",
+                                    "name": "foreignFields",
+                                    "type": {
+                                        "items": "string",
+                                        "type": "array"
+                                    },
+                                    "urn_is_array": true
+                                },
+                                {
+                                    "Urn": "Urn",
+                                    "doc": "Fields the constraint maps to on the source dataset",
+                                    "name": "sourceFields",
+                                    "type": {
+                                        "items": "string",
+                                        "type": "array"
+                                    },
+                                    "urn_is_array": true
+                                },
+                                {
+                                    "Relationship": {
+                                        "entityTypes": [
+                                            "dataset"
+                                        ],
+                                        "name": "ForeignKeyToDataset"
+                                    },
+                                    "Urn": "Urn",
+                                    "doc": "Reference to the foreign dataset for ease of lookup",
+                                    "java": {
+                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                    },
+                                    "name": "foreignDataset",
+                                    "type": "string"
+                                }
+                            ],
+                            "name": "ForeignKeyConstraint",
+                            "namespace": "com.linkedin.pegasus2avro.schema",
+                            "type": "record"
+                        },
+                        "type": "array"
+                    }
                 ]
             }
         ],
-        "name": "DataPlatformInstanceProperties",
-        "namespace": "com.linkedin.pegasus2avro.dataplatforminstance",
+        "name": "SchemaMetadata",
+        "namespace": "com.linkedin.pegasus2avro.schema",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "testInfo"
+            "name": "dataHubIngestionSourceInfo"
         },
-        "doc": "Information about a DataHub Test",
+        "doc": "Info about a DataHub ingestion source",
         "fields": [
             {
                 "Searchable": {
                     "fieldType": "TEXT_PARTIAL"
                 },
-                "doc": "The name of the test",
+                "doc": "The display name of the ingestion source",
                 "name": "name",
                 "type": "string"
             },
             {
-                "Searchable": {
-                    "fieldType": "KEYWORD"
-                },
-                "doc": "Category of the test",
-                "name": "category",
+                "doc": "The type of the source itself, e.g. mysql, bigquery, bigquery-usage. Should match the recipe.",
+                "name": "type",
                 "type": "string"
             },
             {
-                "Searchable": {
-                    "fieldType": "TEXT"
-                },
+                "Urn": "Urn",
                 "default": null,
-                "doc": "Description of the test",
-                "name": "description",
+                "doc": "Data Platform URN associated with the source",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                },
+                "name": "platform",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
-                "doc": "Configuration for the Test",
-                "name": "definition",
+                "default": null,
+                "doc": "The schedule on which the ingestion source is executed",
+                "name": "schedule",
+                "type": [
+                    "null",
+                    {
+                        "doc": "The schedule associated with an ingestion source.",
+                        "fields": [
+                            {
+                                "doc": "A cron-formatted execution interval, as a cron string, e.g. * * * * *",
+                                "name": "interval",
+                                "type": "string"
+                            },
+                            {
+                                "doc": "Timezone in which the cron interval applies, e.g. America/Los Angeles",
+                                "name": "timezone",
+                                "type": "string"
+                            }
+                        ],
+                        "name": "DataHubIngestionSourceSchedule",
+                        "namespace": "com.linkedin.pegasus2avro.ingestion",
+                        "type": "record"
+                    }
+                ]
+            },
+            {
+                "doc": "Parameters associated with the Ingestion Source",
+                "name": "config",
                 "type": {
                     "fields": [
                         {
-                            "doc": "The Test Definition Type",
-                            "name": "type",
-                            "type": {
-                                "name": "TestDefinitionType",
-                                "namespace": "com.linkedin.pegasus2avro.test",
-                                "symbolDocs": {
-                                    "JSON": "JSON / YAML test def"
-                                },
-                                "symbols": [
-                                    "JSON"
-                                ],
-                                "type": "enum"
-                            }
+                            "doc": "The JSON recipe to use for ingestion",
+                            "name": "recipe",
+                            "type": "string"
                         },
                         {
                             "default": null,
-                            "doc": "JSON format configuration for the test",
-                            "name": "json",
+                            "doc": "The PyPI version of the datahub CLI to use when executing a recipe",
+                            "name": "version",
+                            "type": [
+                                "null",
+                                "string"
+                            ]
+                        },
+                        {
+                            "default": null,
+                            "doc": "The id of the executor to use to execute the ingestion run",
+                            "name": "executorId",
                             "type": [
                                 "null",
                                 "string"
                             ]
+                        },
+                        {
+                            "default": null,
+                            "doc": "Whether or not to run this ingestion source in debug mode",
+                            "name": "debugMode",
+                            "type": [
+                                "null",
+                                "boolean"
+                            ]
                         }
                     ],
-                    "name": "TestDefinition",
-                    "namespace": "com.linkedin.pegasus2avro.test",
+                    "name": "DataHubIngestionSourceConfig",
+                    "namespace": "com.linkedin.pegasus2avro.ingestion",
                     "type": "record"
                 }
             }
         ],
-        "name": "TestInfo",
-        "namespace": "com.linkedin.pegasus2avro.test",
+        "name": "DataHubIngestionSourceInfo",
+        "namespace": "com.linkedin.pegasus2avro.ingestion",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "testResults"
+            "name": "viewProperties"
         },
-        "doc": "Information about a Test Result",
+        "doc": "Details about a View. \ne.g. Gets activated when subTypes is view",
         "fields": [
             {
-                "Relationship": {
-                    "/*/test": {
-                        "entityTypes": [
-                            "test"
-                        ],
-                        "name": "IsFailing"
+                "Searchable": {
+                    "fieldType": "BOOLEAN",
+                    "weightsPerFieldValue": {
+                        "true": 0.5
                     }
                 },
+                "doc": "Whether the view is materialized",
+                "name": "materialized",
+                "type": "boolean"
+            },
+            {
+                "doc": "The view logic",
+                "name": "viewLogic",
+                "type": "string"
+            },
+            {
+                "doc": "The view logic language / dialect",
+                "name": "viewLanguage",
+                "type": "string"
+            }
+        ],
+        "name": "ViewProperties",
+        "namespace": "com.linkedin.pegasus2avro.dataset",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "datasetDeprecation"
+        },
+        "Deprecated": true,
+        "doc": "Dataset deprecation status\nDeprecated! This aspect is deprecated in favor of the more-general-purpose 'Deprecation' aspect.",
+        "fields": [
+            {
                 "Searchable": {
-                    "/*/test": {
-                        "fieldName": "failingTests",
-                        "fieldType": "URN",
-                        "hasValuesFieldName": "hasFailingTests"
+                    "fieldType": "BOOLEAN",
+                    "weightsPerFieldValue": {
+                        "true": 0.5
                     }
                 },
-                "doc": "Results that are failing",
-                "name": "failing",
+                "doc": "Whether the dataset is deprecated by owner.",
+                "name": "deprecated",
+                "type": "boolean"
+            },
+            {
+                "default": null,
+                "doc": "The time user plan to decommission this dataset.",
+                "name": "decommissionTime",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            },
+            {
+                "doc": "Additional information about the dataset deprecation plan, such as the wiki, doc, RB.",
+                "name": "note",
+                "type": "string"
+            },
+            {
+                "Urn": "Urn",
+                "default": null,
+                "doc": "The corpuser URN which will be credited for modifying this deprecation content.",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                },
+                "name": "actor",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            }
+        ],
+        "name": "DatasetDeprecation",
+        "namespace": "com.linkedin.pegasus2avro.dataset",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "datasetUpstreamLineage"
+        },
+        "deprecated": "use UpstreamLineage.fineGrainedLineages instead",
+        "doc": "Fine Grained upstream lineage for fields in a dataset",
+        "fields": [
+            {
+                "doc": "Upstream to downstream field level lineage mappings",
+                "name": "fieldMappings",
                 "type": {
                     "items": {
-                        "doc": "Information about a Test Result",
+                        "deprecated": "use FineGrainedLineage instead",
+                        "doc": "Representation of mapping between fields in source dataset to the field in destination dataset",
                         "fields": [
                             {
-                                "doc": "The urn of the test",
-                                "java": {
-                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                },
-                                "name": "test",
-                                "type": "string"
+                                "doc": "Audit stamp containing who reported the field mapping and when",
+                                "name": "created",
+                                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
                             },
                             {
-                                "doc": "The type of the result",
-                                "name": "type",
-                                "type": {
-                                    "name": "TestResultType",
-                                    "namespace": "com.linkedin.pegasus2avro.test",
-                                    "symbolDocs": {
-                                        "FAILURE": " The Test Failed",
-                                        "SUCCESS": " The Test Succeeded"
+                                "doc": "Transfomration function between the fields involved",
+                                "name": "transformation",
+                                "type": [
+                                    {
+                                        "doc": "Type of the transformation involved in generating destination fields from source fields.",
+                                        "name": "TransformationType",
+                                        "namespace": "com.linkedin.pegasus2avro.common.fieldtransformer",
+                                        "symbolDocs": {
+                                            "BLACKBOX": "Field transformation expressed as unknown black box function.",
+                                            "IDENTITY": "Field transformation expressed as Identity function."
+                                        },
+                                        "symbols": [
+                                            "BLACKBOX",
+                                            "IDENTITY"
+                                        ],
+                                        "type": "enum"
                                     },
-                                    "symbols": [
-                                        "SUCCESS",
-                                        "FAILURE"
+                                    {
+                                        "doc": "Field transformation expressed in UDF",
+                                        "fields": [
+                                            {
+                                                "doc": "A UDF mentioning how the source fields got transformed to destination field. This is the FQCN(Fully Qualified Class Name) of the udf.",
+                                                "name": "udf",
+                                                "type": "string"
+                                            }
+                                        ],
+                                        "name": "UDFTransformer",
+                                        "namespace": "com.linkedin.pegasus2avro.common.fieldtransformer",
+                                        "type": "record"
+                                    }
+                                ]
+                            },
+                            {
+                                "doc": "Source fields from which the fine grained lineage is derived",
+                                "name": "sourceFields",
+                                "type": {
+                                    "items": [
+                                        "string"
                                     ],
-                                    "type": "enum"
+                                    "type": "array"
                                 }
+                            },
+                            {
+                                "Urn": "DatasetFieldUrn",
+                                "deprecated": "use SchemaFieldPath and represent as generic Urn instead",
+                                "doc": "Destination field which is derived from source fields",
+                                "java": {
+                                    "class": "com.linkedin.pegasus2avro.common.urn.DatasetFieldUrn"
+                                },
+                                "name": "destinationField",
+                                "type": "string"
                             }
                         ],
-                        "name": "TestResult",
-                        "namespace": "com.linkedin.pegasus2avro.test",
+                        "name": "DatasetFieldMapping",
+                        "namespace": "com.linkedin.pegasus2avro.dataset",
                         "type": "record"
                     },
                     "type": "array"
                 }
-            },
-            {
-                "Relationship": {
-                    "/*/test": {
-                        "entityTypes": [
-                            "test"
-                        ],
-                        "name": "IsPassing"
-                    }
-                },
-                "Searchable": {
-                    "/*/test": {
-                        "fieldName": "passingTests",
-                        "fieldType": "URN",
-                        "hasValuesFieldName": "hasPassingTests"
-                    }
-                },
-                "doc": "Results that are passing",
-                "name": "passing",
-                "type": {
-                    "items": "com.linkedin.pegasus2avro.test.TestResult",
-                    "type": "array"
-                }
             }
         ],
-        "name": "TestResults",
-        "namespace": "com.linkedin.pegasus2avro.test",
+        "name": "DatasetUpstreamLineage",
+        "namespace": "com.linkedin.pegasus2avro.dataset",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "versionInfo"
+            "name": "datasetUsageStatistics",
+            "type": "timeseries"
         },
-        "doc": "Information about a Data processing job",
+        "doc": "Stats corresponding to dataset's usage.",
         "fields": [
             {
-                "Searchable": {
-                    "/*": {
-                        "queryByDefault": true
-                    }
-                },
-                "default": {},
-                "doc": "Custom property bag.",
-                "name": "customProperties",
-                "type": {
-                    "type": "map",
-                    "values": "string"
-                }
+                "doc": "The event timestamp field as epoch at UTC in milli seconds.",
+                "name": "timestampMillis",
+                "type": "long"
             },
             {
                 "default": null,
-                "doc": "URL where the reference exist",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                "doc": "Granularity of the event if applicable",
+                "name": "eventGranularity",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
+                ]
+            },
+            {
+                "default": {
+                    "partition": "FULL_TABLE_SNAPSHOT",
+                    "timePartition": null,
+                    "type": "FULL_TABLE"
                 },
-                "name": "externalUrl",
+                "doc": "The optional partition specification.",
+                "name": "partitionSpec",
+                "type": [
+                    "com.linkedin.pegasus2avro.timeseries.PartitionSpec",
+                    "null"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
+                "name": "messageId",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
-                "doc": "The version which can indentify a job version like a commit hash or md5 hash",
-                "name": "version",
-                "type": "string"
+                "TimeseriesField": {},
+                "default": null,
+                "doc": "Unique user count",
+                "name": "uniqueUserCount",
+                "type": [
+                    "null",
+                    "int"
+                ]
             },
             {
-                "doc": "The type of the version like git hash or md5 hash",
-                "name": "versionType",
-                "type": "string"
+                "TimeseriesField": {},
+                "default": null,
+                "doc": "Total SQL query count",
+                "name": "totalSqlQueries",
+                "type": [
+                    "null",
+                    "int"
+                ]
+            },
+            {
+                "TimeseriesField": {},
+                "default": null,
+                "doc": "Frequent SQL queries; mostly makes sense for datasets in SQL databases",
+                "name": "topSqlQueries",
+                "type": [
+                    "null",
+                    {
+                        "items": "string",
+                        "type": "array"
+                    }
+                ]
+            },
+            {
+                "TimeseriesFieldCollection": {
+                    "key": "user"
+                },
+                "default": null,
+                "doc": "Users within this bucket, with frequency counts",
+                "name": "userCounts",
+                "type": [
+                    "null",
+                    {
+                        "items": {
+                            "doc": "Records a single user's usage counts for a given resource",
+                            "fields": [
+                                {
+                                    "Urn": "Urn",
+                                    "doc": "The unique id of the user.",
+                                    "java": {
+                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                    },
+                                    "name": "user",
+                                    "type": "string"
+                                },
+                                {
+                                    "TimeseriesField": {},
+                                    "doc": "Number of times the dataset has been used by the user.",
+                                    "name": "count",
+                                    "type": "int"
+                                },
+                                {
+                                    "TimeseriesField": {},
+                                    "default": null,
+                                    "doc": "If user_email is set, we attempt to resolve the user's urn upon ingest",
+                                    "name": "userEmail",
+                                    "type": [
+                                        "null",
+                                        "string"
+                                    ]
+                                }
+                            ],
+                            "name": "DatasetUserUsageCounts",
+                            "namespace": "com.linkedin.pegasus2avro.dataset",
+                            "type": "record"
+                        },
+                        "type": "array"
+                    }
+                ]
+            },
+            {
+                "TimeseriesFieldCollection": {
+                    "key": "fieldPath"
+                },
+                "default": null,
+                "doc": "Field-level usage stats",
+                "name": "fieldCounts",
+                "type": [
+                    "null",
+                    {
+                        "items": {
+                            "doc": "Records field-level usage counts for a given dataset",
+                            "fields": [
+                                {
+                                    "doc": "The name of the field.",
+                                    "name": "fieldPath",
+                                    "type": "string"
+                                },
+                                {
+                                    "TimeseriesField": {},
+                                    "doc": "Number of times the field has been used.",
+                                    "name": "count",
+                                    "type": "int"
+                                }
+                            ],
+                            "name": "DatasetFieldUsageCounts",
+                            "namespace": "com.linkedin.pegasus2avro.dataset",
+                            "type": "record"
+                        },
+                        "type": "array"
+                    }
+                ]
             }
         ],
-        "name": "VersionInfo",
-        "namespace": "com.linkedin.pegasus2avro.datajob",
+        "name": "DatasetUsageStatistics",
+        "namespace": "com.linkedin.pegasus2avro.dataset",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dataJobInfo"
+            "name": "datasetProperties"
         },
-        "doc": "Information about a Data processing job",
+        "doc": "Properties associated with a Dataset",
         "fields": [
             {
                 "Searchable": {
                     "/*": {
                         "queryByDefault": true
                     }
                 },
@@ -9239,69 +6684,58 @@
             },
             {
                 "Searchable": {
                     "boostScore": 10.0,
                     "enableAutocomplete": true,
                     "fieldType": "TEXT_PARTIAL"
                 },
-                "doc": "Job name",
+                "default": null,
+                "doc": "Display name of the Dataset",
                 "name": "name",
-                "type": "string"
+                "type": [
+                    "null",
+                    "string"
+                ]
             },
             {
                 "Searchable": {
-                    "fieldType": "TEXT",
-                    "hasValuesFieldName": "hasDescription"
+                    "addToFilters": false,
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT"
                 },
                 "default": null,
-                "doc": "Job description",
-                "name": "description",
+                "doc": "Fully-qualified name of the Dataset",
+                "name": "qualifiedName",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
-                "doc": "Datajob type\n*NOTE**: AzkabanJobType is deprecated. Please use strings instead.",
-                "name": "type",
+                "Searchable": {
+                    "fieldType": "TEXT",
+                    "hasValuesFieldName": "hasDescription"
+                },
+                "default": null,
+                "doc": "Documentation of the dataset",
+                "name": "description",
                 "type": [
-                    {
-                        "doc": "The various types of support azkaban jobs",
-                        "name": "AzkabanJobType",
-                        "namespace": "com.linkedin.pegasus2avro.datajob.azkaban",
-                        "symbolDocs": {
-                            "COMMAND": "The command job type is one of the basic built-in types. It runs multiple UNIX commands using java processbuilder.\nUpon execution, Azkaban spawns off a process to run the command.",
-                            "GLUE": "Glue type is for running AWS Glue job transforms.",
-                            "HADOOP_JAVA": "Runs a java program with ability to access Hadoop cluster.\nhttps://azkaban.readthedocs.io/en/latest/jobTypes.html#java-job-type",
-                            "HADOOP_SHELL": "In large part, this is the same Command type. The difference is its ability to talk to a Hadoop cluster\nsecurely, via Hadoop tokens.",
-                            "HIVE": "Hive type is for running Hive jobs.",
-                            "PIG": "Pig type is for running Pig jobs.",
-                            "SQL": "SQL is for running Presto, mysql queries etc"
-                        },
-                        "symbols": [
-                            "COMMAND",
-                            "HADOOP_JAVA",
-                            "HADOOP_SHELL",
-                            "HIVE",
-                            "PIG",
-                            "SQL",
-                            "GLUE"
-                        ],
-                        "type": "enum"
-                    },
+                    "null",
                     "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "DataFlow urn that this job is part of",
+                "deprecated": "Use ExternalReference.externalUrl field instead.",
+                "doc": "The abstracted URI such as hdfs:///data/tracking/PageViewEvent, file:///dir/file_name. Uri should not include any environment specific properties. Some datasets might not have a standardized uri, which makes this field optional (i.e. kafka topic).",
                 "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.DataFlowUrn"
+                    "class": "java.net.URI"
                 },
-                "name": "flowUrn",
+                "name": "uri",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "Searchable": {
@@ -9330,307 +6764,426 @@
                 "name": "lastModified",
                 "type": [
                     "null",
                     "com.linkedin.pegasus2avro.common.TimeStamp"
                 ]
             },
             {
-                "default": null,
-                "deprecated": "Use Data Process Instance model, instead",
-                "doc": "Status of the job - Deprecated for Data Process Instance model.",
-                "name": "status",
-                "type": [
-                    "null",
-                    {
-                        "doc": "Job statuses",
-                        "name": "JobStatus",
-                        "namespace": "com.linkedin.pegasus2avro.datajob",
-                        "symbolDocs": {
-                            "COMPLETED": "Jobs with successful completion.",
-                            "FAILED": "Jobs that have failed.",
-                            "IN_PROGRESS": "Jobs currently running.",
-                            "SKIPPED": "Jobs that have been skipped.",
-                            "STARTING": "Jobs being initialized.",
-                            "STOPPED": "Jobs that have stopped.",
-                            "STOPPING": "Jobs being stopped.",
-                            "UNKNOWN": "Jobs with unknown status (either unmappable or unavailable)"
-                        },
-                        "symbols": [
-                            "STARTING",
-                            "IN_PROGRESS",
-                            "STOPPING",
-                            "STOPPED",
-                            "COMPLETED",
-                            "FAILED",
-                            "UNKNOWN",
-                            "SKIPPED"
-                        ],
-                        "type": "enum"
-                    }
-                ]
+                "default": [],
+                "deprecated": "Use GlobalTags aspect instead.",
+                "doc": "[Legacy] Unstructured tags for the dataset. Structured tags can be applied via the `GlobalTags` aspect.\nThis is now deprecated.",
+                "name": "tags",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                }
             }
         ],
-        "name": "DataJobInfo",
-        "namespace": "com.linkedin.pegasus2avro.datajob",
+        "name": "DatasetProperties",
+        "namespace": "com.linkedin.pegasus2avro.dataset",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dataJobInputOutput"
+            "name": "upstreamLineage"
         },
-        "doc": "Information about the inputs and outputs of a Data processing job",
+        "doc": "Upstream lineage of a dataset",
         "fields": [
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "dataset"
-                        ],
-                        "isLineage": true,
-                        "name": "Consumes"
-                    }
-                },
-                "Searchable": {
-                    "/*": {
-                        "fieldName": "inputs",
-                        "fieldType": "URN",
-                        "numValuesFieldName": "numInputDatasets",
-                        "queryByDefault": false
-                    }
-                },
-                "deprecated": true,
-                "doc": "Input datasets consumed by the data job during processing\nDeprecated! Use inputDatasetEdges instead.",
-                "name": "inputDatasets",
+                "doc": "List of upstream dataset lineage information",
+                "name": "upstreams",
                 "type": {
-                    "items": "string",
+                    "items": {
+                        "doc": "Upstream lineage information about a dataset including the source reporting the lineage",
+                        "fields": [
+                            {
+                                "default": {
+                                    "actor": "urn:li:corpuser:unknown",
+                                    "impersonator": null,
+                                    "message": null,
+                                    "time": 0
+                                },
+                                "doc": "Audit stamp containing who reported the lineage and when.",
+                                "name": "auditStamp",
+                                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+                            },
+                            {
+                                "default": null,
+                                "doc": "Audit stamp containing who created the lineage and when.",
+                                "name": "created",
+                                "type": [
+                                    "null",
+                                    "com.linkedin.pegasus2avro.common.AuditStamp"
+                                ]
+                            },
+                            {
+                                "Relationship": {
+                                    "createdActor": "upstreams/*/created/actor",
+                                    "createdOn": "upstreams/*/created/time",
+                                    "entityTypes": [
+                                        "dataset"
+                                    ],
+                                    "isLineage": true,
+                                    "name": "DownstreamOf",
+                                    "properties": "upstreams/*/properties",
+                                    "updatedActor": "upstreams/*/auditStamp/actor",
+                                    "updatedOn": "upstreams/*/auditStamp/time"
+                                },
+                                "Searchable": {
+                                    "fieldName": "upstreams",
+                                    "fieldType": "URN",
+                                    "queryByDefault": false
+                                },
+                                "Urn": "DatasetUrn",
+                                "doc": "The upstream dataset the lineage points to",
+                                "java": {
+                                    "class": "com.linkedin.pegasus2avro.common.urn.DatasetUrn"
+                                },
+                                "name": "dataset",
+                                "type": "string"
+                            },
+                            {
+                                "doc": "The type of the lineage",
+                                "name": "type",
+                                "type": {
+                                    "doc": "The various types of supported dataset lineage",
+                                    "name": "DatasetLineageType",
+                                    "namespace": "com.linkedin.pegasus2avro.dataset",
+                                    "symbolDocs": {
+                                        "COPY": "Direct copy without modification",
+                                        "TRANSFORMED": "Transformed data with modification (format or content change)",
+                                        "VIEW": "Represents a view defined on the sources e.g. Hive view defined on underlying hive tables or a Hive table pointing to a HDFS dataset or DALI view defined on multiple sources"
+                                    },
+                                    "symbols": [
+                                        "COPY",
+                                        "TRANSFORMED",
+                                        "VIEW"
+                                    ],
+                                    "type": "enum"
+                                }
+                            },
+                            {
+                                "default": null,
+                                "doc": "A generic properties bag that allows us to store specific information on this graph edge.",
+                                "name": "properties",
+                                "type": [
+                                    "null",
+                                    {
+                                        "type": "map",
+                                        "values": "string"
+                                    }
+                                ]
+                            }
+                        ],
+                        "name": "Upstream",
+                        "namespace": "com.linkedin.pegasus2avro.dataset",
+                        "type": "record"
+                    },
                     "type": "array"
                 }
             },
             {
                 "Relationship": {
-                    "/*/destinationUrn": {
-                        "createdActor": "inputDatasetEdges/*/created/actor",
-                        "createdOn": "inputDatasetEdges/*/created/time",
+                    "/*/upstreams/*": {
                         "entityTypes": [
-                            "dataset"
+                            "dataset",
+                            "schemaField"
                         ],
-                        "isLineage": true,
-                        "name": "Consumes",
-                        "properties": "inputDatasetEdges/*/properties",
-                        "updatedActor": "inputDatasetEdges/*/lastModified/actor",
-                        "updatedOn": "inputDatasetEdges/*/lastModified/time"
-                    }
-                },
-                "Searchable": {
-                    "/*/destinationUrn": {
-                        "fieldName": "inputDatasetEdges",
-                        "fieldType": "URN",
-                        "numValuesFieldName": "numInputDatasets",
-                        "queryByDefault": false
+                        "name": "DownstreamOf"
                     }
                 },
                 "default": null,
-                "doc": "Input datasets consumed by the data job during processing",
-                "name": "inputDatasetEdges",
+                "doc": " List of fine-grained lineage information, including field-level lineage",
+                "name": "fineGrainedLineages",
                 "type": [
                     "null",
                     {
-                        "items": "com.linkedin.pegasus2avro.common.Edge",
+                        "items": "com.linkedin.pegasus2avro.dataset.FineGrainedLineage",
                         "type": "array"
                     }
                 ]
-            },
+            }
+        ],
+        "name": "UpstreamLineage",
+        "namespace": "com.linkedin.pegasus2avro.dataset",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "datasetProfile",
+            "type": "timeseries"
+        },
+        "doc": "Stats corresponding to datasets",
+        "fields": [
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "dataset"
-                        ],
-                        "isLineage": true,
-                        "isUpstream": false,
-                        "name": "Produces"
-                    }
-                },
-                "Searchable": {
-                    "/*": {
-                        "fieldName": "outputs",
-                        "fieldType": "URN",
-                        "numValuesFieldName": "numOutputDatasets",
-                        "queryByDefault": false
-                    }
-                },
-                "deprecated": true,
-                "doc": "Output datasets produced by the data job during processing\nDeprecated! Use outputDatasetEdges instead.",
-                "name": "outputDatasets",
-                "type": {
-                    "items": "string",
-                    "type": "array"
-                }
+                "doc": "The event timestamp field as epoch at UTC in milli seconds.",
+                "name": "timestampMillis",
+                "type": "long"
             },
             {
-                "Relationship": {
-                    "/*/destinationUrn": {
-                        "createdActor": "outputDatasetEdges/*/created/actor",
-                        "createdOn": "outputDatasetEdges/*/created/time",
-                        "entityTypes": [
-                            "dataset"
-                        ],
-                        "isLineage": true,
-                        "isUpstream": false,
-                        "name": "Produces",
-                        "properties": "outputDatasetEdges/*/properties",
-                        "updatedActor": "outputDatasetEdges/*/lastModified/actor",
-                        "updatedOn": "outputDatasetEdges/*/lastModified/time"
-                    }
-                },
-                "Searchable": {
-                    "/*/destinationUrn": {
-                        "fieldName": "outputDatasetEdges",
-                        "fieldType": "URN",
-                        "numValuesFieldName": "numOutputDatasets",
-                        "queryByDefault": false
-                    }
-                },
                 "default": null,
-                "doc": "Output datasets produced by the data job during processing",
-                "name": "outputDatasetEdges",
+                "doc": "Granularity of the event if applicable",
+                "name": "eventGranularity",
                 "type": [
                     "null",
-                    {
-                        "items": "com.linkedin.pegasus2avro.common.Edge",
-                        "type": "array"
-                    }
+                    "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
                 ]
             },
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "dataJob"
-                        ],
-                        "isLineage": true,
-                        "name": "DownstreamOf"
-                    }
+                "default": {
+                    "partition": "FULL_TABLE_SNAPSHOT",
+                    "timePartition": null,
+                    "type": "FULL_TABLE"
                 },
+                "doc": "The optional partition specification.",
+                "name": "partitionSpec",
+                "type": [
+                    "com.linkedin.pegasus2avro.timeseries.PartitionSpec",
+                    "null"
+                ]
+            },
+            {
                 "default": null,
-                "deprecated": true,
-                "doc": "Input datajobs that this data job depends on\nDeprecated! Use inputDatajobEdges instead.",
-                "name": "inputDatajobs",
+                "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
+                "name": "messageId",
                 "type": [
                     "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
+                    "string"
                 ]
             },
             {
-                "Relationship": {
-                    "/*/destinationUrn": {
-                        "createdActor": "inputDatajobEdges/*/created/actor",
-                        "createdOn": "inputDatajobEdges/*/created/time",
-                        "entityTypes": [
-                            "dataJob"
-                        ],
-                        "isLineage": true,
-                        "name": "DownstreamOf",
-                        "properties": "inputDatajobEdges/*/properties",
-                        "updatedActor": "inputDatajobEdges/*/lastModified/actor",
-                        "updatedOn": "inputDatajobEdges/*/lastModified/time"
-                    }
+                "Searchable": {
+                    "fieldType": "COUNT"
                 },
                 "default": null,
-                "doc": "Input datajobs that this data job depends on",
-                "name": "inputDatajobEdges",
+                "doc": "The total number of rows",
+                "name": "rowCount",
                 "type": [
                     "null",
-                    {
-                        "items": "com.linkedin.pegasus2avro.common.Edge",
-                        "type": "array"
-                    }
+                    "long"
                 ]
             },
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "schemaField"
-                        ],
-                        "name": "Consumes"
-                    }
-                },
                 "Searchable": {
-                    "/*": {
-                        "fieldName": "inputFields",
-                        "fieldType": "URN",
-                        "numValuesFieldName": "numInputFields",
-                        "queryByDefault": false
-                    }
+                    "fieldType": "COUNT"
                 },
                 "default": null,
-                "doc": "Fields of the input datasets used by this job",
-                "name": "inputDatasetFields",
+                "doc": "The total number of columns (or schema fields)",
+                "name": "columnCount",
                 "type": [
                     "null",
-                    {
-                        "items": "string",
-                        "type": "array"
-                    }
+                    "long"
                 ]
             },
             {
-                "Relationship": {
-                    "/*": {
-                        "entityTypes": [
-                            "schemaField"
-                        ],
-                        "name": "Produces"
-                    }
-                },
-                "Searchable": {
-                    "/*": {
-                        "fieldName": "outputFields",
-                        "fieldType": "URN",
-                        "numValuesFieldName": "numOutputFields",
-                        "queryByDefault": false
-                    }
-                },
                 "default": null,
-                "doc": "Fields of the output datasets this job writes to",
-                "name": "outputDatasetFields",
+                "doc": "Profiles for each column (or schema field)",
+                "name": "fieldProfiles",
                 "type": [
                     "null",
                     {
-                        "items": "string",
+                        "items": {
+                            "doc": "Stats corresponding to fields in a dataset",
+                            "fields": [
+                                {
+                                    "name": "fieldPath",
+                                    "type": "string"
+                                },
+                                {
+                                    "default": null,
+                                    "name": "uniqueCount",
+                                    "type": [
+                                        "null",
+                                        "long"
+                                    ]
+                                },
+                                {
+                                    "default": null,
+                                    "name": "uniqueProportion",
+                                    "type": [
+                                        "null",
+                                        "float"
+                                    ]
+                                },
+                                {
+                                    "default": null,
+                                    "name": "nullCount",
+                                    "type": [
+                                        "null",
+                                        "long"
+                                    ]
+                                },
+                                {
+                                    "default": null,
+                                    "name": "nullProportion",
+                                    "type": [
+                                        "null",
+                                        "float"
+                                    ]
+                                },
+                                {
+                                    "default": null,
+                                    "name": "min",
+                                    "type": [
+                                        "null",
+                                        "string"
+                                    ]
+                                },
+                                {
+                                    "default": null,
+                                    "name": "max",
+                                    "type": [
+                                        "null",
+                                        "string"
+                                    ]
+                                },
+                                {
+                                    "default": null,
+                                    "name": "mean",
+                                    "type": [
+                                        "null",
+                                        "string"
+                                    ]
+                                },
+                                {
+                                    "default": null,
+                                    "name": "median",
+                                    "type": [
+                                        "null",
+                                        "string"
+                                    ]
+                                },
+                                {
+                                    "default": null,
+                                    "name": "stdev",
+                                    "type": [
+                                        "null",
+                                        "string"
+                                    ]
+                                },
+                                {
+                                    "default": null,
+                                    "name": "quantiles",
+                                    "type": [
+                                        "null",
+                                        {
+                                            "items": {
+                                                "fields": [
+                                                    {
+                                                        "name": "quantile",
+                                                        "type": "string"
+                                                    },
+                                                    {
+                                                        "name": "value",
+                                                        "type": "string"
+                                                    }
+                                                ],
+                                                "name": "Quantile",
+                                                "namespace": "com.linkedin.pegasus2avro.dataset",
+                                                "type": "record"
+                                            },
+                                            "type": "array"
+                                        }
+                                    ]
+                                },
+                                {
+                                    "default": null,
+                                    "name": "distinctValueFrequencies",
+                                    "type": [
+                                        "null",
+                                        {
+                                            "items": {
+                                                "fields": [
+                                                    {
+                                                        "name": "value",
+                                                        "type": "string"
+                                                    },
+                                                    {
+                                                        "name": "frequency",
+                                                        "type": "long"
+                                                    }
+                                                ],
+                                                "name": "ValueFrequency",
+                                                "namespace": "com.linkedin.pegasus2avro.dataset",
+                                                "type": "record"
+                                            },
+                                            "type": "array"
+                                        }
+                                    ]
+                                },
+                                {
+                                    "default": null,
+                                    "name": "histogram",
+                                    "type": [
+                                        "null",
+                                        {
+                                            "fields": [
+                                                {
+                                                    "name": "boundaries",
+                                                    "type": {
+                                                        "items": "string",
+                                                        "type": "array"
+                                                    }
+                                                },
+                                                {
+                                                    "name": "heights",
+                                                    "type": {
+                                                        "items": "float",
+                                                        "type": "array"
+                                                    }
+                                                }
+                                            ],
+                                            "name": "Histogram",
+                                            "namespace": "com.linkedin.pegasus2avro.dataset",
+                                            "type": "record"
+                                        }
+                                    ]
+                                },
+                                {
+                                    "default": null,
+                                    "name": "sampleValues",
+                                    "type": [
+                                        "null",
+                                        {
+                                            "items": "string",
+                                            "type": "array"
+                                        }
+                                    ]
+                                }
+                            ],
+                            "name": "DatasetFieldProfile",
+                            "namespace": "com.linkedin.pegasus2avro.dataset",
+                            "type": "record"
+                        },
                         "type": "array"
                     }
                 ]
             },
             {
+                "Searchable": {
+                    "fieldType": "COUNT"
+                },
                 "default": null,
-                "doc": "Fine-grained column-level lineages",
-                "name": "fineGrainedLineages",
+                "doc": "Storage size in bytes",
+                "name": "sizeInBytes",
                 "type": [
                     "null",
-                    {
-                        "items": "com.linkedin.pegasus2avro.dataset.FineGrainedLineage",
-                        "type": "array"
-                    }
+                    "long"
                 ]
             }
         ],
-        "name": "DataJobInputOutput",
-        "namespace": "com.linkedin.pegasus2avro.datajob",
+        "name": "DatasetProfile",
+        "namespace": "com.linkedin.pegasus2avro.dataset",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "editableDataJobProperties"
+            "name": "editableDatasetProperties"
         },
-        "doc": "Stores editable changes made to properties. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines",
+        "doc": "EditableDatasetProperties stores editable changes made to dataset properties. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines",
         "fields": [
             {
                 "default": {
                     "actor": "urn:li:corpuser:unknown",
                     "impersonator": null,
                     "message": null,
                     "time": 0
@@ -9661,29 +7214,29 @@
             },
             {
                 "Searchable": {
                     "fieldName": "editedDescription",
                     "fieldType": "TEXT"
                 },
                 "default": null,
-                "doc": "Edited documentation of the data job ",
+                "doc": "Documentation of the dataset",
                 "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             }
         ],
-        "name": "EditableDataJobProperties",
-        "namespace": "com.linkedin.pegasus2avro.datajob",
+        "name": "EditableDatasetProperties",
+        "namespace": "com.linkedin.pegasus2avro.dataset",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "editableDataFlowProperties"
+            "name": "editableDashboardProperties"
         },
         "doc": "Stores editable changes made to properties. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines",
         "fields": [
             {
                 "default": {
                     "actor": "urn:li:corpuser:unknown",
                     "impersonator": null,
@@ -9716,31 +7269,200 @@
             },
             {
                 "Searchable": {
                     "fieldName": "editedDescription",
                     "fieldType": "TEXT"
                 },
                 "default": null,
-                "doc": "Edited documentation of the data flow",
+                "doc": "Edited documentation of the dashboard",
                 "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             }
         ],
-        "name": "EditableDataFlowProperties",
-        "namespace": "com.linkedin.pegasus2avro.datajob",
+        "name": "EditableDashboardProperties",
+        "namespace": "com.linkedin.pegasus2avro.dashboard",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dataFlowInfo"
+            "name": "dashboardUsageStatistics",
+            "type": "timeseries"
         },
-        "doc": "Information about a Data processing flow",
+        "doc": "Experimental (Subject to breaking change) -- Stats corresponding to dashboard's usage.\n\nIf this aspect represents the latest snapshot of the statistics about a Dashboard, the eventGranularity field should be null. \nIf this aspect represents a bucketed window of usage statistics (e.g. over a day), then the eventGranularity field should be set accordingly. ",
+        "fields": [
+            {
+                "doc": "The event timestamp field as epoch at UTC in milli seconds.",
+                "name": "timestampMillis",
+                "type": "long"
+            },
+            {
+                "default": null,
+                "doc": "Granularity of the event if applicable",
+                "name": "eventGranularity",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
+                ]
+            },
+            {
+                "default": {
+                    "partition": "FULL_TABLE_SNAPSHOT",
+                    "timePartition": null,
+                    "type": "FULL_TABLE"
+                },
+                "doc": "The optional partition specification.",
+                "name": "partitionSpec",
+                "type": [
+                    "com.linkedin.pegasus2avro.timeseries.PartitionSpec",
+                    "null"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
+                "name": "messageId",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "TimeseriesField": {},
+                "default": null,
+                "doc": "The total number of times dashboard has been viewed",
+                "name": "viewsCount",
+                "type": [
+                    "null",
+                    "int"
+                ]
+            },
+            {
+                "TimeseriesField": {},
+                "default": null,
+                "doc": "The total number of dashboard executions (refreshes / syncs) ",
+                "name": "executionsCount",
+                "type": [
+                    "null",
+                    "int"
+                ]
+            },
+            {
+                "TimeseriesField": {},
+                "default": null,
+                "doc": "Unique user count",
+                "name": "uniqueUserCount",
+                "type": [
+                    "null",
+                    "int"
+                ]
+            },
+            {
+                "TimeseriesFieldCollection": {
+                    "key": "user"
+                },
+                "default": null,
+                "doc": "Users within this bucket, with frequency counts",
+                "name": "userCounts",
+                "type": [
+                    "null",
+                    {
+                        "items": {
+                            "doc": "Records a single user's usage counts for a given resource",
+                            "fields": [
+                                {
+                                    "Urn": "Urn",
+                                    "doc": "The unique id of the user.",
+                                    "java": {
+                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                    },
+                                    "name": "user",
+                                    "type": "string"
+                                },
+                                {
+                                    "TimeseriesField": {},
+                                    "default": null,
+                                    "doc": "The number of times the user has viewed the dashboard",
+                                    "name": "viewsCount",
+                                    "type": [
+                                        "null",
+                                        "int"
+                                    ]
+                                },
+                                {
+                                    "TimeseriesField": {},
+                                    "default": null,
+                                    "doc": "The number of times the user has executed (refreshed) the dashboard",
+                                    "name": "executionsCount",
+                                    "type": [
+                                        "null",
+                                        "int"
+                                    ]
+                                },
+                                {
+                                    "TimeseriesField": {},
+                                    "default": null,
+                                    "doc": "Normalized numeric metric representing user's dashboard usage -- the number of times the user executed or viewed the dashboard. ",
+                                    "name": "usageCount",
+                                    "type": [
+                                        "null",
+                                        "int"
+                                    ]
+                                },
+                                {
+                                    "TimeseriesField": {},
+                                    "default": null,
+                                    "doc": "If user_email is set, we attempt to resolve the user's urn upon ingest",
+                                    "name": "userEmail",
+                                    "type": [
+                                        "null",
+                                        "string"
+                                    ]
+                                }
+                            ],
+                            "name": "DashboardUserUsageCounts",
+                            "namespace": "com.linkedin.pegasus2avro.dashboard",
+                            "type": "record"
+                        },
+                        "type": "array"
+                    }
+                ]
+            },
+            {
+                "TimeseriesField": {},
+                "default": null,
+                "doc": "The total number of times that the dashboard has been favorited ",
+                "name": "favoritesCount",
+                "type": [
+                    "null",
+                    "int"
+                ]
+            },
+            {
+                "TimeseriesField": {},
+                "default": null,
+                "doc": "Last viewed at\n\nThis should not be set in cases where statistics are windowed. ",
+                "name": "lastViewedAt",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            }
+        ],
+        "name": "DashboardUsageStatistics",
+        "namespace": "com.linkedin.pegasus2avro.dashboard",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "dashboardInfo"
+        },
+        "doc": "Information about a dashboard",
         "fields": [
             {
                 "Searchable": {
                     "/*": {
                         "queryByDefault": true
                     }
                 },
@@ -9767,808 +7489,1061 @@
             },
             {
                 "Searchable": {
                     "boostScore": 10.0,
                     "enableAutocomplete": true,
                     "fieldType": "TEXT_PARTIAL"
                 },
-                "doc": "Flow name",
-                "name": "name",
+                "doc": "Title of the dashboard",
+                "name": "title",
                 "type": "string"
             },
             {
                 "Searchable": {
                     "fieldType": "TEXT",
                     "hasValuesFieldName": "hasDescription"
                 },
-                "default": null,
-                "doc": "Flow description",
+                "doc": "Detailed description about the dashboard",
                 "name": "description",
+                "type": "string"
+            },
+            {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "chart"
+                        ],
+                        "isLineage": true,
+                        "name": "Contains"
+                    }
+                },
+                "Urn": "ChartUrn",
+                "default": [],
+                "deprecated": true,
+                "doc": "Charts in a dashboard\nDeprecated! Use chartEdges instead.",
+                "name": "charts",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
+            },
+            {
+                "Relationship": {
+                    "/*/destinationUrn": {
+                        "createdActor": "chartEdges/*/created/actor",
+                        "createdOn": "chartEdges/*/created/time",
+                        "entityTypes": [
+                            "chart"
+                        ],
+                        "isLineage": true,
+                        "name": "Contains",
+                        "properties": "chartEdges/*/properties",
+                        "updatedActor": "chartEdges/*/lastModified/actor",
+                        "updatedOn": "chartEdges/*/lastModified/time"
+                    }
+                },
+                "default": null,
+                "doc": "Charts in a dashboard",
+                "name": "chartEdges",
                 "type": [
                     "null",
-                    "string"
+                    {
+                        "items": "com.linkedin.pegasus2avro.common.Edge",
+                        "type": "array"
+                    }
                 ]
             },
             {
-                "Searchable": {
-                    "fieldType": "TEXT_PARTIAL",
-                    "queryByDefault": false
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "dataset"
+                        ],
+                        "isLineage": true,
+                        "name": "Consumes"
+                    }
+                },
+                "Urn": "Urn",
+                "default": [],
+                "deprecated": true,
+                "doc": "Datasets consumed by a dashboard\nDeprecated! Use datasetEdges instead.",
+                "name": "datasets",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
+            },
+            {
+                "Relationship": {
+                    "/*/destinationUrn": {
+                        "createdActor": "datasetEdges/*/created/actor",
+                        "createdOn": "datasetEdges/*/created/time",
+                        "entityTypes": [
+                            "dataset"
+                        ],
+                        "isLineage": true,
+                        "name": "Consumes",
+                        "properties": "datasetEdges/*/properties",
+                        "updatedActor": "datasetEdges/*/lastModified/actor",
+                        "updatedOn": "datasetEdges/*/lastModified/time"
+                    }
                 },
                 "default": null,
-                "doc": "Optional project/namespace associated with the flow",
-                "name": "project",
+                "doc": "Datasets consumed by a dashboard",
+                "name": "datasetEdges",
+                "type": [
+                    "null",
+                    {
+                        "items": "com.linkedin.pegasus2avro.common.Edge",
+                        "type": "array"
+                    }
+                ]
+            },
+            {
+                "doc": "Captures information about who created/last modified/deleted this dashboard and when",
+                "name": "lastModified",
+                "type": "com.linkedin.pegasus2avro.common.ChangeAuditStamps"
+            },
+            {
+                "default": null,
+                "doc": "URL for the dashboard. This could be used as an external link on DataHub to allow users access/view the dashboard",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "dashboardUrl",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "Searchable": {
-                    "/time": {
-                        "fieldName": "createdAt",
-                        "fieldType": "DATETIME"
-                    }
+                    "addToFilters": true,
+                    "fieldType": "KEYWORD",
+                    "filterNameOverride": "Access Level"
                 },
                 "default": null,
-                "doc": "A timestamp documenting when the asset was created in the source Data Platform (not on DataHub)",
-                "name": "created",
+                "doc": "Access level for the dashboard",
+                "name": "access",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.common.TimeStamp"
+                    "com.linkedin.pegasus2avro.common.AccessLevel"
                 ]
             },
             {
+                "default": null,
+                "doc": "The time when this dashboard last refreshed",
+                "name": "lastRefreshed",
+                "type": [
+                    "null",
+                    "long"
+                ]
+            }
+        ],
+        "name": "DashboardInfo",
+        "namespace": "com.linkedin.pegasus2avro.dashboard",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "dataHubRetentionConfig"
+        },
+        "fields": [
+            {
+                "name": "retention",
+                "type": {
+                    "doc": "Base class that encapsulates different retention policies.\nOnly one of the fields should be set",
+                    "fields": [
+                        {
+                            "default": null,
+                            "name": "version",
+                            "type": [
+                                "null",
+                                {
+                                    "doc": "Keep max N latest records",
+                                    "fields": [
+                                        {
+                                            "name": "maxVersions",
+                                            "type": "int"
+                                        }
+                                    ],
+                                    "name": "VersionBasedRetention",
+                                    "namespace": "com.linkedin.pegasus2avro.retention",
+                                    "type": "record"
+                                }
+                            ]
+                        },
+                        {
+                            "default": null,
+                            "name": "time",
+                            "type": [
+                                "null",
+                                {
+                                    "doc": "Keep records that are less than X seconds old",
+                                    "fields": [
+                                        {
+                                            "name": "maxAgeInSeconds",
+                                            "type": "int"
+                                        }
+                                    ],
+                                    "name": "TimeBasedRetention",
+                                    "namespace": "com.linkedin.pegasus2avro.retention",
+                                    "type": "record"
+                                }
+                            ]
+                        }
+                    ],
+                    "name": "Retention",
+                    "namespace": "com.linkedin.pegasus2avro.retention",
+                    "type": "record"
+                }
+            }
+        ],
+        "name": "DataHubRetentionConfig",
+        "namespace": "com.linkedin.pegasus2avro.retention",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "corpUserStatus"
+        },
+        "doc": "The status of the user, e.g. provisioned, active, suspended, etc.",
+        "fields": [
+            {
                 "Searchable": {
-                    "/time": {
-                        "fieldName": "lastModifiedAt",
-                        "fieldType": "DATETIME"
-                    }
+                    "fieldType": "KEYWORD"
                 },
-                "default": null,
-                "doc": "A timestamp documenting when the asset was last modified in the source Data Platform (not on DataHub)",
+                "doc": "Status of the user, e.g. PROVISIONED / ACTIVE / SUSPENDED",
+                "name": "status",
+                "type": "string"
+            },
+            {
+                "doc": "Audit stamp containing who last modified the status and when.",
                 "name": "lastModified",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.common.TimeStamp"
-                ]
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
             }
         ],
-        "name": "DataFlowInfo",
-        "namespace": "com.linkedin.pegasus2avro.datajob",
+        "name": "CorpUserStatus",
+        "namespace": "com.linkedin.pegasus2avro.identity",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "datahubIngestionRunSummary",
-            "type": "timeseries"
+            "EntityUrns": [
+                "com.linkedin.pegasus2avro.common.CorpuserUrn"
+            ],
+            "name": "corpUserCredentials"
         },
-        "doc": "Summary of a datahub ingestion run for a given platform.",
+        "doc": "Corp user credentials",
         "fields": [
             {
-                "doc": "The event timestamp field as epoch at UTC in milli seconds.",
-                "name": "timestampMillis",
-                "type": "long"
+                "doc": "Salt used to hash password",
+                "name": "salt",
+                "type": "string"
+            },
+            {
+                "doc": "Hashed password generated by concatenating salt and password, then hashing",
+                "name": "hashedPassword",
+                "type": "string"
             },
             {
                 "default": null,
-                "doc": "Granularity of the event if applicable",
-                "name": "eventGranularity",
+                "doc": "Optional token needed to reset a user's password. Can only be set by the admin.",
+                "name": "passwordResetToken",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
+                    "string"
                 ]
             },
             {
-                "default": {
-                    "partition": "FULL_TABLE_SNAPSHOT",
-                    "timePartition": null,
-                    "type": "FULL_TABLE"
-                },
-                "doc": "The optional partition specification.",
-                "name": "partitionSpec",
+                "default": null,
+                "doc": "When the password reset token expires.",
+                "name": "passwordResetTokenExpirationTimeMillis",
                 "type": [
-                    "com.linkedin.pegasus2avro.timeseries.PartitionSpec",
-                    "null"
+                    "null",
+                    "long"
                 ]
-            },
+            }
+        ],
+        "name": "CorpUserCredentials",
+        "namespace": "com.linkedin.pegasus2avro.identity",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "EntityUrns": [
+                "com.linkedin.pegasus2avro.common.CorpuserUrn"
+            ],
+            "name": "corpUserEditableInfo"
+        },
+        "doc": "Linkedin corp user information that can be edited from UI",
+        "fields": [
             {
                 "default": null,
-                "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
-                "name": "messageId",
+                "doc": "About me section of the user",
+                "name": "aboutMe",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
-                "TimeseriesField": {},
-                "doc": "The name of the pipeline that ran ingestion, a stable unique user provided identifier.\n e.g. my_snowflake1-to-datahub.",
-                "name": "pipelineName",
-                "type": "string"
+                "Searchable": {
+                    "/*": {
+                        "fieldType": "TEXT"
+                    }
+                },
+                "default": [],
+                "doc": "Teams that the user belongs to e.g. Metadata",
+                "name": "teams",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                }
             },
             {
-                "TimeseriesField": {},
-                "doc": "The id of the instance against which the ingestion pipeline ran.\ne.g.: Bigquery project ids, MySQL hostnames etc.",
-                "name": "platformInstanceId",
-                "type": "string"
+                "Searchable": {
+                    "/*": {
+                        "fieldType": "TEXT"
+                    }
+                },
+                "default": [],
+                "doc": "Skills that the user possesses e.g. Machine Learning",
+                "name": "skills",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                }
             },
             {
-                "TimeseriesField": {},
-                "doc": "The runId for this pipeline instance.",
-                "name": "runId",
+                "default": "https://raw.githubusercontent.com/datahub-project/datahub/master/datahub-web-react/src/images/default_avatar.png",
+                "doc": "A URL which points to a picture which user wants to set as a profile photo",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "pictureLink",
                 "type": "string"
             },
             {
-                "TimeseriesField": {},
-                "doc": "Run Status - Succeeded/Skipped/Failed etc.",
-                "name": "runStatus",
-                "type": "com.linkedin.pegasus2avro.datajob.JobStatus"
-            },
-            {
+                "Searchable": {
+                    "boostScore": 10.0,
+                    "fieldType": "TEXT_PARTIAL",
+                    "queryByDefault": true
+                },
                 "default": null,
-                "doc": "The number of workunits written to sink.",
-                "name": "numWorkUnitsCommitted",
+                "doc": "DataHub-native display name",
+                "name": "displayName",
                 "type": [
                     "null",
-                    "long"
+                    "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "The number of workunits that are produced.",
-                "name": "numWorkUnitsCreated",
+                "doc": "DataHub-native Title, e.g. 'Software Engineer'",
+                "name": "title",
                 "type": [
                     "null",
-                    "long"
+                    "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "The number of events produced (MCE + MCP).",
-                "name": "numEvents",
+                "doc": "Slack handle for the user",
+                "name": "slack",
                 "type": [
                     "null",
-                    "long"
+                    "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "The total number of entities produced (unique entity urns).",
-                "name": "numEntities",
+                "doc": "Phone number to contact the user",
+                "name": "phone",
                 "type": [
                     "null",
-                    "long"
+                    "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "The total number of aspects produced across all entities.",
-                "name": "numAspects",
+                "doc": "Email address to contact the user",
+                "name": "email",
                 "type": [
                     "null",
-                    "long"
+                    "string"
                 ]
+            }
+        ],
+        "name": "CorpUserEditableInfo",
+        "namespace": "com.linkedin.pegasus2avro.identity",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "inviteToken"
+        },
+        "doc": "Aspect used to store invite tokens.",
+        "fields": [
+            {
+                "doc": "The encrypted invite token.",
+                "name": "token",
+                "type": "string"
             },
             {
+                "Searchable": {
+                    "fieldName": "role",
+                    "fieldType": "KEYWORD",
+                    "hasValuesFieldName": "hasRole"
+                },
+                "Urn": "Urn",
                 "default": null,
-                "doc": "Total number of source API calls.",
-                "name": "numSourceAPICalls",
+                "doc": "The role that this invite token may be associated with",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                },
+                "name": "role",
                 "type": [
                     "null",
-                    "long"
+                    "string"
                 ]
+            }
+        ],
+        "name": "InviteToken",
+        "namespace": "com.linkedin.pegasus2avro.identity",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "EntityUrns": [
+                "com.linkedin.pegasus2avro.common.CorpuserUrn"
+            ],
+            "name": "corpUserInfo"
+        },
+        "doc": "Linkedin corp user information",
+        "fields": [
+            {
+                "Searchable": {
+                    "/*": {
+                        "queryByDefault": true
+                    }
+                },
+                "default": {},
+                "doc": "Custom property bag.",
+                "name": "customProperties",
+                "type": {
+                    "type": "map",
+                    "values": "string"
+                }
+            },
+            {
+                "Searchable": {
+                    "fieldType": "BOOLEAN",
+                    "weightsPerFieldValue": {
+                        "true": 2.0
+                    }
+                },
+                "doc": "Deprecated! Use CorpUserStatus instead. Whether the corpUser is active, ref: https://iwww.corp.linkedin.com/wiki/cf/display/GTSD/Accessing+Active+Directory+via+LDAP+tools",
+                "name": "active",
+                "type": "boolean"
             },
             {
+                "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL",
+                    "queryByDefault": true
+                },
                 "default": null,
-                "doc": "Total latency across all source API calls.",
-                "name": "totalLatencySourceAPICalls",
+                "doc": "displayName of this user ,  e.g.  Hang Zhang(DataHQ)",
+                "name": "displayName",
                 "type": [
                     "null",
-                    "long"
+                    "string"
                 ]
             },
             {
+                "Searchable": {
+                    "fieldType": "KEYWORD",
+                    "queryByDefault": true
+                },
                 "default": null,
-                "doc": "Total number of sink API calls.",
-                "name": "numSinkAPICalls",
+                "doc": "email address of this user",
+                "name": "email",
                 "type": [
                     "null",
-                    "long"
+                    "string"
                 ]
             },
             {
+                "Searchable": {
+                    "fieldType": "KEYWORD",
+                    "queryByDefault": true
+                },
                 "default": null,
-                "doc": "Total latency across all sink API calls.",
-                "name": "totalLatencySinkAPICalls",
+                "doc": "title of this user",
+                "name": "title",
                 "type": [
                     "null",
-                    "long"
+                    "string"
                 ]
             },
             {
+                "Relationship": {
+                    "entityTypes": [
+                        "corpuser"
+                    ],
+                    "name": "ReportsTo"
+                },
+                "Searchable": {
+                    "fieldName": "managerLdap",
+                    "fieldType": "URN",
+                    "queryByDefault": true
+                },
+                "Urn": "CorpuserUrn",
                 "default": null,
-                "doc": "Number of warnings generated.",
-                "name": "numWarnings",
+                "doc": "direct manager of this user",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.CorpuserUrn"
+                },
+                "name": "managerUrn",
                 "type": [
                     "null",
-                    "long"
+                    "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "Number of errors generated.",
-                "name": "numErrors",
+                "doc": "department id this user belong to",
+                "name": "departmentId",
                 "type": [
                     "null",
                     "long"
                 ]
             },
             {
                 "default": null,
-                "doc": "Number of entities skipped.",
-                "name": "numEntitiesSkipped",
+                "doc": "department name this user belong to",
+                "name": "departmentName",
                 "type": [
                     "null",
-                    "long"
+                    "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "The non-sensitive key-value pairs of the yaml config used as json string.",
-                "name": "config",
+                "doc": "first name of this user",
+                "name": "firstName",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "Custom value.",
-                "name": "custom_summary",
+                "doc": "last name of this user",
+                "name": "lastName",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
-                "TimeseriesField": {},
+                "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL",
+                    "queryByDefault": true
+                },
                 "default": null,
-                "doc": "The software version of this ingestion.",
-                "name": "softwareVersion",
+                "doc": "Common name of this user, format is firstName + lastName (split by a whitespace)",
+                "name": "fullName",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "The hostname the ingestion pipeline ran on.",
-                "name": "systemHostName",
+                "doc": "two uppercase letters country code. e.g.  US",
+                "name": "countryCode",
                 "type": [
                     "null",
                     "string"
                 ]
+            }
+        ],
+        "name": "CorpUserInfo",
+        "namespace": "com.linkedin.pegasus2avro.identity",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "roleMembership"
+        },
+        "doc": "Carries information about which roles a user is assigned to.",
+        "fields": [
+            {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "dataHubRole"
+                        ],
+                        "name": "IsMemberOfRole"
+                    }
+                },
+                "Urn": "Urn",
+                "name": "roles",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
+            }
+        ],
+        "name": "RoleMembership",
+        "namespace": "com.linkedin.pegasus2avro.identity",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "groupMembership"
+        },
+        "doc": "Carries information about the CorpGroups a user is in.",
+        "fields": [
+            {
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "corpGroup"
+                        ],
+                        "name": "IsMemberOfGroup"
+                    }
+                },
+                "Urn": "Urn",
+                "name": "groups",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
+            }
+        ],
+        "name": "GroupMembership",
+        "namespace": "com.linkedin.pegasus2avro.identity",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "corpUserSettings"
+        },
+        "doc": "Settings that a user can customize through the datahub ui",
+        "fields": [
+            {
+                "doc": "Settings for a user around the appearance of their DataHub U",
+                "name": "appearance",
+                "type": {
+                    "doc": "Settings for a user around the appearance of their DataHub UI",
+                    "fields": [
+                        {
+                            "default": null,
+                            "doc": "Flag whether the user should see a homepage with only datasets, charts and dashboards. Intended for users\nwho have less operational use cases for the datahub tool.",
+                            "name": "showSimplifiedHomepage",
+                            "type": [
+                                "null",
+                                "boolean"
+                            ]
+                        }
+                    ],
+                    "name": "CorpUserAppearanceSettings",
+                    "namespace": "com.linkedin.pegasus2avro.identity",
+                    "type": "record"
+                }
             },
             {
-                "TimeseriesField": {},
                 "default": null,
-                "doc": "The os the ingestion pipeline ran on.",
-                "name": "operatingSystemName",
+                "doc": "User preferences for the Views feature.",
+                "name": "views",
                 "type": [
                     "null",
-                    "string"
+                    {
+                        "doc": "Settings related to the 'Views' feature.",
+                        "fields": [
+                            {
+                                "Urn": "Urn",
+                                "default": null,
+                                "doc": "The default View which is selected for the user.\nIf none is chosen, then this value will be left blank.",
+                                "java": {
+                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                },
+                                "name": "defaultView",
+                                "type": [
+                                    "null",
+                                    "string"
+                                ]
+                            }
+                        ],
+                        "name": "CorpUserViewsSettings",
+                        "namespace": "com.linkedin.pegasus2avro.identity",
+                        "type": "record"
+                    }
                 ]
-            },
+            }
+        ],
+        "name": "CorpUserSettings",
+        "namespace": "com.linkedin.pegasus2avro.identity",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "corpGroupEditableInfo"
+        },
+        "doc": "Group information that can be edited from UI",
+        "fields": [
             {
+                "Searchable": {
+                    "fieldName": "editedDescription",
+                    "fieldType": "TEXT"
+                },
                 "default": null,
-                "doc": "The number of processors on the host the ingestion pipeline ran on.",
-                "name": "numProcessors",
+                "doc": "A description of the group",
+                "name": "description",
                 "type": [
                     "null",
-                    "int"
+                    "string"
                 ]
             },
             {
+                "default": "https://raw.githubusercontent.com/datahub-project/datahub/master/datahub-web-react/src/images/default_avatar.png",
+                "doc": "A URL which points to a picture which user wants to set as the photo for the group",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "pictureLink",
+                "type": "string"
+            },
+            {
                 "default": null,
-                "doc": "The total amount of memory on the host the ingestion pipeline ran on.",
-                "name": "totalMemory",
+                "doc": "Slack channel for the group",
+                "name": "slack",
                 "type": [
                     "null",
-                    "long"
+                    "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "The available memory on the host the ingestion pipeline ran on.",
-                "name": "availableMemory",
+                "doc": "Email address to contact the group",
+                "name": "email",
                 "type": [
                     "null",
-                    "long"
+                    "string"
                 ]
             }
         ],
-        "name": "DatahubIngestionRunSummary",
-        "namespace": "com.linkedin.pegasus2avro.datajob.datahub",
+        "name": "CorpGroupEditableInfo",
+        "namespace": "com.linkedin.pegasus2avro.identity",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "datahubIngestionCheckpoint",
-            "type": "timeseries"
+            "name": "nativeGroupMembership"
         },
-        "doc": "Checkpoint of a datahub ingestion run for a given job.",
+        "doc": "Carries information about the native CorpGroups a user is in.",
         "fields": [
             {
-                "doc": "The event timestamp field as epoch at UTC in milli seconds.",
-                "name": "timestampMillis",
-                "type": "long"
-            },
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "corpGroup"
+                        ],
+                        "name": "IsMemberOfNativeGroup"
+                    }
+                },
+                "Urn": "Urn",
+                "name": "nativeGroups",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
+            }
+        ],
+        "name": "NativeGroupMembership",
+        "namespace": "com.linkedin.pegasus2avro.identity",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "EntityUrns": [
+                "com.linkedin.pegasus2avro.common.CorpGroupUrn"
+            ],
+            "name": "corpGroupInfo"
+        },
+        "doc": "Information about a Corp Group ingested from a third party source",
+        "fields": [
             {
+                "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL",
+                    "queryByDefault": true
+                },
                 "default": null,
-                "doc": "Granularity of the event if applicable",
-                "name": "eventGranularity",
+                "doc": "The name of the group.",
+                "name": "displayName",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
-                ]
-            },
-            {
-                "default": {
-                    "partition": "FULL_TABLE_SNAPSHOT",
-                    "timePartition": null,
-                    "type": "FULL_TABLE"
-                },
-                "doc": "The optional partition specification.",
-                "name": "partitionSpec",
-                "type": [
-                    "com.linkedin.pegasus2avro.timeseries.PartitionSpec",
-                    "null"
+                    "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
-                "name": "messageId",
+                "doc": "email of this group",
+                "name": "email",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
-                "TimeseriesField": {},
-                "doc": "The name of the pipeline that ran ingestion, a stable unique user provided identifier.\n e.g. my_snowflake1-to-datahub.",
-                "name": "pipelineName",
-                "type": "string"
-            },
-            {
-                "TimeseriesField": {},
-                "doc": "The id of the instance against which the ingestion pipeline ran.\ne.g.: Bigquery project ids, MySQL hostnames etc.",
-                "name": "platformInstanceId",
-                "type": "string"
-            },
-            {
-                "doc": "Json-encoded string representation of the non-secret members of the config .",
-                "name": "config",
-                "type": "string"
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "corpuser"
+                        ],
+                        "name": "OwnedBy"
+                    }
+                },
+                "Urn": "CorpuserUrn",
+                "deprecated": true,
+                "doc": "owners of this group\nDeprecated! Replaced by Ownership aspect.",
+                "name": "admins",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
             },
             {
-                "doc": "Opaque blob of the state representation.",
-                "name": "state",
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "corpuser"
+                        ],
+                        "name": "IsPartOf"
+                    }
+                },
+                "Urn": "CorpuserUrn",
+                "deprecated": true,
+                "doc": "List of ldap urn in this group.\nDeprecated! Replaced by GroupMembership aspect.",
+                "name": "members",
                 "type": {
-                    "doc": "The checkpoint state object of a datahub ingestion run for a given job.",
-                    "fields": [
-                        {
-                            "doc": "The version of the state format.",
-                            "name": "formatVersion",
-                            "type": "string"
-                        },
-                        {
-                            "doc": "The serialization/deserialization protocol.",
-                            "name": "serde",
-                            "type": "string"
-                        },
-                        {
-                            "default": null,
-                            "doc": "Opaque blob of the state representation.",
-                            "name": "payload",
-                            "type": [
-                                "null",
-                                "bytes"
-                            ]
-                        }
-                    ],
-                    "name": "IngestionCheckpointState",
-                    "namespace": "com.linkedin.pegasus2avro.datajob.datahub",
-                    "type": "record"
-                }
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
             },
             {
-                "TimeseriesField": {},
-                "doc": "The run identifier of this job.",
-                "name": "runId",
-                "type": "string"
-            }
-        ],
-        "name": "DatahubIngestionCheckpoint",
-        "namespace": "com.linkedin.pegasus2avro.datajob.datahub",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "dataPlatformInfo"
-        },
-        "doc": "Information about a data platform",
-        "fields": [
-            {
-                "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": false,
-                    "fieldType": "TEXT_PARTIAL"
-                },
-                "doc": "Name of the data platform",
-                "name": "name",
-                "type": "string",
-                "validate": {
-                    "strlen": {
-                        "max": 15
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "corpGroup"
+                        ],
+                        "name": "IsPartOf"
                     }
-                }
+                },
+                "Urn": "CorpGroupUrn",
+                "deprecated": true,
+                "doc": "List of groups in this group.\nDeprecated! This field is unused.",
+                "name": "groups",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
             },
             {
                 "Searchable": {
-                    "boostScore": 10.0,
-                    "enableAutocomplete": true,
                     "fieldType": "TEXT_PARTIAL"
                 },
                 "default": null,
-                "doc": "The name that will be used for displaying a platform type.",
-                "name": "displayName",
+                "doc": "A description of the group.",
+                "name": "description",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
-                "doc": "Platform type this data platform describes",
-                "name": "type",
-                "type": {
-                    "doc": "Platform types available at LinkedIn",
-                    "name": "PlatformType",
-                    "namespace": "com.linkedin.pegasus2avro.dataplatform",
-                    "symbolDocs": {
-                        "FILE_SYSTEM": "Value for a file system, e.g. hdfs",
-                        "KEY_VALUE_STORE": "Value for a key value store, e.g. espresso, voldemort",
-                        "MESSAGE_BROKER": "Value for a message broker, e.g. kafka",
-                        "OBJECT_STORE": "Value for an object store, e.g. ambry",
-                        "OLAP_DATASTORE": "Value for an OLAP datastore, e.g. pinot",
-                        "OTHERS": "Value for other platforms, e.g salesforce, dovetail",
-                        "QUERY_ENGINE": "Value for a query engine, e.g. presto",
-                        "RELATIONAL_DB": "Value for a relational database, e.g. oracle, mysql",
-                        "SEARCH_ENGINE": "Value for a search engine, e.g seas"
-                    },
-                    "symbols": [
-                        "FILE_SYSTEM",
-                        "KEY_VALUE_STORE",
-                        "MESSAGE_BROKER",
-                        "OBJECT_STORE",
-                        "OLAP_DATASTORE",
-                        "OTHERS",
-                        "QUERY_ENGINE",
-                        "RELATIONAL_DB",
-                        "SEARCH_ENGINE"
-                    ],
-                    "type": "enum"
-                }
-            },
-            {
-                "doc": "The delimiter in the dataset names on the data platform, e.g. '/' for HDFS and '.' for Oracle",
-                "name": "datasetNameDelimiter",
-                "type": "string"
+                "default": null,
+                "doc": "Slack channel for the group",
+                "name": "slack",
+                "type": [
+                    "null",
+                    "string"
+                ]
             },
             {
-                "default": null,
-                "doc": "The URL for a logo associated with the platform",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.url.Url",
-                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                "Searchable": {
+                    "/time": {
+                        "fieldName": "createdTime",
+                        "fieldType": "DATETIME"
+                    }
                 },
-                "name": "logoUrl",
+                "default": null,
+                "doc": "Created Audit stamp",
+                "name": "created",
                 "type": [
                     "null",
-                    "string"
+                    "com.linkedin.pegasus2avro.common.AuditStamp"
                 ]
             }
         ],
-        "name": "DataPlatformInfo",
-        "namespace": "com.linkedin.pegasus2avro.dataplatform",
+        "name": "CorpGroupInfo",
+        "namespace": "com.linkedin.pegasus2avro.identity",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dataHubRoleInfo"
+            "name": "dataHubAccessTokenInfo"
         },
-        "doc": "Information about a DataHub Role.",
+        "doc": "Information about a DataHub Access Token",
         "fields": [
             {
                 "Searchable": {
                     "fieldType": "TEXT_PARTIAL"
                 },
-                "doc": "Name of the Role",
+                "doc": "User defined name for the access token if defined.",
                 "name": "name",
                 "type": "string"
             },
             {
                 "Searchable": {
-                    "fieldType": "TEXT"
+                    "fieldType": "URN"
                 },
-                "doc": "Description of the Role",
-                "name": "description",
+                "Urn": "Urn",
+                "doc": "Urn of the actor to which this access token belongs to.",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                },
+                "name": "actorUrn",
                 "type": "string"
             },
             {
-                "default": false,
-                "doc": "Whether the role should be editable via the UI",
-                "name": "editable",
-                "type": "boolean"
-            }
-        ],
-        "name": "DataHubRoleInfo",
-        "namespace": "com.linkedin.pegasus2avro.policy",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "dataHubPolicyInfo"
-        },
-        "doc": "Information about a DataHub (UI) access policy.",
-        "fields": [
-            {
                 "Searchable": {
-                    "fieldType": "TEXT_PARTIAL"
+                    "fieldType": "URN"
                 },
-                "doc": "Display name of the Policy",
-                "name": "displayName",
+                "Urn": "Urn",
+                "doc": "Urn of the actor which created this access token.",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                },
+                "name": "ownerUrn",
                 "type": "string"
             },
             {
                 "Searchable": {
-                    "fieldType": "TEXT"
+                    "fieldType": "COUNT",
+                    "queryByDefault": false
                 },
-                "doc": "Description of the Policy",
-                "name": "description",
-                "type": "string"
-            },
-            {
-                "doc": "The type of policy",
-                "name": "type",
-                "type": "string"
+                "doc": "When the token was created.",
+                "name": "createdAt",
+                "type": "long"
             },
             {
-                "doc": "The state of policy, ACTIVE or INACTIVE",
-                "name": "state",
-                "type": "string"
+                "Searchable": {
+                    "fieldType": "COUNT",
+                    "queryByDefault": false
+                },
+                "default": null,
+                "doc": "When the token expires.",
+                "name": "expiresAt",
+                "type": [
+                    "null",
+                    "long"
+                ]
             },
             {
                 "default": null,
-                "doc": "The resource that the policy applies to. Not required for some 'Platform' privileges.",
-                "name": "resources",
+                "doc": "Description of the token if defined.",
+                "name": "description",
                 "type": [
                     "null",
-                    {
-                        "doc": "Information used to filter DataHub resource.",
-                        "fields": [
-                            {
-                                "default": null,
-                                "deprecated": true,
-                                "doc": "The type of resource that the policy applies to. This will most often be a data asset entity name, for\nexample 'dataset'. It is not strictly required because in the future we will want to support filtering a resource\nby domain, as well.",
-                                "name": "type",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "deprecated": true,
-                                "doc": "A specific set of resources to apply the policy to, e.g. asset urns",
-                                "name": "resources",
-                                "type": [
-                                    "null",
-                                    {
-                                        "items": "string",
-                                        "type": "array"
-                                    }
-                                ]
-                            },
-                            {
-                                "default": false,
-                                "deprecated": true,
-                                "doc": "Whether the policy should be applied to all assets matching the filter.",
-                                "name": "allResources",
-                                "type": "boolean"
-                            },
-                            {
-                                "default": null,
-                                "doc": "Filter to apply privileges to",
-                                "name": "filter",
-                                "type": [
-                                    "null",
-                                    {
-                                        "doc": "The filter for specifying the resource or actor to apply privileges to",
-                                        "fields": [
-                                            {
-                                                "doc": "A list of criteria to apply conjunctively (so all criteria must pass)",
-                                                "name": "criteria",
-                                                "type": {
-                                                    "items": {
-                                                        "doc": "A criterion for matching a field with given value",
-                                                        "fields": [
-                                                            {
-                                                                "doc": "The name of the field that the criterion refers to",
-                                                                "name": "field",
-                                                                "type": "string"
-                                                            },
-                                                            {
-                                                                "doc": "Values. Matches criterion if any one of the values matches condition (OR-relationship)",
-                                                                "name": "values",
-                                                                "type": {
-                                                                    "items": "string",
-                                                                    "type": "array"
-                                                                }
-                                                            },
-                                                            {
-                                                                "default": "EQUALS",
-                                                                "doc": "The condition for the criterion",
-                                                                "name": "condition",
-                                                                "type": {
-                                                                    "doc": "The matching condition in a filter criterion",
-                                                                    "name": "PolicyMatchCondition",
-                                                                    "namespace": "com.linkedin.pegasus2avro.policy",
-                                                                    "symbolDocs": {
-                                                                        "EQUALS": "Whether the field matches the value"
-                                                                    },
-                                                                    "symbols": [
-                                                                        "EQUALS"
-                                                                    ],
-                                                                    "type": "enum"
-                                                                }
-                                                            }
-                                                        ],
-                                                        "name": "PolicyMatchCriterion",
-                                                        "namespace": "com.linkedin.pegasus2avro.policy",
-                                                        "type": "record"
-                                                    },
-                                                    "type": "array"
-                                                }
-                                            }
-                                        ],
-                                        "name": "PolicyMatchFilter",
-                                        "namespace": "com.linkedin.pegasus2avro.policy",
-                                        "type": "record"
-                                    }
-                                ]
-                            }
-                        ],
-                        "name": "DataHubResourceFilter",
-                        "namespace": "com.linkedin.pegasus2avro.policy",
-                        "type": "record"
-                    }
+                    "string"
                 ]
-            },
+            }
+        ],
+        "name": "DataHubAccessTokenInfo",
+        "namespace": "com.linkedin.pegasus2avro.access.token",
+        "type": "record"
+    },
+    {
+        "doc": "A DataHub Platform Event.",
+        "fields": [
             {
-                "doc": "The privileges that the policy grants.",
-                "name": "privileges",
+                "doc": "Header information stored with the event.",
+                "name": "header",
                 "type": {
-                    "items": "string",
-                    "type": "array"
+                    "doc": "A header included with each DataHub platform event.",
+                    "fields": [
+                        {
+                            "doc": "The event timestamp field as epoch at UTC in milli seconds.",
+                            "name": "timestampMillis",
+                            "type": "long"
+                        }
+                    ],
+                    "name": "PlatformEventHeader",
+                    "namespace": "com.linkedin.pegasus2avro.mxe",
+                    "type": "record"
                 }
             },
             {
-                "doc": "The actors that the policy applies to.",
-                "name": "actors",
+                "doc": "The name of the event, e.g. the type of event. For example, 'notificationRequestEvent', 'entityChangeEvent'",
+                "name": "name",
+                "type": "string"
+            },
+            {
+                "doc": "The event payload.",
+                "name": "payload",
                 "type": {
-                    "doc": "Information used to filter DataHub actors.",
+                    "doc": "Generic payload record structure for serializing a Platform Event.",
                     "fields": [
                         {
-                            "default": null,
-                            "doc": "A specific set of users to apply the policy to (disjunctive)",
-                            "name": "users",
-                            "type": [
-                                "null",
-                                {
-                                    "items": "string",
-                                    "type": "array"
-                                }
-                            ]
-                        },
-                        {
-                            "default": null,
-                            "doc": "A specific set of groups to apply the policy to (disjunctive)",
-                            "name": "groups",
-                            "type": [
-                                "null",
-                                {
-                                    "items": "string",
-                                    "type": "array"
-                                }
-                            ]
-                        },
-                        {
-                            "default": false,
-                            "doc": "Whether the filter should return true for owners of a particular resource.\nOnly applies to policies of type 'Metadata', which have a resource associated with them.",
-                            "name": "resourceOwners",
-                            "type": "boolean"
-                        },
-                        {
-                            "default": false,
-                            "doc": "Whether the filter should apply to all users.",
-                            "name": "allUsers",
-                            "type": "boolean"
-                        },
-                        {
-                            "default": false,
-                            "doc": "Whether the filter should apply to all groups.",
-                            "name": "allGroups",
-                            "type": "boolean"
+                            "doc": "The value of the event, serialized as bytes.",
+                            "name": "value",
+                            "type": "bytes"
                         },
                         {
-                            "Relationship": {
-                                "/*": {
-                                    "entityTypes": [
-                                        "dataHubRole"
-                                    ],
-                                    "name": "IsAssociatedWithRole"
-                                }
-                            },
-                            "default": null,
-                            "doc": "A specific set of roles to apply the policy to (disjunctive).",
-                            "name": "roles",
-                            "type": [
-                                "null",
-                                {
-                                    "items": "string",
-                                    "type": "array"
-                                }
-                            ]
+                            "doc": "The content type, which represents the fashion in which the event was serialized.\nThe only type currently supported is application/json.",
+                            "name": "contentType",
+                            "type": "string"
                         }
                     ],
-                    "name": "DataHubActorFilter",
-                    "namespace": "com.linkedin.pegasus2avro.policy",
+                    "name": "GenericPayload",
+                    "namespace": "com.linkedin.pegasus2avro.mxe",
                     "type": "record"
                 }
-            },
-            {
-                "default": true,
-                "doc": "Whether the policy should be editable via the UI",
-                "name": "editable",
-                "type": "boolean"
-            },
-            {
-                "Searchable": {
-                    "fieldType": "DATETIME"
-                },
-                "default": null,
-                "doc": "Timestamp when the policy was last updated",
-                "name": "lastUpdatedTimestamp",
-                "type": [
-                    "null",
-                    "long"
-                ]
             }
         ],
-        "name": "DataHubPolicyInfo",
-        "namespace": "com.linkedin.pegasus2avro.policy",
+        "name": "PlatformEvent",
+        "namespace": "com.linkedin.pegasus2avro.mxe",
         "type": "record"
     },
     {
-        "doc": "Kafka event for proposing a metadata change for an entity. A corresponding MetadataAuditEvent is emitted when the change is accepted and committed, otherwise a FailedMetadataChangeEvent will be emitted instead.",
+        "doc": "Kafka event for capturing update made to an entity's metadata.",
         "fields": [
             {
                 "default": null,
-                "doc": "Kafka audit header. See go/kafkaauditheader for more info.",
+                "doc": "Kafka audit header. Currently remains unused in the open source.",
                 "name": "auditHeader",
                 "type": [
                     "null",
                     {
                         "doc": "This header records information about the context of an event as it is emitted into kafka and is intended to be used by the kafka audit application.  For more information see go/kafkaauditheader",
                         "fields": [
                             {
@@ -10648,38 +8623,260 @@
                         "name": "KafkaAuditHeader",
                         "namespace": "com.linkedin.events",
                         "type": "record"
                     }
                 ]
             },
             {
+                "doc": "Type of the entity being written to",
+                "name": "entityType",
+                "type": "string"
+            },
+            {
+                "Urn": "Urn",
+                "default": null,
+                "doc": "Urn of the entity being written",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                },
+                "name": "entityUrn",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Key aspect of the entity being written",
+                "name": "entityKeyAspect",
+                "type": [
+                    "null",
+                    {
+                        "doc": "Generic record structure for serializing an Aspect",
+                        "fields": [
+                            {
+                                "doc": "The value of the aspect, serialized as bytes.",
+                                "name": "value",
+                                "type": "bytes"
+                            },
+                            {
+                                "doc": "The content type, which represents the fashion in which the aspect was serialized.\nThe only type currently supported is application/json.",
+                                "name": "contentType",
+                                "type": "string"
+                            }
+                        ],
+                        "name": "GenericAspect",
+                        "namespace": "com.linkedin.pegasus2avro.mxe",
+                        "type": "record"
+                    }
+                ]
+            },
+            {
+                "doc": "Type of change being proposed",
+                "name": "changeType",
+                "type": {
+                    "doc": "Descriptor for a change action",
+                    "name": "ChangeType",
+                    "namespace": "com.linkedin.pegasus2avro.events.metadata",
+                    "symbolDocs": {
+                        "CREATE": "NOT SUPPORTED YET\ninsert if not exists. otherwise fail",
+                        "DELETE": "NOT SUPPORTED YET\ndelete action",
+                        "PATCH": "NOT SUPPORTED YET\npatch the changes instead of full replace",
+                        "RESTATE": "Restate an aspect, eg. in a index refresh.",
+                        "UPDATE": "NOT SUPPORTED YET\nupdate if exists. otherwise fail",
+                        "UPSERT": "insert if not exists. otherwise update"
+                    },
+                    "symbols": [
+                        "UPSERT",
+                        "CREATE",
+                        "UPDATE",
+                        "DELETE",
+                        "PATCH",
+                        "RESTATE"
+                    ],
+                    "type": "enum"
+                }
+            },
+            {
+                "default": null,
+                "doc": "Aspect of the entity being written to\nNot filling this out implies that the writer wants to affect the entire entity\nNote: This is only valid for CREATE, UPSERT, and DELETE operations.",
+                "name": "aspectName",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "The value of the new aspect.",
+                "name": "aspect",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.mxe.GenericAspect"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "A string->string map of custom properties that one might want to attach to an event",
+                "name": "systemMetadata",
+                "type": [
+                    "null",
+                    {
+                        "doc": "Metadata associated with each metadata change that is processed by the system",
+                        "fields": [
+                            {
+                                "default": 0,
+                                "doc": "The timestamp the metadata was observed at",
+                                "name": "lastObserved",
+                                "type": [
+                                    "long",
+                                    "null"
+                                ]
+                            },
+                            {
+                                "default": "no-run-id-provided",
+                                "doc": "The run id that produced the metadata. Populated in case of batch-ingestion.",
+                                "name": "runId",
+                                "type": [
+                                    "string",
+                                    "null"
+                                ]
+                            },
+                            {
+                                "default": null,
+                                "doc": "The model registry name that was used to process this event",
+                                "name": "registryName",
+                                "type": [
+                                    "null",
+                                    "string"
+                                ]
+                            },
+                            {
+                                "default": null,
+                                "doc": "The model registry version that was used to process this event",
+                                "name": "registryVersion",
+                                "type": [
+                                    "null",
+                                    "string"
+                                ]
+                            },
+                            {
+                                "default": null,
+                                "doc": "Additional properties",
+                                "name": "properties",
+                                "type": [
+                                    "null",
+                                    {
+                                        "type": "map",
+                                        "values": "string"
+                                    }
+                                ]
+                            }
+                        ],
+                        "name": "SystemMetadata",
+                        "namespace": "com.linkedin.pegasus2avro.mxe",
+                        "type": "record"
+                    }
+                ]
+            },
+            {
+                "default": null,
+                "doc": "The previous value of the aspect that has changed.",
+                "name": "previousAspectValue",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.mxe.GenericAspect"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "The previous value of the system metadata field that has changed.",
+                "name": "previousSystemMetadata",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.mxe.SystemMetadata"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "An audit stamp detailing who and when the aspect was changed by. Required for all intents and purposes.",
+                "name": "created",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.common.AuditStamp"
+                ]
+            }
+        ],
+        "name": "MetadataChangeLog",
+        "namespace": "com.linkedin.pegasus2avro.mxe",
+        "type": "record"
+    },
+    {
+        "doc": "Kafka event for proposing a metadata change for an entity. A corresponding MetadataAuditEvent is emitted when the change is accepted and committed, otherwise a FailedMetadataChangeEvent will be emitted instead.",
+        "fields": [
+            {
+                "default": null,
+                "doc": "Kafka audit header. See go/kafkaauditheader for more info.",
+                "name": "auditHeader",
+                "type": [
+                    "null",
+                    "com.linkedin.events.KafkaAuditHeader"
+                ]
+            },
+            {
                 "doc": "Snapshot of the proposed metadata change. Include only the aspects affected by the change in the snapshot.",
                 "name": "proposedSnapshot",
                 "type": [
                     {
                         "Entity": {
                             "keyAspect": "chartKey",
                             "name": "chart"
                         },
                         "doc": "A metadata snapshot for a specific Chart entity.",
                         "fields": [
                             {
+                                "Urn": "ChartUrn",
                                 "doc": "URN for the entity the metadata snapshot is associated with.",
                                 "java": {
                                     "class": "com.linkedin.pegasus2avro.common.urn.ChartUrn"
                                 },
                                 "name": "urn",
                                 "type": "string"
                             },
                             {
                                 "doc": "The list of metadata aspects associated with the chart. Depending on the use case, this can either be all, or a selection, of supported aspects.",
                                 "name": "aspects",
                                 "type": {
                                     "items": [
-                                        "com.linkedin.pegasus2avro.metadata.key.ChartKey",
+                                        {
+                                            "Aspect": {
+                                                "name": "chartKey"
+                                            },
+                                            "doc": "Key for a Chart",
+                                            "fields": [
+                                                {
+                                                    "Searchable": {
+                                                        "boostScore": 4.0,
+                                                        "fieldName": "tool",
+                                                        "fieldType": "TEXT_PARTIAL"
+                                                    },
+                                                    "doc": "The name of the dashboard tool such as looker, redash etc.",
+                                                    "name": "dashboardTool",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "doc": "Unique id for the chart. This id should be globally unique for a dashboarding tool even when there are multiple deployments of it. As an example, chart URL could be used here for Looker such as 'looker.linkedin.com/looks/1234'",
+                                                    "name": "chartId",
+                                                    "type": "string"
+                                                }
+                                            ],
+                                            "name": "ChartKey",
+                                            "namespace": "com.linkedin.pegasus2avro.metadata.key",
+                                            "type": "record"
+                                        },
                                         "com.linkedin.pegasus2avro.chart.ChartInfo",
                                         "com.linkedin.pegasus2avro.chart.ChartQuery",
                                         "com.linkedin.pegasus2avro.chart.EditableChartProperties",
                                         "com.linkedin.pegasus2avro.common.Ownership",
                                         "com.linkedin.pegasus2avro.common.Status",
                                         "com.linkedin.pegasus2avro.common.GlobalTags",
                                         "com.linkedin.pegasus2avro.common.BrowsePaths",
@@ -10699,27 +8896,49 @@
                         "Entity": {
                             "keyAspect": "corpGroupKey",
                             "name": "corpGroup"
                         },
                         "doc": "A metadata snapshot for a specific CorpGroup entity.",
                         "fields": [
                             {
+                                "Urn": "CorpGroupUrn",
                                 "doc": "URN for the entity the metadata snapshot is associated with.",
                                 "java": {
                                     "class": "com.linkedin.pegasus2avro.common.urn.CorpGroupUrn"
                                 },
                                 "name": "urn",
                                 "type": "string"
                             },
                             {
                                 "doc": "The list of metadata aspects associated with the LdapUser. Depending on the use case, this can either be all, or a selection, of supported aspects.",
                                 "name": "aspects",
                                 "type": {
                                     "items": [
-                                        "com.linkedin.pegasus2avro.metadata.key.CorpGroupKey",
+                                        {
+                                            "Aspect": {
+                                                "name": "corpGroupKey"
+                                            },
+                                            "doc": "Key for a CorpGroup",
+                                            "fields": [
+                                                {
+                                                    "Searchable": {
+                                                        "boostScore": 10.0,
+                                                        "enableAutocomplete": true,
+                                                        "fieldType": "TEXT_PARTIAL",
+                                                        "queryByDefault": true
+                                                    },
+                                                    "doc": "The URL-encoded name of the AD/LDAP group. Serves as a globally unique identifier within DataHub.",
+                                                    "name": "name",
+                                                    "type": "string"
+                                                }
+                                            ],
+                                            "name": "CorpGroupKey",
+                                            "namespace": "com.linkedin.pegasus2avro.metadata.key",
+                                            "type": "record"
+                                        },
                                         "com.linkedin.pegasus2avro.identity.CorpGroupInfo",
                                         "com.linkedin.pegasus2avro.common.GlobalTags",
                                         "com.linkedin.pegasus2avro.common.Status"
                                     ],
                                     "type": "array"
                                 }
                             }
@@ -10732,27 +8951,49 @@
                         "Entity": {
                             "keyAspect": "corpUserKey",
                             "name": "corpuser"
                         },
                         "doc": "A metadata snapshot for a specific CorpUser entity.",
                         "fields": [
                             {
+                                "Urn": "CorpuserUrn",
                                 "doc": "URN for the entity the metadata snapshot is associated with.",
                                 "java": {
                                     "class": "com.linkedin.pegasus2avro.common.urn.CorpuserUrn"
                                 },
                                 "name": "urn",
                                 "type": "string"
                             },
                             {
                                 "doc": "The list of metadata aspects associated with the CorpUser. Depending on the use case, this can either be all, or a selection, of supported aspects.",
                                 "name": "aspects",
                                 "type": {
                                     "items": [
-                                        "com.linkedin.pegasus2avro.metadata.key.CorpUserKey",
+                                        {
+                                            "Aspect": {
+                                                "name": "corpUserKey"
+                                            },
+                                            "doc": "Key for a CorpUser",
+                                            "fields": [
+                                                {
+                                                    "Searchable": {
+                                                        "boostScore": 2.0,
+                                                        "enableAutocomplete": true,
+                                                        "fieldName": "ldap",
+                                                        "fieldType": "TEXT_PARTIAL"
+                                                    },
+                                                    "doc": "The name of the AD/LDAP user.",
+                                                    "name": "username",
+                                                    "type": "string"
+                                                }
+                                            ],
+                                            "name": "CorpUserKey",
+                                            "namespace": "com.linkedin.pegasus2avro.metadata.key",
+                                            "type": "record"
+                                        },
                                         "com.linkedin.pegasus2avro.identity.CorpUserInfo",
                                         "com.linkedin.pegasus2avro.identity.CorpUserEditableInfo",
                                         "com.linkedin.pegasus2avro.identity.CorpUserStatus",
                                         "com.linkedin.pegasus2avro.identity.GroupMembership",
                                         "com.linkedin.pegasus2avro.common.GlobalTags",
                                         "com.linkedin.pegasus2avro.common.Status"
                                     ],
@@ -10768,27 +9009,53 @@
                         "Entity": {
                             "keyAspect": "dashboardKey",
                             "name": "dashboard"
                         },
                         "doc": "A metadata snapshot for a specific Dashboard entity.",
                         "fields": [
                             {
+                                "Urn": "DashboardUrn",
                                 "doc": "URN for the entity the metadata snapshot is associated with.",
                                 "java": {
                                     "class": "com.linkedin.pegasus2avro.common.urn.DashboardUrn"
                                 },
                                 "name": "urn",
                                 "type": "string"
                             },
                             {
                                 "doc": "The list of metadata aspects associated with the dashboard. Depending on the use case, this can either be all, or a selection, of supported aspects.",
                                 "name": "aspects",
                                 "type": {
                                     "items": [
-                                        "com.linkedin.pegasus2avro.metadata.key.DashboardKey",
+                                        {
+                                            "Aspect": {
+                                                "name": "dashboardKey"
+                                            },
+                                            "doc": "Key for a Dashboard",
+                                            "fields": [
+                                                {
+                                                    "Searchable": {
+                                                        "boostScore": 4.0,
+                                                        "fieldName": "tool",
+                                                        "fieldType": "TEXT_PARTIAL"
+                                                    },
+                                                    "doc": "The name of the dashboard tool such as looker, redash etc.",
+                                                    "name": "dashboardTool",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "doc": "Unique id for the dashboard. This id should be globally unique for a dashboarding tool even when there are multiple deployments of it. As an example, dashboard URL could be used here for Looker such as 'looker.linkedin.com/dashboards/1234'",
+                                                    "name": "dashboardId",
+                                                    "type": "string"
+                                                }
+                                            ],
+                                            "name": "DashboardKey",
+                                            "namespace": "com.linkedin.pegasus2avro.metadata.key",
+                                            "type": "record"
+                                        },
                                         "com.linkedin.pegasus2avro.dashboard.DashboardInfo",
                                         "com.linkedin.pegasus2avro.dashboard.EditableDashboardProperties",
                                         "com.linkedin.pegasus2avro.common.Ownership",
                                         "com.linkedin.pegasus2avro.common.Status",
                                         "com.linkedin.pegasus2avro.common.GlobalTags",
                                         "com.linkedin.pegasus2avro.common.BrowsePaths",
                                         "com.linkedin.pegasus2avro.common.GlossaryTerms",
@@ -10807,27 +9074,63 @@
                         "Entity": {
                             "keyAspect": "dataFlowKey",
                             "name": "dataFlow"
                         },
                         "doc": "A metadata snapshot for a specific DataFlow entity.",
                         "fields": [
                             {
+                                "Urn": "DataFlowUrn",
                                 "doc": "URN for the entity the metadata snapshot is associated with.",
                                 "java": {
                                     "class": "com.linkedin.pegasus2avro.common.urn.DataFlowUrn"
                                 },
                                 "name": "urn",
                                 "type": "string"
                             },
                             {
                                 "doc": "The list of metadata aspects associated with the data flow. Depending on the use case, this can either be all, or a selection, of supported aspects.",
                                 "name": "aspects",
                                 "type": {
                                     "items": [
-                                        "com.linkedin.pegasus2avro.metadata.key.DataFlowKey",
+                                        {
+                                            "Aspect": {
+                                                "name": "dataFlowKey"
+                                            },
+                                            "doc": "Key for a Data Flow",
+                                            "fields": [
+                                                {
+                                                    "Searchable": {
+                                                        "fieldType": "TEXT_PARTIAL"
+                                                    },
+                                                    "doc": "Workflow manager like azkaban, airflow which orchestrates the flow",
+                                                    "name": "orchestrator",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "Searchable": {
+                                                        "enableAutocomplete": true,
+                                                        "fieldType": "TEXT_PARTIAL"
+                                                    },
+                                                    "doc": "Unique Identifier of the data flow",
+                                                    "name": "flowId",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "Searchable": {
+                                                        "fieldType": "TEXT_PARTIAL"
+                                                    },
+                                                    "doc": "Cluster where the flow is executed",
+                                                    "name": "cluster",
+                                                    "type": "string"
+                                                }
+                                            ],
+                                            "name": "DataFlowKey",
+                                            "namespace": "com.linkedin.pegasus2avro.metadata.key",
+                                            "type": "record"
+                                        },
                                         "com.linkedin.pegasus2avro.datajob.DataFlowInfo",
                                         "com.linkedin.pegasus2avro.datajob.EditableDataFlowProperties",
                                         "com.linkedin.pegasus2avro.common.Ownership",
                                         "com.linkedin.pegasus2avro.common.Status",
                                         "com.linkedin.pegasus2avro.common.GlobalTags",
                                         "com.linkedin.pegasus2avro.common.BrowsePaths",
                                         "com.linkedin.pegasus2avro.common.GlossaryTerms",
@@ -10846,27 +9149,67 @@
                         "Entity": {
                             "keyAspect": "dataJobKey",
                             "name": "dataJob"
                         },
                         "doc": "A metadata snapshot for a specific DataJob entity.",
                         "fields": [
                             {
+                                "Urn": "DataJobUrn",
                                 "doc": "URN for the entity the metadata snapshot is associated with.",
                                 "java": {
                                     "class": "com.linkedin.pegasus2avro.common.urn.DataJobUrn"
                                 },
                                 "name": "urn",
                                 "type": "string"
                             },
                             {
                                 "doc": "The list of metadata aspects associated with the data job. Depending on the use case, this can either be all, or a selection, of supported aspects.",
                                 "name": "aspects",
                                 "type": {
                                     "items": [
-                                        "com.linkedin.pegasus2avro.metadata.key.DataJobKey",
+                                        {
+                                            "Aspect": {
+                                                "name": "dataJobKey"
+                                            },
+                                            "doc": "Key for a Data Job",
+                                            "fields": [
+                                                {
+                                                    "Relationship": {
+                                                        "entityTypes": [
+                                                            "dataFlow"
+                                                        ],
+                                                        "name": "IsPartOf"
+                                                    },
+                                                    "Searchable": {
+                                                        "fieldName": "dataFlow",
+                                                        "fieldType": "URN_PARTIAL",
+                                                        "queryByDefault": false
+                                                    },
+                                                    "Urn": "Urn",
+                                                    "doc": "Standardized data processing flow urn representing the flow for the job",
+                                                    "java": {
+                                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                                    },
+                                                    "name": "flow",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "Searchable": {
+                                                        "enableAutocomplete": true,
+                                                        "fieldType": "TEXT_PARTIAL"
+                                                    },
+                                                    "doc": "Unique Identifier of the data job",
+                                                    "name": "jobId",
+                                                    "type": "string"
+                                                }
+                                            ],
+                                            "name": "DataJobKey",
+                                            "namespace": "com.linkedin.pegasus2avro.metadata.key",
+                                            "type": "record"
+                                        },
                                         "com.linkedin.pegasus2avro.datajob.DataJobInfo",
                                         "com.linkedin.pegasus2avro.datajob.DataJobInputOutput",
                                         "com.linkedin.pegasus2avro.datajob.EditableDataJobProperties",
                                         "com.linkedin.pegasus2avro.common.Ownership",
                                         "com.linkedin.pegasus2avro.common.Status",
                                         "com.linkedin.pegasus2avro.common.GlobalTags",
                                         "com.linkedin.pegasus2avro.common.BrowsePaths",
@@ -10886,27 +9229,102 @@
                         "Entity": {
                             "keyAspect": "datasetKey",
                             "name": "dataset"
                         },
                         "doc": "A metadata snapshot for a specific dataset entity.",
                         "fields": [
                             {
+                                "Urn": "DatasetUrn",
                                 "doc": "URN for the entity the metadata snapshot is associated with.",
                                 "java": {
                                     "class": "com.linkedin.pegasus2avro.common.urn.DatasetUrn"
                                 },
                                 "name": "urn",
                                 "type": "string"
                             },
                             {
                                 "doc": "The list of metadata aspects associated with the dataset. Depending on the use case, this can either be all, or a selection, of supported aspects.",
                                 "name": "aspects",
                                 "type": {
                                     "items": [
-                                        "com.linkedin.pegasus2avro.metadata.key.DatasetKey",
+                                        {
+                                            "Aspect": {
+                                                "name": "datasetKey"
+                                            },
+                                            "doc": "Key for a Dataset",
+                                            "fields": [
+                                                {
+                                                    "Searchable": {
+                                                        "enableAutocomplete": true,
+                                                        "fieldType": "URN"
+                                                    },
+                                                    "Urn": "Urn",
+                                                    "doc": "Data platform urn associated with the dataset",
+                                                    "java": {
+                                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                                    },
+                                                    "name": "platform",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "Searchable": {
+                                                        "boostScore": 10.0,
+                                                        "enableAutocomplete": true,
+                                                        "fieldName": "id",
+                                                        "fieldType": "TEXT_PARTIAL"
+                                                    },
+                                                    "doc": "Unique guid for dataset",
+                                                    "name": "name",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "Searchable": {
+                                                        "addToFilters": true,
+                                                        "fieldType": "TEXT_PARTIAL",
+                                                        "filterNameOverride": "Environment",
+                                                        "queryByDefault": false
+                                                    },
+                                                    "doc": "Fabric type where dataset belongs to or where it was generated.",
+                                                    "name": "origin",
+                                                    "type": {
+                                                        "doc": "Fabric group type",
+                                                        "name": "FabricType",
+                                                        "namespace": "com.linkedin.pegasus2avro.common",
+                                                        "symbolDocs": {
+                                                            "CORP": "Designates corporation fabrics",
+                                                            "DEV": "Designates development fabrics",
+                                                            "EI": "Designates early-integration fabrics",
+                                                            "NON_PROD": "Designates non-production fabrics",
+                                                            "PRE": "Designates pre-production fabrics",
+                                                            "PROD": "Designates production fabrics",
+                                                            "QA": "Designates quality assurance fabrics",
+                                                            "STG": "Designates staging fabrics",
+                                                            "TEST": "Designates testing fabrics",
+                                                            "UAT": "Designates user acceptance testing fabrics"
+                                                        },
+                                                        "symbols": [
+                                                            "DEV",
+                                                            "TEST",
+                                                            "QA",
+                                                            "UAT",
+                                                            "EI",
+                                                            "PRE",
+                                                            "STG",
+                                                            "NON_PROD",
+                                                            "PROD",
+                                                            "CORP"
+                                                        ],
+                                                        "type": "enum"
+                                                    }
+                                                }
+                                            ],
+                                            "name": "DatasetKey",
+                                            "namespace": "com.linkedin.pegasus2avro.metadata.key",
+                                            "type": "record"
+                                        },
                                         "com.linkedin.pegasus2avro.dataset.DatasetProperties",
                                         "com.linkedin.pegasus2avro.dataset.EditableDatasetProperties",
                                         "com.linkedin.pegasus2avro.dataset.DatasetDeprecation",
                                         "com.linkedin.pegasus2avro.dataset.DatasetUpstreamLineage",
                                         "com.linkedin.pegasus2avro.dataset.UpstreamLineage",
                                         "com.linkedin.pegasus2avro.common.InstitutionalMemory",
                                         "com.linkedin.pegasus2avro.common.Ownership",
@@ -10932,29 +9350,140 @@
                             "keyAspect": "dataProcessKey",
                             "name": "dataProcess"
                         },
                         "deprecated": "Use DataJob instead.",
                         "doc": "A metadata snapshot for a specific Data process entity.",
                         "fields": [
                             {
+                                "Urn": "DataProcessUrn",
                                 "doc": "URN for the entity the metadata snapshot is associated with.",
                                 "java": {
                                     "class": "com.linkedin.pegasus2avro.common.urn.DataProcessUrn"
                                 },
                                 "name": "urn",
                                 "type": "string"
                             },
                             {
                                 "doc": "The list of metadata aspects associated with the data process. Depending on the use case, this can either be all, or a selection, of supported aspects.",
                                 "name": "aspects",
                                 "type": {
                                     "items": [
-                                        "com.linkedin.pegasus2avro.metadata.key.DataProcessKey",
+                                        {
+                                            "Aspect": {
+                                                "name": "dataProcessKey"
+                                            },
+                                            "doc": "Key for a Data Process",
+                                            "fields": [
+                                                {
+                                                    "Searchable": {
+                                                        "boostScore": 4.0,
+                                                        "enableAutocomplete": true,
+                                                        "fieldType": "TEXT_PARTIAL"
+                                                    },
+                                                    "doc": "Process name i.e. an ETL job name",
+                                                    "name": "name",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "Searchable": {
+                                                        "enableAutocomplete": true,
+                                                        "fieldType": "TEXT_PARTIAL"
+                                                    },
+                                                    "doc": "Standardized Orchestrator where data process is defined.\nTODO: Migrate towards something that can be validated like DataPlatform urn",
+                                                    "name": "orchestrator",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "Searchable": {
+                                                        "fieldType": "TEXT_PARTIAL",
+                                                        "queryByDefault": false
+                                                    },
+                                                    "doc": "Fabric type where dataset belongs to or where it was generated.",
+                                                    "name": "origin",
+                                                    "type": "com.linkedin.pegasus2avro.common.FabricType"
+                                                }
+                                            ],
+                                            "name": "DataProcessKey",
+                                            "namespace": "com.linkedin.pegasus2avro.metadata.key",
+                                            "type": "record"
+                                        },
                                         "com.linkedin.pegasus2avro.common.Ownership",
-                                        "com.linkedin.pegasus2avro.dataprocess.DataProcessInfo",
+                                        {
+                                            "Aspect": {
+                                                "name": "dataProcessInfo"
+                                            },
+                                            "doc": "The inputs and outputs of this data process",
+                                            "fields": [
+                                                {
+                                                    "Relationship": {
+                                                        "/*": {
+                                                            "entityTypes": [
+                                                                "dataset"
+                                                            ],
+                                                            "isLineage": true,
+                                                            "name": "Consumes"
+                                                        }
+                                                    },
+                                                    "Searchable": {
+                                                        "/*": {
+                                                            "fieldName": "inputs",
+                                                            "fieldType": "URN",
+                                                            "numValuesFieldName": "numInputDatasets",
+                                                            "queryByDefault": false
+                                                        }
+                                                    },
+                                                    "Urn": "DatasetUrn",
+                                                    "default": null,
+                                                    "doc": "the inputs of the data process",
+                                                    "name": "inputs",
+                                                    "type": [
+                                                        "null",
+                                                        {
+                                                            "items": "string",
+                                                            "type": "array"
+                                                        }
+                                                    ],
+                                                    "urn_is_array": true
+                                                },
+                                                {
+                                                    "Relationship": {
+                                                        "/*": {
+                                                            "entityTypes": [
+                                                                "dataset"
+                                                            ],
+                                                            "isLineage": true,
+                                                            "name": "Consumes"
+                                                        }
+                                                    },
+                                                    "Searchable": {
+                                                        "/*": {
+                                                            "fieldName": "outputs",
+                                                            "fieldType": "URN",
+                                                            "numValuesFieldName": "numOutputDatasets",
+                                                            "queryByDefault": false
+                                                        }
+                                                    },
+                                                    "Urn": "DatasetUrn",
+                                                    "default": null,
+                                                    "doc": "the outputs of the data process",
+                                                    "name": "outputs",
+                                                    "type": [
+                                                        "null",
+                                                        {
+                                                            "items": "string",
+                                                            "type": "array"
+                                                        }
+                                                    ],
+                                                    "urn_is_array": true
+                                                }
+                                            ],
+                                            "name": "DataProcessInfo",
+                                            "namespace": "com.linkedin.pegasus2avro.dataprocess",
+                                            "type": "record"
+                                        },
                                         "com.linkedin.pegasus2avro.common.Status"
                                     ],
                                     "type": "array"
                                 }
                             }
                         ],
                         "name": "DataProcessSnapshot",
@@ -10965,27 +9494,43 @@
                         "Entity": {
                             "keyAspect": "dataPlatformKey",
                             "name": "dataPlatform"
                         },
                         "doc": "A metadata snapshot for a specific dataplatform entity.",
                         "fields": [
                             {
+                                "Urn": "DataPlatformUrn",
                                 "doc": "URN for the entity the metadata snapshot is associated with.",
                                 "java": {
                                     "class": "com.linkedin.pegasus2avro.common.urn.DataPlatformUrn"
                                 },
                                 "name": "urn",
                                 "type": "string"
                             },
                             {
                                 "doc": "The list of metadata aspects associated with the data platform. Depending on the use case, this can either be all, or a selection, of supported aspects.",
                                 "name": "aspects",
                                 "type": {
                                     "items": [
-                                        "com.linkedin.pegasus2avro.metadata.key.DataPlatformKey",
+                                        {
+                                            "Aspect": {
+                                                "name": "dataPlatformKey"
+                                            },
+                                            "doc": "Key for a Data Platform",
+                                            "fields": [
+                                                {
+                                                    "doc": "Data platform name i.e. hdfs, oracle, espresso",
+                                                    "name": "platformName",
+                                                    "type": "string"
+                                                }
+                                            ],
+                                            "name": "DataPlatformKey",
+                                            "namespace": "com.linkedin.pegasus2avro.metadata.key",
+                                            "type": "record"
+                                        },
                                         "com.linkedin.pegasus2avro.dataplatform.DataPlatformInfo"
                                     ],
                                     "type": "array"
                                 }
                             }
                         ],
                         "name": "DataPlatformSnapshot",
@@ -10996,27 +9541,68 @@
                         "Entity": {
                             "keyAspect": "mlModelKey",
                             "name": "mlModel"
                         },
                         "doc": "MLModel Snapshot entity details.",
                         "fields": [
                             {
+                                "Urn": "MLModelUrn",
                                 "doc": "URN for the entity the metadata snapshot is associated with.",
                                 "java": {
                                     "class": "com.linkedin.pegasus2avro.common.urn.MLModelUrn"
                                 },
                                 "name": "urn",
                                 "type": "string"
                             },
                             {
                                 "doc": "The list of metadata aspects associated with the MLModel. Depending on the use case, this can either be all, or a selection, of supported aspects.",
                                 "name": "aspects",
                                 "type": {
                                     "items": [
-                                        "com.linkedin.pegasus2avro.metadata.key.MLModelKey",
+                                        {
+                                            "Aspect": {
+                                                "name": "mlModelKey"
+                                            },
+                                            "doc": "Key for an ML model",
+                                            "fields": [
+                                                {
+                                                    "Urn": "Urn",
+                                                    "doc": "Standardized platform urn for the model",
+                                                    "java": {
+                                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                                    },
+                                                    "name": "platform",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "Searchable": {
+                                                        "boostScore": 10.0,
+                                                        "enableAutocomplete": true,
+                                                        "fieldType": "TEXT_PARTIAL"
+                                                    },
+                                                    "doc": "Name of the MLModel",
+                                                    "name": "name",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "Searchable": {
+                                                        "addToFilters": true,
+                                                        "fieldType": "TEXT_PARTIAL",
+                                                        "filterNameOverride": "Environment",
+                                                        "queryByDefault": false
+                                                    },
+                                                    "doc": "Fabric type where model belongs to or where it was generated",
+                                                    "name": "origin",
+                                                    "type": "com.linkedin.pegasus2avro.common.FabricType"
+                                                }
+                                            ],
+                                            "name": "MLModelKey",
+                                            "namespace": "com.linkedin.pegasus2avro.metadata.key",
+                                            "type": "record"
+                                        },
                                         "com.linkedin.pegasus2avro.common.Ownership",
                                         "com.linkedin.pegasus2avro.ml.metadata.MLModelProperties",
                                         "com.linkedin.pegasus2avro.ml.metadata.IntendedUse",
                                         "com.linkedin.pegasus2avro.ml.metadata.MLModelFactorPrompts",
                                         "com.linkedin.pegasus2avro.ml.metadata.Metrics",
                                         "com.linkedin.pegasus2avro.ml.metadata.EvaluationData",
                                         "com.linkedin.pegasus2avro.ml.metadata.TrainingData",
@@ -11043,27 +9629,56 @@
                     {
                         "Entity": {
                             "keyAspect": "mlPrimaryKeyKey",
                             "name": "mlPrimaryKey"
                         },
                         "fields": [
                             {
+                                "Urn": "Urn",
                                 "doc": "URN for the entity the metadata snapshot is associated with.",
                                 "java": {
                                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                 },
                                 "name": "urn",
                                 "type": "string"
                             },
                             {
                                 "doc": "The list of metadata aspects associated with the MLPrimaryKey. Depending on the use case, this can either be all, or a selection, of supported aspects.",
                                 "name": "aspects",
                                 "type": {
                                     "items": [
-                                        "com.linkedin.pegasus2avro.metadata.key.MLPrimaryKeyKey",
+                                        {
+                                            "Aspect": {
+                                                "name": "mlPrimaryKeyKey"
+                                            },
+                                            "doc": "Key for an MLPrimaryKey",
+                                            "fields": [
+                                                {
+                                                    "Searchable": {
+                                                        "fieldType": "TEXT_PARTIAL"
+                                                    },
+                                                    "doc": "Namespace for the primary key",
+                                                    "name": "featureNamespace",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "Searchable": {
+                                                        "boostScore": 8.0,
+                                                        "enableAutocomplete": true,
+                                                        "fieldType": "TEXT_PARTIAL"
+                                                    },
+                                                    "doc": "Name of the primary key",
+                                                    "name": "name",
+                                                    "type": "string"
+                                                }
+                                            ],
+                                            "name": "MLPrimaryKeyKey",
+                                            "namespace": "com.linkedin.pegasus2avro.metadata.key",
+                                            "type": "record"
+                                        },
                                         "com.linkedin.pegasus2avro.ml.metadata.MLPrimaryKeyProperties",
                                         "com.linkedin.pegasus2avro.common.Ownership",
                                         "com.linkedin.pegasus2avro.common.InstitutionalMemory",
                                         "com.linkedin.pegasus2avro.common.Status",
                                         "com.linkedin.pegasus2avro.common.Deprecation",
                                         "com.linkedin.pegasus2avro.common.GlobalTags",
                                         "com.linkedin.pegasus2avro.common.DataPlatformInstance"
@@ -11079,27 +9694,56 @@
                     {
                         "Entity": {
                             "keyAspect": "mlFeatureKey",
                             "name": "mlFeature"
                         },
                         "fields": [
                             {
+                                "Urn": "MLFeatureUrn",
                                 "doc": "URN for the entity the metadata snapshot is associated with.",
                                 "java": {
                                     "class": "com.linkedin.pegasus2avro.common.urn.MLFeatureUrn"
                                 },
                                 "name": "urn",
                                 "type": "string"
                             },
                             {
                                 "doc": "The list of metadata aspects associated with the MLFeature. Depending on the use case, this can either be all, or a selection, of supported aspects.",
                                 "name": "aspects",
                                 "type": {
                                     "items": [
-                                        "com.linkedin.pegasus2avro.metadata.key.MLFeatureKey",
+                                        {
+                                            "Aspect": {
+                                                "name": "mlFeatureKey"
+                                            },
+                                            "doc": "Key for an MLFeature",
+                                            "fields": [
+                                                {
+                                                    "Searchable": {
+                                                        "fieldType": "TEXT_PARTIAL"
+                                                    },
+                                                    "doc": "Namespace for the feature",
+                                                    "name": "featureNamespace",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "Searchable": {
+                                                        "boostScore": 8.0,
+                                                        "enableAutocomplete": true,
+                                                        "fieldType": "TEXT_PARTIAL"
+                                                    },
+                                                    "doc": "Name of the feature",
+                                                    "name": "name",
+                                                    "type": "string"
+                                                }
+                                            ],
+                                            "name": "MLFeatureKey",
+                                            "namespace": "com.linkedin.pegasus2avro.metadata.key",
+                                            "type": "record"
+                                        },
                                         "com.linkedin.pegasus2avro.ml.metadata.MLFeatureProperties",
                                         "com.linkedin.pegasus2avro.common.Ownership",
                                         "com.linkedin.pegasus2avro.common.InstitutionalMemory",
                                         "com.linkedin.pegasus2avro.common.Status",
                                         "com.linkedin.pegasus2avro.common.Deprecation",
                                         "com.linkedin.pegasus2avro.common.BrowsePaths",
                                         "com.linkedin.pegasus2avro.common.GlobalTags",
@@ -11116,27 +9760,63 @@
                     {
                         "Entity": {
                             "keyAspect": "mlFeatureTableKey",
                             "name": "mlFeatureTable"
                         },
                         "fields": [
                             {
+                                "Urn": "Urn",
                                 "doc": "URN for the entity the metadata snapshot is associated with.",
                                 "java": {
                                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                 },
                                 "name": "urn",
                                 "type": "string"
                             },
                             {
                                 "doc": "The list of metadata aspects associated with the MLFeatureTable. Depending on the use case, this can either be all, or a selection, of supported aspects.",
                                 "name": "aspects",
                                 "type": {
                                     "items": [
-                                        "com.linkedin.pegasus2avro.metadata.key.MLFeatureTableKey",
+                                        {
+                                            "Aspect": {
+                                                "name": "mlFeatureTableKey"
+                                            },
+                                            "doc": "Key for an MLFeatureTable",
+                                            "fields": [
+                                                {
+                                                    "Relationship": {
+                                                        "entityTypes": [
+                                                            "dataPlatform"
+                                                        ],
+                                                        "name": "SourcePlatform"
+                                                    },
+                                                    "Urn": "Urn",
+                                                    "doc": "Data platform urn associated with the feature table",
+                                                    "java": {
+                                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                                    },
+                                                    "name": "platform",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "Searchable": {
+                                                        "boostScore": 8.0,
+                                                        "enableAutocomplete": true,
+                                                        "fieldType": "TEXT_PARTIAL"
+                                                    },
+                                                    "doc": "Name of the feature table",
+                                                    "name": "name",
+                                                    "type": "string"
+                                                }
+                                            ],
+                                            "name": "MLFeatureTableKey",
+                                            "namespace": "com.linkedin.pegasus2avro.metadata.key",
+                                            "type": "record"
+                                        },
                                         "com.linkedin.pegasus2avro.ml.metadata.MLFeatureTableProperties",
                                         "com.linkedin.pegasus2avro.common.Ownership",
                                         "com.linkedin.pegasus2avro.common.InstitutionalMemory",
                                         "com.linkedin.pegasus2avro.common.Status",
                                         "com.linkedin.pegasus2avro.common.Deprecation",
                                         "com.linkedin.pegasus2avro.common.BrowsePaths",
                                         "com.linkedin.pegasus2avro.common.GlobalTags",
@@ -11153,27 +9833,68 @@
                     {
                         "Entity": {
                             "keyAspect": "mlModelDeploymentKey",
                             "name": "mlModelDeployment"
                         },
                         "fields": [
                             {
+                                "Urn": "Urn",
                                 "doc": "URN for the entity the metadata snapshot is associated with.",
                                 "java": {
                                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                 },
                                 "name": "urn",
                                 "type": "string"
                             },
                             {
                                 "doc": "The list of metadata aspects associated with the MLModelDeployment. Depending on the use case, this can either be all, or a selection, of supported aspects.",
                                 "name": "aspects",
                                 "type": {
                                     "items": [
-                                        "com.linkedin.pegasus2avro.metadata.key.MLModelDeploymentKey",
+                                        {
+                                            "Aspect": {
+                                                "name": "mlModelDeploymentKey"
+                                            },
+                                            "doc": "Key for an ML model deployment",
+                                            "fields": [
+                                                {
+                                                    "Urn": "Urn",
+                                                    "doc": "Standardized platform urn for the model Deployment",
+                                                    "java": {
+                                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                                    },
+                                                    "name": "platform",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "Searchable": {
+                                                        "boostScore": 10.0,
+                                                        "enableAutocomplete": true,
+                                                        "fieldType": "TEXT_PARTIAL"
+                                                    },
+                                                    "doc": "Name of the MLModelDeployment",
+                                                    "name": "name",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "Searchable": {
+                                                        "addToFilters": true,
+                                                        "fieldType": "TEXT_PARTIAL",
+                                                        "filterNameOverride": "Environment",
+                                                        "queryByDefault": false
+                                                    },
+                                                    "doc": "Fabric type where model Deployment belongs to or where it was generated",
+                                                    "name": "origin",
+                                                    "type": "com.linkedin.pegasus2avro.common.FabricType"
+                                                }
+                                            ],
+                                            "name": "MLModelDeploymentKey",
+                                            "namespace": "com.linkedin.pegasus2avro.metadata.key",
+                                            "type": "record"
+                                        },
                                         "com.linkedin.pegasus2avro.ml.metadata.MLModelDeploymentProperties",
                                         "com.linkedin.pegasus2avro.common.Ownership",
                                         "com.linkedin.pegasus2avro.common.Status",
                                         "com.linkedin.pegasus2avro.common.Deprecation",
                                         "com.linkedin.pegasus2avro.common.GlobalTags",
                                         "com.linkedin.pegasus2avro.common.DataPlatformInstance"
                                     ],
@@ -11188,27 +9909,66 @@
                     {
                         "Entity": {
                             "keyAspect": "mlModelGroupKey",
                             "name": "mlModelGroup"
                         },
                         "fields": [
                             {
+                                "Urn": "Urn",
                                 "doc": "URN for the entity the metadata snapshot is associated with.",
                                 "java": {
                                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                 },
                                 "name": "urn",
                                 "type": "string"
                             },
                             {
                                 "doc": "The list of metadata aspects associated with the MLModelGroup. Depending on the use case, this can either be all, or a selection, of supported aspects.",
                                 "name": "aspects",
                                 "type": {
                                     "items": [
-                                        "com.linkedin.pegasus2avro.metadata.key.MLModelGroupKey",
+                                        {
+                                            "Aspect": {
+                                                "name": "mlModelGroupKey"
+                                            },
+                                            "doc": "Key for an ML model group",
+                                            "fields": [
+                                                {
+                                                    "Urn": "Urn",
+                                                    "doc": "Standardized platform urn for the model group",
+                                                    "java": {
+                                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                                    },
+                                                    "name": "platform",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "Searchable": {
+                                                        "boostScore": 10.0,
+                                                        "enableAutocomplete": true,
+                                                        "fieldType": "TEXT_PARTIAL"
+                                                    },
+                                                    "doc": "Name of the MLModelGroup",
+                                                    "name": "name",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "Searchable": {
+                                                        "fieldType": "TEXT_PARTIAL",
+                                                        "queryByDefault": false
+                                                    },
+                                                    "doc": "Fabric type where model group belongs to or where it was generated",
+                                                    "name": "origin",
+                                                    "type": "com.linkedin.pegasus2avro.common.FabricType"
+                                                }
+                                            ],
+                                            "name": "MLModelGroupKey",
+                                            "namespace": "com.linkedin.pegasus2avro.metadata.key",
+                                            "type": "record"
+                                        },
                                         "com.linkedin.pegasus2avro.ml.metadata.MLModelGroupProperties",
                                         "com.linkedin.pegasus2avro.common.Ownership",
                                         "com.linkedin.pegasus2avro.common.Status",
                                         "com.linkedin.pegasus2avro.common.Deprecation",
                                         "com.linkedin.pegasus2avro.common.BrowsePaths",
                                         "com.linkedin.pegasus2avro.common.GlobalTags",
                                         "com.linkedin.pegasus2avro.common.DataPlatformInstance"
@@ -11225,27 +9985,49 @@
                         "Entity": {
                             "keyAspect": "tagKey",
                             "name": "tag"
                         },
                         "doc": "A metadata snapshot for a specific dataset entity.",
                         "fields": [
                             {
+                                "Urn": "TagUrn",
                                 "doc": "URN for the entity the metadata snapshot is associated with.",
                                 "java": {
                                     "class": "com.linkedin.pegasus2avro.common.urn.TagUrn"
                                 },
                                 "name": "urn",
                                 "type": "string"
                             },
                             {
                                 "doc": "The list of metadata aspects associated with the dataset. Depending on the use case, this can either be all, or a selection, of supported aspects.",
                                 "name": "aspects",
                                 "type": {
                                     "items": [
-                                        "com.linkedin.pegasus2avro.metadata.key.TagKey",
+                                        {
+                                            "Aspect": {
+                                                "name": "tagKey"
+                                            },
+                                            "doc": "Key for a Tag",
+                                            "fields": [
+                                                {
+                                                    "Searchable": {
+                                                        "boostScore": 10.0,
+                                                        "enableAutocomplete": true,
+                                                        "fieldName": "id",
+                                                        "fieldType": "TEXT_PARTIAL"
+                                                    },
+                                                    "doc": "The tag name, which serves as a unique id",
+                                                    "name": "name",
+                                                    "type": "string"
+                                                }
+                                            ],
+                                            "name": "TagKey",
+                                            "namespace": "com.linkedin.pegasus2avro.metadata.key",
+                                            "type": "record"
+                                        },
                                         "com.linkedin.pegasus2avro.common.Ownership",
                                         "com.linkedin.pegasus2avro.tag.TagProperties",
                                         "com.linkedin.pegasus2avro.common.Status"
                                     ],
                                     "type": "array"
                                 }
                             }
@@ -11258,270 +10040,53 @@
                         "Entity": {
                             "keyAspect": "glossaryTermKey",
                             "name": "glossaryTerm"
                         },
                         "doc": "A metadata snapshot for a specific GlossaryTerm entity.",
                         "fields": [
                             {
+                                "Urn": "GlossaryTermUrn",
                                 "doc": "URN for the entity the metadata snapshot is associated with.",
                                 "java": {
                                     "class": "com.linkedin.pegasus2avro.common.urn.GlossaryTermUrn"
                                 },
                                 "name": "urn",
                                 "type": "string"
                             },
                             {
                                 "doc": "The list of metadata aspects associated with the GlossaryTerm. Depending on the use case, this can either be all, or a selection, of supported aspects.",
                                 "name": "aspects",
                                 "type": {
                                     "items": [
-                                        "com.linkedin.pegasus2avro.metadata.key.GlossaryTermKey",
                                         {
                                             "Aspect": {
-                                                "name": "glossaryTermInfo"
+                                                "name": "glossaryTermKey"
                                             },
-                                            "doc": "Properties associated with a GlossaryTerm",
+                                            "doc": "Key for a GlossaryTerm",
                                             "fields": [
                                                 {
                                                     "Searchable": {
-                                                        "/*": {
-                                                            "queryByDefault": true
-                                                        }
-                                                    },
-                                                    "default": {},
-                                                    "doc": "Custom property bag.",
-                                                    "name": "customProperties",
-                                                    "type": {
-                                                        "type": "map",
-                                                        "values": "string"
-                                                    }
-                                                },
-                                                {
-                                                    "Searchable": {
-                                                        "fieldType": "TEXT_PARTIAL"
-                                                    },
-                                                    "default": null,
-                                                    "doc": "Optional id for the term",
-                                                    "name": "id",
-                                                    "type": [
-                                                        "null",
-                                                        "string"
-                                                    ]
-                                                },
-                                                {
-                                                    "Searchable": {
-                                                        "boostScore": 10.0,
                                                         "enableAutocomplete": true,
+                                                        "fieldName": "id",
                                                         "fieldType": "TEXT_PARTIAL"
                                                     },
-                                                    "default": null,
-                                                    "doc": "Display name of the term",
+                                                    "doc": "The term name, which serves as a unique id",
                                                     "name": "name",
-                                                    "type": [
-                                                        "null",
-                                                        "string"
-                                                    ]
-                                                },
-                                                {
-                                                    "Searchable": {},
-                                                    "doc": "Definition of business term.",
-                                                    "name": "definition",
-                                                    "type": "string"
-                                                },
-                                                {
-                                                    "Relationship": {
-                                                        "entityTypes": [
-                                                            "glossaryNode"
-                                                        ],
-                                                        "name": "IsPartOf"
-                                                    },
-                                                    "Searchable": {
-                                                        "fieldName": "parentNode",
-                                                        "fieldType": "URN",
-                                                        "hasValuesFieldName": "hasParentNode"
-                                                    },
-                                                    "default": null,
-                                                    "doc": "Parent node of the glossary term",
-                                                    "java": {
-                                                        "class": "com.linkedin.pegasus2avro.common.urn.GlossaryNodeUrn"
-                                                    },
-                                                    "name": "parentNode",
-                                                    "type": [
-                                                        "null",
-                                                        "string"
-                                                    ]
-                                                },
-                                                {
-                                                    "Searchable": {
-                                                        "fieldType": "KEYWORD"
-                                                    },
-                                                    "doc": "Source of the Business Term (INTERNAL or EXTERNAL) with default value as INTERNAL",
-                                                    "name": "termSource",
                                                     "type": "string"
-                                                },
-                                                {
-                                                    "Searchable": {
-                                                        "fieldType": "KEYWORD"
-                                                    },
-                                                    "default": null,
-                                                    "doc": "External Reference to the business-term",
-                                                    "name": "sourceRef",
-                                                    "type": [
-                                                        "null",
-                                                        "string"
-                                                    ]
-                                                },
-                                                {
-                                                    "default": null,
-                                                    "doc": "The abstracted URL such as https://spec.edmcouncil.org/fibo/ontology/FBC/FinancialInstruments/FinancialInstruments/CashInstrument.",
-                                                    "java": {
-                                                        "class": "com.linkedin.pegasus2avro.common.url.Url",
-                                                        "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-                                                    },
-                                                    "name": "sourceUrl",
-                                                    "type": [
-                                                        "null",
-                                                        "string"
-                                                    ]
-                                                },
-                                                {
-                                                    "default": null,
-                                                    "deprecated": true,
-                                                    "doc": "Schema definition of the glossary term",
-                                                    "name": "rawSchema",
-                                                    "type": [
-                                                        "null",
-                                                        "string"
-                                                    ]
                                                 }
                                             ],
-                                            "name": "GlossaryTermInfo",
-                                            "namespace": "com.linkedin.pegasus2avro.glossary",
+                                            "name": "GlossaryTermKey",
+                                            "namespace": "com.linkedin.pegasus2avro.metadata.key",
                                             "type": "record"
                                         },
+                                        "com.linkedin.pegasus2avro.glossary.GlossaryTermInfo",
                                         "com.linkedin.pegasus2avro.common.Ownership",
                                         "com.linkedin.pegasus2avro.common.Status",
                                         "com.linkedin.pegasus2avro.common.BrowsePaths",
-                                        {
-                                            "Aspect": {
-                                                "name": "glossaryRelatedTerms"
-                                            },
-                                            "doc": "Has A / Is A lineage information about a glossary Term reporting the lineage",
-                                            "fields": [
-                                                {
-                                                    "Relationship": {
-                                                        "/*": {
-                                                            "entityTypes": [
-                                                                "glossaryTerm"
-                                                            ],
-                                                            "name": "IsA"
-                                                        }
-                                                    },
-                                                    "Searchable": {
-                                                        "/*": {
-                                                            "boostScore": 2.0,
-                                                            "fieldName": "isRelatedTerms",
-                                                            "fieldType": "URN"
-                                                        }
-                                                    },
-                                                    "default": null,
-                                                    "doc": "The relationship Is A with glossary term",
-                                                    "name": "isRelatedTerms",
-                                                    "type": [
-                                                        "null",
-                                                        {
-                                                            "items": "string",
-                                                            "type": "array"
-                                                        }
-                                                    ]
-                                                },
-                                                {
-                                                    "Relationship": {
-                                                        "/*": {
-                                                            "entityTypes": [
-                                                                "glossaryTerm"
-                                                            ],
-                                                            "name": "HasA"
-                                                        }
-                                                    },
-                                                    "Searchable": {
-                                                        "/*": {
-                                                            "boostScore": 2.0,
-                                                            "fieldName": "hasRelatedTerms",
-                                                            "fieldType": "URN"
-                                                        }
-                                                    },
-                                                    "default": null,
-                                                    "doc": "The relationship Has A with glossary term",
-                                                    "name": "hasRelatedTerms",
-                                                    "type": [
-                                                        "null",
-                                                        {
-                                                            "items": "string",
-                                                            "type": "array"
-                                                        }
-                                                    ]
-                                                },
-                                                {
-                                                    "Relationship": {
-                                                        "/*": {
-                                                            "entityTypes": [
-                                                                "glossaryTerm"
-                                                            ],
-                                                            "name": "HasValue"
-                                                        }
-                                                    },
-                                                    "Searchable": {
-                                                        "/*": {
-                                                            "fieldName": "values",
-                                                            "fieldType": "URN"
-                                                        }
-                                                    },
-                                                    "default": null,
-                                                    "doc": "The relationship Has Value with glossary term.\nThese are fixed value a term has. For example a ColorEnum where RED, GREEN and YELLOW are fixed values.",
-                                                    "name": "values",
-                                                    "type": [
-                                                        "null",
-                                                        {
-                                                            "items": "string",
-                                                            "type": "array"
-                                                        }
-                                                    ]
-                                                },
-                                                {
-                                                    "Relationship": {
-                                                        "/*": {
-                                                            "entityTypes": [
-                                                                "glossaryTerm"
-                                                            ],
-                                                            "name": "IsRelatedTo"
-                                                        }
-                                                    },
-                                                    "Searchable": {
-                                                        "/*": {
-                                                            "fieldName": "relatedTerms",
-                                                            "fieldType": "URN"
-                                                        }
-                                                    },
-                                                    "default": null,
-                                                    "doc": "The relationship isRelatedTo with glossary term",
-                                                    "name": "relatedTerms",
-                                                    "type": [
-                                                        "null",
-                                                        {
-                                                            "items": "string",
-                                                            "type": "array"
-                                                        }
-                                                    ]
-                                                }
-                                            ],
-                                            "name": "GlossaryRelatedTerms",
-                                            "namespace": "com.linkedin.pegasus2avro.glossary",
-                                            "type": "record"
-                                        }
+                                        "com.linkedin.pegasus2avro.glossary.GlossaryRelatedTerms"
                                     ],
                                     "type": "array"
                                 }
                             }
                         ],
                         "name": "GlossaryTermSnapshot",
                         "namespace": "com.linkedin.pegasus2avro.metadata.snapshot",
@@ -11531,94 +10096,47 @@
                         "Entity": {
                             "keyAspect": "glossaryNodeKey",
                             "name": "glossaryNode"
                         },
                         "doc": "A metadata snapshot for a specific GlossaryNode entity.",
                         "fields": [
                             {
+                                "Urn": "GlossaryNodeUrn",
                                 "doc": "URN for the entity the metadata snapshot is associated with.",
                                 "java": {
                                     "class": "com.linkedin.pegasus2avro.common.urn.GlossaryNodeUrn"
                                 },
                                 "name": "urn",
                                 "type": "string"
                             },
                             {
                                 "doc": "The list of metadata aspects associated with the GlossaryNode. Depending on the use case, this can either be all, or a selection, of supported aspects.",
                                 "name": "aspects",
                                 "type": {
                                     "items": [
-                                        "com.linkedin.pegasus2avro.metadata.key.GlossaryNodeKey",
                                         {
                                             "Aspect": {
-                                                "name": "glossaryNodeInfo"
+                                                "name": "glossaryNodeKey"
                                             },
-                                            "doc": "Properties associated with a GlossaryNode",
+                                            "doc": "Key for a GlossaryNode",
                                             "fields": [
                                                 {
-                                                    "Searchable": {},
-                                                    "doc": "Definition of business node",
-                                                    "name": "definition",
-                                                    "type": "string"
-                                                },
-                                                {
-                                                    "Relationship": {
-                                                        "entityTypes": [
-                                                            "glossaryNode"
-                                                        ],
-                                                        "name": "IsPartOf"
-                                                    },
-                                                    "Searchable": {
-                                                        "fieldName": "parentNode",
-                                                        "fieldType": "URN",
-                                                        "hasValuesFieldName": "hasParentNode"
-                                                    },
-                                                    "default": null,
-                                                    "doc": "Parent node of the glossary term",
-                                                    "java": {
-                                                        "class": "com.linkedin.pegasus2avro.common.urn.GlossaryNodeUrn"
-                                                    },
-                                                    "name": "parentNode",
-                                                    "type": [
-                                                        "null",
-                                                        "string"
-                                                    ]
-                                                },
-                                                {
                                                     "Searchable": {
-                                                        "boostScore": 10.0,
                                                         "enableAutocomplete": true,
-                                                        "fieldName": "displayName",
                                                         "fieldType": "TEXT_PARTIAL"
                                                     },
-                                                    "default": null,
-                                                    "doc": "Display name of the node",
                                                     "name": "name",
-                                                    "type": [
-                                                        "null",
-                                                        "string"
-                                                    ]
-                                                },
-                                                {
-                                                    "Searchable": {
-                                                        "fieldType": "TEXT_PARTIAL"
-                                                    },
-                                                    "default": null,
-                                                    "doc": "Optional id for the GlossaryNode",
-                                                    "name": "id",
-                                                    "type": [
-                                                        "null",
-                                                        "string"
-                                                    ]
+                                                    "type": "string"
                                                 }
                                             ],
-                                            "name": "GlossaryNodeInfo",
-                                            "namespace": "com.linkedin.pegasus2avro.glossary",
+                                            "name": "GlossaryNodeKey",
+                                            "namespace": "com.linkedin.pegasus2avro.metadata.key",
                                             "type": "record"
                                         },
+                                        "com.linkedin.pegasus2avro.glossary.GlossaryNodeInfo",
                                         "com.linkedin.pegasus2avro.common.Ownership",
                                         "com.linkedin.pegasus2avro.common.Status"
                                     ],
                                     "type": "array"
                                 }
                             }
                         ],
@@ -11630,27 +10148,43 @@
                         "Entity": {
                             "keyAspect": "dataHubPolicyKey",
                             "name": "dataHubPolicy"
                         },
                         "doc": "A metadata snapshot for DataHub Access Policy data.",
                         "fields": [
                             {
+                                "Urn": "Urn",
                                 "doc": "URN for the entity the metadata snapshot is associated with.",
                                 "java": {
                                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                 },
                                 "name": "urn",
                                 "type": "string"
                             },
                             {
                                 "doc": "The list of metadata aspects associated with the DataHub access policy.",
                                 "name": "aspects",
                                 "type": {
                                     "items": [
-                                        "com.linkedin.pegasus2avro.metadata.key.DataHubPolicyKey",
+                                        {
+                                            "Aspect": {
+                                                "name": "dataHubPolicyKey"
+                                            },
+                                            "doc": "Key for a DataHub Policy",
+                                            "fields": [
+                                                {
+                                                    "doc": "A unique id for the DataHub access policy record. Generated on the server side at policy creation time.",
+                                                    "name": "id",
+                                                    "type": "string"
+                                                }
+                                            ],
+                                            "name": "DataHubPolicyKey",
+                                            "namespace": "com.linkedin.pegasus2avro.metadata.key",
+                                            "type": "record"
+                                        },
                                         "com.linkedin.pegasus2avro.policy.DataHubPolicyInfo"
                                     ],
                                     "type": "array"
                                 }
                             }
                         ],
                         "name": "DataHubPolicySnapshot",
@@ -11661,27 +10195,58 @@
                         "Entity": {
                             "keyAspect": "schemaFieldKey",
                             "name": "schemaField"
                         },
                         "doc": "A metadata snapshot for a specific schema field entity.",
                         "fields": [
                             {
+                                "Urn": "Urn",
                                 "doc": "URN for the entity the metadata snapshot is associated with.",
                                 "java": {
                                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                 },
                                 "name": "urn",
                                 "type": "string"
                             },
                             {
                                 "doc": "The list of metadata aspects associated with the dataset. Depending on the use case, this can either be all, or a selection, of supported aspects.",
                                 "name": "aspects",
                                 "type": {
                                     "items": [
-                                        "com.linkedin.pegasus2avro.metadata.key.SchemaFieldKey"
+                                        {
+                                            "Aspect": {
+                                                "name": "schemaFieldKey"
+                                            },
+                                            "doc": "Key for a SchemaField",
+                                            "fields": [
+                                                {
+                                                    "Searchable": {
+                                                        "fieldType": "URN"
+                                                    },
+                                                    "Urn": "Urn",
+                                                    "doc": "Parent associated with the schema field",
+                                                    "java": {
+                                                        "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                                    },
+                                                    "name": "parent",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "Searchable": {
+                                                        "fieldType": "KEYWORD"
+                                                    },
+                                                    "doc": "fieldPath identifying the schema field",
+                                                    "name": "fieldPath",
+                                                    "type": "string"
+                                                }
+                                            ],
+                                            "name": "SchemaFieldKey",
+                                            "namespace": "com.linkedin.pegasus2avro.metadata.key",
+                                            "type": "record"
+                                        }
                                     ],
                                     "type": "array"
                                 }
                             }
                         ],
                         "name": "SchemaFieldSnapshot",
                         "namespace": "com.linkedin.pegasus2avro.metadata.snapshot",
@@ -11691,27 +10256,48 @@
                         "Entity": {
                             "keyAspect": "dataHubRetentionKey",
                             "name": "dataHubRetention"
                         },
                         "doc": "A metadata snapshot for DataHub Access Policy data.",
                         "fields": [
                             {
+                                "Urn": "Urn",
                                 "doc": "URN for the entity the metadata snapshot is associated with.",
                                 "java": {
                                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                 },
                                 "name": "urn",
                                 "type": "string"
                             },
                             {
                                 "doc": "The list of metadata aspects associated with the DataHub access policy.",
                                 "name": "aspects",
                                 "type": {
                                     "items": [
-                                        "com.linkedin.pegasus2avro.metadata.key.DataHubRetentionKey",
+                                        {
+                                            "Aspect": {
+                                                "name": "dataHubRetentionKey"
+                                            },
+                                            "doc": "Key for a DataHub Retention",
+                                            "fields": [
+                                                {
+                                                    "doc": "Entity name to apply retention to. * (or empty) for applying defaults.",
+                                                    "name": "entityName",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "doc": "Aspect name to apply retention to. * (or empty) for applying defaults.",
+                                                    "name": "aspectName",
+                                                    "type": "string"
+                                                }
+                                            ],
+                                            "name": "DataHubRetentionKey",
+                                            "namespace": "com.linkedin.pegasus2avro.metadata.key",
+                                            "type": "record"
+                                        },
                                         "com.linkedin.pegasus2avro.retention.DataHubRetentionConfig"
                                     ],
                                     "type": "array"
                                 }
                             }
                         ],
                         "name": "DataHubRetentionSnapshot",
@@ -11730,70 +10316,15 @@
             },
             {
                 "default": null,
                 "doc": "Metadata around how the snapshot was ingested",
                 "name": "systemMetadata",
                 "type": [
                     "null",
-                    {
-                        "doc": "Metadata associated with each metadata change that is processed by the system",
-                        "fields": [
-                            {
-                                "default": 0,
-                                "doc": "The timestamp the metadata was observed at",
-                                "name": "lastObserved",
-                                "type": [
-                                    "long",
-                                    "null"
-                                ]
-                            },
-                            {
-                                "default": "no-run-id-provided",
-                                "doc": "The run id that produced the metadata. Populated in case of batch-ingestion.",
-                                "name": "runId",
-                                "type": [
-                                    "string",
-                                    "null"
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "The model registry name that was used to process this event",
-                                "name": "registryName",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "The model registry version that was used to process this event",
-                                "name": "registryVersion",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            },
-                            {
-                                "default": null,
-                                "doc": "Additional properties",
-                                "name": "properties",
-                                "type": [
-                                    "null",
-                                    {
-                                        "type": "map",
-                                        "values": "string"
-                                    }
-                                ]
-                            }
-                        ],
-                        "name": "SystemMetadata",
-                        "namespace": "com.linkedin.pegasus2avro.mxe",
-                        "type": "record"
-                    }
+                    "com.linkedin.pegasus2avro.mxe.SystemMetadata"
                 ]
             }
         ],
         "name": "MetadataChangeEvent",
         "namespace": "com.linkedin.pegasus2avro.mxe",
         "type": "record"
     },
@@ -11811,14 +10342,15 @@
             },
             {
                 "doc": "Type of the entity being written to",
                 "name": "entityType",
                 "type": "string"
             },
             {
+                "Urn": "Urn",
                 "default": null,
                 "doc": "Urn of the entity being written",
                 "java": {
                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                 },
                 "name": "entityUrn",
                 "type": [
@@ -11828,59 +10360,21 @@
             },
             {
                 "default": null,
                 "doc": "Key aspect of the entity being written",
                 "name": "entityKeyAspect",
                 "type": [
                     "null",
-                    {
-                        "doc": "Generic record structure for serializing an Aspect",
-                        "fields": [
-                            {
-                                "doc": "The value of the aspect, serialized as bytes.",
-                                "name": "value",
-                                "type": "bytes"
-                            },
-                            {
-                                "doc": "The content type, which represents the fashion in which the aspect was serialized.\nThe only type currently supported is application/json.",
-                                "name": "contentType",
-                                "type": "string"
-                            }
-                        ],
-                        "name": "GenericAspect",
-                        "namespace": "com.linkedin.pegasus2avro.mxe",
-                        "type": "record"
-                    }
+                    "com.linkedin.pegasus2avro.mxe.GenericAspect"
                 ]
             },
             {
                 "doc": "Type of change being proposed",
                 "name": "changeType",
-                "type": {
-                    "doc": "Descriptor for a change action",
-                    "name": "ChangeType",
-                    "namespace": "com.linkedin.pegasus2avro.events.metadata",
-                    "symbolDocs": {
-                        "CREATE": "NOT SUPPORTED YET\ninsert if not exists. otherwise fail",
-                        "DELETE": "NOT SUPPORTED YET\ndelete action",
-                        "PATCH": "NOT SUPPORTED YET\npatch the changes instead of full replace",
-                        "RESTATE": "Restate an aspect, eg. in a index refresh.",
-                        "UPDATE": "NOT SUPPORTED YET\nupdate if exists. otherwise fail",
-                        "UPSERT": "insert if not exists. otherwise update"
-                    },
-                    "symbols": [
-                        "UPSERT",
-                        "CREATE",
-                        "UPDATE",
-                        "DELETE",
-                        "PATCH",
-                        "RESTATE"
-                    ],
-                    "type": "enum"
-                }
+                "type": "com.linkedin.pegasus2avro.events.metadata.ChangeType"
             },
             {
                 "default": null,
                 "doc": "Aspect of the entity being written to\nNot filling this out implies that the writer wants to affect the entire entity\nNote: This is only valid for CREATE, UPSERT, and DELETE operations.",
                 "name": "aspectName",
                 "type": [
                     "null",
@@ -11907,396 +10401,391 @@
             }
         ],
         "name": "MetadataChangeProposal",
         "namespace": "com.linkedin.pegasus2avro.mxe",
         "type": "record"
     },
     {
-        "doc": "Kafka event for capturing update made to an entity's metadata.",
+        "Aspect": {
+            "name": "dataProcessInstanceRunEvent",
+            "type": "timeseries"
+        },
+        "doc": "An event representing the current status of data process run.\nDataProcessRunEvent should be used for reporting the status of a dataProcess' run.",
         "fields": [
             {
-                "default": null,
-                "doc": "Kafka audit header. Currently remains unused in the open source.",
-                "name": "auditHeader",
-                "type": [
-                    "null",
-                    "com.linkedin.events.KafkaAuditHeader"
-                ]
-            },
-            {
-                "doc": "Type of the entity being written to",
-                "name": "entityType",
-                "type": "string"
+                "doc": "The event timestamp field as epoch at UTC in milli seconds.",
+                "name": "timestampMillis",
+                "type": "long"
             },
             {
                 "default": null,
-                "doc": "Urn of the entity being written",
-                "java": {
-                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                },
-                "name": "entityUrn",
+                "doc": "Granularity of the event if applicable",
+                "name": "eventGranularity",
                 "type": [
                     "null",
-                    "string"
+                    "com.linkedin.pegasus2avro.timeseries.TimeWindowSize"
                 ]
             },
             {
-                "default": null,
-                "doc": "Key aspect of the entity being written",
-                "name": "entityKeyAspect",
+                "default": {
+                    "partition": "FULL_TABLE_SNAPSHOT",
+                    "timePartition": null,
+                    "type": "FULL_TABLE"
+                },
+                "doc": "The optional partition specification.",
+                "name": "partitionSpec",
                 "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.mxe.GenericAspect"
+                    "com.linkedin.pegasus2avro.timeseries.PartitionSpec",
+                    "null"
                 ]
             },
             {
-                "doc": "Type of change being proposed",
-                "name": "changeType",
-                "type": "com.linkedin.pegasus2avro.events.metadata.ChangeType"
-            },
-            {
                 "default": null,
-                "doc": "Aspect of the entity being written to\nNot filling this out implies that the writer wants to affect the entire entity\nNote: This is only valid for CREATE, UPSERT, and DELETE operations.",
-                "name": "aspectName",
+                "doc": "The optional messageId, if provided serves as a custom user-defined unique identifier for an aspect value.",
+                "name": "messageId",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
                 "default": null,
-                "doc": "The value of the new aspect.",
-                "name": "aspect",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.mxe.GenericAspect"
-                ]
-            },
-            {
-                "default": null,
-                "doc": "A string->string map of custom properties that one might want to attach to an event",
-                "name": "systemMetadata",
+                "doc": "URL where the reference exist",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "externalUrl",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.mxe.SystemMetadata"
+                    "string"
                 ]
             },
             {
-                "default": null,
-                "doc": "The previous value of the aspect that has changed.",
-                "name": "previousAspectValue",
-                "type": [
-                    "null",
-                    "com.linkedin.pegasus2avro.mxe.GenericAspect"
-                ]
+                "TimeseriesField": {},
+                "name": "status",
+                "type": {
+                    "name": "DataProcessRunStatus",
+                    "namespace": "com.linkedin.pegasus2avro.dataprocess",
+                    "symbolDocs": {
+                        "STARTED": "The status where the Data processing run is in."
+                    },
+                    "symbols": [
+                        "STARTED",
+                        "COMPLETE"
+                    ],
+                    "type": "enum"
+                }
             },
             {
                 "default": null,
-                "doc": "The previous value of the system metadata field that has changed.",
-                "name": "previousSystemMetadata",
+                "doc": "Return the try number that this Instance Run is in",
+                "name": "attempt",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.mxe.SystemMetadata"
+                    "int"
                 ]
             },
             {
+                "TimeseriesField": {},
                 "default": null,
-                "doc": "An audit stamp detailing who and when the aspect was changed by. Required for all intents and purposes.",
-                "name": "created",
+                "doc": "The final result of the Data Processing run.",
+                "name": "result",
                 "type": [
                     "null",
-                    "com.linkedin.pegasus2avro.common.AuditStamp"
+                    {
+                        "fields": [
+                            {
+                                "doc": " The final result, e.g. SUCCESS, FAILURE, SKIPPED, or UP_FOR_RETRY.",
+                                "name": "type",
+                                "type": {
+                                    "name": "RunResultType",
+                                    "namespace": "com.linkedin.pegasus2avro.dataprocess",
+                                    "symbolDocs": {
+                                        "FAILURE": " The Run Failed",
+                                        "SKIPPED": " The Run Skipped",
+                                        "SUCCESS": " The Run Succeeded",
+                                        "UP_FOR_RETRY": " The Run Failed and will Retry"
+                                    },
+                                    "symbols": [
+                                        "SUCCESS",
+                                        "FAILURE",
+                                        "SKIPPED",
+                                        "UP_FOR_RETRY"
+                                    ],
+                                    "type": "enum"
+                                }
+                            },
+                            {
+                                "doc": "It identifies the system where the native result comes from like Airflow, Azkaban, etc..",
+                                "name": "nativeResultType",
+                                "type": "string"
+                            }
+                        ],
+                        "name": "DataProcessInstanceRunResult",
+                        "namespace": "com.linkedin.pegasus2avro.dataprocess",
+                        "type": "record"
+                    }
                 ]
             }
         ],
-        "name": "MetadataChangeLog",
-        "namespace": "com.linkedin.pegasus2avro.mxe",
+        "name": "DataProcessInstanceRunEvent",
+        "namespace": "com.linkedin.pegasus2avro.dataprocess",
         "type": "record"
     },
     {
-        "doc": "A DataHub Platform Event.",
+        "Aspect": {
+            "name": "dataProcessInstanceInput"
+        },
+        "doc": "Information about the inputs datasets of a Data process",
         "fields": [
             {
-                "doc": "Header information stored with the event.",
-                "name": "header",
-                "type": {
-                    "doc": "A header included with each DataHub platform event.",
-                    "fields": [
-                        {
-                            "doc": "The event timestamp field as epoch at UTC in milli seconds.",
-                            "name": "timestampMillis",
-                            "type": "long"
-                        }
-                    ],
-                    "name": "PlatformEventHeader",
-                    "namespace": "com.linkedin.pegasus2avro.mxe",
-                    "type": "record"
-                }
-            },
-            {
-                "doc": "The name of the event, e.g. the type of event. For example, 'notificationRequestEvent', 'entityChangeEvent'",
-                "name": "name",
-                "type": "string"
-            },
-            {
-                "doc": "The event payload.",
-                "name": "payload",
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "dataset"
+                        ],
+                        "name": "Consumes"
+                    }
+                },
+                "Searchable": {
+                    "/*": {
+                        "addToFilters": true,
+                        "fieldName": "inputs",
+                        "fieldType": "URN",
+                        "numValuesFieldName": "numInputs",
+                        "queryByDefault": false
+                    }
+                },
+                "Urn": "Urn",
+                "doc": "Input datasets to be consumed",
+                "name": "inputs",
                 "type": {
-                    "doc": "Generic payload record structure for serializing a Platform Event.",
-                    "fields": [
-                        {
-                            "doc": "The value of the event, serialized as bytes.",
-                            "name": "value",
-                            "type": "bytes"
-                        },
-                        {
-                            "doc": "The content type, which represents the fashion in which the event was serialized.\nThe only type currently supported is application/json.",
-                            "name": "contentType",
-                            "type": "string"
-                        }
-                    ],
-                    "name": "GenericPayload",
-                    "namespace": "com.linkedin.pegasus2avro.mxe",
-                    "type": "record"
-                }
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
             }
         ],
-        "name": "PlatformEvent",
-        "namespace": "com.linkedin.pegasus2avro.mxe",
+        "name": "DataProcessInstanceInput",
+        "namespace": "com.linkedin.pegasus2avro.dataprocess",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dataHubExecutionRequestInput"
+            "name": "dataProcessInstanceProperties"
         },
-        "doc": "An request to execution some remote logic or action.\nTODO: Determine who is responsible for emitting execution request success or failure. Executor?",
+        "doc": "The inputs and outputs of this data process",
         "fields": [
             {
-                "doc": "The name of the task to execute, for example RUN_INGEST",
-                "name": "task",
-                "type": "string"
-            },
-            {
-                "doc": "Arguments provided to the task",
-                "name": "args",
+                "Searchable": {
+                    "/*": {
+                        "queryByDefault": true
+                    }
+                },
+                "default": {},
+                "doc": "Custom property bag.",
+                "name": "customProperties",
                 "type": {
                     "type": "map",
                     "values": "string"
                 }
             },
             {
-                "doc": "Advanced: specify a specific executor to route the request to. If none is provided, a \"default\" executor is used.",
-                "name": "executorId",
+                "default": null,
+                "doc": "URL where the reference exist",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "externalUrl",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL"
+                },
+                "doc": "Process name",
+                "name": "name",
                 "type": "string"
             },
             {
-                "doc": "Source which created the execution request",
-                "name": "source",
-                "type": {
-                    "fields": [
-                        {
-                            "doc": "The type of the execution request source, e.g. INGESTION_SOURCE",
-                            "name": "type",
-                            "type": "string"
-                        },
-                        {
-                            "Relationship": {
-                                "entityTypes": [
-                                    "dataHubIngestionSource"
-                                ],
-                                "name": "ingestionSource"
-                            },
-                            "Searchable": {
-                                "fieldName": "ingestionSource",
-                                "fieldType": "KEYWORD",
-                                "queryByDefault": false
-                            },
-                            "default": null,
-                            "doc": "The urn of the ingestion source associated with the ingestion request. Present if type is INGESTION_SOURCE",
-                            "java": {
-                                "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                            },
-                            "name": "ingestionSource",
-                            "type": [
-                                "null",
-                                "string"
-                            ]
-                        }
-                    ],
-                    "name": "ExecutionRequestSource",
-                    "namespace": "com.linkedin.pegasus2avro.execution",
-                    "type": "record"
-                }
+                "Searchable": {
+                    "addToFilters": true,
+                    "fieldType": "KEYWORD",
+                    "filterNameOverride": "Process Type"
+                },
+                "default": null,
+                "doc": "Process type",
+                "name": "type",
+                "type": [
+                    "null",
+                    {
+                        "name": "DataProcessType",
+                        "namespace": "com.linkedin.pegasus2avro.dataprocess",
+                        "symbols": [
+                            "BATCH_SCHEDULED",
+                            "BATCH_AD_HOC",
+                            "STREAMING"
+                        ],
+                        "type": "enum"
+                    }
+                ]
             },
             {
                 "Searchable": {
-                    "fieldName": "requestTimeMs",
-                    "fieldType": "COUNT",
-                    "queryByDefault": false
+                    "/time": {
+                        "fieldName": "created",
+                        "fieldType": "COUNT",
+                        "queryByDefault": false
+                    }
                 },
-                "doc": "Time at which the execution request input was created",
-                "name": "requestedAt",
-                "type": "long"
+                "doc": "Audit stamp containing who reported the lineage and when",
+                "name": "created",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
             }
         ],
-        "name": "ExecutionRequestInput",
-        "namespace": "com.linkedin.pegasus2avro.execution",
+        "name": "DataProcessInstanceProperties",
+        "namespace": "com.linkedin.pegasus2avro.dataprocess",
         "type": "record"
     },
     {
         "Aspect": {
-            "name": "dataHubExecutionRequestSignal"
+            "name": "dataProcessInstanceOutput"
         },
-        "doc": "An signal sent to a running execution request",
+        "doc": "Information about the outputs of a Data process",
         "fields": [
             {
-                "doc": "The signal to issue, e.g. KILL",
-                "name": "signal",
-                "type": "string"
-            },
-            {
-                "default": null,
-                "doc": "Advanced: specify a specific executor to route the request to. If none is provided, a \"default\" executor is used.",
-                "name": "executorId",
-                "type": [
-                    "null",
-                    "string"
-                ]
-            },
-            {
-                "doc": "Audit Stamp",
-                "name": "createdAt",
-                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "dataset"
+                        ],
+                        "name": "Produces"
+                    }
+                },
+                "Searchable": {
+                    "/*": {
+                        "addToFilters": true,
+                        "fieldName": "outputs",
+                        "fieldType": "URN",
+                        "numValuesFieldName": "numOutputs",
+                        "queryByDefault": false
+                    }
+                },
+                "Urn": "Urn",
+                "doc": "Output datasets to be produced",
+                "name": "outputs",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
             }
         ],
-        "name": "ExecutionRequestSignal",
-        "namespace": "com.linkedin.pegasus2avro.execution",
+        "name": "DataProcessInstanceOutput",
+        "namespace": "com.linkedin.pegasus2avro.dataprocess",
         "type": "record"
     },
+    "com.linkedin.pegasus2avro.dataprocess.DataProcessInfo",
     {
         "Aspect": {
-            "name": "dataHubExecutionRequestResult"
+            "name": "dataProcessInstanceRelationships"
         },
-        "doc": "The result of an execution request",
+        "doc": "Information about Data process relationships",
         "fields": [
             {
-                "doc": "The status of the execution request",
-                "name": "status",
-                "type": "string"
-            },
-            {
+                "Relationship": {
+                    "entityTypes": [
+                        "dataJob",
+                        "dataFlow"
+                    ],
+                    "name": "InstanceOf"
+                },
+                "Searchable": {
+                    "/*": {
+                        "fieldName": "parentTemplate",
+                        "fieldType": "URN",
+                        "queryByDefault": false
+                    }
+                },
+                "Urn": "Urn",
                 "default": null,
-                "doc": "The pretty-printed execution report.",
-                "name": "report",
+                "doc": "The parent entity whose run instance it is",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                },
+                "name": "parentTemplate",
                 "type": [
                     "null",
                     "string"
                 ]
             },
             {
-                "default": null,
-                "doc": "A structured report if available.",
-                "name": "structuredReport",
-                "type": [
-                    "null",
-                    {
-                        "doc": "A flexible carrier for structured results of an execution request.\nThe goal is to allow for free flow of structured responses from execution tasks to the orchestrator or observer.\nThe full spectrum of different execution report types is not intended to be modeled by this object.",
-                        "fields": [
-                            {
-                                "doc": "The type of the structured report. (e.g. INGESTION_REPORT, TEST_CONNECTION_REPORT, etc.)",
-                                "name": "type",
-                                "type": "string"
-                            },
-                            {
-                                "doc": "The serialized value of the structured report",
-                                "name": "serializedValue",
-                                "type": "string"
-                            },
-                            {
-                                "doc": "The content-type of the serialized value (e.g. application/json, application/json;gzip etc.)",
-                                "name": "contentType",
-                                "type": "string"
-                            }
-                        ],
-                        "name": "StructuredExecutionReport",
-                        "namespace": "com.linkedin.pegasus2avro.execution",
-                        "type": "record"
-                    }
-                ]
-            },
-            {
+                "Relationship": {
+                    "entityTypes": [
+                        "dataProcessInstance"
+                    ],
+                    "name": "ChildOf"
+                },
                 "Searchable": {
-                    "fieldName": "startTimeMs",
-                    "fieldType": "COUNT",
-                    "queryByDefault": false
+                    "/*": {
+                        "fieldName": "parentInstance",
+                        "fieldType": "URN",
+                        "queryByDefault": false
+                    }
                 },
+                "Urn": "Urn",
                 "default": null,
-                "doc": "Time at which the request was created",
-                "name": "startTimeMs",
+                "doc": "The parent DataProcessInstance where it belongs to.\nIf it is a Airflow Task then it should belong to an Airflow Dag run as well\nwhich will be another DataProcessInstance",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                },
+                "name": "parentInstance",
                 "type": [
                     "null",
-                    "long"
+                    "string"
                 ]
             },
             {
-                "default": null,
-                "doc": "Duration in milliseconds",
-                "name": "durationMs",
-                "type": [
-                    "null",
-                    "long"
-                ]
-            }
-        ],
-        "name": "ExecutionRequestResult",
-        "namespace": "com.linkedin.pegasus2avro.execution",
-        "type": "record"
-    },
-    {
-        "Aspect": {
-            "name": "globalSettingsInfo"
-        },
-        "doc": "DataHub Global platform settings. Careful - these should not be modified by the outside world!",
-        "fields": [
-            {
-                "default": null,
-                "doc": "Settings related to the Views Feature",
-                "name": "views",
-                "type": [
-                    "null",
-                    {
-                        "doc": "Settings for DataHub Views feature.",
-                        "fields": [
-                            {
-                                "default": null,
-                                "doc": "The default View for the instance, or organization.",
-                                "java": {
-                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                },
-                                "name": "defaultView",
-                                "type": [
-                                    "null",
-                                    "string"
-                                ]
-                            }
+                "Relationship": {
+                    "/*": {
+                        "entityTypes": [
+                            "dataProcessInstance"
                         ],
-                        "name": "GlobalViewsSettings",
-                        "namespace": "com.linkedin.pegasus2avro.settings.global",
-                        "type": "record"
+                        "name": "UpstreamOf"
                     }
-                ]
+                },
+                "Searchable": {
+                    "/*": {
+                        "fieldName": "upstream",
+                        "fieldType": "URN",
+                        "numValuesFieldName": "numUpstreams",
+                        "queryByDefault": false
+                    }
+                },
+                "Urn": "Urn",
+                "doc": "Input DataProcessInstance which triggered this dataprocess instance",
+                "name": "upstreamInstances",
+                "type": {
+                    "items": "string",
+                    "type": "array"
+                },
+                "urn_is_array": true
             }
         ],
-        "name": "GlobalSettingsInfo",
-        "namespace": "com.linkedin.pegasus2avro.settings.global",
+        "name": "DataProcessInstanceRelationships",
+        "namespace": "com.linkedin.pegasus2avro.dataprocess",
         "type": "record"
     },
-    "com.linkedin.pegasus2avro.glossary.GlossaryTermInfo",
-    "com.linkedin.pegasus2avro.glossary.GlossaryNodeInfo",
-    "com.linkedin.pegasus2avro.glossary.GlossaryRelatedTerms",
     {
         "Aspect": {
             "name": "assertionInfo"
         },
         "doc": "Information about an assertion",
         "fields": [
             {
@@ -12350,14 +10839,15 @@
                             {
                                 "Relationship": {
                                     "entityTypes": [
                                         "dataset"
                                     ],
                                     "name": "Asserts"
                                 },
+                                "Urn": "Urn",
                                 "doc": "The dataset targeted by this assertion.",
                                 "java": {
                                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                 },
                                 "name": "dataset",
                                 "type": "string"
                             },
@@ -12387,24 +10877,26 @@
                                     "/*": {
                                         "entityTypes": [
                                             "schemaField"
                                         ],
                                         "name": "Asserts"
                                     }
                                 },
+                                "Urn": "Urn",
                                 "default": null,
                                 "doc": "One or more dataset schema fields that are targeted by this assertion",
                                 "name": "fields",
                                 "type": [
                                     "null",
                                     {
                                         "items": "string",
                                         "type": "array"
                                     }
-                                ]
+                                ],
+                                "urn_is_array": true
                             },
                             {
                                 "default": null,
                                 "doc": "Standardized assertion operator",
                                 "name": "aggregation",
                                 "type": [
                                     "null",
@@ -12650,22 +11142,24 @@
             {
                 "doc": " Native (platform-specific) identifier for this run",
                 "name": "runId",
                 "type": "string"
             },
             {
                 "TimeseriesField": {},
+                "Urn": "Urn",
                 "java": {
                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                 },
                 "name": "assertionUrn",
                 "type": "string"
             },
             {
                 "TimeseriesField": {},
+                "Urn": "Urn",
                 "java": {
                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                 },
                 "name": "asserteeUrn",
                 "type": "string"
             },
             {
@@ -12846,32 +11340,668 @@
             }
         ],
         "name": "AssertionRunEvent",
         "namespace": "com.linkedin.pegasus2avro.assertion",
         "type": "record"
     },
     {
+        "Event": {
+            "name": "entityChangeEvent"
+        },
+        "doc": "Shared fields for all entity change events.",
+        "fields": [
+            {
+                "doc": "The type of the entity affected. Corresponds to the entity registry, e.g. 'dataset', 'chart', 'dashboard', etc.",
+                "name": "entityType",
+                "type": "string"
+            },
+            {
+                "Urn": "Urn",
+                "doc": "The urn of the entity which was affected.",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                },
+                "name": "entityUrn",
+                "type": "string"
+            },
+            {
+                "doc": "The category type (TAG, GLOSSARY_TERM, OWNERSHIP, TECHNICAL_SCHEMA, etc). This is used to determine what the rest of the schema will look like.",
+                "name": "category",
+                "type": "string"
+            },
+            {
+                "doc": "The operation type. This is used to determine what the rest of the schema will look like.",
+                "name": "operation",
+                "type": "string"
+            },
+            {
+                "default": null,
+                "doc": "The urn of the entity which was affected.",
+                "name": "modifier",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Arbitrary key-value parameters corresponding to the event.",
+                "name": "parameters",
+                "type": [
+                    "null",
+                    {
+                        "doc": "Arbitrary key-value parameters for an Entity Change Event. (any record).",
+                        "fields": [],
+                        "name": "Parameters",
+                        "namespace": "com.linkedin.pegasus2avro.platform.event.v1",
+                        "type": "record"
+                    }
+                ]
+            },
+            {
+                "doc": "Audit stamp of the operation",
+                "name": "auditStamp",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+            },
+            {
+                "doc": "The version of the event type, incremented in integers.",
+                "name": "version",
+                "type": "int"
+            }
+        ],
+        "name": "EntityChangeEvent",
+        "namespace": "com.linkedin.pegasus2avro.platform.event.v1",
+        "type": "record"
+    },
+    {
         "Aspect": {
-            "name": "dataHubUpgradeRequest"
+            "name": "notebookContent"
         },
-        "doc": "Information collected when kicking off a DataHubUpgrade",
+        "doc": "Content in a Notebook\nNote: This is IN BETA version",
         "fields": [
             {
-                "doc": "Timestamp when we started this DataHubUpgrade",
-                "name": "timestampMs",
+                "default": [],
+                "doc": "The content of a Notebook which is composed by a list of NotebookCell",
+                "name": "cells",
+                "type": {
+                    "items": {
+                        "doc": "A record of all supported cells for a Notebook. Only one type of cell will be non-null.",
+                        "fields": [
+                            {
+                                "default": null,
+                                "doc": "The text cell content. The will be non-null only when all other cell field is null.",
+                                "name": "textCell",
+                                "type": [
+                                    "null",
+                                    {
+                                        "doc": "Text cell in a Notebook, which will present content in text format",
+                                        "fields": [
+                                            {
+                                                "default": null,
+                                                "doc": "Title of the cell",
+                                                "name": "cellTitle",
+                                                "type": [
+                                                    "null",
+                                                    "string"
+                                                ]
+                                            },
+                                            {
+                                                "doc": "Unique id for the cell. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773/?cellId=1234'",
+                                                "name": "cellId",
+                                                "type": "string"
+                                            },
+                                            {
+                                                "doc": "Captures information about who created/last modified/deleted this Notebook cell and when",
+                                                "name": "changeAuditStamps",
+                                                "type": "com.linkedin.pegasus2avro.common.ChangeAuditStamps"
+                                            },
+                                            {
+                                                "doc": "The actual text in a TextCell in a Notebook",
+                                                "name": "text",
+                                                "type": "string"
+                                            }
+                                        ],
+                                        "name": "TextCell",
+                                        "namespace": "com.linkedin.pegasus2avro.notebook",
+                                        "type": "record"
+                                    }
+                                ]
+                            },
+                            {
+                                "default": null,
+                                "doc": "The query cell content. The will be non-null only when all other cell field is null.",
+                                "name": "queryCell",
+                                "type": [
+                                    "null",
+                                    {
+                                        "doc": "Query cell in a Notebook, which will present content in query format",
+                                        "fields": [
+                                            {
+                                                "default": null,
+                                                "doc": "Title of the cell",
+                                                "name": "cellTitle",
+                                                "type": [
+                                                    "null",
+                                                    "string"
+                                                ]
+                                            },
+                                            {
+                                                "doc": "Unique id for the cell. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773/?cellId=1234'",
+                                                "name": "cellId",
+                                                "type": "string"
+                                            },
+                                            {
+                                                "doc": "Captures information about who created/last modified/deleted this Notebook cell and when",
+                                                "name": "changeAuditStamps",
+                                                "type": "com.linkedin.pegasus2avro.common.ChangeAuditStamps"
+                                            },
+                                            {
+                                                "doc": "Raw query to explain some specific logic in a Notebook",
+                                                "name": "rawQuery",
+                                                "type": "string"
+                                            },
+                                            {
+                                                "default": null,
+                                                "doc": "Captures information about who last executed this query cell and when",
+                                                "name": "lastExecuted",
+                                                "type": [
+                                                    "null",
+                                                    "com.linkedin.pegasus2avro.common.AuditStamp"
+                                                ]
+                                            }
+                                        ],
+                                        "name": "QueryCell",
+                                        "namespace": "com.linkedin.pegasus2avro.notebook",
+                                        "type": "record"
+                                    }
+                                ]
+                            },
+                            {
+                                "default": null,
+                                "doc": "The chart cell content. The will be non-null only when all other cell field is null.",
+                                "name": "chartCell",
+                                "type": [
+                                    "null",
+                                    {
+                                        "doc": "Chart cell in a notebook, which will present content in chart format",
+                                        "fields": [
+                                            {
+                                                "default": null,
+                                                "doc": "Title of the cell",
+                                                "name": "cellTitle",
+                                                "type": [
+                                                    "null",
+                                                    "string"
+                                                ]
+                                            },
+                                            {
+                                                "doc": "Unique id for the cell. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773/?cellId=1234'",
+                                                "name": "cellId",
+                                                "type": "string"
+                                            },
+                                            {
+                                                "doc": "Captures information about who created/last modified/deleted this Notebook cell and when",
+                                                "name": "changeAuditStamps",
+                                                "type": "com.linkedin.pegasus2avro.common.ChangeAuditStamps"
+                                            }
+                                        ],
+                                        "name": "ChartCell",
+                                        "namespace": "com.linkedin.pegasus2avro.notebook",
+                                        "type": "record"
+                                    }
+                                ]
+                            },
+                            {
+                                "doc": "The type of this Notebook cell",
+                                "name": "type",
+                                "type": {
+                                    "doc": "Type of Notebook Cell",
+                                    "name": "NotebookCellType",
+                                    "namespace": "com.linkedin.pegasus2avro.notebook",
+                                    "symbolDocs": {
+                                        "CHART_CELL": "CHART Notebook cell type. The cell content is chart only.",
+                                        "QUERY_CELL": "QUERY Notebook cell type. The cell context is query only.",
+                                        "TEXT_CELL": "TEXT Notebook cell type. The cell context is text only."
+                                    },
+                                    "symbols": [
+                                        "TEXT_CELL",
+                                        "QUERY_CELL",
+                                        "CHART_CELL"
+                                    ],
+                                    "type": "enum"
+                                }
+                            }
+                        ],
+                        "name": "NotebookCell",
+                        "namespace": "com.linkedin.pegasus2avro.notebook",
+                        "type": "record"
+                    },
+                    "type": "array"
+                }
+            }
+        ],
+        "name": "NotebookContent",
+        "namespace": "com.linkedin.pegasus2avro.notebook",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "notebookInfo"
+        },
+        "doc": "Information about a Notebook\nNote: This is IN BETA version",
+        "fields": [
+            {
+                "Searchable": {
+                    "/*": {
+                        "queryByDefault": true
+                    }
+                },
+                "default": {},
+                "doc": "Custom property bag.",
+                "name": "customProperties",
+                "type": {
+                    "type": "map",
+                    "values": "string"
+                }
+            },
+            {
+                "default": null,
+                "doc": "URL where the reference exist",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "externalUrl",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL"
+                },
+                "doc": "Title of the Notebook",
+                "name": "title",
+                "type": "string"
+            },
+            {
+                "Searchable": {
+                    "fieldType": "TEXT",
+                    "hasValuesFieldName": "hasDescription"
+                },
+                "default": null,
+                "doc": "Detailed description about the Notebook",
+                "name": "description",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "doc": "Captures information about who created/last modified/deleted this Notebook and when",
+                "name": "changeAuditStamps",
+                "type": "com.linkedin.pegasus2avro.common.ChangeAuditStamps"
+            }
+        ],
+        "name": "NotebookInfo",
+        "namespace": "com.linkedin.pegasus2avro.notebook",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "editableNotebookProperties"
+        },
+        "doc": "Stores editable changes made to properties. This separates changes made from\ningestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines\nNote: This is IN BETA version",
+        "fields": [
+            {
+                "default": {
+                    "actor": "urn:li:corpuser:unknown",
+                    "impersonator": null,
+                    "message": null,
+                    "time": 0
+                },
+                "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
+                "name": "created",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+            },
+            {
+                "default": {
+                    "actor": "urn:li:corpuser:unknown",
+                    "impersonator": null,
+                    "message": null,
+                    "time": 0
+                },
+                "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
+                "name": "lastModified",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+            },
+            {
+                "default": null,
+                "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
+                "name": "deleted",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.common.AuditStamp"
+                ]
+            },
+            {
+                "Searchable": {
+                    "fieldName": "editedDescription",
+                    "fieldType": "TEXT"
+                },
+                "default": null,
+                "doc": "Edited documentation of the Notebook",
+                "name": "description",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            }
+        ],
+        "name": "EditableNotebookProperties",
+        "namespace": "com.linkedin.pegasus2avro.notebook",
+        "type": "record"
+    },
+    {
+        "doc": "Usage data for a given resource, rolled up into a bucket.",
+        "fields": [
+            {
+                "doc": " Bucket start time in milliseconds ",
+                "name": "bucket",
                 "type": "long"
             },
             {
-                "doc": "Version of this upgrade",
-                "name": "version",
+                "doc": " Bucket duration ",
+                "name": "duration",
+                "type": {
+                    "doc": "Enum to define the length of a bucket when doing aggregations",
+                    "name": "WindowDuration",
+                    "namespace": "com.linkedin.pegasus2avro.common",
+                    "symbols": [
+                        "YEAR",
+                        "MONTH",
+                        "WEEK",
+                        "DAY",
+                        "HOUR"
+                    ],
+                    "type": "enum"
+                }
+            },
+            {
+                "Urn": "Urn",
+                "doc": " Resource associated with these usage stats ",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                },
+                "name": "resource",
                 "type": "string"
+            },
+            {
+                "doc": " Metrics associated with this bucket ",
+                "name": "metrics",
+                "type": {
+                    "doc": "Metrics for usage data for a given resource and bucket. Not all fields\nmake sense for all buckets, so every field is optional.",
+                    "fields": [
+                        {
+                            "default": null,
+                            "doc": " Unique user count ",
+                            "name": "uniqueUserCount",
+                            "type": [
+                                "null",
+                                "int"
+                            ]
+                        },
+                        {
+                            "default": null,
+                            "doc": " Users within this bucket, with frequency counts ",
+                            "name": "users",
+                            "type": [
+                                "null",
+                                {
+                                    "items": {
+                                        "doc": " Records a single user's usage counts for a given resource ",
+                                        "fields": [
+                                            {
+                                                "Urn": "Urn",
+                                                "default": null,
+                                                "java": {
+                                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                                },
+                                                "name": "user",
+                                                "type": [
+                                                    "null",
+                                                    "string"
+                                                ]
+                                            },
+                                            {
+                                                "name": "count",
+                                                "type": "int"
+                                            },
+                                            {
+                                                "default": null,
+                                                "doc": " If user_email is set, we attempt to resolve the user's urn upon ingest ",
+                                                "name": "userEmail",
+                                                "type": [
+                                                    "null",
+                                                    "string"
+                                                ]
+                                            }
+                                        ],
+                                        "name": "UserUsageCounts",
+                                        "namespace": "com.linkedin.pegasus2avro.usage",
+                                        "type": "record"
+                                    },
+                                    "type": "array"
+                                }
+                            ]
+                        },
+                        {
+                            "default": null,
+                            "doc": " Total SQL query count ",
+                            "name": "totalSqlQueries",
+                            "type": [
+                                "null",
+                                "int"
+                            ]
+                        },
+                        {
+                            "default": null,
+                            "doc": " Frequent SQL queries; mostly makes sense for datasets in SQL databases ",
+                            "name": "topSqlQueries",
+                            "type": [
+                                "null",
+                                {
+                                    "items": "string",
+                                    "type": "array"
+                                }
+                            ]
+                        },
+                        {
+                            "default": null,
+                            "doc": " Field-level usage stats ",
+                            "name": "fields",
+                            "type": [
+                                "null",
+                                {
+                                    "items": {
+                                        "doc": " Records field-level usage counts for a given resource ",
+                                        "fields": [
+                                            {
+                                                "name": "fieldName",
+                                                "type": "string"
+                                            },
+                                            {
+                                                "name": "count",
+                                                "type": "int"
+                                            }
+                                        ],
+                                        "name": "FieldUsageCounts",
+                                        "namespace": "com.linkedin.pegasus2avro.usage",
+                                        "type": "record"
+                                    },
+                                    "type": "array"
+                                }
+                            ]
+                        }
+                    ],
+                    "name": "UsageAggregationMetrics",
+                    "namespace": "com.linkedin.pegasus2avro.usage",
+                    "type": "record"
+                }
             }
         ],
-        "name": "DataHubUpgradeRequest",
-        "namespace": "com.linkedin.pegasus2avro.upgrade",
+        "name": "UsageAggregation",
+        "namespace": "com.linkedin.pegasus2avro.usage",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "postInfo"
+        },
+        "doc": "Information about a DataHub Post.",
+        "fields": [
+            {
+                "doc": "Type of the Post.",
+                "name": "type",
+                "type": {
+                    "doc": "Enum defining types of Posts.",
+                    "name": "PostType",
+                    "namespace": "com.linkedin.pegasus2avro.post",
+                    "symbolDocs": {
+                        "HOME_PAGE_ANNOUNCEMENT": "The Post is an Home Page announcement."
+                    },
+                    "symbols": [
+                        "HOME_PAGE_ANNOUNCEMENT"
+                    ],
+                    "type": "enum"
+                }
+            },
+            {
+                "doc": "Content stored in the post.",
+                "name": "content",
+                "type": {
+                    "doc": "Content stored inside a Post.",
+                    "fields": [
+                        {
+                            "Searchable": {
+                                "fieldType": "TEXT_PARTIAL"
+                            },
+                            "doc": "Title of the post.",
+                            "name": "title",
+                            "type": "string"
+                        },
+                        {
+                            "doc": "Type of content held in the post.",
+                            "name": "type",
+                            "type": {
+                                "doc": "Enum defining the type of content held in a Post.",
+                                "name": "PostContentType",
+                                "namespace": "com.linkedin.pegasus2avro.post",
+                                "symbolDocs": {
+                                    "LINK": "Link content",
+                                    "TEXT": "Text content"
+                                },
+                                "symbols": [
+                                    "TEXT",
+                                    "LINK"
+                                ],
+                                "type": "enum"
+                            }
+                        },
+                        {
+                            "default": null,
+                            "doc": "Optional description of the post.",
+                            "name": "description",
+                            "type": [
+                                "null",
+                                "string"
+                            ]
+                        },
+                        {
+                            "default": null,
+                            "doc": "Optional link that the post is associated with.",
+                            "java": {
+                                "class": "com.linkedin.pegasus2avro.common.url.Url",
+                                "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                            },
+                            "name": "link",
+                            "type": [
+                                "null",
+                                "string"
+                            ]
+                        },
+                        {
+                            "default": null,
+                            "doc": "Optional media that the post is storing",
+                            "name": "media",
+                            "type": [
+                                "null",
+                                {
+                                    "doc": "Carries information about which roles a user is assigned to.",
+                                    "fields": [
+                                        {
+                                            "doc": "Type of content the Media is storing, e.g. image, video, etc.",
+                                            "name": "type",
+                                            "type": {
+                                                "doc": "Enum defining the type of content a Media object holds.",
+                                                "name": "MediaType",
+                                                "namespace": "com.linkedin.pegasus2avro.common",
+                                                "symbolDocs": {
+                                                    "IMAGE": "The Media holds an image."
+                                                },
+                                                "symbols": [
+                                                    "IMAGE"
+                                                ],
+                                                "type": "enum"
+                                            }
+                                        },
+                                        {
+                                            "doc": "Where the media content is stored.",
+                                            "java": {
+                                                "class": "com.linkedin.pegasus2avro.common.url.Url",
+                                                "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                                            },
+                                            "name": "location",
+                                            "type": "string"
+                                        }
+                                    ],
+                                    "name": "Media",
+                                    "namespace": "com.linkedin.pegasus2avro.common",
+                                    "type": "record"
+                                }
+                            ]
+                        }
+                    ],
+                    "name": "PostContent",
+                    "namespace": "com.linkedin.pegasus2avro.post",
+                    "type": "record"
+                }
+            },
+            {
+                "Searchable": {
+                    "fieldType": "COUNT"
+                },
+                "doc": "The time at which the post was initially created",
+                "name": "created",
+                "type": "long"
+            },
+            {
+                "Searchable": {
+                    "fieldType": "COUNT"
+                },
+                "doc": "The time at which the post was last modified",
+                "name": "lastModified",
+                "type": "long"
+            }
+        ],
+        "name": "PostInfo",
+        "namespace": "com.linkedin.pegasus2avro.post",
         "type": "record"
     },
     {
         "Aspect": {
             "name": "dataHubUpgradeResult"
         },
         "doc": "Information collected when a DataHubUpgrade successfully finishes",
@@ -12893,9 +12023,1064 @@
                     }
                 ]
             }
         ],
         "name": "DataHubUpgradeResult",
         "namespace": "com.linkedin.pegasus2avro.upgrade",
         "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "dataHubUpgradeRequest"
+        },
+        "doc": "Information collected when kicking off a DataHubUpgrade",
+        "fields": [
+            {
+                "doc": "Timestamp when we started this DataHubUpgrade",
+                "name": "timestampMs",
+                "type": "long"
+            },
+            {
+                "doc": "Version of this upgrade",
+                "name": "version",
+                "type": "string"
+            }
+        ],
+        "name": "DataHubUpgradeRequest",
+        "namespace": "com.linkedin.pegasus2avro.upgrade",
+        "type": "record"
+    },
+    {
+        "doc": "The filter for finding a record or a collection of records",
+        "fields": [
+            {
+                "default": null,
+                "doc": "A list of disjunctive criterion for the filter. (or operation to combine filters)",
+                "name": "or",
+                "type": [
+                    "null",
+                    {
+                        "items": {
+                            "doc": "A list of criterion and'd together.",
+                            "fields": [
+                                {
+                                    "doc": "A list of and criteria the filter applies to the query",
+                                    "name": "and",
+                                    "type": {
+                                        "items": {
+                                            "doc": "A criterion for matching a field with given value",
+                                            "fields": [
+                                                {
+                                                    "doc": "The name of the field that the criterion refers to",
+                                                    "name": "field",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "doc": "The value of the intended field",
+                                                    "name": "value",
+                                                    "type": "string"
+                                                },
+                                                {
+                                                    "default": [],
+                                                    "doc": "Values. one of which the intended field should match\nNote, if values is set, the above \"value\" field will be ignored",
+                                                    "name": "values",
+                                                    "type": {
+                                                        "items": "string",
+                                                        "type": "array"
+                                                    }
+                                                },
+                                                {
+                                                    "default": "EQUAL",
+                                                    "doc": "The condition for the criterion, e.g. EQUAL, START_WITH",
+                                                    "name": "condition",
+                                                    "type": {
+                                                        "doc": "The matching condition in a filter criterion",
+                                                        "name": "Condition",
+                                                        "namespace": "com.linkedin.pegasus2avro.metadata.query.filter",
+                                                        "symbolDocs": {
+                                                            "CONTAIN": "Represent the relation: String field contains value, e.g. name contains Profile",
+                                                            "END_WITH": "Represent the relation: String field ends with value, e.g. name ends with Event",
+                                                            "EQUAL": "Represent the relation: field = value, e.g. platform = hdfs",
+                                                            "GREATER_THAN": "Represent the relation greater than, e.g. ownerCount > 5",
+                                                            "GREATER_THAN_OR_EQUAL_TO": "Represent the relation greater than or equal to, e.g. ownerCount >= 5",
+                                                            "IN": "Represent the relation: String field is one of the array values to, e.g. name in [\"Profile\", \"Event\"]",
+                                                            "IS_NULL": "Represent the relation: field is null, e.g. platform is null",
+                                                            "LESS_THAN": "Represent the relation less than, e.g. ownerCount < 3",
+                                                            "LESS_THAN_OR_EQUAL_TO": "Represent the relation less than or equal to, e.g. ownerCount <= 3",
+                                                            "START_WITH": "Represent the relation: String field starts with value, e.g. name starts with PageView"
+                                                        },
+                                                        "symbols": [
+                                                            "CONTAIN",
+                                                            "END_WITH",
+                                                            "EQUAL",
+                                                            "IS_NULL",
+                                                            "GREATER_THAN",
+                                                            "GREATER_THAN_OR_EQUAL_TO",
+                                                            "IN",
+                                                            "LESS_THAN",
+                                                            "LESS_THAN_OR_EQUAL_TO",
+                                                            "START_WITH"
+                                                        ],
+                                                        "type": "enum"
+                                                    }
+                                                },
+                                                {
+                                                    "default": false,
+                                                    "doc": "Whether the condition should be negated",
+                                                    "name": "negated",
+                                                    "type": "boolean"
+                                                }
+                                            ],
+                                            "name": "Criterion",
+                                            "namespace": "com.linkedin.pegasus2avro.metadata.query.filter",
+                                            "type": "record"
+                                        },
+                                        "type": "array"
+                                    }
+                                }
+                            ],
+                            "name": "ConjunctiveCriterion",
+                            "namespace": "com.linkedin.pegasus2avro.metadata.query.filter",
+                            "type": "record"
+                        },
+                        "type": "array"
+                    }
+                ]
+            },
+            {
+                "default": null,
+                "doc": "Deprecated! A list of conjunctive criterion for the filter. If \"or\" field is provided, then this field is ignored.",
+                "name": "criteria",
+                "type": [
+                    "null",
+                    {
+                        "items": "com.linkedin.pegasus2avro.metadata.query.filter.Criterion",
+                        "type": "array"
+                    }
+                ]
+            }
+        ],
+        "name": "Filter",
+        "namespace": "com.linkedin.pegasus2avro.metadata.query.filter",
+        "type": "record"
+    },
+    "com.linkedin.pegasus2avro.metadata.key.MLFeatureTableKey",
+    {
+        "Aspect": {
+            "entityAspects": [
+                "containerProperties",
+                "editableContainerProperties",
+                "dataPlatformInstance",
+                "subTypes",
+                "ownership",
+                "container",
+                "globalTags",
+                "glossaryTerms",
+                "institutionalMemory",
+                "browsePaths",
+                "status",
+                "domains"
+            ],
+            "entityCategory": "_unset_",
+            "entityDoc": "A container of related data assets.",
+            "keyForEntity": "container",
+            "name": "containerKey"
+        },
+        "doc": "Key for an Asset Container",
+        "fields": [
+            {
+                "default": null,
+                "doc": "Unique guid for container",
+                "name": "guid",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            }
+        ],
+        "name": "ContainerKey",
+        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "type": "record"
+    },
+    "com.linkedin.pegasus2avro.metadata.key.DataHubRetentionKey",
+    "com.linkedin.pegasus2avro.metadata.key.DataJobKey",
+    "com.linkedin.pegasus2avro.metadata.key.MLModelDeploymentKey",
+    "com.linkedin.pegasus2avro.metadata.key.SchemaFieldKey",
+    "com.linkedin.pegasus2avro.metadata.key.DataProcessKey",
+    "com.linkedin.pegasus2avro.metadata.key.CorpGroupKey",
+    "com.linkedin.pegasus2avro.metadata.key.MLPrimaryKeyKey",
+    "com.linkedin.pegasus2avro.metadata.key.DataFlowKey",
+    "com.linkedin.pegasus2avro.metadata.key.DashboardKey",
+    "com.linkedin.pegasus2avro.metadata.key.MLModelGroupKey",
+    {
+        "Aspect": {
+            "entityAspects": [
+                "telemetryClientId"
+            ],
+            "entityCategory": "internal",
+            "keyForEntity": "telemetry",
+            "name": "telemetryKey"
+        },
+        "doc": "Key for the telemetry client ID, only one should ever exist",
+        "fields": [
+            {
+                "doc": "The telemetry entity name, which serves as a unique id",
+                "name": "name",
+                "type": "string"
+            }
+        ],
+        "name": "TelemetryKey",
+        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "type": "record"
+    },
+    "com.linkedin.pegasus2avro.metadata.key.GlossaryTermKey",
+    "com.linkedin.pegasus2avro.metadata.key.GlossaryNodeKey",
+    {
+        "Aspect": {
+            "entityAspects": [
+                "dataProcessInstanceInput",
+                "dataProcessInstanceOutput",
+                "dataProcessInstanceProperties",
+                "dataProcessInstanceRelationships",
+                "dataProcessInstanceRunEvent"
+            ],
+            "entityCategory": "_unset_",
+            "entityDoc": "DataProcessInstance represents an instance of a datajob/jobflow run",
+            "keyForEntity": "dataProcessInstance",
+            "name": "dataProcessInstanceKey"
+        },
+        "doc": "Key for an Asset DataProcessInstance",
+        "fields": [
+            {
+                "doc": "A unique id for the DataProcessInstance . Should be separate from the name used for displaying a DataProcessInstance.",
+                "name": "id",
+                "type": "string"
+            }
+        ],
+        "name": "DataProcessInstanceKey",
+        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "entityAspects": [
+                "dataHubRoleInfo"
+            ],
+            "entityCategory": "core",
+            "keyForEntity": "dataHubRole",
+            "name": "dataHubRoleKey"
+        },
+        "doc": "Key for a DataHub Role",
+        "fields": [
+            {
+                "doc": "A unique id for the DataHub role record. Generated on the server side at role creation time.",
+                "name": "id",
+                "type": "string"
+            }
+        ],
+        "name": "DataHubRoleKey",
+        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "type": "record"
+    },
+    "com.linkedin.pegasus2avro.metadata.key.MLModelKey",
+    {
+        "Aspect": {
+            "entityAspects": [
+                "dataHubViewInfo"
+            ],
+            "entityCategory": "core",
+            "keyForEntity": "dataHubView",
+            "name": "dataHubViewKey"
+        },
+        "doc": "Key for a DataHub View",
+        "fields": [
+            {
+                "doc": "A unique id for the View",
+                "name": "id",
+                "type": "string"
+            }
+        ],
+        "name": "DataHubViewKey",
+        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "entityAspects": [
+                "queryProperties",
+                "querySubjects",
+                "status"
+            ],
+            "entityCategory": "core",
+            "keyForEntity": "query",
+            "name": "queryKey"
+        },
+        "doc": "Key for a Query",
+        "fields": [
+            {
+                "doc": "A unique id for the Query.",
+                "name": "id",
+                "type": "string"
+            }
+        ],
+        "name": "QueryKey",
+        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "type": "record"
+    },
+    "com.linkedin.pegasus2avro.metadata.key.CorpUserKey",
+    {
+        "Aspect": {
+            "entityAspects": [
+                "dataPlatformInstanceProperties",
+                "ownership",
+                "globalTags",
+                "institutionalMemory",
+                "deprecation",
+                "status"
+            ],
+            "entityCategory": "internal",
+            "keyForEntity": "dataPlatformInstance",
+            "name": "dataPlatformInstanceKey"
+        },
+        "doc": "Key for a Dataset",
+        "fields": [
+            {
+                "Urn": "Urn",
+                "doc": "Data platform urn associated with the instance",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                },
+                "name": "platform",
+                "type": "string"
+            },
+            {
+                "doc": "Unique instance id",
+                "name": "instance",
+                "type": "string"
+            }
+        ],
+        "name": "DataPlatformInstanceKey",
+        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "entityAspects": [
+                "notebookInfo",
+                "notebookContent",
+                "editableNotebookProperties",
+                "ownership",
+                "status",
+                "globalTags",
+                "glossaryTerms",
+                "browsePaths",
+                "institutionalMemory",
+                "domains",
+                "subTypes",
+                "dataPlatformInstance"
+            ],
+            "entityCategory": "_unset_",
+            "entityDoc": "Notebook represents a combination of query, text, chart and etc. This is in BETA version",
+            "keyForEntity": "notebook",
+            "name": "notebookKey"
+        },
+        "doc": "Key for a Notebook",
+        "fields": [
+            {
+                "doc": "The name of the Notebook tool such as QueryBook, etc.",
+                "name": "notebookTool",
+                "type": "string"
+            },
+            {
+                "doc": "Unique id for the Notebook. This id should be globally unique for a Notebook tool even when there are multiple deployments of it. As an example, Notebook URL could be used here for QueryBook such as 'querybook.com/notebook/773'",
+                "name": "notebookId",
+                "type": "string"
+            }
+        ],
+        "name": "NotebookKey",
+        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "type": "record"
+    },
+    "com.linkedin.pegasus2avro.metadata.key.MLFeatureKey",
+    "com.linkedin.pegasus2avro.metadata.key.ChartKey",
+    {
+        "Aspect": {
+            "entityAspects": [
+                "assertionInfo",
+                "dataPlatformInstance",
+                "assertionRunEvent",
+                "status"
+            ],
+            "entityCategory": "core",
+            "entityDoc": "Assertion represents a data quality rule applied on one or more dataset.",
+            "keyForEntity": "assertion",
+            "name": "assertionKey"
+        },
+        "doc": "Key for a Assertion",
+        "fields": [
+            {
+                "doc": "Unique id for the assertion.",
+                "name": "assertionId",
+                "type": "string"
+            }
+        ],
+        "name": "AssertionKey",
+        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "entityAspects": [
+                "dataHubSecretValue"
+            ],
+            "entityCategory": "internal",
+            "keyForEntity": "dataHubSecret",
+            "name": "dataHubSecretKey"
+        },
+        "doc": "Key for a DataHub Secret",
+        "fields": [
+            {
+                "doc": "A unique id for the Secret",
+                "name": "id",
+                "type": "string"
+            }
+        ],
+        "name": "DataHubSecretKey",
+        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "type": "record"
+    },
+    "com.linkedin.pegasus2avro.metadata.key.DataHubPolicyKey",
+    {
+        "Aspect": {
+            "entityAspects": [
+                "testInfo"
+            ],
+            "entityCategory": "core",
+            "entityDoc": "A DataHub test",
+            "keyForEntity": "test",
+            "name": "testKey"
+        },
+        "doc": "Key for a Test",
+        "fields": [
+            {
+                "doc": "Unique id for the test",
+                "name": "id",
+                "type": "string"
+            }
+        ],
+        "name": "TestKey",
+        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "entityAspects": [
+                "dataHubAccessTokenInfo"
+            ],
+            "entityCategory": "internal",
+            "keyForEntity": "dataHubAccessToken",
+            "name": "dataHubAccessTokenKey"
+        },
+        "doc": "Key for a DataHub Access Token",
+        "fields": [
+            {
+                "doc": "Access token's SHA-256 hashed JWT signature",
+                "name": "id",
+                "type": "string"
+            }
+        ],
+        "name": "DataHubAccessTokenKey",
+        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "entityAspects": [
+                "dataHubUpgradeRequest",
+                "dataHubUpgradeResult"
+            ],
+            "entityCategory": "internal",
+            "keyForEntity": "dataHubUpgrade",
+            "name": "dataHubUpgradeKey"
+        },
+        "doc": "Key for a DataHubUpgrade",
+        "fields": [
+            {
+                "name": "id",
+                "type": "string"
+            }
+        ],
+        "name": "DataHubUpgradeKey",
+        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "type": "record"
+    },
+    "com.linkedin.pegasus2avro.metadata.key.DataPlatformKey",
+    {
+        "Aspect": {
+            "entityAspects": [
+                "inviteToken"
+            ],
+            "entityCategory": "core",
+            "keyForEntity": "inviteToken",
+            "name": "inviteTokenKey"
+        },
+        "doc": "Key for an InviteToken.",
+        "fields": [
+            {
+                "doc": "A unique id for the invite token.",
+                "name": "id",
+                "type": "string"
+            }
+        ],
+        "name": "InviteTokenKey",
+        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "entityAspects": [
+                "postInfo"
+            ],
+            "entityCategory": "core",
+            "keyForEntity": "post",
+            "name": "postKey"
+        },
+        "doc": "Key for a Post.",
+        "fields": [
+            {
+                "doc": "A unique id for the DataHub Post record. Generated on the server side at Post creation time.",
+                "name": "id",
+                "type": "string"
+            }
+        ],
+        "name": "PostKey",
+        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "entityAspects": [
+                "dataHubExecutionRequestInput",
+                "dataHubExecutionRequestSignal",
+                "dataHubExecutionRequestResult"
+            ],
+            "entityCategory": "internal",
+            "keyForEntity": "dataHubExecutionRequest",
+            "name": "dataHubExecutionRequestKey"
+        },
+        "doc": "Key for an DataHub Execution Request",
+        "fields": [
+            {
+                "doc": "A unique id for the DataHub execution request.",
+                "name": "id",
+                "type": "string"
+            }
+        ],
+        "name": "ExecutionRequestKey",
+        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "entityAspects": [
+                "dataHubIngestionSourceInfo"
+            ],
+            "entityCategory": "internal",
+            "keyForEntity": "dataHubIngestionSource",
+            "name": "dataHubIngestionSourceKey"
+        },
+        "doc": "Key for a DataHub ingestion source",
+        "fields": [
+            {
+                "doc": "A unique id for the Ingestion Source, either generated or provided",
+                "name": "id",
+                "type": "string"
+            }
+        ],
+        "name": "DataHubIngestionSourceKey",
+        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "entityAspects": [
+                "domainProperties",
+                "institutionalMemory",
+                "ownership"
+            ],
+            "entityCategory": "_unset_",
+            "entityDoc": "A data domain within an organization.",
+            "keyForEntity": "domain",
+            "name": "domainKey"
+        },
+        "doc": "Key for an Asset Domain",
+        "fields": [
+            {
+                "doc": "A unique id for the domain. Should be separate from the name used for displaying a Domain.",
+                "name": "id",
+                "type": "string"
+            }
+        ],
+        "name": "DomainKey",
+        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "type": "record"
+    },
+    "com.linkedin.pegasus2avro.metadata.key.DatasetKey",
+    {
+        "Aspect": {
+            "entityAspects": [
+                "dataHubStepStateProperties"
+            ],
+            "entityCategory": "core",
+            "keyForEntity": "dataHubStepState",
+            "name": "dataHubStepStateKey"
+        },
+        "doc": "Key for a DataHub Step State",
+        "fields": [
+            {
+                "doc": "A unique id for the state",
+                "name": "id",
+                "type": "string"
+            }
+        ],
+        "name": "DataHubStepStateKey",
+        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "entityAspects": [
+                "globalSettingsInfo"
+            ],
+            "entityCategory": "internal",
+            "entityDoc": "Global settings for an the platform",
+            "keyForEntity": "globalSettings",
+            "name": "globalSettingsKey"
+        },
+        "doc": "Key for a Global Settings",
+        "fields": [
+            {
+                "doc": "Id for the settings. There should be only 1 global settings urn: urn:li:globalSettings:0",
+                "name": "id",
+                "type": "string"
+            }
+        ],
+        "name": "GlobalSettingsKey",
+        "namespace": "com.linkedin.pegasus2avro.metadata.key",
+        "type": "record"
+    },
+    "com.linkedin.pegasus2avro.metadata.key.TagKey",
+    {
+        "Aspect": {
+            "name": "globalSettingsInfo"
+        },
+        "doc": "DataHub Global platform settings. Careful - these should not be modified by the outside world!",
+        "fields": [
+            {
+                "default": null,
+                "doc": "Settings related to the Views Feature",
+                "name": "views",
+                "type": [
+                    "null",
+                    {
+                        "doc": "Settings for DataHub Views feature.",
+                        "fields": [
+                            {
+                                "Urn": "Urn",
+                                "default": null,
+                                "doc": "The default View for the instance, or organization.",
+                                "java": {
+                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                },
+                                "name": "defaultView",
+                                "type": [
+                                    "null",
+                                    "string"
+                                ]
+                            }
+                        ],
+                        "name": "GlobalViewsSettings",
+                        "namespace": "com.linkedin.pegasus2avro.settings.global",
+                        "type": "record"
+                    }
+                ]
+            }
+        ],
+        "name": "GlobalSettingsInfo",
+        "namespace": "com.linkedin.pegasus2avro.settings.global",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "testResults"
+        },
+        "doc": "Information about a Test Result",
+        "fields": [
+            {
+                "Relationship": {
+                    "/*/test": {
+                        "entityTypes": [
+                            "test"
+                        ],
+                        "name": "IsFailing"
+                    }
+                },
+                "Searchable": {
+                    "/*/test": {
+                        "fieldName": "failingTests",
+                        "fieldType": "URN",
+                        "hasValuesFieldName": "hasFailingTests"
+                    }
+                },
+                "doc": "Results that are failing",
+                "name": "failing",
+                "type": {
+                    "items": {
+                        "doc": "Information about a Test Result",
+                        "fields": [
+                            {
+                                "Urn": "Urn",
+                                "doc": "The urn of the test",
+                                "java": {
+                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+                                },
+                                "name": "test",
+                                "type": "string"
+                            },
+                            {
+                                "doc": "The type of the result",
+                                "name": "type",
+                                "type": {
+                                    "name": "TestResultType",
+                                    "namespace": "com.linkedin.pegasus2avro.test",
+                                    "symbolDocs": {
+                                        "FAILURE": " The Test Failed",
+                                        "SUCCESS": " The Test Succeeded"
+                                    },
+                                    "symbols": [
+                                        "SUCCESS",
+                                        "FAILURE"
+                                    ],
+                                    "type": "enum"
+                                }
+                            }
+                        ],
+                        "name": "TestResult",
+                        "namespace": "com.linkedin.pegasus2avro.test",
+                        "type": "record"
+                    },
+                    "type": "array"
+                }
+            },
+            {
+                "Relationship": {
+                    "/*/test": {
+                        "entityTypes": [
+                            "test"
+                        ],
+                        "name": "IsPassing"
+                    }
+                },
+                "Searchable": {
+                    "/*/test": {
+                        "fieldName": "passingTests",
+                        "fieldType": "URN",
+                        "hasValuesFieldName": "hasPassingTests"
+                    }
+                },
+                "doc": "Results that are passing",
+                "name": "passing",
+                "type": {
+                    "items": "com.linkedin.pegasus2avro.test.TestResult",
+                    "type": "array"
+                }
+            }
+        ],
+        "name": "TestResults",
+        "namespace": "com.linkedin.pegasus2avro.test",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "testInfo"
+        },
+        "doc": "Information about a DataHub Test",
+        "fields": [
+            {
+                "Searchable": {
+                    "fieldType": "TEXT_PARTIAL"
+                },
+                "doc": "The name of the test",
+                "name": "name",
+                "type": "string"
+            },
+            {
+                "Searchable": {
+                    "fieldType": "KEYWORD"
+                },
+                "doc": "Category of the test",
+                "name": "category",
+                "type": "string"
+            },
+            {
+                "Searchable": {
+                    "fieldType": "TEXT"
+                },
+                "default": null,
+                "doc": "Description of the test",
+                "name": "description",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "doc": "Configuration for the Test",
+                "name": "definition",
+                "type": {
+                    "fields": [
+                        {
+                            "doc": "The Test Definition Type",
+                            "name": "type",
+                            "type": {
+                                "name": "TestDefinitionType",
+                                "namespace": "com.linkedin.pegasus2avro.test",
+                                "symbolDocs": {
+                                    "JSON": "JSON / YAML test def"
+                                },
+                                "symbols": [
+                                    "JSON"
+                                ],
+                                "type": "enum"
+                            }
+                        },
+                        {
+                            "default": null,
+                            "doc": "JSON format configuration for the test",
+                            "name": "json",
+                            "type": [
+                                "null",
+                                "string"
+                            ]
+                        }
+                    ],
+                    "name": "TestDefinition",
+                    "namespace": "com.linkedin.pegasus2avro.test",
+                    "type": "record"
+                }
+            }
+        ],
+        "name": "TestInfo",
+        "namespace": "com.linkedin.pegasus2avro.test",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "dataPlatformInstanceProperties"
+        },
+        "doc": "Properties associated with a Data Platform Instance",
+        "fields": [
+            {
+                "Searchable": {
+                    "/*": {
+                        "queryByDefault": true
+                    }
+                },
+                "default": {},
+                "doc": "Custom property bag.",
+                "name": "customProperties",
+                "type": {
+                    "type": "map",
+                    "values": "string"
+                }
+            },
+            {
+                "default": null,
+                "doc": "URL where the reference exist",
+                "java": {
+                    "class": "com.linkedin.pegasus2avro.common.url.Url",
+                    "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
+                },
+                "name": "externalUrl",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "Searchable": {
+                    "boostScore": 10.0,
+                    "enableAutocomplete": true,
+                    "fieldType": "TEXT_PARTIAL"
+                },
+                "default": null,
+                "doc": "Display name of the Data Platform Instance",
+                "name": "name",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "Searchable": {
+                    "fieldType": "TEXT",
+                    "hasValuesFieldName": "hasDescription"
+                },
+                "default": null,
+                "doc": "Documentation of the Data Platform Instance",
+                "name": "description",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            }
+        ],
+        "name": "DataPlatformInstanceProperties",
+        "namespace": "com.linkedin.pegasus2avro.dataplatforminstance",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "dataHubViewInfo"
+        },
+        "doc": "Information about a DataHub View. -- TODO: Understand whether an entity type filter is required.",
+        "fields": [
+            {
+                "Searchable": {
+                    "fieldType": "TEXT_PARTIAL"
+                },
+                "doc": "The name of the View",
+                "name": "name",
+                "type": "string"
+            },
+            {
+                "default": null,
+                "doc": "Description of the view",
+                "name": "description",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "Searchable": {},
+                "doc": "The type of View",
+                "name": "type",
+                "type": {
+                    "name": "DataHubViewType",
+                    "namespace": "com.linkedin.pegasus2avro.view",
+                    "symbolDocs": {
+                        "GLOBAL": "A global view, which all users can see and use.",
+                        "PERSONAL": "A view private for a specific person."
+                    },
+                    "symbols": [
+                        "PERSONAL",
+                        "GLOBAL"
+                    ],
+                    "type": "enum"
+                }
+            },
+            {
+                "doc": "The view itself",
+                "name": "definition",
+                "type": {
+                    "doc": "A View definition.",
+                    "fields": [
+                        {
+                            "doc": "The Entity Types in the scope of the View.",
+                            "name": "entityTypes",
+                            "type": {
+                                "items": "string",
+                                "type": "array"
+                            }
+                        },
+                        {
+                            "doc": "The filter criteria, which represents the view itself",
+                            "name": "filter",
+                            "type": "com.linkedin.pegasus2avro.metadata.query.filter.Filter"
+                        }
+                    ],
+                    "name": "DataHubViewDefinition",
+                    "namespace": "com.linkedin.pegasus2avro.view",
+                    "type": "record"
+                }
+            },
+            {
+                "Searchable": {
+                    "/actor": {
+                        "fieldName": "createdBy",
+                        "fieldType": "URN"
+                    },
+                    "/time": {
+                        "fieldName": "createdAt",
+                        "fieldType": "DATETIME"
+                    }
+                },
+                "doc": "Audit stamp capturing the time and actor who created the View.",
+                "name": "created",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+            },
+            {
+                "Searchable": {
+                    "/time": {
+                        "fieldName": "lastModifiedAt",
+                        "fieldType": "DATETIME"
+                    }
+                },
+                "doc": "Audit stamp capturing the time and actor who last modified the View.",
+                "name": "lastModified",
+                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
+            }
+        ],
+        "name": "DataHubViewInfo",
+        "namespace": "com.linkedin.pegasus2avro.view",
+        "type": "record"
+    },
+    {
+        "Aspect": {
+            "name": "dataHubSecretValue"
+        },
+        "doc": "The value of a DataHub Secret",
+        "fields": [
+            {
+                "Searchable": {
+                    "fieldType": "TEXT_PARTIAL"
+                },
+                "doc": "The display name for the secret",
+                "name": "name",
+                "type": "string"
+            },
+            {
+                "doc": "The AES-encrypted value of the DataHub secret.",
+                "name": "value",
+                "type": "string"
+            },
+            {
+                "default": null,
+                "doc": "Description of the secret",
+                "name": "description",
+                "type": [
+                    "null",
+                    "string"
+                ]
+            },
+            {
+                "Searchable": {
+                    "/time": {
+                        "fieldName": "createdTime",
+                        "fieldType": "DATETIME"
+                    }
+                },
+                "default": null,
+                "doc": "Created Audit stamp",
+                "name": "created",
+                "type": [
+                    "null",
+                    "com.linkedin.pegasus2avro.common.AuditStamp"
+                ]
+            }
+        ],
+        "name": "DataHubSecretValue",
+        "namespace": "com.linkedin.pegasus2avro.secret",
+        "type": "record"
     }
 ]
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schema_classes.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/schema_classes.py`

 * *Files 0% similar despite different names*

```diff
@@ -46,17 +46,17 @@
 SCHEMA_JSON_STR = __read_file(os.path.join(os.path.dirname(__file__), "schema.avsc"))
 
 
 __NAMES, SCHEMA = __get_names_and_schema(SCHEMA_JSON_STR)
 __SCHEMAS: Dict[str, RecordSchema] = {}
 
 class _Aspect(DictWrapper):
-    ASPECT_NAME: str = None  # type: ignore
-    ASPECT_TYPE: str = "default"
-    ASPECT_INFO: dict = None  # type: ignore
+    ASPECT_NAME: ClassVar[str] = None  # type: ignore
+    ASPECT_TYPE: ClassVar[str] = "default"
+    ASPECT_INFO: ClassVar[dict] = None  # type: ignore
 
     def __init__(self):
         if type(self) is _Aspect:
             raise TypeError("_Aspect is an abstract class, and cannot be instantiated directly.")
         super().__init__()
 
     @classmethod
@@ -211,15 +211,15 @@
     
     
 class DataHubAccessTokenInfoClass(_Aspect):
     """Information about a DataHub Access Token"""
 
 
     ASPECT_NAME = 'dataHubAccessTokenInfo'
-    ASPECT_INFO = {'name': 'dataHubAccessTokenInfo'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.access.token.DataHubAccessTokenInfo")
 
     def __init__(self,
         name: str,
         actorUrn: str,
         ownerUrn: str,
         createdAt: int,
@@ -318,15 +318,15 @@
     
     
 class AssertionInfoClass(_Aspect):
     """Information about an assertion"""
 
 
     ASPECT_NAME = 'assertionInfo'
-    ASPECT_INFO = {'name': 'assertionInfo'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.assertion.AssertionInfo")
 
     def __init__(self,
         type: Union[str, "AssertionTypeClass"],
         customProperties: Optional[Dict[str, str]]=None,
         externalUrl: Union[None, str]=None,
         datasetAssertion: Union[None, "DatasetAssertionInfoClass"]=None,
@@ -531,15 +531,15 @@
 class AssertionRunEventClass(_Aspect):
     """An event representing the current status of evaluating an assertion on a batch.
     AssertionRunEvent should be used for reporting the status of a run as an assertion evaluation progresses."""
 
 
     ASPECT_NAME = 'assertionRunEvent'
     ASPECT_TYPE = 'timeseries'
-    ASPECT_INFO = {'name': 'assertionRunEvent', 'type': 'timeseries'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.assertion.AssertionRunEvent")
 
     def __init__(self,
         timestampMillis: int,
         runId: str,
         assertionUrn: str,
         asserteeUrn: str,
@@ -1182,15 +1182,15 @@
     
     
 class ChartInfoClass(_Aspect):
     """Information about a chart"""
 
 
     ASPECT_NAME = 'chartInfo'
-    ASPECT_INFO = {'name': 'chartInfo'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.chart.ChartInfo")
 
     def __init__(self,
         title: str,
         description: str,
         lastModified: "ChangeAuditStampsClass",
         customProperties: Optional[Dict[str, str]]=None,
@@ -1365,15 +1365,15 @@
     
     
 class ChartQueryClass(_Aspect):
     """Information for chart query which is used for getting data of the chart"""
 
 
     ASPECT_NAME = 'chartQuery'
-    ASPECT_INFO = {'name': 'chartQuery'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.chart.ChartQuery")
 
     def __init__(self,
         rawQuery: str,
         type: Union[str, "ChartQueryTypeClass"],
     ):
         super().__init__()
@@ -1463,15 +1463,15 @@
     
     If this aspect represents the latest snapshot of the statistics about a Chart, the eventGranularity field should be null.
     If this aspect represents a bucketed window of usage statistics (e.g. over a day), then the eventGranularity field should be set accordingly."""
 
 
     ASPECT_NAME = 'chartUsageStatistics'
     ASPECT_TYPE = 'timeseries'
-    ASPECT_INFO = {'name': 'chartUsageStatistics', 'type': 'timeseries'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.chart.ChartUsageStatistics")
 
     def __init__(self,
         timestampMillis: int,
         eventGranularity: Union[None, "TimeWindowSizeClass"]=None,
         partitionSpec: Optional[Union["PartitionSpecClass", None]]=None,
         messageId: Union[None, str]=None,
@@ -1636,15 +1636,15 @@
     
 class EditableChartPropertiesClass(_Aspect):
     """Stores editable changes made to properties. This separates changes made from
     ingestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines"""
 
 
     ASPECT_NAME = 'editableChartProperties'
-    ASPECT_INFO = {'name': 'editableChartProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.chart.EditableChartProperties")
 
     def __init__(self,
         created: Optional["AuditStampClass"]=None,
         lastModified: Optional["AuditStampClass"]=None,
         deleted: Union[None, "AuditStampClass"]=None,
         description: Union[None, str]=None,
@@ -1809,15 +1809,15 @@
     
     
 class BrowsePathsClass(_Aspect):
     """Shared aspect containing Browse Paths to be indexed for an entity."""
 
 
     ASPECT_NAME = 'browsePaths'
-    ASPECT_INFO = {'name': 'browsePaths'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.BrowsePaths")
 
     def __init__(self,
         paths: List[str],
     ):
         super().__init__()
         
@@ -1919,15 +1919,15 @@
     
     
 class CostClass(_Aspect):
     # No docs available.
 
 
     ASPECT_NAME = 'cost'
-    ASPECT_INFO = {'name': 'cost'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.Cost")
 
     def __init__(self,
         costType: Union[str, "CostTypeClass"],
         cost: "CostCostClass",
     ):
         super().__init__()
@@ -2046,15 +2046,15 @@
     
     
 class DataPlatformInstanceClass(_Aspect):
     """The specific instance of the data platform that this entity belongs to"""
 
 
     ASPECT_NAME = 'dataPlatformInstance'
-    ASPECT_INFO = {'name': 'dataPlatformInstance'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.DataPlatformInstance")
 
     def __init__(self,
         platform: str,
         instance: Union[None, str]=None,
     ):
         super().__init__()
@@ -2097,15 +2097,15 @@
     
     
 class DeprecationClass(_Aspect):
     """Deprecation status of an entity"""
 
 
     ASPECT_NAME = 'deprecation'
-    ASPECT_INFO = {'name': 'deprecation'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.Deprecation")
 
     def __init__(self,
         deprecated: bool,
         note: str,
         actor: str,
         decommissionTime: Union[None, int]=None,
@@ -2265,15 +2265,15 @@
     
     
 class EmbedClass(_Aspect):
     """Information regarding rendering an embed for an asset."""
 
 
     ASPECT_NAME = 'embed'
-    ASPECT_INFO = {'name': 'embed'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.Embed")
 
     def __init__(self,
         renderUrl: Union[None, str]=None,
     ):
         super().__init__()
         
@@ -2337,15 +2337,15 @@
     
     
 class GlobalTagsClass(_Aspect):
     """Tag aspect used for applying tags to an entity"""
 
 
     ASPECT_NAME = 'globalTags'
-    ASPECT_INFO = {'name': 'globalTags'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.GlobalTags")
 
     def __init__(self,
         tags: List["TagAssociationClass"],
     ):
         super().__init__()
         
@@ -2421,15 +2421,15 @@
     
     
 class GlossaryTermsClass(_Aspect):
     """Related business terms information"""
 
 
     ASPECT_NAME = 'glossaryTerms'
-    ASPECT_INFO = {'name': 'glossaryTerms'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.GlossaryTerms")
 
     def __init__(self,
         terms: List["GlossaryTermAssociationClass"],
         auditStamp: "AuditStampClass",
     ):
         super().__init__()
@@ -2519,15 +2519,15 @@
     
     
 class InputFieldsClass(_Aspect):
     """Information about the fields a chart or dashboard references"""
 
 
     ASPECT_NAME = 'inputFields'
-    ASPECT_INFO = {'name': 'inputFields'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.InputFields")
 
     def __init__(self,
         fields: List["InputFieldClass"],
     ):
         super().__init__()
         
@@ -2556,15 +2556,15 @@
     
     
 class InstitutionalMemoryClass(_Aspect):
     """Institutional memory of an entity. This is a way to link to relevant documentation and provide description of the documentation. Institutional or tribal knowledge is very important for users to leverage the entity."""
 
 
     ASPECT_NAME = 'institutionalMemory'
-    ASPECT_INFO = {'name': 'institutionalMemory'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.InstitutionalMemory")
 
     def __init__(self,
         elements: List["InstitutionalMemoryMetadataClass"],
     ):
         super().__init__()
         
@@ -2772,15 +2772,15 @@
     
 class OperationClass(_Aspect):
     """Operational info for an entity."""
 
 
     ASPECT_NAME = 'operation'
     ASPECT_TYPE = 'timeseries'
-    ASPECT_INFO = {'name': 'operation', 'type': 'timeseries'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.Operation")
 
     def __init__(self,
         timestampMillis: int,
         operationType: Union[str, "OperationTypeClass"],
         lastUpdatedTimestamp: int,
         eventGranularity: Union[None, "TimeWindowSizeClass"]=None,
@@ -3006,15 +3006,15 @@
     
     
 class OriginClass(_Aspect):
     """Carries information about where an entity originated from."""
 
 
     ASPECT_NAME = 'origin'
-    ASPECT_INFO = {'name': 'origin'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.Origin")
 
     def __init__(self,
         type: Union[str, "OriginTypeClass"],
         externalType: Union[None, str]=None,
     ):
         super().__init__()
@@ -3131,15 +3131,15 @@
     
     
 class OwnershipClass(_Aspect):
     """Ownership information of an entity."""
 
 
     ASPECT_NAME = 'ownership'
-    ASPECT_INFO = {'name': 'ownership'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.Ownership")
 
     def __init__(self,
         owners: List["OwnerClass"],
         lastModified: Optional["AuditStampClass"]=None,
     ):
         super().__init__()
@@ -3303,15 +3303,15 @@
     
     
 class SiblingsClass(_Aspect):
     """Siblings information of an entity."""
 
 
     ASPECT_NAME = 'siblings'
-    ASPECT_INFO = {'name': 'siblings'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.Siblings")
 
     def __init__(self,
         siblings: List[str],
         primary: bool,
     ):
         super().__init__()
@@ -3355,15 +3355,15 @@
     
 class StatusClass(_Aspect):
     """The lifecycle status metadata of an entity, e.g. dataset, metric, feature, etc.
     This aspect is used to represent soft deletes conventionally."""
 
 
     ASPECT_NAME = 'status'
-    ASPECT_INFO = {'name': 'status'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.Status")
 
     def __init__(self,
         removed: Optional[bool]=None,
     ):
         super().__init__()
         
@@ -3397,15 +3397,15 @@
     
 class SubTypesClass(_Aspect):
     """Sub Types. Use this aspect to specialize a generic Entity
     e.g. Making a Dataset also be a View or also be a LookerExplore"""
 
 
     ASPECT_NAME = 'subTypes'
-    ASPECT_INFO = {'name': 'subTypes'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.common.SubTypes")
 
     def __init__(self,
         typeNames: List[str],
     ):
         super().__init__()
         
@@ -3616,15 +3616,15 @@
     
     
 class ContainerClass(_Aspect):
     """Link from an asset to its parent container"""
 
 
     ASPECT_NAME = 'container'
-    ASPECT_INFO = {'name': 'container'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.container.Container")
 
     def __init__(self,
         container: str,
     ):
         super().__init__()
         
@@ -3653,15 +3653,15 @@
     
     
 class ContainerPropertiesClass(_Aspect):
     """Information about a Asset Container as received from a 3rd party source system"""
 
 
     ASPECT_NAME = 'containerProperties'
-    ASPECT_INFO = {'name': 'containerProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.container.ContainerProperties")
 
     def __init__(self,
         name: str,
         customProperties: Optional[Dict[str, str]]=None,
         externalUrl: Union[None, str]=None,
         qualifiedName: Union[None, str]=None,
@@ -3778,15 +3778,15 @@
     
     
 class EditableContainerPropertiesClass(_Aspect):
     """Editable information about an Asset Container as defined on the DataHub Platform"""
 
 
     ASPECT_NAME = 'editableContainerProperties'
-    ASPECT_INFO = {'name': 'editableContainerProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.container.EditableContainerProperties")
 
     def __init__(self,
         description: Union[None, str]=None,
     ):
         super().__init__()
         
@@ -3815,15 +3815,15 @@
     
     
 class DashboardInfoClass(_Aspect):
     """Information about a dashboard"""
 
 
     ASPECT_NAME = 'dashboardInfo'
-    ASPECT_INFO = {'name': 'dashboardInfo'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dashboard.DashboardInfo")
 
     def __init__(self,
         title: str,
         description: str,
         lastModified: "ChangeAuditStampsClass",
         customProperties: Optional[Dict[str, str]]=None,
@@ -4026,15 +4026,15 @@
     
     If this aspect represents the latest snapshot of the statistics about a Dashboard, the eventGranularity field should be null. 
     If this aspect represents a bucketed window of usage statistics (e.g. over a day), then the eventGranularity field should be set accordingly. """
 
 
     ASPECT_NAME = 'dashboardUsageStatistics'
     ASPECT_TYPE = 'timeseries'
-    ASPECT_INFO = {'name': 'dashboardUsageStatistics', 'type': 'timeseries'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dashboard.DashboardUsageStatistics")
 
     def __init__(self,
         timestampMillis: int,
         eventGranularity: Union[None, "TimeWindowSizeClass"]=None,
         partitionSpec: Optional[Union["PartitionSpecClass", None]]=None,
         messageId: Union[None, str]=None,
@@ -4287,15 +4287,15 @@
     
 class EditableDashboardPropertiesClass(_Aspect):
     """Stores editable changes made to properties. This separates changes made from
     ingestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines"""
 
 
     ASPECT_NAME = 'editableDashboardProperties'
-    ASPECT_INFO = {'name': 'editableDashboardProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dashboard.EditableDashboardProperties")
 
     def __init__(self,
         created: Optional["AuditStampClass"]=None,
         lastModified: Optional["AuditStampClass"]=None,
         deleted: Union[None, "AuditStampClass"]=None,
         description: Union[None, str]=None,
@@ -4374,15 +4374,15 @@
     
     
 class DataFlowInfoClass(_Aspect):
     """Information about a Data processing flow"""
 
 
     ASPECT_NAME = 'dataFlowInfo'
-    ASPECT_INFO = {'name': 'dataFlowInfo'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.datajob.DataFlowInfo")
 
     def __init__(self,
         name: str,
         customProperties: Optional[Dict[str, str]]=None,
         externalUrl: Union[None, str]=None,
         description: Union[None, str]=None,
@@ -4499,15 +4499,15 @@
     
     
 class DataJobInfoClass(_Aspect):
     """Information about a Data processing job"""
 
 
     ASPECT_NAME = 'dataJobInfo'
-    ASPECT_INFO = {'name': 'dataJobInfo'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.datajob.DataJobInfo")
 
     def __init__(self,
         name: str,
         type: Union[Union[str, "AzkabanJobTypeClass"], str],
         customProperties: Optional[Dict[str, str]]=None,
         externalUrl: Union[None, str]=None,
@@ -4654,15 +4654,15 @@
     
     
 class DataJobInputOutputClass(_Aspect):
     """Information about the inputs and outputs of a Data processing job"""
 
 
     ASPECT_NAME = 'dataJobInputOutput'
-    ASPECT_INFO = {'name': 'dataJobInputOutput'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.datajob.DataJobInputOutput")
 
     def __init__(self,
         inputDatasets: List[str],
         outputDatasets: List[str],
         inputDatasetEdges: Union[None, List["EdgeClass"]]=None,
         outputDatasetEdges: Union[None, List["EdgeClass"]]=None,
@@ -4810,15 +4810,15 @@
     
 class EditableDataFlowPropertiesClass(_Aspect):
     """Stores editable changes made to properties. This separates changes made from
     ingestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines"""
 
 
     ASPECT_NAME = 'editableDataFlowProperties'
-    ASPECT_INFO = {'name': 'editableDataFlowProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.datajob.EditableDataFlowProperties")
 
     def __init__(self,
         created: Optional["AuditStampClass"]=None,
         lastModified: Optional["AuditStampClass"]=None,
         deleted: Union[None, "AuditStampClass"]=None,
         description: Union[None, str]=None,
@@ -4898,15 +4898,15 @@
     
 class EditableDataJobPropertiesClass(_Aspect):
     """Stores editable changes made to properties. This separates changes made from
     ingestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines"""
 
 
     ASPECT_NAME = 'editableDataJobProperties'
-    ASPECT_INFO = {'name': 'editableDataJobProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.datajob.EditableDataJobProperties")
 
     def __init__(self,
         created: Optional["AuditStampClass"]=None,
         lastModified: Optional["AuditStampClass"]=None,
         deleted: Union[None, "AuditStampClass"]=None,
         description: Union[None, str]=None,
@@ -5014,15 +5014,15 @@
     
     
 class VersionInfoClass(_Aspect):
     """Information about a Data processing job"""
 
 
     ASPECT_NAME = 'versionInfo'
-    ASPECT_INFO = {'name': 'versionInfo'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.datajob.VersionInfo")
 
     def __init__(self,
         version: str,
         versionType: str,
         customProperties: Optional[Dict[str, str]]=None,
         externalUrl: Union[None, str]=None,
@@ -5127,15 +5127,15 @@
     
 class DatahubIngestionCheckpointClass(_Aspect):
     """Checkpoint of a datahub ingestion run for a given job."""
 
 
     ASPECT_NAME = 'datahubIngestionCheckpoint'
     ASPECT_TYPE = 'timeseries'
-    ASPECT_INFO = {'name': 'datahubIngestionCheckpoint', 'type': 'timeseries'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.datajob.datahub.DatahubIngestionCheckpoint")
 
     def __init__(self,
         timestampMillis: int,
         pipelineName: str,
         platformInstanceId: str,
         config: str,
@@ -5285,15 +5285,15 @@
     
 class DatahubIngestionRunSummaryClass(_Aspect):
     """Summary of a datahub ingestion run for a given platform."""
 
 
     ASPECT_NAME = 'datahubIngestionRunSummary'
     ASPECT_TYPE = 'timeseries'
-    ASPECT_INFO = {'name': 'datahubIngestionRunSummary', 'type': 'timeseries'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.datajob.datahub.DatahubIngestionRunSummary")
 
     def __init__(self,
         timestampMillis: int,
         pipelineName: str,
         platformInstanceId: str,
         runId: str,
@@ -5769,15 +5769,15 @@
     
     
 class DataPlatformInfoClass(_Aspect):
     """Information about a data platform"""
 
 
     ASPECT_NAME = 'dataPlatformInfo'
-    ASPECT_INFO = {'name': 'dataPlatformInfo'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataplatform.DataPlatformInfo")
 
     def __init__(self,
         name: str,
         type: Union[str, "PlatformTypeClass"],
         datasetNameDelimiter: str,
         displayName: Union[None, str]=None,
@@ -5894,15 +5894,15 @@
     
     
 class DataPlatformInstancePropertiesClass(_Aspect):
     """Properties associated with a Data Platform Instance"""
 
 
     ASPECT_NAME = 'dataPlatformInstanceProperties'
-    ASPECT_INFO = {'name': 'dataPlatformInstanceProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataplatforminstance.DataPlatformInstanceProperties")
 
     def __init__(self,
         customProperties: Optional[Dict[str, str]]=None,
         externalUrl: Union[None, str]=None,
         name: Union[None, str]=None,
         description: Union[None, str]=None,
@@ -5977,15 +5977,15 @@
     
     
 class DataProcessInfoClass(_Aspect):
     """The inputs and outputs of this data process"""
 
 
     ASPECT_NAME = 'dataProcessInfo'
-    ASPECT_INFO = {'name': 'dataProcessInfo'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataprocess.DataProcessInfo")
 
     def __init__(self,
         inputs: Union[None, List[str]]=None,
         outputs: Union[None, List[str]]=None,
     ):
         super().__init__()
@@ -6028,15 +6028,15 @@
     
     
 class DataProcessInstanceInputClass(_Aspect):
     """Information about the inputs datasets of a Data process"""
 
 
     ASPECT_NAME = 'dataProcessInstanceInput'
-    ASPECT_INFO = {'name': 'dataProcessInstanceInput'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataprocess.DataProcessInstanceInput")
 
     def __init__(self,
         inputs: List[str],
     ):
         super().__init__()
         
@@ -6065,15 +6065,15 @@
     
     
 class DataProcessInstanceOutputClass(_Aspect):
     """Information about the outputs of a Data process"""
 
 
     ASPECT_NAME = 'dataProcessInstanceOutput'
-    ASPECT_INFO = {'name': 'dataProcessInstanceOutput'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataprocess.DataProcessInstanceOutput")
 
     def __init__(self,
         outputs: List[str],
     ):
         super().__init__()
         
@@ -6102,15 +6102,15 @@
     
     
 class DataProcessInstancePropertiesClass(_Aspect):
     """The inputs and outputs of this data process"""
 
 
     ASPECT_NAME = 'dataProcessInstanceProperties'
-    ASPECT_INFO = {'name': 'dataProcessInstanceProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataprocess.DataProcessInstanceProperties")
 
     def __init__(self,
         name: str,
         created: "AuditStampClass",
         customProperties: Optional[Dict[str, str]]=None,
         externalUrl: Union[None, str]=None,
@@ -6199,15 +6199,15 @@
     
     
 class DataProcessInstanceRelationshipsClass(_Aspect):
     """Information about Data process relationships"""
 
 
     ASPECT_NAME = 'dataProcessInstanceRelationships'
-    ASPECT_INFO = {'name': 'dataProcessInstanceRelationships'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataprocess.DataProcessInstanceRelationships")
 
     def __init__(self,
         upstreamInstances: List[str],
         parentTemplate: Union[None, str]=None,
         parentInstance: Union[None, str]=None,
     ):
@@ -6270,15 +6270,15 @@
 class DataProcessInstanceRunEventClass(_Aspect):
     """An event representing the current status of data process run.
     DataProcessRunEvent should be used for reporting the status of a dataProcess' run."""
 
 
     ASPECT_NAME = 'dataProcessInstanceRunEvent'
     ASPECT_TYPE = 'timeseries'
-    ASPECT_INFO = {'name': 'dataProcessInstanceRunEvent', 'type': 'timeseries'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataprocess.DataProcessInstanceRunEvent")
 
     def __init__(self,
         timestampMillis: int,
         status: Union[str, "DataProcessRunStatusClass"],
         eventGranularity: Union[None, "TimeWindowSizeClass"]=None,
         partitionSpec: Optional[Union["PartitionSpecClass", None]]=None,
@@ -6492,15 +6492,15 @@
     
 class DatasetDeprecationClass(_Aspect):
     """Dataset deprecation status
     Deprecated! This aspect is deprecated in favor of the more-general-purpose 'Deprecation' aspect."""
 
 
     ASPECT_NAME = 'datasetDeprecation'
-    ASPECT_INFO = {'name': 'datasetDeprecation'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataset.DatasetDeprecation")
 
     def __init__(self,
         deprecated: bool,
         note: str,
         decommissionTime: Union[None, int]=None,
         actor: Union[None, str]=None,
@@ -6923,15 +6923,15 @@
     
 class DatasetProfileClass(_Aspect):
     """Stats corresponding to datasets"""
 
 
     ASPECT_NAME = 'datasetProfile'
     ASPECT_TYPE = 'timeseries'
-    ASPECT_INFO = {'name': 'datasetProfile', 'type': 'timeseries'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataset.DatasetProfile")
 
     def __init__(self,
         timestampMillis: int,
         eventGranularity: Union[None, "TimeWindowSizeClass"]=None,
         partitionSpec: Optional[Union["PartitionSpecClass", None]]=None,
         messageId: Union[None, str]=None,
@@ -7062,15 +7062,15 @@
     
     
 class DatasetPropertiesClass(_Aspect):
     """Properties associated with a Dataset"""
 
 
     ASPECT_NAME = 'datasetProperties'
-    ASPECT_INFO = {'name': 'datasetProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataset.DatasetProperties")
 
     def __init__(self,
         customProperties: Optional[Dict[str, str]]=None,
         externalUrl: Union[None, str]=None,
         name: Union[None, str]=None,
         qualifiedName: Union[None, str]=None,
@@ -7221,15 +7221,15 @@
     
     
 class DatasetUpstreamLineageClass(_Aspect):
     """Fine Grained upstream lineage for fields in a dataset"""
 
 
     ASPECT_NAME = 'datasetUpstreamLineage'
-    ASPECT_INFO = {'name': 'datasetUpstreamLineage'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataset.DatasetUpstreamLineage")
 
     def __init__(self,
         fieldMappings: List["DatasetFieldMappingClass"],
     ):
         super().__init__()
         
@@ -7259,15 +7259,15 @@
     
 class DatasetUsageStatisticsClass(_Aspect):
     """Stats corresponding to dataset's usage."""
 
 
     ASPECT_NAME = 'datasetUsageStatistics'
     ASPECT_TYPE = 'timeseries'
-    ASPECT_INFO = {'name': 'datasetUsageStatistics', 'type': 'timeseries'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataset.DatasetUsageStatistics")
 
     def __init__(self,
         timestampMillis: int,
         eventGranularity: Union[None, "TimeWindowSizeClass"]=None,
         partitionSpec: Optional[Union["PartitionSpecClass", None]]=None,
         messageId: Union[None, str]=None,
@@ -7474,15 +7474,15 @@
     
 class EditableDatasetPropertiesClass(_Aspect):
     """EditableDatasetProperties stores editable changes made to dataset properties. This separates changes made from
     ingestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines"""
 
 
     ASPECT_NAME = 'editableDatasetProperties'
-    ASPECT_INFO = {'name': 'editableDatasetProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataset.EditableDatasetProperties")
 
     def __init__(self,
         created: Optional["AuditStampClass"]=None,
         lastModified: Optional["AuditStampClass"]=None,
         deleted: Union[None, "AuditStampClass"]=None,
         description: Union[None, str]=None,
@@ -7880,15 +7880,15 @@
     
     
 class UpstreamLineageClass(_Aspect):
     """Upstream lineage of a dataset"""
 
 
     ASPECT_NAME = 'upstreamLineage'
-    ASPECT_INFO = {'name': 'upstreamLineage'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataset.UpstreamLineage")
 
     def __init__(self,
         upstreams: List["UpstreamClass"],
         fineGrainedLineages: Union[None, List["FineGrainedLineageClass"]]=None,
     ):
         super().__init__()
@@ -7979,15 +7979,15 @@
     
 class ViewPropertiesClass(_Aspect):
     """Details about a View. 
     e.g. Gets activated when subTypes is view"""
 
 
     ASPECT_NAME = 'viewProperties'
-    ASPECT_INFO = {'name': 'viewProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.dataset.ViewProperties")
 
     def __init__(self,
         materialized: bool,
         viewLogic: str,
         viewLanguage: str,
     ):
@@ -8044,15 +8044,15 @@
     
     
 class DomainPropertiesClass(_Aspect):
     """Information about a Domain"""
 
 
     ASPECT_NAME = 'domainProperties'
-    ASPECT_INFO = {'name': 'domainProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.domain.DomainProperties")
 
     def __init__(self,
         name: str,
         description: Union[None, str]=None,
         created: Union[None, "AuditStampClass"]=None,
     ):
@@ -8109,15 +8109,15 @@
     
     
 class DomainsClass(_Aspect):
     """Links from an Asset to its Domains"""
 
 
     ASPECT_NAME = 'domains'
-    ASPECT_INFO = {'name': 'domains'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.domain.Domains")
 
     def __init__(self,
         domains: List[str],
     ):
         super().__init__()
         
@@ -8174,15 +8174,15 @@
     
 class ExecutionRequestInputClass(_Aspect):
     """An request to execution some remote logic or action.
     TODO: Determine who is responsible for emitting execution request success or failure. Executor?"""
 
 
     ASPECT_NAME = 'dataHubExecutionRequestInput'
-    ASPECT_INFO = {'name': 'dataHubExecutionRequestInput'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.execution.ExecutionRequestInput")
 
     def __init__(self,
         task: str,
         args: Dict[str, str],
         executorId: str,
         source: "ExecutionRequestSourceClass",
@@ -8267,15 +8267,15 @@
     
     
 class ExecutionRequestResultClass(_Aspect):
     """The result of an execution request"""
 
 
     ASPECT_NAME = 'dataHubExecutionRequestResult'
-    ASPECT_INFO = {'name': 'dataHubExecutionRequestResult'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.execution.ExecutionRequestResult")
 
     def __init__(self,
         status: str,
         report: Union[None, str]=None,
         structuredReport: Union[None, "StructuredExecutionReportClass"]=None,
         startTimeMs: Union[None, int]=None,
@@ -8360,15 +8360,15 @@
     
     
 class ExecutionRequestSignalClass(_Aspect):
     """An signal sent to a running execution request"""
 
 
     ASPECT_NAME = 'dataHubExecutionRequestSignal'
-    ASPECT_INFO = {'name': 'dataHubExecutionRequestSignal'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.execution.ExecutionRequestSignal")
 
     def __init__(self,
         signal: str,
         createdAt: "AuditStampClass",
         executorId: Union[None, str]=None,
     ):
@@ -8535,15 +8535,15 @@
     
     
 class GlossaryNodeInfoClass(_Aspect):
     """Properties associated with a GlossaryNode"""
 
 
     ASPECT_NAME = 'glossaryNodeInfo'
-    ASPECT_INFO = {'name': 'glossaryNodeInfo'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.glossary.GlossaryNodeInfo")
 
     def __init__(self,
         definition: str,
         parentNode: Union[None, str]=None,
         name: Union[None, str]=None,
         id: Union[None, str]=None,
@@ -8614,15 +8614,15 @@
     
     
 class GlossaryRelatedTermsClass(_Aspect):
     """Has A / Is A lineage information about a glossary Term reporting the lineage"""
 
 
     ASPECT_NAME = 'glossaryRelatedTerms'
-    ASPECT_INFO = {'name': 'glossaryRelatedTerms'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.glossary.GlossaryRelatedTerms")
 
     def __init__(self,
         isRelatedTerms: Union[None, List[str]]=None,
         hasRelatedTerms: Union[None, List[str]]=None,
         values: Union[None, List[str]]=None,
         relatedTerms: Union[None, List[str]]=None,
@@ -8695,15 +8695,15 @@
     
     
 class GlossaryTermInfoClass(_Aspect):
     """Properties associated with a GlossaryTerm"""
 
 
     ASPECT_NAME = 'glossaryTermInfo'
-    ASPECT_INFO = {'name': 'glossaryTermInfo'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.glossary.GlossaryTermInfo")
 
     def __init__(self,
         definition: str,
         termSource: str,
         customProperties: Optional[Dict[str, str]]=None,
         id: Union[None, str]=None,
@@ -8848,15 +8848,15 @@
     
     
 class CorpGroupEditableInfoClass(_Aspect):
     """Group information that can be edited from UI"""
 
 
     ASPECT_NAME = 'corpGroupEditableInfo'
-    ASPECT_INFO = {'name': 'corpGroupEditableInfo'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.identity.CorpGroupEditableInfo")
 
     def __init__(self,
         description: Union[None, str]=None,
         pictureLink: Optional[str]=None,
         slack: Union[None, str]=None,
         email: Union[None, str]=None,
@@ -8931,15 +8931,15 @@
     
     
 class CorpGroupInfoClass(_Aspect):
     """Information about a Corp Group ingested from a third party source"""
 
 
     ASPECT_NAME = 'corpGroupInfo'
-    ASPECT_INFO = {'EntityUrns': ['com.linkedin.pegasus2avro.common.CorpGroupUrn'], 'name': 'corpGroupInfo'}
+    ASPECT_INFO = {'EntityUrns': ['com.linkedin.pegasus2avro.common.CorpGroupUrn']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.identity.CorpGroupInfo")
 
     def __init__(self,
         admins: List[str],
         members: List[str],
         groups: List[str],
         displayName: Union[None, str]=None,
@@ -9107,15 +9107,15 @@
     
     
 class CorpUserCredentialsClass(_Aspect):
     """Corp user credentials"""
 
 
     ASPECT_NAME = 'corpUserCredentials'
-    ASPECT_INFO = {'EntityUrns': ['com.linkedin.pegasus2avro.common.CorpuserUrn'], 'name': 'corpUserCredentials'}
+    ASPECT_INFO = {'EntityUrns': ['com.linkedin.pegasus2avro.common.CorpuserUrn']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.identity.CorpUserCredentials")
 
     def __init__(self,
         salt: str,
         hashedPassword: str,
         passwordResetToken: Union[None, str]=None,
         passwordResetTokenExpirationTimeMillis: Union[None, int]=None,
@@ -9186,15 +9186,15 @@
     
     
 class CorpUserEditableInfoClass(_Aspect):
     """Linkedin corp user information that can be edited from UI"""
 
 
     ASPECT_NAME = 'corpUserEditableInfo'
-    ASPECT_INFO = {'EntityUrns': ['com.linkedin.pegasus2avro.common.CorpuserUrn'], 'name': 'corpUserEditableInfo'}
+    ASPECT_INFO = {'EntityUrns': ['com.linkedin.pegasus2avro.common.CorpuserUrn']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.identity.CorpUserEditableInfo")
 
     def __init__(self,
         aboutMe: Union[None, str]=None,
         teams: Optional[List[str]]=None,
         skills: Optional[List[str]]=None,
         pictureLink: Optional[str]=None,
@@ -9347,15 +9347,15 @@
     
     
 class CorpUserInfoClass(_Aspect):
     """Linkedin corp user information"""
 
 
     ASPECT_NAME = 'corpUserInfo'
-    ASPECT_INFO = {'EntityUrns': ['com.linkedin.pegasus2avro.common.CorpuserUrn'], 'name': 'corpUserInfo'}
+    ASPECT_INFO = {'EntityUrns': ['com.linkedin.pegasus2avro.common.CorpuserUrn']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.identity.CorpUserInfo")
 
     def __init__(self,
         active: bool,
         customProperties: Optional[Dict[str, str]]=None,
         displayName: Union[None, str]=None,
         email: Union[None, str]=None,
@@ -9542,15 +9542,15 @@
     
     
 class CorpUserSettingsClass(_Aspect):
     """Settings that a user can customize through the datahub ui"""
 
 
     ASPECT_NAME = 'corpUserSettings'
-    ASPECT_INFO = {'name': 'corpUserSettings'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.identity.CorpUserSettings")
 
     def __init__(self,
         appearance: "CorpUserAppearanceSettingsClass",
         views: Union[None, "CorpUserViewsSettingsClass"]=None,
     ):
         super().__init__()
@@ -9593,15 +9593,15 @@
     
     
 class CorpUserStatusClass(_Aspect):
     """The status of the user, e.g. provisioned, active, suspended, etc."""
 
 
     ASPECT_NAME = 'corpUserStatus'
-    ASPECT_INFO = {'name': 'corpUserStatus'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.identity.CorpUserStatus")
 
     def __init__(self,
         status: str,
         lastModified: "AuditStampClass",
     ):
         super().__init__()
@@ -9679,15 +9679,15 @@
     
     
 class GroupMembershipClass(_Aspect):
     """Carries information about the CorpGroups a user is in."""
 
 
     ASPECT_NAME = 'groupMembership'
-    ASPECT_INFO = {'name': 'groupMembership'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.identity.GroupMembership")
 
     def __init__(self,
         groups: List[str],
     ):
         super().__init__()
         
@@ -9716,15 +9716,15 @@
     
     
 class InviteTokenClass(_Aspect):
     """Aspect used to store invite tokens."""
 
 
     ASPECT_NAME = 'inviteToken'
-    ASPECT_INFO = {'name': 'inviteToken'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.identity.InviteToken")
 
     def __init__(self,
         token: str,
         role: Union[None, str]=None,
     ):
         super().__init__()
@@ -9767,15 +9767,15 @@
     
     
 class NativeGroupMembershipClass(_Aspect):
     """Carries information about the native CorpGroups a user is in."""
 
 
     ASPECT_NAME = 'nativeGroupMembership'
-    ASPECT_INFO = {'name': 'nativeGroupMembership'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.identity.NativeGroupMembership")
 
     def __init__(self,
         nativeGroups: List[str],
     ):
         super().__init__()
         
@@ -9804,15 +9804,15 @@
     
     
 class RoleMembershipClass(_Aspect):
     """Carries information about which roles a user is assigned to."""
 
 
     ASPECT_NAME = 'roleMembership'
-    ASPECT_INFO = {'name': 'roleMembership'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.identity.RoleMembership")
 
     def __init__(self,
         roles: List[str],
     ):
         super().__init__()
         
@@ -9916,15 +9916,15 @@
     
     
 class DataHubIngestionSourceInfoClass(_Aspect):
     """Info about a DataHub ingestion source"""
 
 
     ASPECT_NAME = 'dataHubIngestionSourceInfo'
-    ASPECT_INFO = {'name': 'dataHubIngestionSourceInfo'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ingestion.DataHubIngestionSourceInfo")
 
     def __init__(self,
         name: str,
         type: str,
         config: "DataHubIngestionSourceConfigClass",
         platform: Union[None, str]=None,
@@ -10056,15 +10056,15 @@
     
     
 class AssertionKeyClass(_Aspect):
     """Key for a Assertion"""
 
 
     ASPECT_NAME = 'assertionKey'
-    ASPECT_INFO = {'name': 'assertionKey', 'keyForEntity': 'assertion', 'entityCategory': 'core', 'entityAspects': ['assertionInfo', 'dataPlatformInstance', 'assertionRunEvent', 'status'], 'entityDoc': 'Assertion represents a data quality rule applied on one or more dataset.'}
+    ASPECT_INFO = {'keyForEntity': 'assertion', 'entityCategory': 'core', 'entityAspects': ['assertionInfo', 'dataPlatformInstance', 'assertionRunEvent', 'status'], 'entityDoc': 'Assertion represents a data quality rule applied on one or more dataset.'}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.AssertionKey")
 
     def __init__(self,
         assertionId: str,
     ):
         super().__init__()
         
@@ -10093,15 +10093,15 @@
     
     
 class ChartKeyClass(_Aspect):
     """Key for a Chart"""
 
 
     ASPECT_NAME = 'chartKey'
-    ASPECT_INFO = {'name': 'chartKey', 'keyForEntity': 'chart', 'entityCategory': '_unset_', 'entityAspects': ['domains', 'container', 'deprecation', 'inputFields', 'chartUsageStatistics', 'embed']}
+    ASPECT_INFO = {'keyForEntity': 'chart', 'entityCategory': '_unset_', 'entityAspects': ['domains', 'container', 'deprecation', 'inputFields', 'chartUsageStatistics', 'embed']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.ChartKey")
 
     def __init__(self,
         dashboardTool: str,
         chartId: str,
     ):
         super().__init__()
@@ -10144,15 +10144,15 @@
     
     
 class ContainerKeyClass(_Aspect):
     """Key for an Asset Container"""
 
 
     ASPECT_NAME = 'containerKey'
-    ASPECT_INFO = {'name': 'containerKey', 'keyForEntity': 'container', 'entityCategory': '_unset_', 'entityAspects': ['containerProperties', 'editableContainerProperties', 'dataPlatformInstance', 'subTypes', 'ownership', 'container', 'globalTags', 'glossaryTerms', 'institutionalMemory', 'browsePaths', 'status', 'domains'], 'entityDoc': 'A container of related data assets.'}
+    ASPECT_INFO = {'keyForEntity': 'container', 'entityCategory': '_unset_', 'entityAspects': ['containerProperties', 'editableContainerProperties', 'dataPlatformInstance', 'subTypes', 'ownership', 'container', 'globalTags', 'glossaryTerms', 'institutionalMemory', 'browsePaths', 'status', 'domains'], 'entityDoc': 'A container of related data assets.'}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.ContainerKey")
 
     def __init__(self,
         guid: Union[None, str]=None,
     ):
         super().__init__()
         
@@ -10181,15 +10181,15 @@
     
     
 class CorpGroupKeyClass(_Aspect):
     """Key for a CorpGroup"""
 
 
     ASPECT_NAME = 'corpGroupKey'
-    ASPECT_INFO = {'name': 'corpGroupKey', 'keyForEntity': 'corpGroup', 'entityCategory': '_unset_', 'entityAspects': ['corpGroupInfo', 'corpGroupEditableInfo', 'globalTags', 'ownership', 'status', 'origin'], 'entityDoc': 'CorpGroup represents an identity of a group of users in the enterprise.'}
+    ASPECT_INFO = {'keyForEntity': 'corpGroup', 'entityCategory': '_unset_', 'entityAspects': ['corpGroupInfo', 'corpGroupEditableInfo', 'globalTags', 'ownership', 'status', 'origin'], 'entityDoc': 'CorpGroup represents an identity of a group of users in the enterprise.'}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.CorpGroupKey")
 
     def __init__(self,
         name: str,
     ):
         super().__init__()
         
@@ -10218,15 +10218,15 @@
     
     
 class CorpUserKeyClass(_Aspect):
     """Key for a CorpUser"""
 
 
     ASPECT_NAME = 'corpUserKey'
-    ASPECT_INFO = {'name': 'corpUserKey', 'keyForEntity': 'corpuser', 'entityCategory': '_unset_', 'entityAspects': ['corpUserInfo', 'corpUserEditableInfo', 'corpUserStatus', 'groupMembership', 'globalTags', 'status', 'corpUserCredentials', 'nativeGroupMembership', 'corpUserSettings', 'origin', 'roleMembership'], 'entityDoc': 'CorpUser represents an identity of a person (or an account) in the enterprise.'}
+    ASPECT_INFO = {'keyForEntity': 'corpuser', 'entityCategory': '_unset_', 'entityAspects': ['corpUserInfo', 'corpUserEditableInfo', 'corpUserStatus', 'groupMembership', 'globalTags', 'status', 'corpUserCredentials', 'nativeGroupMembership', 'corpUserSettings', 'origin', 'roleMembership'], 'entityDoc': 'CorpUser represents an identity of a person (or an account) in the enterprise.'}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.CorpUserKey")
 
     def __init__(self,
         username: str,
     ):
         super().__init__()
         
@@ -10255,15 +10255,15 @@
     
     
 class DashboardKeyClass(_Aspect):
     """Key for a Dashboard"""
 
 
     ASPECT_NAME = 'dashboardKey'
-    ASPECT_INFO = {'name': 'dashboardKey', 'keyForEntity': 'dashboard', 'entityCategory': '_unset_', 'entityAspects': ['domains', 'container', 'deprecation', 'dashboardUsageStatistics', 'inputFields', 'subTypes', 'embed']}
+    ASPECT_INFO = {'keyForEntity': 'dashboard', 'entityCategory': '_unset_', 'entityAspects': ['domains', 'container', 'deprecation', 'dashboardUsageStatistics', 'inputFields', 'subTypes', 'embed']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.DashboardKey")
 
     def __init__(self,
         dashboardTool: str,
         dashboardId: str,
     ):
         super().__init__()
@@ -10306,15 +10306,15 @@
     
     
 class DataFlowKeyClass(_Aspect):
     """Key for a Data Flow"""
 
 
     ASPECT_NAME = 'dataFlowKey'
-    ASPECT_INFO = {'name': 'dataFlowKey', 'keyForEntity': 'dataFlow', 'entityCategory': 'core', 'entityAspects': ['domains', 'deprecation', 'versionInfo']}
+    ASPECT_INFO = {'keyForEntity': 'dataFlow', 'entityCategory': 'core', 'entityAspects': ['domains', 'deprecation', 'versionInfo']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.DataFlowKey")
 
     def __init__(self,
         orchestrator: str,
         flowId: str,
         cluster: str,
     ):
@@ -10371,15 +10371,15 @@
     
     
 class DataHubAccessTokenKeyClass(_Aspect):
     """Key for a DataHub Access Token"""
 
 
     ASPECT_NAME = 'dataHubAccessTokenKey'
-    ASPECT_INFO = {'name': 'dataHubAccessTokenKey', 'keyForEntity': 'dataHubAccessToken', 'entityCategory': 'internal', 'entityAspects': ['dataHubAccessTokenInfo']}
+    ASPECT_INFO = {'keyForEntity': 'dataHubAccessToken', 'entityCategory': 'internal', 'entityAspects': ['dataHubAccessTokenInfo']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.DataHubAccessTokenKey")
 
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
@@ -10408,15 +10408,15 @@
     
     
 class DataHubIngestionSourceKeyClass(_Aspect):
     """Key for a DataHub ingestion source"""
 
 
     ASPECT_NAME = 'dataHubIngestionSourceKey'
-    ASPECT_INFO = {'name': 'dataHubIngestionSourceKey', 'keyForEntity': 'dataHubIngestionSource', 'entityCategory': 'internal', 'entityAspects': ['dataHubIngestionSourceInfo']}
+    ASPECT_INFO = {'keyForEntity': 'dataHubIngestionSource', 'entityCategory': 'internal', 'entityAspects': ['dataHubIngestionSourceInfo']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.DataHubIngestionSourceKey")
 
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
@@ -10445,15 +10445,15 @@
     
     
 class DataHubPolicyKeyClass(_Aspect):
     """Key for a DataHub Policy"""
 
 
     ASPECT_NAME = 'dataHubPolicyKey'
-    ASPECT_INFO = {'name': 'dataHubPolicyKey', 'keyForEntity': 'dataHubPolicy', 'entityCategory': 'internal', 'entityAspects': ['dataHubPolicyInfo'], 'entityDoc': 'DataHub Policies represent access policies granted to users or groups on metadata operations like edit, view etc.'}
+    ASPECT_INFO = {'keyForEntity': 'dataHubPolicy', 'entityCategory': 'internal', 'entityAspects': ['dataHubPolicyInfo'], 'entityDoc': 'DataHub Policies represent access policies granted to users or groups on metadata operations like edit, view etc.'}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.DataHubPolicyKey")
 
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
@@ -10482,15 +10482,15 @@
     
     
 class DataHubRetentionKeyClass(_Aspect):
     """Key for a DataHub Retention"""
 
 
     ASPECT_NAME = 'dataHubRetentionKey'
-    ASPECT_INFO = {'name': 'dataHubRetentionKey', 'keyForEntity': 'dataHubRetention', 'entityCategory': 'internal', 'entityAspects': ['dataHubRetentionConfig']}
+    ASPECT_INFO = {'keyForEntity': 'dataHubRetention', 'entityCategory': 'internal', 'entityAspects': ['dataHubRetentionConfig']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.DataHubRetentionKey")
 
     def __init__(self,
         entityName: str,
         aspectName: str,
     ):
         super().__init__()
@@ -10533,15 +10533,15 @@
     
     
 class DataHubRoleKeyClass(_Aspect):
     """Key for a DataHub Role"""
 
 
     ASPECT_NAME = 'dataHubRoleKey'
-    ASPECT_INFO = {'name': 'dataHubRoleKey', 'keyForEntity': 'dataHubRole', 'entityCategory': 'core', 'entityAspects': ['dataHubRoleInfo']}
+    ASPECT_INFO = {'keyForEntity': 'dataHubRole', 'entityCategory': 'core', 'entityAspects': ['dataHubRoleInfo']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.DataHubRoleKey")
 
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
@@ -10570,15 +10570,15 @@
     
     
 class DataHubSecretKeyClass(_Aspect):
     """Key for a DataHub Secret"""
 
 
     ASPECT_NAME = 'dataHubSecretKey'
-    ASPECT_INFO = {'name': 'dataHubSecretKey', 'keyForEntity': 'dataHubSecret', 'entityCategory': 'internal', 'entityAspects': ['dataHubSecretValue']}
+    ASPECT_INFO = {'keyForEntity': 'dataHubSecret', 'entityCategory': 'internal', 'entityAspects': ['dataHubSecretValue']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.DataHubSecretKey")
 
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
@@ -10607,15 +10607,15 @@
     
     
 class DataHubStepStateKeyClass(_Aspect):
     """Key for a DataHub Step State"""
 
 
     ASPECT_NAME = 'dataHubStepStateKey'
-    ASPECT_INFO = {'name': 'dataHubStepStateKey', 'keyForEntity': 'dataHubStepState', 'entityCategory': 'core', 'entityAspects': ['dataHubStepStateProperties']}
+    ASPECT_INFO = {'keyForEntity': 'dataHubStepState', 'entityCategory': 'core', 'entityAspects': ['dataHubStepStateProperties']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.DataHubStepStateKey")
 
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
@@ -10644,15 +10644,15 @@
     
     
 class DataHubUpgradeKeyClass(_Aspect):
     """Key for a DataHubUpgrade"""
 
 
     ASPECT_NAME = 'dataHubUpgradeKey'
-    ASPECT_INFO = {'name': 'dataHubUpgradeKey', 'keyForEntity': 'dataHubUpgrade', 'entityCategory': 'internal', 'entityAspects': ['dataHubUpgradeRequest', 'dataHubUpgradeResult']}
+    ASPECT_INFO = {'keyForEntity': 'dataHubUpgrade', 'entityCategory': 'internal', 'entityAspects': ['dataHubUpgradeRequest', 'dataHubUpgradeResult']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.DataHubUpgradeKey")
 
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
@@ -10681,15 +10681,15 @@
     
     
 class DataHubViewKeyClass(_Aspect):
     """Key for a DataHub View"""
 
 
     ASPECT_NAME = 'dataHubViewKey'
-    ASPECT_INFO = {'name': 'dataHubViewKey', 'keyForEntity': 'dataHubView', 'entityCategory': 'core', 'entityAspects': ['dataHubViewInfo']}
+    ASPECT_INFO = {'keyForEntity': 'dataHubView', 'entityCategory': 'core', 'entityAspects': ['dataHubViewInfo']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.DataHubViewKey")
 
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
@@ -10718,15 +10718,15 @@
     
     
 class DataJobKeyClass(_Aspect):
     """Key for a Data Job"""
 
 
     ASPECT_NAME = 'dataJobKey'
-    ASPECT_INFO = {'name': 'dataJobKey', 'keyForEntity': 'dataJob', 'entityCategory': '_unset_', 'entityAspects': ['datahubIngestionRunSummary', 'datahubIngestionCheckpoint', 'domains', 'deprecation', 'versionInfo']}
+    ASPECT_INFO = {'keyForEntity': 'dataJob', 'entityCategory': '_unset_', 'entityAspects': ['datahubIngestionRunSummary', 'datahubIngestionCheckpoint', 'domains', 'deprecation', 'versionInfo']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.DataJobKey")
 
     def __init__(self,
         flow: str,
         jobId: str,
     ):
         super().__init__()
@@ -10769,15 +10769,15 @@
     
     
 class DataPlatformInstanceKeyClass(_Aspect):
     """Key for a Dataset"""
 
 
     ASPECT_NAME = 'dataPlatformInstanceKey'
-    ASPECT_INFO = {'name': 'dataPlatformInstanceKey', 'keyForEntity': 'dataPlatformInstance', 'entityCategory': 'internal', 'entityAspects': ['dataPlatformInstanceProperties', 'ownership', 'globalTags', 'institutionalMemory', 'deprecation', 'status']}
+    ASPECT_INFO = {'keyForEntity': 'dataPlatformInstance', 'entityCategory': 'internal', 'entityAspects': ['dataPlatformInstanceProperties', 'ownership', 'globalTags', 'institutionalMemory', 'deprecation', 'status']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.DataPlatformInstanceKey")
 
     def __init__(self,
         platform: str,
         instance: str,
     ):
         super().__init__()
@@ -10820,15 +10820,15 @@
     
     
 class DataPlatformKeyClass(_Aspect):
     """Key for a Data Platform"""
 
 
     ASPECT_NAME = 'dataPlatformKey'
-    ASPECT_INFO = {'name': 'dataPlatformKey', 'keyForEntity': 'dataPlatform', 'entityCategory': 'core', 'entityAspects': ['dataPlatformInfo']}
+    ASPECT_INFO = {'keyForEntity': 'dataPlatform', 'entityCategory': 'core', 'entityAspects': ['dataPlatformInfo']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.DataPlatformKey")
 
     def __init__(self,
         platformName: str,
     ):
         super().__init__()
         
@@ -10857,15 +10857,15 @@
     
     
 class DataProcessInstanceKeyClass(_Aspect):
     """Key for an Asset DataProcessInstance"""
 
 
     ASPECT_NAME = 'dataProcessInstanceKey'
-    ASPECT_INFO = {'name': 'dataProcessInstanceKey', 'keyForEntity': 'dataProcessInstance', 'entityCategory': '_unset_', 'entityAspects': ['dataProcessInstanceInput', 'dataProcessInstanceOutput', 'dataProcessInstanceProperties', 'dataProcessInstanceRelationships', 'dataProcessInstanceRunEvent'], 'entityDoc': 'DataProcessInstance represents an instance of a datajob/jobflow run'}
+    ASPECT_INFO = {'keyForEntity': 'dataProcessInstance', 'entityCategory': '_unset_', 'entityAspects': ['dataProcessInstanceInput', 'dataProcessInstanceOutput', 'dataProcessInstanceProperties', 'dataProcessInstanceRelationships', 'dataProcessInstanceRunEvent'], 'entityDoc': 'DataProcessInstance represents an instance of a datajob/jobflow run'}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.DataProcessInstanceKey")
 
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
@@ -10894,15 +10894,15 @@
     
     
 class DataProcessKeyClass(_Aspect):
     """Key for a Data Process"""
 
 
     ASPECT_NAME = 'dataProcessKey'
-    ASPECT_INFO = {'name': 'dataProcessKey'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.DataProcessKey")
 
     def __init__(self,
         name: str,
         orchestrator: str,
         origin: Union[str, "FabricTypeClass"],
     ):
@@ -10961,15 +10961,15 @@
     
     
 class DatasetKeyClass(_Aspect):
     """Key for a Dataset"""
 
 
     ASPECT_NAME = 'datasetKey'
-    ASPECT_INFO = {'name': 'datasetKey', 'keyForEntity': 'dataset', 'entityCategory': 'core', 'entityAspects': ['viewProperties', 'subTypes', 'datasetProfile', 'datasetUsageStatistics', 'operation', 'domains', 'schemaMetadata', 'status', 'container', 'deprecation', 'testResults', 'siblings', 'embed'], 'entityDoc': 'Datasets represent logical or physical data assets stored or represented in various data platforms. Tables, Views, Streams are all instances of datasets.'}
+    ASPECT_INFO = {'keyForEntity': 'dataset', 'entityCategory': 'core', 'entityAspects': ['viewProperties', 'subTypes', 'datasetProfile', 'datasetUsageStatistics', 'operation', 'domains', 'schemaMetadata', 'status', 'container', 'deprecation', 'testResults', 'siblings', 'embed'], 'entityDoc': 'Datasets represent logical or physical data assets stored or represented in various data platforms. Tables, Views, Streams are all instances of datasets.'}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.DatasetKey")
 
     def __init__(self,
         platform: str,
         name: str,
         origin: Union[str, "FabricTypeClass"],
     ):
@@ -11026,15 +11026,15 @@
     
     
 class DomainKeyClass(_Aspect):
     """Key for an Asset Domain"""
 
 
     ASPECT_NAME = 'domainKey'
-    ASPECT_INFO = {'name': 'domainKey', 'keyForEntity': 'domain', 'entityCategory': '_unset_', 'entityAspects': ['domainProperties', 'institutionalMemory', 'ownership'], 'entityDoc': 'A data domain within an organization.'}
+    ASPECT_INFO = {'keyForEntity': 'domain', 'entityCategory': '_unset_', 'entityAspects': ['domainProperties', 'institutionalMemory', 'ownership'], 'entityDoc': 'A data domain within an organization.'}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.DomainKey")
 
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
@@ -11063,15 +11063,15 @@
     
     
 class ExecutionRequestKeyClass(_Aspect):
     """Key for an DataHub Execution Request"""
 
 
     ASPECT_NAME = 'dataHubExecutionRequestKey'
-    ASPECT_INFO = {'name': 'dataHubExecutionRequestKey', 'keyForEntity': 'dataHubExecutionRequest', 'entityCategory': 'internal', 'entityAspects': ['dataHubExecutionRequestInput', 'dataHubExecutionRequestSignal', 'dataHubExecutionRequestResult']}
+    ASPECT_INFO = {'keyForEntity': 'dataHubExecutionRequest', 'entityCategory': 'internal', 'entityAspects': ['dataHubExecutionRequestInput', 'dataHubExecutionRequestSignal', 'dataHubExecutionRequestResult']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.ExecutionRequestKey")
 
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
@@ -11100,15 +11100,15 @@
     
     
 class GlobalSettingsKeyClass(_Aspect):
     """Key for a Global Settings"""
 
 
     ASPECT_NAME = 'globalSettingsKey'
-    ASPECT_INFO = {'name': 'globalSettingsKey', 'keyForEntity': 'globalSettings', 'entityCategory': 'internal', 'entityAspects': ['globalSettingsInfo'], 'entityDoc': 'Global settings for an the platform'}
+    ASPECT_INFO = {'keyForEntity': 'globalSettings', 'entityCategory': 'internal', 'entityAspects': ['globalSettingsInfo'], 'entityDoc': 'Global settings for an the platform'}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.GlobalSettingsKey")
 
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
@@ -11137,15 +11137,15 @@
     
     
 class GlossaryNodeKeyClass(_Aspect):
     """Key for a GlossaryNode"""
 
 
     ASPECT_NAME = 'glossaryNodeKey'
-    ASPECT_INFO = {'name': 'glossaryNodeKey', 'keyForEntity': 'glossaryNode', 'entityCategory': '_unset_', 'entityAspects': ['glossaryNodeInfo', 'institutionalMemory', 'ownership', 'status']}
+    ASPECT_INFO = {'keyForEntity': 'glossaryNode', 'entityCategory': '_unset_', 'entityAspects': ['glossaryNodeInfo', 'institutionalMemory', 'ownership', 'status']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.GlossaryNodeKey")
 
     def __init__(self,
         name: str,
     ):
         super().__init__()
         
@@ -11174,15 +11174,15 @@
     
     
 class GlossaryTermKeyClass(_Aspect):
     """Key for a GlossaryTerm"""
 
 
     ASPECT_NAME = 'glossaryTermKey'
-    ASPECT_INFO = {'name': 'glossaryTermKey', 'keyForEntity': 'glossaryTerm', 'entityCategory': '_unset_', 'entityAspects': ['glossaryTermInfo', 'institutionalMemory', 'schemaMetadata', 'ownership', 'deprecation', 'domains']}
+    ASPECT_INFO = {'keyForEntity': 'glossaryTerm', 'entityCategory': '_unset_', 'entityAspects': ['glossaryTermInfo', 'institutionalMemory', 'schemaMetadata', 'ownership', 'deprecation', 'domains']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.GlossaryTermKey")
 
     def __init__(self,
         name: str,
     ):
         super().__init__()
         
@@ -11211,15 +11211,15 @@
     
     
 class InviteTokenKeyClass(_Aspect):
     """Key for an InviteToken."""
 
 
     ASPECT_NAME = 'inviteTokenKey'
-    ASPECT_INFO = {'name': 'inviteTokenKey', 'keyForEntity': 'inviteToken', 'entityCategory': 'core', 'entityAspects': ['inviteToken']}
+    ASPECT_INFO = {'keyForEntity': 'inviteToken', 'entityCategory': 'core', 'entityAspects': ['inviteToken']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.InviteTokenKey")
 
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
@@ -11248,15 +11248,15 @@
     
     
 class MLFeatureKeyClass(_Aspect):
     """Key for an MLFeature"""
 
 
     ASPECT_NAME = 'mlFeatureKey'
-    ASPECT_INFO = {'name': 'mlFeatureKey', 'keyForEntity': 'mlFeature', 'entityCategory': 'core', 'entityAspects': ['glossaryTerms', 'editableMlFeatureProperties', 'domains']}
+    ASPECT_INFO = {'keyForEntity': 'mlFeature', 'entityCategory': 'core', 'entityAspects': ['glossaryTerms', 'editableMlFeatureProperties', 'domains']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.MLFeatureKey")
 
     def __init__(self,
         featureNamespace: str,
         name: str,
     ):
         super().__init__()
@@ -11299,15 +11299,15 @@
     
     
 class MLFeatureTableKeyClass(_Aspect):
     """Key for an MLFeatureTable"""
 
 
     ASPECT_NAME = 'mlFeatureTableKey'
-    ASPECT_INFO = {'name': 'mlFeatureTableKey', 'keyForEntity': 'mlFeatureTable', 'entityCategory': 'core', 'entityAspects': ['glossaryTerms', 'editableMlFeatureTableProperties', 'domains']}
+    ASPECT_INFO = {'keyForEntity': 'mlFeatureTable', 'entityCategory': 'core', 'entityAspects': ['glossaryTerms', 'editableMlFeatureTableProperties', 'domains']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.MLFeatureTableKey")
 
     def __init__(self,
         platform: str,
         name: str,
     ):
         super().__init__()
@@ -11350,15 +11350,15 @@
     
     
 class MLModelDeploymentKeyClass(_Aspect):
     """Key for an ML model deployment"""
 
 
     ASPECT_NAME = 'mlModelDeploymentKey'
-    ASPECT_INFO = {'name': 'mlModelDeploymentKey'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.MLModelDeploymentKey")
 
     def __init__(self,
         platform: str,
         name: str,
         origin: Union[str, "FabricTypeClass"],
     ):
@@ -11415,15 +11415,15 @@
     
     
 class MLModelGroupKeyClass(_Aspect):
     """Key for an ML model group"""
 
 
     ASPECT_NAME = 'mlModelGroupKey'
-    ASPECT_INFO = {'name': 'mlModelGroupKey', 'keyForEntity': 'mlModelGroup', 'entityCategory': 'core', 'entityAspects': ['glossaryTerms', 'editableMlModelGroupProperties', 'domains']}
+    ASPECT_INFO = {'keyForEntity': 'mlModelGroup', 'entityCategory': 'core', 'entityAspects': ['glossaryTerms', 'editableMlModelGroupProperties', 'domains']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.MLModelGroupKey")
 
     def __init__(self,
         platform: str,
         name: str,
         origin: Union[str, "FabricTypeClass"],
     ):
@@ -11480,15 +11480,15 @@
     
     
 class MLModelKeyClass(_Aspect):
     """Key for an ML model"""
 
 
     ASPECT_NAME = 'mlModelKey'
-    ASPECT_INFO = {'name': 'mlModelKey', 'keyForEntity': 'mlModel', 'entityCategory': 'core', 'entityAspects': ['glossaryTerms', 'editableMlModelProperties', 'domains']}
+    ASPECT_INFO = {'keyForEntity': 'mlModel', 'entityCategory': 'core', 'entityAspects': ['glossaryTerms', 'editableMlModelProperties', 'domains']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.MLModelKey")
 
     def __init__(self,
         platform: str,
         name: str,
         origin: Union[str, "FabricTypeClass"],
     ):
@@ -11545,15 +11545,15 @@
     
     
 class MLPrimaryKeyKeyClass(_Aspect):
     """Key for an MLPrimaryKey"""
 
 
     ASPECT_NAME = 'mlPrimaryKeyKey'
-    ASPECT_INFO = {'name': 'mlPrimaryKeyKey', 'keyForEntity': 'mlPrimaryKey', 'entityCategory': 'core', 'entityAspects': ['glossaryTerms', 'editableMlPrimaryKeyProperties', 'domains']}
+    ASPECT_INFO = {'keyForEntity': 'mlPrimaryKey', 'entityCategory': 'core', 'entityAspects': ['glossaryTerms', 'editableMlPrimaryKeyProperties', 'domains']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.MLPrimaryKeyKey")
 
     def __init__(self,
         featureNamespace: str,
         name: str,
     ):
         super().__init__()
@@ -11596,15 +11596,15 @@
     
     
 class NotebookKeyClass(_Aspect):
     """Key for a Notebook"""
 
 
     ASPECT_NAME = 'notebookKey'
-    ASPECT_INFO = {'name': 'notebookKey', 'keyForEntity': 'notebook', 'entityCategory': '_unset_', 'entityAspects': ['notebookInfo', 'notebookContent', 'editableNotebookProperties', 'ownership', 'status', 'globalTags', 'glossaryTerms', 'browsePaths', 'institutionalMemory', 'domains', 'subTypes', 'dataPlatformInstance'], 'entityDoc': 'Notebook represents a combination of query, text, chart and etc. This is in BETA version'}
+    ASPECT_INFO = {'keyForEntity': 'notebook', 'entityCategory': '_unset_', 'entityAspects': ['notebookInfo', 'notebookContent', 'editableNotebookProperties', 'ownership', 'status', 'globalTags', 'glossaryTerms', 'browsePaths', 'institutionalMemory', 'domains', 'subTypes', 'dataPlatformInstance'], 'entityDoc': 'Notebook represents a combination of query, text, chart and etc. This is in BETA version'}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.NotebookKey")
 
     def __init__(self,
         notebookTool: str,
         notebookId: str,
     ):
         super().__init__()
@@ -11647,15 +11647,15 @@
     
     
 class PostKeyClass(_Aspect):
     """Key for a Post."""
 
 
     ASPECT_NAME = 'postKey'
-    ASPECT_INFO = {'name': 'postKey', 'keyForEntity': 'post', 'entityCategory': 'core', 'entityAspects': ['postInfo']}
+    ASPECT_INFO = {'keyForEntity': 'post', 'entityCategory': 'core', 'entityAspects': ['postInfo']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.PostKey")
 
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
@@ -11679,20 +11679,57 @@
     
     @id.setter
     def id(self, value: str) -> None:
         """Setter: A unique id for the DataHub Post record. Generated on the server side at Post creation time."""
         self._inner_dict['id'] = value
     
     
+class QueryKeyClass(_Aspect):
+    """Key for a Query"""
+
+
+    ASPECT_NAME = 'queryKey'
+    ASPECT_INFO = {'keyForEntity': 'query', 'entityCategory': 'core', 'entityAspects': ['queryProperties', 'querySubjects', 'status']}
+    RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.QueryKey")
+
+    def __init__(self,
+        id: str,
+    ):
+        super().__init__()
+        
+        self.id = id
+    
+    @classmethod
+    def construct_with_defaults(cls) -> "QueryKeyClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
+    def _restore_defaults(self) -> None:
+        self.id = str()
+    
+    
+    @property
+    def id(self) -> str:
+        """Getter: A unique id for the Query."""
+        return self._inner_dict.get('id')  # type: ignore
+    
+    @id.setter
+    def id(self, value: str) -> None:
+        """Setter: A unique id for the Query."""
+        self._inner_dict['id'] = value
+    
+    
 class SchemaFieldKeyClass(_Aspect):
     """Key for a SchemaField"""
 
 
     ASPECT_NAME = 'schemaFieldKey'
-    ASPECT_INFO = {'name': 'schemaFieldKey', 'keyForEntity': 'schemaField', 'entityCategory': 'core', 'entityAspects': []}
+    ASPECT_INFO = {'keyForEntity': 'schemaField', 'entityCategory': 'core', 'entityAspects': []}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.SchemaFieldKey")
 
     def __init__(self,
         parent: str,
         fieldPath: str,
     ):
         super().__init__()
@@ -11735,15 +11772,15 @@
     
     
 class TagKeyClass(_Aspect):
     """Key for a Tag"""
 
 
     ASPECT_NAME = 'tagKey'
-    ASPECT_INFO = {'name': 'tagKey', 'keyForEntity': 'tag', 'entityCategory': '_unset_', 'entityAspects': ['tagProperties', 'ownership', 'deprecation']}
+    ASPECT_INFO = {'keyForEntity': 'tag', 'entityCategory': '_unset_', 'entityAspects': ['tagProperties', 'ownership', 'deprecation']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.TagKey")
 
     def __init__(self,
         name: str,
     ):
         super().__init__()
         
@@ -11772,15 +11809,15 @@
     
     
 class TelemetryKeyClass(_Aspect):
     """Key for the telemetry client ID, only one should ever exist"""
 
 
     ASPECT_NAME = 'telemetryKey'
-    ASPECT_INFO = {'name': 'telemetryKey', 'keyForEntity': 'telemetry', 'entityCategory': 'internal', 'entityAspects': ['telemetryClientId']}
+    ASPECT_INFO = {'keyForEntity': 'telemetry', 'entityCategory': 'internal', 'entityAspects': ['telemetryClientId']}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.TelemetryKey")
 
     def __init__(self,
         name: str,
     ):
         super().__init__()
         
@@ -11809,15 +11846,15 @@
     
     
 class TestKeyClass(_Aspect):
     """Key for a Test"""
 
 
     ASPECT_NAME = 'testKey'
-    ASPECT_INFO = {'name': 'testKey', 'keyForEntity': 'test', 'entityCategory': 'core', 'entityAspects': ['testInfo'], 'entityDoc': 'A DataHub test'}
+    ASPECT_INFO = {'keyForEntity': 'test', 'entityCategory': 'core', 'entityAspects': ['testInfo'], 'entityDoc': 'A DataHub test'}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.metadata.key.TestKey")
 
     def __init__(self,
         id: str,
     ):
         super().__init__()
         
@@ -13175,15 +13212,15 @@
     
     
 class CaveatsAndRecommendationsClass(_Aspect):
     """This section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset? Are there additional recommendations for model use?"""
 
 
     ASPECT_NAME = 'mlModelCaveatsAndRecommendations'
-    ASPECT_INFO = {'name': 'mlModelCaveatsAndRecommendations'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.CaveatsAndRecommendations")
 
     def __init__(self,
         caveats: Union[None, "CaveatDetailsClass"]=None,
         recommendations: Union[None, str]=None,
         idealDatasetCharacteristics: Union[None, List[str]]=None,
     ):
@@ -13269,15 +13306,15 @@
     
     
 class EditableMLFeaturePropertiesClass(_Aspect):
     """Properties associated with a MLFeature editable from the UI"""
 
 
     ASPECT_NAME = 'editableMlFeatureProperties'
-    ASPECT_INFO = {'name': 'editableMlFeatureProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.EditableMLFeatureProperties")
 
     def __init__(self,
         description: Union[None, str]=None,
     ):
         super().__init__()
         
@@ -13306,15 +13343,15 @@
     
     
 class EditableMLFeatureTablePropertiesClass(_Aspect):
     """Properties associated with a MLFeatureTable editable from the ui"""
 
 
     ASPECT_NAME = 'editableMlFeatureTableProperties'
-    ASPECT_INFO = {'name': 'editableMlFeatureTableProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.EditableMLFeatureTableProperties")
 
     def __init__(self,
         description: Union[None, str]=None,
     ):
         super().__init__()
         
@@ -13343,15 +13380,15 @@
     
     
 class EditableMLModelGroupPropertiesClass(_Aspect):
     """Properties associated with an ML Model Group editable from the UI"""
 
 
     ASPECT_NAME = 'editableMlModelGroupProperties'
-    ASPECT_INFO = {'name': 'editableMlModelGroupProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.EditableMLModelGroupProperties")
 
     def __init__(self,
         description: Union[None, str]=None,
     ):
         super().__init__()
         
@@ -13380,15 +13417,15 @@
     
     
 class EditableMLModelPropertiesClass(_Aspect):
     """Properties associated with a ML Model editable from the UI"""
 
 
     ASPECT_NAME = 'editableMlModelProperties'
-    ASPECT_INFO = {'name': 'editableMlModelProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.EditableMLModelProperties")
 
     def __init__(self,
         description: Union[None, str]=None,
     ):
         super().__init__()
         
@@ -13417,15 +13454,15 @@
     
     
 class EditableMLPrimaryKeyPropertiesClass(_Aspect):
     """Properties associated with a MLPrimaryKey editable from the UI"""
 
 
     ASPECT_NAME = 'editableMlPrimaryKeyProperties'
-    ASPECT_INFO = {'name': 'editableMlPrimaryKeyProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.EditableMLPrimaryKeyProperties")
 
     def __init__(self,
         description: Union[None, str]=None,
     ):
         super().__init__()
         
@@ -13454,15 +13491,15 @@
     
     
 class EthicalConsiderationsClass(_Aspect):
     """This section is intended to demonstrate the ethical considerations that went into MLModel development, surfacing ethical challenges and solutions to stakeholders."""
 
 
     ASPECT_NAME = 'mlModelEthicalConsiderations'
-    ASPECT_INFO = {'name': 'mlModelEthicalConsiderations'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.EthicalConsiderations")
 
     def __init__(self,
         data: Union[None, List[str]]=None,
         humanLife: Union[None, List[str]]=None,
         mitigations: Union[None, List[str]]=None,
         risksAndHarms: Union[None, List[str]]=None,
@@ -13547,15 +13584,15 @@
     
     
 class EvaluationDataClass(_Aspect):
     """All referenced datasets would ideally point to any set of documents that provide visibility into the source and composition of the dataset."""
 
 
     ASPECT_NAME = 'mlModelEvaluationData'
-    ASPECT_INFO = {'name': 'mlModelEvaluationData'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.EvaluationData")
 
     def __init__(self,
         evaluationData: List["BaseDataClass"],
     ):
         super().__init__()
         
@@ -13584,15 +13621,15 @@
     
     
 class IntendedUseClass(_Aspect):
     """Intended Use for the ML Model"""
 
 
     ASPECT_NAME = 'intendedUse'
-    ASPECT_INFO = {'name': 'intendedUse'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.IntendedUse")
 
     def __init__(self,
         primaryUses: Union[None, List[str]]=None,
         primaryUsers: Union[None, List[Union[str, "IntendedUserTypeClass"]]]=None,
         outOfScopeUses: Union[None, List[str]]=None,
     ):
@@ -13657,15 +13694,15 @@
     
     
 class MLFeaturePropertiesClass(_Aspect):
     """Properties associated with a MLFeature"""
 
 
     ASPECT_NAME = 'mlFeatureProperties'
-    ASPECT_INFO = {'name': 'mlFeatureProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.MLFeatureProperties")
 
     def __init__(self,
         description: Union[None, str]=None,
         dataType: Union[None, Union[str, "MLFeatureDataTypeClass"]]=None,
         version: Union[None, "VersionTagClass"]=None,
         sources: Union[None, List[str]]=None,
@@ -13736,15 +13773,15 @@
     
     
 class MLFeatureTablePropertiesClass(_Aspect):
     """Properties associated with a MLFeatureTable"""
 
 
     ASPECT_NAME = 'mlFeatureTableProperties'
-    ASPECT_INFO = {'name': 'mlFeatureTableProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.MLFeatureTableProperties")
 
     def __init__(self,
         customProperties: Optional[Dict[str, str]]=None,
         description: Union[None, str]=None,
         mlFeatures: Union[None, List[str]]=None,
         mlPrimaryKeys: Union[None, List[str]]=None,
@@ -13819,15 +13856,15 @@
     
     
 class MLHyperParamClass(_Aspect):
     """Properties associated with an ML Hyper Param"""
 
 
     ASPECT_NAME = 'mlHyperParam'
-    ASPECT_INFO = {'name': 'mlHyperParam'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.MLHyperParam")
 
     def __init__(self,
         name: str,
         description: Union[None, str]=None,
         value: Union[None, str]=None,
         createdAt: Union[None, int]=None,
@@ -13898,15 +13935,15 @@
     
     
 class MLMetricClass(_Aspect):
     """Properties associated with an ML Metric"""
 
 
     ASPECT_NAME = 'mlMetric'
-    ASPECT_INFO = {'name': 'mlMetric'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.MLMetric")
 
     def __init__(self,
         name: str,
         description: Union[None, str]=None,
         value: Union[None, str]=None,
         createdAt: Union[None, int]=None,
@@ -13977,15 +14014,15 @@
     
     
 class MLModelDeploymentPropertiesClass(_Aspect):
     """Properties associated with an ML Model Deployment"""
 
 
     ASPECT_NAME = 'mlModelDeploymentProperties'
-    ASPECT_INFO = {'name': 'mlModelDeploymentProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.MLModelDeploymentProperties")
 
     def __init__(self,
         customProperties: Optional[Dict[str, str]]=None,
         externalUrl: Union[None, str]=None,
         description: Union[None, str]=None,
         createdAt: Union[None, int]=None,
@@ -14088,15 +14125,15 @@
     
     
 class MLModelFactorPromptsClass(_Aspect):
     """Prompts which affect the performance of the MLModel"""
 
 
     ASPECT_NAME = 'mlModelFactorPrompts'
-    ASPECT_INFO = {'name': 'mlModelFactorPrompts'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.MLModelFactorPrompts")
 
     def __init__(self,
         relevantFactors: Union[None, List["MLModelFactorsClass"]]=None,
         evaluationFactors: Union[None, List["MLModelFactorsClass"]]=None,
     ):
         super().__init__()
@@ -14206,15 +14243,15 @@
     
     
 class MLModelGroupPropertiesClass(_Aspect):
     """Properties associated with an ML Model Group"""
 
 
     ASPECT_NAME = 'mlModelGroupProperties'
-    ASPECT_INFO = {'name': 'mlModelGroupProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.MLModelGroupProperties")
 
     def __init__(self,
         customProperties: Optional[Dict[str, str]]=None,
         description: Union[None, str]=None,
         createdAt: Union[None, int]=None,
         version: Union[None, "VersionTagClass"]=None,
@@ -14289,15 +14326,15 @@
     
     
 class MLModelPropertiesClass(_Aspect):
     """Properties associated with a ML Model"""
 
 
     ASPECT_NAME = 'mlModelProperties'
-    ASPECT_INFO = {'name': 'mlModelProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.MLModelProperties")
 
     def __init__(self,
         customProperties: Optional[Dict[str, str]]=None,
         externalUrl: Union[None, str]=None,
         description: Union[None, str]=None,
         date: Union[None, int]=None,
@@ -14548,15 +14585,15 @@
     
     
 class MLPrimaryKeyPropertiesClass(_Aspect):
     """Properties associated with a MLPrimaryKey"""
 
 
     ASPECT_NAME = 'mlPrimaryKeyProperties'
-    ASPECT_INFO = {'name': 'mlPrimaryKeyProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.MLPrimaryKeyProperties")
 
     def __init__(self,
         sources: List[str],
         description: Union[None, str]=None,
         dataType: Union[None, Union[str, "MLFeatureDataTypeClass"]]=None,
         version: Union[None, "VersionTagClass"]=None,
@@ -14627,15 +14664,15 @@
     
     
 class MetricsClass(_Aspect):
     """Metrics to be featured for the MLModel."""
 
 
     ASPECT_NAME = 'mlModelMetrics'
-    ASPECT_INFO = {'name': 'mlModelMetrics'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.Metrics")
 
     def __init__(self,
         performanceMeasures: Union[None, List[str]]=None,
         decisionThreshold: Union[None, List[str]]=None,
     ):
         super().__init__()
@@ -14678,15 +14715,15 @@
     
     
 class QuantitativeAnalysesClass(_Aspect):
     """Quantitative analyses should be disaggregated, that is, broken down by the chosen factors. Quantitative analyses should provide the results of evaluating the MLModel according to the chosen metrics, providing confidence interval values when possible."""
 
 
     ASPECT_NAME = 'mlModelQuantitativeAnalyses'
-    ASPECT_INFO = {'name': 'mlModelQuantitativeAnalyses'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.QuantitativeAnalyses")
 
     def __init__(self,
         unitaryResults: Union[None, str]=None,
         intersectionalResults: Union[None, str]=None,
     ):
         super().__init__()
@@ -14729,15 +14766,15 @@
     
     
 class SourceCodeClass(_Aspect):
     """Source Code"""
 
 
     ASPECT_NAME = 'sourceCode'
-    ASPECT_INFO = {'name': 'sourceCode'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.SourceCode")
 
     def __init__(self,
         sourceCode: List["SourceCodeUrlClass"],
     ):
         super().__init__()
         
@@ -14821,15 +14858,15 @@
     
     
 class TrainingDataClass(_Aspect):
     """Ideally, the MLModel card would contain as much information about the training data as the evaluation data. However, there might be cases where it is not feasible to provide this level of detailed information about the training data. For example, the data may be proprietary, or require a non-disclosure agreement. In these cases, we advocate for basic details about the distributions over groups in the data, as well as any other details that could inform stakeholders on the kinds of biases the model may have encoded."""
 
 
     ASPECT_NAME = 'mlModelTrainingData'
-    ASPECT_INFO = {'name': 'mlModelTrainingData'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.ml.metadata.TrainingData")
 
     def __init__(self,
         trainingData: List["BaseDataClass"],
     ):
         super().__init__()
         
@@ -15597,15 +15634,15 @@
 class EditableNotebookPropertiesClass(_Aspect):
     """Stores editable changes made to properties. This separates changes made from
     ingestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines
     Note: This is IN BETA version"""
 
 
     ASPECT_NAME = 'editableNotebookProperties'
-    ASPECT_INFO = {'name': 'editableNotebookProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.notebook.EditableNotebookProperties")
 
     def __init__(self,
         created: Optional["AuditStampClass"]=None,
         lastModified: Optional["AuditStampClass"]=None,
         deleted: Union[None, "AuditStampClass"]=None,
         description: Union[None, str]=None,
@@ -15774,15 +15811,15 @@
     
 class NotebookContentClass(_Aspect):
     """Content in a Notebook
     Note: This is IN BETA version"""
 
 
     ASPECT_NAME = 'notebookContent'
-    ASPECT_INFO = {'name': 'notebookContent'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.notebook.NotebookContent")
 
     def __init__(self,
         cells: Optional[List["NotebookCellClass"]]=None,
     ):
         super().__init__()
         
@@ -15816,15 +15853,15 @@
     
 class NotebookInfoClass(_Aspect):
     """Information about a Notebook
     Note: This is IN BETA version"""
 
 
     ASPECT_NAME = 'notebookInfo'
-    ASPECT_INFO = {'name': 'notebookInfo'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.notebook.NotebookInfo")
 
     def __init__(self,
         title: str,
         changeAuditStamps: "ChangeAuditStampsClass",
         customProperties: Optional[Dict[str, str]]=None,
         externalUrl: Union[None, str]=None,
@@ -16345,15 +16382,15 @@
     
     
 class DataHubPolicyInfoClass(_Aspect):
     """Information about a DataHub (UI) access policy."""
 
 
     ASPECT_NAME = 'dataHubPolicyInfo'
-    ASPECT_INFO = {'name': 'dataHubPolicyInfo'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.policy.DataHubPolicyInfo")
 
     def __init__(self,
         displayName: str,
         description: str,
         type: str,
         state: str,
@@ -16581,15 +16618,15 @@
     
     
 class DataHubRoleInfoClass(_Aspect):
     """Information about a DataHub Role."""
 
 
     ASPECT_NAME = 'dataHubRoleInfo'
-    ASPECT_INFO = {'name': 'dataHubRoleInfo'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.policy.DataHubRoleInfo")
 
     def __init__(self,
         name: str,
         description: str,
         editable: Optional[bool]=None,
     ):
@@ -16856,15 +16893,15 @@
     
     
 class PostInfoClass(_Aspect):
     """Information about a DataHub Post."""
 
 
     ASPECT_NAME = 'postInfo'
-    ASPECT_INFO = {'name': 'postInfo'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.post.PostInfo")
 
     def __init__(self,
         type: Union[str, "PostTypeClass"],
         content: "PostContentClass",
         created: int,
         lastModified: int,
@@ -16938,20 +16975,279 @@
     """Enum defining types of Posts."""
     
     
     """The Post is an Home Page announcement."""
     HOME_PAGE_ANNOUNCEMENT = "HOME_PAGE_ANNOUNCEMENT"
     
     
+class QueryLanguageClass(object):
+    # No docs available.
+    
+    
+    """A SQL Query"""
+    SQL = "SQL"
+    
+    
+class QueryPropertiesClass(_Aspect):
+    """Information about a Query against one or more data assets (e.g. Tables or Views)."""
+
+
+    ASPECT_NAME = 'queryProperties'
+    ASPECT_INFO = {}
+    RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.query.QueryProperties")
+
+    def __init__(self,
+        statement: "QueryStatementClass",
+        source: Union[str, "QuerySourceClass"],
+        created: "AuditStampClass",
+        lastModified: "AuditStampClass",
+        name: Union[None, str]=None,
+        description: Union[None, str]=None,
+    ):
+        super().__init__()
+        
+        self.statement = statement
+        self.source = source
+        self.name = name
+        self.description = description
+        self.created = created
+        self.lastModified = lastModified
+    
+    @classmethod
+    def construct_with_defaults(cls) -> "QueryPropertiesClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
+    def _restore_defaults(self) -> None:
+        self.statement = QueryStatementClass.construct_with_defaults()
+        self.source = QuerySourceClass.MANUAL
+        self.name = self.RECORD_SCHEMA.fields_dict["name"].default
+        self.description = self.RECORD_SCHEMA.fields_dict["description"].default
+        self.created = AuditStampClass.construct_with_defaults()
+        self.lastModified = AuditStampClass.construct_with_defaults()
+    
+    
+    @property
+    def statement(self) -> "QueryStatementClass":
+        """Getter: The Query Statement."""
+        return self._inner_dict.get('statement')  # type: ignore
+    
+    @statement.setter
+    def statement(self, value: "QueryStatementClass") -> None:
+        """Setter: The Query Statement."""
+        self._inner_dict['statement'] = value
+    
+    
+    @property
+    def source(self) -> Union[str, "QuerySourceClass"]:
+        """Getter: The source of the Query"""
+        return self._inner_dict.get('source')  # type: ignore
+    
+    @source.setter
+    def source(self, value: Union[str, "QuerySourceClass"]) -> None:
+        """Setter: The source of the Query"""
+        self._inner_dict['source'] = value
+    
+    
+    @property
+    def name(self) -> Union[None, str]:
+        """Getter: Optional display name to identify the query."""
+        return self._inner_dict.get('name')  # type: ignore
+    
+    @name.setter
+    def name(self, value: Union[None, str]) -> None:
+        """Setter: Optional display name to identify the query."""
+        self._inner_dict['name'] = value
+    
+    
+    @property
+    def description(self) -> Union[None, str]:
+        """Getter: The Query description."""
+        return self._inner_dict.get('description')  # type: ignore
+    
+    @description.setter
+    def description(self, value: Union[None, str]) -> None:
+        """Setter: The Query description."""
+        self._inner_dict['description'] = value
+    
+    
+    @property
+    def created(self) -> "AuditStampClass":
+        """Getter: Audit stamp capturing the time and actor who created the Query."""
+        return self._inner_dict.get('created')  # type: ignore
+    
+    @created.setter
+    def created(self, value: "AuditStampClass") -> None:
+        """Setter: Audit stamp capturing the time and actor who created the Query."""
+        self._inner_dict['created'] = value
+    
+    
+    @property
+    def lastModified(self) -> "AuditStampClass":
+        """Getter: Audit stamp capturing the time and actor who last modified the Query."""
+        return self._inner_dict.get('lastModified')  # type: ignore
+    
+    @lastModified.setter
+    def lastModified(self, value: "AuditStampClass") -> None:
+        """Setter: Audit stamp capturing the time and actor who last modified the Query."""
+        self._inner_dict['lastModified'] = value
+    
+    
+class QuerySourceClass(object):
+    # No docs available.
+    
+    
+    """The query was entered manually by a user (via the UI)."""
+    MANUAL = "MANUAL"
+    
+    
+class QueryStatementClass(DictWrapper):
+    """A query statement against one or more data assets."""
+    
+    RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.query.QueryStatement")
+    def __init__(self,
+        value: str,
+        language: Optional[Union[str, "QueryLanguageClass"]]=None,
+    ):
+        super().__init__()
+        
+        self.value = value
+        if language is None:
+            # default: 'SQL'
+            self.language = self.RECORD_SCHEMA.fields_dict["language"].default
+        else:
+            self.language = language
+    
+    @classmethod
+    def construct_with_defaults(cls) -> "QueryStatementClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
+    def _restore_defaults(self) -> None:
+        self.value = str()
+        self.language = self.RECORD_SCHEMA.fields_dict["language"].default
+    
+    
+    @property
+    def value(self) -> str:
+        """Getter: The query text"""
+        return self._inner_dict.get('value')  # type: ignore
+    
+    @value.setter
+    def value(self, value: str) -> None:
+        """Setter: The query text"""
+        self._inner_dict['value'] = value
+    
+    
+    @property
+    def language(self) -> Union[str, "QueryLanguageClass"]:
+        """Getter: The language of the Query, e.g. SQL."""
+        return self._inner_dict.get('language')  # type: ignore
+    
+    @language.setter
+    def language(self, value: Union[str, "QueryLanguageClass"]) -> None:
+        """Setter: The language of the Query, e.g. SQL."""
+        self._inner_dict['language'] = value
+    
+    
+class QuerySubjectClass(DictWrapper):
+    """A single subject of a particular query.
+    In the future, we may evolve this model to include richer details
+    about the Query Subject in relation to the query."""
+    
+    RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.query.QuerySubject")
+    def __init__(self,
+        entity: str,
+    ):
+        super().__init__()
+        
+        self.entity = entity
+    
+    @classmethod
+    def construct_with_defaults(cls) -> "QuerySubjectClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
+    def _restore_defaults(self) -> None:
+        self.entity = str()
+    
+    
+    @property
+    def entity(self) -> str:
+        """Getter: An entity which is the subject of a query."""
+        return self._inner_dict.get('entity')  # type: ignore
+    
+    @entity.setter
+    def entity(self, value: str) -> None:
+        """Setter: An entity which is the subject of a query."""
+        self._inner_dict['entity'] = value
+    
+    
+class QuerySubjectsClass(_Aspect):
+    """Information about the subjects of a particular Query, i.e. the assets
+    being queried."""
+
+
+    ASPECT_NAME = 'querySubjects'
+    ASPECT_INFO = {}
+    RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.query.QuerySubjects")
+
+    def __init__(self,
+        subjects: List["QuerySubjectClass"],
+    ):
+        super().__init__()
+        
+        self.subjects = subjects
+    
+    @classmethod
+    def construct_with_defaults(cls) -> "QuerySubjectsClass":
+        self = cls.construct({})
+        self._restore_defaults()
+        
+        return self
+    
+    def _restore_defaults(self) -> None:
+        self.subjects = list()
+    
+    
+    @property
+    def subjects(self) -> List["QuerySubjectClass"]:
+        """Getter: One or more subjects of the query.
+    
+    In single-asset queries (e.g. table select), this will contain the Table reference
+    and optionally schema field references.
+    
+    In multi-asset queries (e.g. table joins), this may contain multiple Table references
+    and optionally schema field references."""
+        return self._inner_dict.get('subjects')  # type: ignore
+    
+    @subjects.setter
+    def subjects(self, value: List["QuerySubjectClass"]) -> None:
+        """Setter: One or more subjects of the query.
+    
+    In single-asset queries (e.g. table select), this will contain the Table reference
+    and optionally schema field references.
+    
+    In multi-asset queries (e.g. table joins), this may contain multiple Table references
+    and optionally schema field references."""
+        self._inner_dict['subjects'] = value
+    
+    
 class DataHubRetentionConfigClass(_Aspect):
     # No docs available.
 
 
     ASPECT_NAME = 'dataHubRetentionConfig'
-    ASPECT_INFO = {'name': 'dataHubRetentionConfig'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.retention.DataHubRetentionConfig")
 
     def __init__(self,
         retention: "RetentionClass",
     ):
         super().__init__()
         
@@ -17357,15 +17653,15 @@
     
 class EditableSchemaMetadataClass(_Aspect):
     """EditableSchemaMetadata stores editable changes made to schema metadata. This separates changes made from
     ingestion pipelines and edits in the UI to avoid accidental overwrites of user-provided data by ingestion pipelines."""
 
 
     ASPECT_NAME = 'editableSchemaMetadata'
-    ASPECT_INFO = {'name': 'editableSchemaMetadata'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.EditableSchemaMetadata")
 
     def __init__(self,
         editableSchemaFieldInfo: List["EditableSchemaFieldInfoClass"],
         created: Optional["AuditStampClass"]=None,
         lastModified: Optional["AuditStampClass"]=None,
         deleted: Union[None, "AuditStampClass"]=None,
@@ -18287,15 +18583,15 @@
     
     
 class SchemaMetadataClass(_Aspect):
     """SchemaMetadata to describe metadata related to store schema"""
 
 
     ASPECT_NAME = 'schemaMetadata'
-    ASPECT_INFO = {'name': 'schemaMetadata'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.schema.SchemaMetadata")
 
     def __init__(self,
         schemaName: str,
         platform: str,
         version: int,
         hash: str,
@@ -18640,15 +18936,15 @@
     
     
 class DataHubSecretValueClass(_Aspect):
     """The value of a DataHub Secret"""
 
 
     ASPECT_NAME = 'dataHubSecretValue'
-    ASPECT_INFO = {'name': 'dataHubSecretValue'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.secret.DataHubSecretValue")
 
     def __init__(self,
         name: str,
         value: str,
         description: Union[None, str]=None,
         created: Union[None, "AuditStampClass"]=None,
@@ -18719,15 +19015,15 @@
     
     
 class GlobalSettingsInfoClass(_Aspect):
     """DataHub Global platform settings. Careful - these should not be modified by the outside world!"""
 
 
     ASPECT_NAME = 'globalSettingsInfo'
-    ASPECT_INFO = {'name': 'globalSettingsInfo'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.settings.global.GlobalSettingsInfo")
 
     def __init__(self,
         views: Union[None, "GlobalViewsSettingsClass"]=None,
     ):
         super().__init__()
         
@@ -18789,15 +19085,15 @@
     
     
 class DataHubStepStatePropertiesClass(_Aspect):
     """The properties associated with a DataHub step state"""
 
 
     ASPECT_NAME = 'dataHubStepStateProperties'
-    ASPECT_INFO = {'name': 'dataHubStepStateProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.step.DataHubStepStateProperties")
 
     def __init__(self,
         lastModified: "AuditStampClass",
         properties: Optional[Dict[str, str]]=None,
     ):
         super().__init__()
@@ -18844,15 +19140,15 @@
     
     
 class TagPropertiesClass(_Aspect):
     """Properties associated with a Tag"""
 
 
     ASPECT_NAME = 'tagProperties'
-    ASPECT_INFO = {'name': 'tagProperties'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.tag.TagProperties")
 
     def __init__(self,
         name: str,
         description: Union[None, str]=None,
         colorHex: Union[None, str]=None,
     ):
@@ -18909,15 +19205,15 @@
     
     
 class TelemetryClientIdClass(_Aspect):
     """A simple wrapper around a String to persist the client ID for telemetry in DataHub's backend DB"""
 
 
     ASPECT_NAME = 'telemetryClientId'
-    ASPECT_INFO = {'name': 'telemetryClientId'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.telemetry.TelemetryClientId")
 
     def __init__(self,
         clientId: str,
     ):
         super().__init__()
         
@@ -19001,15 +19297,15 @@
     
     
 class TestInfoClass(_Aspect):
     """Information about a DataHub Test"""
 
 
     ASPECT_NAME = 'testInfo'
-    ASPECT_INFO = {'name': 'testInfo'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.test.TestInfo")
 
     def __init__(self,
         name: str,
         category: str,
         definition: "TestDefinitionClass",
         description: Union[None, str]=None,
@@ -19138,15 +19434,15 @@
     
     
 class TestResultsClass(_Aspect):
     """Information about a Test Result"""
 
 
     ASPECT_NAME = 'testResults'
-    ASPECT_INFO = {'name': 'testResults'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.test.TestResults")
 
     def __init__(self,
         failing: List["TestResultClass"],
         passing: List["TestResultClass"],
     ):
         super().__init__()
@@ -19373,15 +19669,15 @@
     
     
 class DataHubUpgradeRequestClass(_Aspect):
     """Information collected when kicking off a DataHubUpgrade"""
 
 
     ASPECT_NAME = 'dataHubUpgradeRequest'
-    ASPECT_INFO = {'name': 'dataHubUpgradeRequest'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.upgrade.DataHubUpgradeRequest")
 
     def __init__(self,
         timestampMs: int,
         version: str,
     ):
         super().__init__()
@@ -19424,15 +19720,15 @@
     
     
 class DataHubUpgradeResultClass(_Aspect):
     """Information collected when a DataHubUpgrade successfully finishes"""
 
 
     ASPECT_NAME = 'dataHubUpgradeResult'
-    ASPECT_INFO = {'name': 'dataHubUpgradeResult'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.upgrade.DataHubUpgradeResult")
 
     def __init__(self,
         timestampMs: int,
         result: Union[None, Dict[str, str]]=None,
     ):
         super().__init__()
@@ -19795,15 +20091,15 @@
     
     
 class DataHubViewInfoClass(_Aspect):
     """Information about a DataHub View. -- TODO: Understand whether an entity type filter is required."""
 
 
     ASPECT_NAME = 'dataHubViewInfo'
-    ASPECT_INFO = {'name': 'dataHubViewInfo'}
+    ASPECT_INFO = {}
     RECORD_SCHEMA = get_schema_type("com.linkedin.pegasus2avro.view.DataHubViewInfo")
 
     def __init__(self,
         name: str,
         type: Union[str, "DataHubViewTypeClass"],
         definition: "DataHubViewDefinitionClass",
         created: "AuditStampClass",
@@ -20088,14 +20384,15 @@
     'com.linkedin.pegasus2avro.metadata.key.MLFeatureTableKey': MLFeatureTableKeyClass,
     'com.linkedin.pegasus2avro.metadata.key.MLModelDeploymentKey': MLModelDeploymentKeyClass,
     'com.linkedin.pegasus2avro.metadata.key.MLModelGroupKey': MLModelGroupKeyClass,
     'com.linkedin.pegasus2avro.metadata.key.MLModelKey': MLModelKeyClass,
     'com.linkedin.pegasus2avro.metadata.key.MLPrimaryKeyKey': MLPrimaryKeyKeyClass,
     'com.linkedin.pegasus2avro.metadata.key.NotebookKey': NotebookKeyClass,
     'com.linkedin.pegasus2avro.metadata.key.PostKey': PostKeyClass,
+    'com.linkedin.pegasus2avro.metadata.key.QueryKey': QueryKeyClass,
     'com.linkedin.pegasus2avro.metadata.key.SchemaFieldKey': SchemaFieldKeyClass,
     'com.linkedin.pegasus2avro.metadata.key.TagKey': TagKeyClass,
     'com.linkedin.pegasus2avro.metadata.key.TelemetryKey': TelemetryKeyClass,
     'com.linkedin.pegasus2avro.metadata.key.TestKey': TestKeyClass,
     'com.linkedin.pegasus2avro.metadata.query.filter.Condition': ConditionClass,
     'com.linkedin.pegasus2avro.metadata.query.filter.ConjunctiveCriterion': ConjunctiveCriterionClass,
     'com.linkedin.pegasus2avro.metadata.query.filter.Criterion': CriterionClass,
@@ -20175,14 +20472,20 @@
     'com.linkedin.pegasus2avro.policy.PolicyMatchCondition': PolicyMatchConditionClass,
     'com.linkedin.pegasus2avro.policy.PolicyMatchCriterion': PolicyMatchCriterionClass,
     'com.linkedin.pegasus2avro.policy.PolicyMatchFilter': PolicyMatchFilterClass,
     'com.linkedin.pegasus2avro.post.PostContent': PostContentClass,
     'com.linkedin.pegasus2avro.post.PostContentType': PostContentTypeClass,
     'com.linkedin.pegasus2avro.post.PostInfo': PostInfoClass,
     'com.linkedin.pegasus2avro.post.PostType': PostTypeClass,
+    'com.linkedin.pegasus2avro.query.QueryLanguage': QueryLanguageClass,
+    'com.linkedin.pegasus2avro.query.QueryProperties': QueryPropertiesClass,
+    'com.linkedin.pegasus2avro.query.QuerySource': QuerySourceClass,
+    'com.linkedin.pegasus2avro.query.QueryStatement': QueryStatementClass,
+    'com.linkedin.pegasus2avro.query.QuerySubject': QuerySubjectClass,
+    'com.linkedin.pegasus2avro.query.QuerySubjects': QuerySubjectsClass,
     'com.linkedin.pegasus2avro.retention.DataHubRetentionConfig': DataHubRetentionConfigClass,
     'com.linkedin.pegasus2avro.retention.Retention': RetentionClass,
     'com.linkedin.pegasus2avro.retention.TimeBasedRetention': TimeBasedRetentionClass,
     'com.linkedin.pegasus2avro.retention.VersionBasedRetention': VersionBasedRetentionClass,
     'com.linkedin.pegasus2avro.schema.ArrayType': ArrayTypeClass,
     'com.linkedin.pegasus2avro.schema.BinaryJsonSchema': BinaryJsonSchemaClass,
     'com.linkedin.pegasus2avro.schema.BooleanType': BooleanTypeClass,
@@ -20416,14 +20719,15 @@
     'MLFeatureTableKey': MLFeatureTableKeyClass,
     'MLModelDeploymentKey': MLModelDeploymentKeyClass,
     'MLModelGroupKey': MLModelGroupKeyClass,
     'MLModelKey': MLModelKeyClass,
     'MLPrimaryKeyKey': MLPrimaryKeyKeyClass,
     'NotebookKey': NotebookKeyClass,
     'PostKey': PostKeyClass,
+    'QueryKey': QueryKeyClass,
     'SchemaFieldKey': SchemaFieldKeyClass,
     'TagKey': TagKeyClass,
     'TelemetryKey': TelemetryKeyClass,
     'TestKey': TestKeyClass,
     'Condition': ConditionClass,
     'ConjunctiveCriterion': ConjunctiveCriterionClass,
     'Criterion': CriterionClass,
@@ -20503,14 +20807,20 @@
     'PolicyMatchCondition': PolicyMatchConditionClass,
     'PolicyMatchCriterion': PolicyMatchCriterionClass,
     'PolicyMatchFilter': PolicyMatchFilterClass,
     'PostContent': PostContentClass,
     'PostContentType': PostContentTypeClass,
     'PostInfo': PostInfoClass,
     'PostType': PostTypeClass,
+    'QueryLanguage': QueryLanguageClass,
+    'QueryProperties': QueryPropertiesClass,
+    'QuerySource': QuerySourceClass,
+    'QueryStatement': QueryStatementClass,
+    'QuerySubject': QuerySubjectClass,
+    'QuerySubjects': QuerySubjectsClass,
     'DataHubRetentionConfig': DataHubRetentionConfigClass,
     'Retention': RetentionClass,
     'TimeBasedRetention': TimeBasedRetentionClass,
     'VersionBasedRetention': VersionBasedRetentionClass,
     'ArrayType': ArrayTypeClass,
     'BinaryJsonSchema': BinaryJsonSchemaClass,
     'BooleanType': BooleanTypeClass,
@@ -20572,207 +20882,209 @@
 }
 
 _json_converter = avrojson.AvroJsonConverter(use_logical_types=False, schema_types=__SCHEMA_TYPES)
 
 
     
 
-from typing import Type
-
 ASPECT_CLASSES: List[Type[_Aspect]] = [
-    DataHubViewInfoClass,
-    ViewPropertiesClass,
-    DatasetUsageStatisticsClass,
-    DatasetDeprecationClass,
-    DatasetPropertiesClass,
-    DatasetUpstreamLineageClass,
-    DatasetProfileClass,
-    UpstreamLineageClass,
-    EditableDatasetPropertiesClass,
-    DataHubStepStatePropertiesClass,
-    DataHubSecretValueClass,
-    DashboardKeyClass,
-    MLFeatureTableKeyClass,
-    DataHubAccessTokenKeyClass,
-    DataHubIngestionSourceKeyClass,
-    AssertionKeyClass,
-    MLPrimaryKeyKeyClass,
-    DataHubUpgradeKeyClass,
-    TelemetryKeyClass,
-    DataHubSecretKeyClass,
-    DataHubRetentionKeyClass,
-    DataHubRoleKeyClass,
-    DataPlatformInstanceKeyClass,
-    DataHubStepStateKeyClass,
-    DataProcessInstanceKeyClass,
-    PostKeyClass,
-    GlobalSettingsKeyClass,
-    ChartKeyClass,
-    DataJobKeyClass,
-    DomainKeyClass,
-    MLModelDeploymentKeyClass,
-    DataProcessKeyClass,
-    MLModelKeyClass,
-    DataPlatformKeyClass,
-    MLModelGroupKeyClass,
-    MLFeatureKeyClass,
-    GlossaryNodeKeyClass,
-    GlossaryTermKeyClass,
-    DatasetKeyClass,
-    DataHubViewKeyClass,
-    ExecutionRequestKeyClass,
-    TestKeyClass,
-    DataFlowKeyClass,
-    DataHubPolicyKeyClass,
-    ContainerKeyClass,
-    CorpGroupKeyClass,
-    InviteTokenKeyClass,
-    NotebookKeyClass,
-    SchemaFieldKeyClass,
-    TagKeyClass,
-    CorpUserKeyClass,
-    CaveatsAndRecommendationsClass,
-    EthicalConsiderationsClass,
-    MLFeaturePropertiesClass,
-    MLMetricClass,
-    MLPrimaryKeyPropertiesClass,
-    QuantitativeAnalysesClass,
-    MLFeatureTablePropertiesClass,
-    EditableMLPrimaryKeyPropertiesClass,
+    EditableDataJobPropertiesClass,
+    DataFlowInfoClass,
+    EditableDataFlowPropertiesClass,
+    DataJobInputOutputClass,
+    DataJobInfoClass,
+    VersionInfoClass,
+    DatahubIngestionRunSummaryClass,
+    DatahubIngestionCheckpointClass,
+    EmbedClass,
+    InputFieldsClass,
+    CostClass,
+    SubTypesClass,
+    GlobalTagsClass,
+    SiblingsClass,
+    OperationClass,
+    BrowsePathsClass,
+    GlossaryTermsClass,
+    OriginClass,
+    DataPlatformInstanceClass,
+    InstitutionalMemoryClass,
+    DeprecationClass,
+    OwnershipClass,
+    StatusClass,
+    SourceCodeClass,
     EditableMLModelPropertiesClass,
-    EditableMLModelGroupPropertiesClass,
-    MetricsClass,
-    MLModelGroupPropertiesClass,
-    IntendedUseClass,
-    MLModelFactorPromptsClass,
+    EditableMLPrimaryKeyPropertiesClass,
+    EvaluationDataClass,
     EditableMLFeatureTablePropertiesClass,
+    MLPrimaryKeyPropertiesClass,
+    MLHyperParamClass,
     MLModelDeploymentPropertiesClass,
-    SourceCodeClass,
-    EvaluationDataClass,
+    CaveatsAndRecommendationsClass,
+    MLMetricClass,
     TrainingDataClass,
+    MLModelGroupPropertiesClass,
+    MetricsClass,
+    MLModelFactorPromptsClass,
+    EthicalConsiderationsClass,
+    MLFeaturePropertiesClass,
     MLModelPropertiesClass,
     EditableMLFeaturePropertiesClass,
-    MLHyperParamClass,
-    SchemaMetadataClass,
-    EditableSchemaMetadataClass,
-    ChartInfoClass,
-    EditableChartPropertiesClass,
+    EditableMLModelGroupPropertiesClass,
+    MLFeatureTablePropertiesClass,
+    QuantitativeAnalysesClass,
+    IntendedUseClass,
+    DataHubPolicyInfoClass,
+    DataHubRoleInfoClass,
+    DataHubStepStatePropertiesClass,
+    DataPlatformInfoClass,
+    ExecutionRequestResultClass,
+    ExecutionRequestInputClass,
+    ExecutionRequestSignalClass,
     ChartQueryClass,
+    EditableChartPropertiesClass,
     ChartUsageStatisticsClass,
-    ContainerClass,
-    ContainerPropertiesClass,
+    ChartInfoClass,
     EditableContainerPropertiesClass,
-    PostInfoClass,
-    InstitutionalMemoryClass,
-    DataPlatformInstanceClass,
-    GlobalTagsClass,
-    EmbedClass,
-    OwnershipClass,
-    CostClass,
-    StatusClass,
-    OperationClass,
-    DeprecationClass,
-    InputFieldsClass,
-    BrowsePathsClass,
-    GlossaryTermsClass,
-    SiblingsClass,
-    SubTypesClass,
-    OriginClass,
+    ContainerPropertiesClass,
+    ContainerClass,
+    GlossaryNodeInfoClass,
+    GlossaryTermInfoClass,
+    GlossaryRelatedTermsClass,
     TagPropertiesClass,
-    DataProcessInstanceOutputClass,
-    DataProcessInstancePropertiesClass,
-    DataProcessInstanceInputClass,
-    DataProcessInstanceRunEventClass,
-    DataProcessInfoClass,
-    DataProcessInstanceRelationshipsClass,
-    DataHubRetentionConfigClass,
+    QuerySubjectsClass,
+    QueryPropertiesClass,
     TelemetryClientIdClass,
     DomainPropertiesClass,
     DomainsClass,
-    DataHubAccessTokenInfoClass,
+    EditableSchemaMetadataClass,
+    SchemaMetadataClass,
     DataHubIngestionSourceInfoClass,
-    NotebookInfoClass,
-    EditableNotebookPropertiesClass,
-    NotebookContentClass,
+    ViewPropertiesClass,
+    DatasetDeprecationClass,
+    DatasetUpstreamLineageClass,
+    DatasetUsageStatisticsClass,
+    DatasetPropertiesClass,
+    UpstreamLineageClass,
+    DatasetProfileClass,
+    EditableDatasetPropertiesClass,
     EditableDashboardPropertiesClass,
     DashboardUsageStatisticsClass,
     DashboardInfoClass,
-    RoleMembershipClass,
-    CorpGroupInfoClass,
-    NativeGroupMembershipClass,
-    CorpUserSettingsClass,
+    DataHubRetentionConfigClass,
     CorpUserStatusClass,
+    CorpUserCredentialsClass,
     CorpUserEditableInfoClass,
     InviteTokenClass,
     CorpUserInfoClass,
+    RoleMembershipClass,
     GroupMembershipClass,
-    CorpUserCredentialsClass,
+    CorpUserSettingsClass,
     CorpGroupEditableInfoClass,
-    DataPlatformInstancePropertiesClass,
-    TestInfoClass,
-    TestResultsClass,
-    VersionInfoClass,
-    DataJobInfoClass,
-    DataJobInputOutputClass,
-    EditableDataJobPropertiesClass,
-    EditableDataFlowPropertiesClass,
-    DataFlowInfoClass,
-    DatahubIngestionRunSummaryClass,
-    DatahubIngestionCheckpointClass,
-    DataPlatformInfoClass,
-    DataHubRoleInfoClass,
-    DataHubPolicyInfoClass,
-    ExecutionRequestInputClass,
-    ExecutionRequestSignalClass,
-    ExecutionRequestResultClass,
-    GlobalSettingsInfoClass,
-    GlossaryTermInfoClass,
-    GlossaryNodeInfoClass,
-    GlossaryRelatedTermsClass,
+    NativeGroupMembershipClass,
+    CorpGroupInfoClass,
+    DataHubAccessTokenInfoClass,
+    DataProcessInstanceRunEventClass,
+    DataProcessInstanceInputClass,
+    DataProcessInstancePropertiesClass,
+    DataProcessInstanceOutputClass,
+    DataProcessInfoClass,
+    DataProcessInstanceRelationshipsClass,
     AssertionInfoClass,
     AssertionRunEventClass,
+    NotebookContentClass,
+    NotebookInfoClass,
+    EditableNotebookPropertiesClass,
+    PostInfoClass,
+    DataHubUpgradeResultClass,
     DataHubUpgradeRequestClass,
-    DataHubUpgradeResultClass
+    MLFeatureTableKeyClass,
+    ContainerKeyClass,
+    DataHubRetentionKeyClass,
+    DataJobKeyClass,
+    MLModelDeploymentKeyClass,
+    SchemaFieldKeyClass,
+    DataProcessKeyClass,
+    CorpGroupKeyClass,
+    MLPrimaryKeyKeyClass,
+    DataFlowKeyClass,
+    DashboardKeyClass,
+    MLModelGroupKeyClass,
+    TelemetryKeyClass,
+    GlossaryTermKeyClass,
+    GlossaryNodeKeyClass,
+    DataProcessInstanceKeyClass,
+    DataHubRoleKeyClass,
+    MLModelKeyClass,
+    DataHubViewKeyClass,
+    QueryKeyClass,
+    CorpUserKeyClass,
+    DataPlatformInstanceKeyClass,
+    NotebookKeyClass,
+    MLFeatureKeyClass,
+    ChartKeyClass,
+    AssertionKeyClass,
+    DataHubSecretKeyClass,
+    DataHubPolicyKeyClass,
+    TestKeyClass,
+    DataHubAccessTokenKeyClass,
+    DataHubUpgradeKeyClass,
+    DataPlatformKeyClass,
+    InviteTokenKeyClass,
+    PostKeyClass,
+    ExecutionRequestKeyClass,
+    DataHubIngestionSourceKeyClass,
+    DomainKeyClass,
+    DatasetKeyClass,
+    DataHubStepStateKeyClass,
+    GlobalSettingsKeyClass,
+    TagKeyClass,
+    GlobalSettingsInfoClass,
+    TestResultsClass,
+    TestInfoClass,
+    DataPlatformInstancePropertiesClass,
+    DataHubViewInfoClass,
+    DataHubSecretValueClass
 ]
 
 KEY_ASPECTS: Dict[str, Type[_Aspect]] = {
-    'dashboard': DashboardKeyClass,
     'mlFeatureTable': MLFeatureTableKeyClass,
-    'dataHubAccessToken': DataHubAccessTokenKeyClass,
-    'dataHubIngestionSource': DataHubIngestionSourceKeyClass,
-    'assertion': AssertionKeyClass,
-    'mlPrimaryKey': MLPrimaryKeyKeyClass,
-    'dataHubUpgrade': DataHubUpgradeKeyClass,
-    'telemetry': TelemetryKeyClass,
-    'dataHubSecret': DataHubSecretKeyClass,
+    'container': ContainerKeyClass,
     'dataHubRetention': DataHubRetentionKeyClass,
-    'dataHubRole': DataHubRoleKeyClass,
-    'dataPlatformInstance': DataPlatformInstanceKeyClass,
-    'dataHubStepState': DataHubStepStateKeyClass,
-    'dataProcessInstance': DataProcessInstanceKeyClass,
-    'post': PostKeyClass,
-    'globalSettings': GlobalSettingsKeyClass,
-    'chart': ChartKeyClass,
     'dataJob': DataJobKeyClass,
-    'domain': DomainKeyClass,
-    'mlModel': MLModelKeyClass,
-    'dataPlatform': DataPlatformKeyClass,
+    'schemaField': SchemaFieldKeyClass,
+    'corpGroup': CorpGroupKeyClass,
+    'mlPrimaryKey': MLPrimaryKeyKeyClass,
+    'dataFlow': DataFlowKeyClass,
+    'dashboard': DashboardKeyClass,
     'mlModelGroup': MLModelGroupKeyClass,
-    'mlFeature': MLFeatureKeyClass,
-    'glossaryNode': GlossaryNodeKeyClass,
+    'telemetry': TelemetryKeyClass,
     'glossaryTerm': GlossaryTermKeyClass,
-    'dataset': DatasetKeyClass,
+    'glossaryNode': GlossaryNodeKeyClass,
+    'dataProcessInstance': DataProcessInstanceKeyClass,
+    'dataHubRole': DataHubRoleKeyClass,
+    'mlModel': MLModelKeyClass,
     'dataHubView': DataHubViewKeyClass,
-    'dataHubExecutionRequest': ExecutionRequestKeyClass,
-    'test': TestKeyClass,
-    'dataFlow': DataFlowKeyClass,
+    'query': QueryKeyClass,
+    'corpuser': CorpUserKeyClass,
+    'dataPlatformInstance': DataPlatformInstanceKeyClass,
+    'notebook': NotebookKeyClass,
+    'mlFeature': MLFeatureKeyClass,
+    'chart': ChartKeyClass,
+    'assertion': AssertionKeyClass,
+    'dataHubSecret': DataHubSecretKeyClass,
     'dataHubPolicy': DataHubPolicyKeyClass,
-    'container': ContainerKeyClass,
-    'corpGroup': CorpGroupKeyClass,
+    'test': TestKeyClass,
+    'dataHubAccessToken': DataHubAccessTokenKeyClass,
+    'dataHubUpgrade': DataHubUpgradeKeyClass,
+    'dataPlatform': DataPlatformKeyClass,
     'inviteToken': InviteTokenKeyClass,
-    'notebook': NotebookKeyClass,
-    'schemaField': SchemaFieldKeyClass,
-    'tag': TagKeyClass,
-    'corpuser': CorpUserKeyClass
+    'post': PostKeyClass,
+    'dataHubExecutionRequest': ExecutionRequestKeyClass,
+    'dataHubIngestionSource': DataHubIngestionSourceKeyClass,
+    'domain': DomainKeyClass,
+    'dataset': DatasetKeyClass,
+    'dataHubStepState': DataHubStepStateKeyClass,
+    'globalSettings': GlobalSettingsKeyClass,
+    'tag': TagKeyClass
 }
 
 # fmt: on
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/DashboardInfo.avsc` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/schemas/MetadataChangeProposal.avsc`

 * *Files 27% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.5299727182539683%*

 * *Differences: {"'doc'": "'Kafka event for proposing a metadata change for an entity. A corresponding "*

 * *          'MetadataChangeLog is emitted when the change is accepted and committed, otherwise a '*

 * *          "FailedMetadataChangeProposal will be emitted instead.'",*

 * * "'fields'": "{0: {'name': 'auditHeader', 'type': {insert: [(1, OrderedDict([('type', 'record'), "*

 * *             "('name', 'KafkaAuditHeader'), ('namespace', 'com.linkedin.events'), ('fields', "*

 * *             "[OrderedDict([('compliance', [OrderedDict([('polic []*

```diff
@@ -1,324 +1,249 @@
 {
-    "Aspect": {
-        "name": "dashboardInfo"
-    },
-    "doc": "Information about a dashboard",
+    "doc": "Kafka event for proposing a metadata change for an entity. A corresponding MetadataChangeLog is emitted when the change is accepted and committed, otherwise a FailedMetadataChangeProposal will be emitted instead.",
     "fields": [
         {
-            "Searchable": {
-                "/*": {
-                    "queryByDefault": true
-                }
-            },
-            "default": {},
-            "doc": "Custom property bag.",
-            "name": "customProperties",
-            "type": {
-                "type": "map",
-                "values": "string"
-            }
-        },
-        {
             "default": null,
-            "doc": "URL where the reference exist",
-            "java": {
-                "class": "com.linkedin.pegasus2avro.common.url.Url",
-                "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-            },
-            "name": "externalUrl",
+            "doc": "Kafka audit header. Currently remains unused in the open source.",
+            "name": "auditHeader",
             "type": [
                 "null",
-                "string"
+                {
+                    "doc": "This header records information about the context of an event as it is emitted into kafka and is intended to be used by the kafka audit application.  For more information see go/kafkaauditheader",
+                    "fields": [
+                        {
+                            "compliance": [
+                                {
+                                    "policy": "EVENT_TIME"
+                                }
+                            ],
+                            "doc": "The time at which the event was emitted into kafka.",
+                            "name": "time",
+                            "type": "long"
+                        },
+                        {
+                            "compliance": "NONE",
+                            "doc": "The fully qualified name of the host from which the event is being emitted.",
+                            "name": "server",
+                            "type": "string"
+                        },
+                        {
+                            "compliance": "NONE",
+                            "default": null,
+                            "doc": "The instance on the server from which the event is being emitted. e.g. i001",
+                            "name": "instance",
+                            "type": [
+                                "null",
+                                "string"
+                            ]
+                        },
+                        {
+                            "compliance": "NONE",
+                            "doc": "The name of the application from which the event is being emitted. see go/appname",
+                            "name": "appName",
+                            "type": "string"
+                        },
+                        {
+                            "compliance": "NONE",
+                            "doc": "A unique identifier for the message",
+                            "name": "messageId",
+                            "type": {
+                                "name": "UUID",
+                                "namespace": "com.linkedin.events",
+                                "size": 16,
+                                "type": "fixed"
+                            }
+                        },
+                        {
+                            "compliance": "NONE",
+                            "default": null,
+                            "doc": "The version that is being used for auditing. In version 0, the audit trail buckets events into 10 minute audit windows based on the EventHeader timestamp. In version 1, the audit trail buckets events as follows: if the schema has an outer KafkaAuditHeader, use the outer audit header timestamp for bucketing; else if the EventHeader has an inner KafkaAuditHeader use that inner audit header's timestamp for bucketing",
+                            "name": "auditVersion",
+                            "type": [
+                                "null",
+                                "int"
+                            ]
+                        },
+                        {
+                            "compliance": "NONE",
+                            "default": null,
+                            "doc": "The fabricUrn of the host from which the event is being emitted. Fabric Urn in the format of urn:li:fabric:{fabric_name}. See go/fabric.",
+                            "name": "fabricUrn",
+                            "type": [
+                                "null",
+                                "string"
+                            ]
+                        },
+                        {
+                            "compliance": "NONE",
+                            "default": null,
+                            "doc": "This is a String that the client uses to establish some kind of connection with the Kafka cluster. The exact format of it depends on specific versions of clients and brokers. This information could potentially identify the fabric and cluster with which the client is producing to or consuming from.",
+                            "name": "clusterConnectionString",
+                            "type": [
+                                "null",
+                                "string"
+                            ]
+                        }
+                    ],
+                    "name": "KafkaAuditHeader",
+                    "namespace": "com.linkedin.events",
+                    "type": "record"
+                }
             ]
         },
         {
-            "Searchable": {
-                "boostScore": 10.0,
-                "enableAutocomplete": true,
-                "fieldType": "TEXT_PARTIAL"
-            },
-            "doc": "Title of the dashboard",
-            "name": "title",
+            "doc": "Type of the entity being written to",
+            "name": "entityType",
             "type": "string"
         },
         {
-            "Searchable": {
-                "fieldType": "TEXT",
-                "hasValuesFieldName": "hasDescription"
-            },
-            "doc": "Detailed description about the dashboard",
-            "name": "description",
-            "type": "string"
-        },
-        {
-            "Relationship": {
-                "/*": {
-                    "entityTypes": [
-                        "chart"
-                    ],
-                    "isLineage": true,
-                    "name": "Contains"
-                }
-            },
-            "default": [],
-            "deprecated": true,
-            "doc": "Charts in a dashboard\nDeprecated! Use chartEdges instead.",
-            "name": "charts",
-            "type": {
-                "items": "string",
-                "type": "array"
-            }
-        },
-        {
-            "Relationship": {
-                "/*/destinationUrn": {
-                    "createdActor": "chartEdges/*/created/actor",
-                    "createdOn": "chartEdges/*/created/time",
-                    "entityTypes": [
-                        "chart"
-                    ],
-                    "isLineage": true,
-                    "name": "Contains",
-                    "properties": "chartEdges/*/properties",
-                    "updatedActor": "chartEdges/*/lastModified/actor",
-                    "updatedOn": "chartEdges/*/lastModified/time"
-                }
-            },
+            "Urn": "Urn",
             "default": null,
-            "doc": "Charts in a dashboard",
-            "name": "chartEdges",
+            "doc": "Urn of the entity being written",
+            "java": {
+                "class": "com.linkedin.pegasus2avro.common.urn.Urn"
+            },
+            "name": "entityUrn",
             "type": [
                 "null",
-                {
-                    "items": {
-                        "doc": "Information about a relatonship edge.",
-                        "fields": [
-                            {
-                                "doc": "Urn of the source of this relationship edge.",
-                                "java": {
-                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                },
-                                "name": "sourceUrn",
-                                "type": "string"
-                            },
-                            {
-                                "doc": "Urn of the destination of this relationship edge.",
-                                "java": {
-                                    "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                },
-                                "name": "destinationUrn",
-                                "type": "string"
-                            },
-                            {
-                                "doc": "Audit stamp containing who created this relationship edge and when",
-                                "name": "created",
-                                "type": {
-                                    "doc": "Data captured on a resource/association/sub-resource level giving insight into when that resource/association/sub-resource moved into a particular lifecycle stage, and who acted to move it into that specific lifecycle stage.",
-                                    "fields": [
-                                        {
-                                            "doc": "When did the resource/association/sub-resource move into the specific lifecycle stage represented by this AuditEvent.",
-                                            "name": "time",
-                                            "type": "long"
-                                        },
-                                        {
-                                            "doc": "The entity (e.g. a member URN) which will be credited for moving the resource/association/sub-resource into the specific lifecycle stage. It is also the one used to authorize the change.",
-                                            "java": {
-                                                "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                            },
-                                            "name": "actor",
-                                            "type": "string"
-                                        },
-                                        {
-                                            "default": null,
-                                            "doc": "The entity (e.g. a service URN) which performs the change on behalf of the Actor and must be authorized to act as the Actor.",
-                                            "java": {
-                                                "class": "com.linkedin.pegasus2avro.common.urn.Urn"
-                                            },
-                                            "name": "impersonator",
-                                            "type": [
-                                                "null",
-                                                "string"
-                                            ]
-                                        },
-                                        {
-                                            "default": null,
-                                            "doc": "Additional context around how DataHub was informed of the particular change. For example: was the change created by an automated process, or manually.",
-                                            "name": "message",
-                                            "type": [
-                                                "null",
-                                                "string"
-                                            ]
-                                        }
-                                    ],
-                                    "name": "AuditStamp",
-                                    "type": "record"
-                                }
-                            },
-                            {
-                                "doc": "Audit stamp containing who last modified this relationship edge and when",
-                                "name": "lastModified",
-                                "type": "AuditStamp"
-                            },
-                            {
-                                "default": null,
-                                "doc": "A generic properties bag that allows us to store specific information on this graph edge.",
-                                "name": "properties",
-                                "type": [
-                                    "null",
-                                    {
-                                        "type": "map",
-                                        "values": "string"
-                                    }
-                                ]
-                            }
-                        ],
-                        "name": "Edge",
-                        "namespace": "com.linkedin.pegasus2avro.common",
-                        "type": "record"
-                    },
-                    "type": "array"
-                }
+                "string"
             ]
         },
         {
-            "Relationship": {
-                "/*": {
-                    "entityTypes": [
-                        "dataset"
-                    ],
-                    "isLineage": true,
-                    "name": "Consumes"
-                }
-            },
-            "default": [],
-            "deprecated": true,
-            "doc": "Datasets consumed by a dashboard\nDeprecated! Use datasetEdges instead.",
-            "name": "datasets",
-            "type": {
-                "items": "string",
-                "type": "array"
-            }
-        },
-        {
-            "Relationship": {
-                "/*/destinationUrn": {
-                    "createdActor": "datasetEdges/*/created/actor",
-                    "createdOn": "datasetEdges/*/created/time",
-                    "entityTypes": [
-                        "dataset"
-                    ],
-                    "isLineage": true,
-                    "name": "Consumes",
-                    "properties": "datasetEdges/*/properties",
-                    "updatedActor": "datasetEdges/*/lastModified/actor",
-                    "updatedOn": "datasetEdges/*/lastModified/time"
-                }
-            },
             "default": null,
-            "doc": "Datasets consumed by a dashboard",
-            "name": "datasetEdges",
+            "doc": "Key aspect of the entity being written",
+            "name": "entityKeyAspect",
             "type": [
                 "null",
                 {
-                    "items": "com.linkedin.pegasus2avro.common.Edge",
-                    "type": "array"
+                    "doc": "Generic record structure for serializing an Aspect",
+                    "fields": [
+                        {
+                            "doc": "The value of the aspect, serialized as bytes.",
+                            "name": "value",
+                            "type": "bytes"
+                        },
+                        {
+                            "doc": "The content type, which represents the fashion in which the aspect was serialized.\nThe only type currently supported is application/json.",
+                            "name": "contentType",
+                            "type": "string"
+                        }
+                    ],
+                    "name": "GenericAspect",
+                    "namespace": "com.linkedin.pegasus2avro.mxe",
+                    "type": "record"
                 }
             ]
         },
         {
-            "doc": "Captures information about who created/last modified/deleted this dashboard and when",
-            "name": "lastModified",
+            "doc": "Type of change being proposed",
+            "name": "changeType",
             "type": {
-                "doc": "Data captured on a resource/association/sub-resource level giving insight into when that resource/association/sub-resource moved into various lifecycle stages, and who acted to move it into those lifecycle stages. The recommended best practice is to include this record in your record schema, and annotate its fields as @readOnly in your resource. See https://github.com/linkedin/rest.li/wiki/Validation-in-Rest.li#restli-validation-annotations",
-                "fields": [
-                    {
-                        "default": {
-                            "actor": "urn:li:corpuser:unknown",
-                            "impersonator": null,
-                            "message": null,
-                            "time": 0
-                        },
-                        "doc": "An AuditStamp corresponding to the creation of this resource/association/sub-resource. A value of 0 for time indicates missing data.",
-                        "name": "created",
-                        "type": "AuditStamp"
-                    },
-                    {
-                        "default": {
-                            "actor": "urn:li:corpuser:unknown",
-                            "impersonator": null,
-                            "message": null,
-                            "time": 0
-                        },
-                        "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
-                        "name": "lastModified",
-                        "type": "AuditStamp"
-                    },
-                    {
-                        "default": null,
-                        "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
-                        "name": "deleted",
-                        "type": [
-                            "null",
-                            "AuditStamp"
-                        ]
-                    }
+                "doc": "Descriptor for a change action",
+                "name": "ChangeType",
+                "namespace": "com.linkedin.pegasus2avro.events.metadata",
+                "symbolDocs": {
+                    "CREATE": "NOT SUPPORTED YET\ninsert if not exists. otherwise fail",
+                    "DELETE": "NOT SUPPORTED YET\ndelete action",
+                    "PATCH": "NOT SUPPORTED YET\npatch the changes instead of full replace",
+                    "RESTATE": "Restate an aspect, eg. in a index refresh.",
+                    "UPDATE": "NOT SUPPORTED YET\nupdate if exists. otherwise fail",
+                    "UPSERT": "insert if not exists. otherwise update"
+                },
+                "symbols": [
+                    "UPSERT",
+                    "CREATE",
+                    "UPDATE",
+                    "DELETE",
+                    "PATCH",
+                    "RESTATE"
                 ],
-                "name": "ChangeAuditStamps",
-                "namespace": "com.linkedin.pegasus2avro.common",
-                "type": "record"
+                "type": "enum"
             }
         },
         {
             "default": null,
-            "doc": "URL for the dashboard. This could be used as an external link on DataHub to allow users access/view the dashboard",
-            "java": {
-                "class": "com.linkedin.pegasus2avro.common.url.Url",
-                "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
-            },
-            "name": "dashboardUrl",
+            "doc": "Aspect of the entity being written to\nNot filling this out implies that the writer wants to affect the entire entity\nNote: This is only valid for CREATE, UPSERT, and DELETE operations.",
+            "name": "aspectName",
             "type": [
                 "null",
                 "string"
             ]
         },
         {
-            "Searchable": {
-                "addToFilters": true,
-                "fieldType": "KEYWORD",
-                "filterNameOverride": "Access Level"
-            },
             "default": null,
-            "doc": "Access level for the dashboard",
-            "name": "access",
+            "doc": "The value of the new aspect.",
+            "name": "aspect",
             "type": [
                 "null",
-                {
-                    "doc": "The various access levels",
-                    "name": "AccessLevel",
-                    "namespace": "com.linkedin.pegasus2avro.common",
-                    "symbolDocs": {
-                        "PRIVATE": "Private availability to certain set of users",
-                        "PUBLIC": "Publicly available access level"
-                    },
-                    "symbols": [
-                        "PUBLIC",
-                        "PRIVATE"
-                    ],
-                    "type": "enum"
-                }
+                "com.linkedin.pegasus2avro.mxe.GenericAspect"
             ]
         },
         {
             "default": null,
-            "doc": "The time when this dashboard last refreshed",
-            "name": "lastRefreshed",
+            "doc": "A string->string map of custom properties that one might want to attach to an event",
+            "name": "systemMetadata",
             "type": [
                 "null",
-                "long"
+                {
+                    "doc": "Metadata associated with each metadata change that is processed by the system",
+                    "fields": [
+                        {
+                            "default": 0,
+                            "doc": "The timestamp the metadata was observed at",
+                            "name": "lastObserved",
+                            "type": [
+                                "long",
+                                "null"
+                            ]
+                        },
+                        {
+                            "default": "no-run-id-provided",
+                            "doc": "The run id that produced the metadata. Populated in case of batch-ingestion.",
+                            "name": "runId",
+                            "type": [
+                                "string",
+                                "null"
+                            ]
+                        },
+                        {
+                            "default": null,
+                            "doc": "The model registry name that was used to process this event",
+                            "name": "registryName",
+                            "type": [
+                                "null",
+                                "string"
+                            ]
+                        },
+                        {
+                            "default": null,
+                            "doc": "The model registry version that was used to process this event",
+                            "name": "registryVersion",
+                            "type": [
+                                "null",
+                                "string"
+                            ]
+                        },
+                        {
+                            "default": null,
+                            "doc": "Additional properties",
+                            "name": "properties",
+                            "type": [
+                                "null",
+                                {
+                                    "type": "map",
+                                    "values": "string"
+                                }
+                            ]
+                        }
+                    ],
+                    "name": "SystemMetadata",
+                    "namespace": "com.linkedin.pegasus2avro.mxe",
+                    "type": "record"
+                }
             ]
         }
     ],
-    "name": "DashboardInfo",
-    "namespace": "com.linkedin.pegasus2avro.dashboard",
+    "name": "MetadataChangeProposal",
+    "namespace": "com.linkedin.pegasus2avro.mxe",
     "type": "record"
 }
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/MetadataChangeEvent.avsc` & `acryl-datahub-tc-0.10.2rc1/src/datahub/metadata/schemas/MetadataChangeEvent.avsc`

 * *Files 6% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9996507356064491%*

 * *Differences: {"'fields'": "{0: {'type': {1: {'fields': {4: {'type': {'namespace': 'com.linkedin.events'}}}}}}, "*

 * *             "1: {'type': {0: {'fields': {0: {'Urn': 'ChartUrn'}, 1: {'type': {'items': {1: "*

 * *             "{'fields': {4: {'type': {'fields': {0: {'type': {'fields': {1: {'Urn': 'Urn'}, 2: "*

 * *             "{'Urn': 'Urn'}}, 'namespace': 'com.linkedin.pegasus2avro.common'}}, 1: {'type': "*

 * *             "'com.linkedin.pegasus2avro.common.AuditStamp'}, 2: {'type': {insert: [(1, "*

 * *             "'com.linkedin.pegasu []*

```diff
@@ -44,14 +44,15 @@
                         },
                         {
                             "compliance": "NONE",
                             "doc": "A unique identifier for the message",
                             "name": "messageId",
                             "type": {
                                 "name": "UUID",
+                                "namespace": "com.linkedin.events",
                                 "size": 16,
                                 "type": "fixed"
                             }
                         },
                         {
                             "compliance": "NONE",
                             "default": null,
@@ -97,14 +98,15 @@
                     "Entity": {
                         "keyAspect": "chartKey",
                         "name": "chart"
                     },
                     "doc": "A metadata snapshot for a specific Chart entity.",
                     "fields": [
                         {
+                            "Urn": "ChartUrn",
                             "doc": "URN for the entity the metadata snapshot is associated with.",
                             "java": {
                                 "class": "com.linkedin.pegasus2avro.common.urn.ChartUrn"
                             },
                             "name": "urn",
                             "type": "string"
                         },
@@ -207,22 +209,24 @@
                                                                 "fields": [
                                                                     {
                                                                         "doc": "When did the resource/association/sub-resource move into the specific lifecycle stage represented by this AuditEvent.",
                                                                         "name": "time",
                                                                         "type": "long"
                                                                     },
                                                                     {
+                                                                        "Urn": "Urn",
                                                                         "doc": "The entity (e.g. a member URN) which will be credited for moving the resource/association/sub-resource into the specific lifecycle stage. It is also the one used to authorize the change.",
                                                                         "java": {
                                                                             "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                                                         },
                                                                         "name": "actor",
                                                                         "type": "string"
                                                                     },
                                                                     {
+                                                                        "Urn": "Urn",
                                                                         "default": null,
                                                                         "doc": "The entity (e.g. a service URN) which performs the change on behalf of the Actor and must be authorized to act as the Actor.",
                                                                         "java": {
                                                                             "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                                                         },
                                                                         "name": "impersonator",
                                                                         "type": [
@@ -237,35 +241,36 @@
                                                                         "type": [
                                                                             "null",
                                                                             "string"
                                                                         ]
                                                                     }
                                                                 ],
                                                                 "name": "AuditStamp",
+                                                                "namespace": "com.linkedin.pegasus2avro.common",
                                                                 "type": "record"
                                                             }
                                                         },
                                                         {
                                                             "default": {
                                                                 "actor": "urn:li:corpuser:unknown",
                                                                 "impersonator": null,
                                                                 "message": null,
                                                                 "time": 0
                                                             },
                                                             "doc": "An AuditStamp corresponding to the last modification of this resource/association/sub-resource. If no modification has happened since creation, lastModified should be the same as created. A value of 0 for time indicates missing data.",
                                                             "name": "lastModified",
-                                                            "type": "AuditStamp"
+                                                            "type": "com.linkedin.pegasus2avro.common.AuditStamp"
                                                         },
                                                         {
                                                             "default": null,
                                                             "doc": "An AuditStamp corresponding to the deletion of this resource/association/sub-resource. Logically, deleted MUST have a later timestamp than creation. It may or may not have the same time as lastModified depending upon the resource/association/sub-resource semantics.",
                                                             "name": "deleted",
                                                             "type": [
                                                                 "null",
-                                                                "AuditStamp"
+                                                                "com.linkedin.pegasus2avro.common.AuditStamp"
                                                             ]
                                                         }
                                                     ],
                                                     "name": "ChangeAuditStamps",
                                                     "namespace": "com.linkedin.pegasus2avro.common",
                                                     "type": "record"
                                                 }
@@ -328,38 +333,40 @@
                                                 "type": [
                                                     "null",
                                                     {
                                                         "items": {
                                                             "doc": "Information about a relatonship edge.",
                                                             "fields": [
                                                                 {
+                                                                    "Urn": "Urn",
                                                                     "doc": "Urn of the source of this relationship edge.",
                                                                     "java": {
                                                                         "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                                                     },
                                                                     "name": "sourceUrn",
                                                                     "type": "string"
                                                                 },
                                                                 {
+                                                                    "Urn": "Urn",
                                                                     "doc": "Urn of the destination of this relationship edge.",
                                                                     "java": {
                                                                         "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                                                     },
                                                                     "name": "destinationUrn",
                                                                     "type": "string"
                                                                 },
                                                                 {
                                                                     "doc": "Audit stamp containing who created this relationship edge and when",
                                                                     "name": "created",
-                                                                    "type": "AuditStamp"
+                                                                    "type": "com.linkedin.pegasus2avro.common.AuditStamp"
                                                                 },
                                                                 {
                                                                     "doc": "Audit stamp containing who last modified this relationship edge and when",
                                                                     "name": "lastModified",
-                                                                    "type": "AuditStamp"
+                                                                    "type": "com.linkedin.pegasus2avro.common.AuditStamp"
                                                                 },
                                                                 {
                                                                     "default": null,
                                                                     "doc": "A generic properties bag that allows us to store specific information on this graph edge.",
                                                                     "name": "properties",
                                                                     "type": [
                                                                         "null",
@@ -388,14 +395,15 @@
                                                 "doc": "Type of the chart",
                                                 "name": "type",
                                                 "type": [
                                                     "null",
                                                     {
                                                         "doc": "The various types of charts",
                                                         "name": "ChartType",
+                                                        "namespace": "com.linkedin.pegasus2avro.chart",
                                                         "symbolDocs": {
                                                             "BAR": "Chart showing a Bar chart",
                                                             "PIE": "Chart showing a Pie chart",
                                                             "SCATTER": "Chart showing a Scatter plot",
                                                             "TABLE": "Chart showing a table",
                                                             "TEXT": "Chart showing Markdown formatted text"
                                                         },
@@ -475,14 +483,15 @@
                                                     "fieldType": "KEYWORD",
                                                     "filterNameOverride": "Query Type"
                                                 },
                                                 "doc": "Chart query type",
                                                 "name": "type",
                                                 "type": {
                                                     "name": "ChartQueryType",
+                                                    "namespace": "com.linkedin.pegasus2avro.chart",
                                                     "symbolDocs": {
                                                         "LOOKML": "LookML queries",
                                                         "SQL": "SQL type queries"
                                                     },
                                                     "symbols": [
                                                         "LOOKML",
                                                         "SQL"
@@ -575,14 +584,15 @@
                                                                     "addToFilters": true,
                                                                     "fieldName": "owners",
                                                                     "fieldType": "URN",
                                                                     "filterNameOverride": "Owned By",
                                                                     "hasValuesFieldName": "hasOwners",
                                                                     "queryByDefault": false
                                                                 },
+                                                                "Urn": "Urn",
                                                                 "doc": "Owner URN, e.g. urn:li:corpuser:ldap, urn:li:corpGroup:group_name, and urn:li:multiProduct:mp_name\n(Caveat: only corpuser is currently supported in the frontend.)",
                                                                 "java": {
                                                                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                                                 },
                                                                 "name": "owner",
                                                                 "type": "string"
                                                             },
@@ -596,14 +606,15 @@
                                                                         "DELEGATE": true,
                                                                         "DEVELOPER": true,
                                                                         "PRODUCER": true,
                                                                         "STAKEHOLDER": true
                                                                     },
                                                                     "doc": "Asset owner types",
                                                                     "name": "OwnershipType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.common",
                                                                     "symbolDocs": {
                                                                         "BUSINESS_OWNER": "A person or group who is responsible for logical, or business related, aspects of the asset.",
                                                                         "CONSUMER": "A person, group, or service that consumes the data\nDeprecated! Use TECHNICAL_OWNER or BUSINESS_OWNER instead.",
                                                                         "DATAOWNER": "A person or group that is owning the data\nDeprecated! Use TECHNICAL_OWNER instead.",
                                                                         "DATA_STEWARD": "A steward, expert, or delegate responsible for the asset.",
                                                                         "DELEGATE": "A person or a group that overseas the operation, e.g. a DBA or SRE.\nDeprecated! Use TECHNICAL_OWNER instead.",
                                                                         "DEVELOPER": "A person or group that is in charge of developing the code\nDeprecated! Use TECHNICAL_OWNER instead.",
@@ -637,14 +648,15 @@
                                                                         "doc": "Source/provider of the ownership information",
                                                                         "fields": [
                                                                             {
                                                                                 "doc": "The type of the source",
                                                                                 "name": "type",
                                                                                 "type": {
                                                                                     "name": "OwnershipSourceType",
+                                                                                    "namespace": "com.linkedin.pegasus2avro.common",
                                                                                     "symbolDocs": {
                                                                                         "AUDIT": "Auditing system or audit logs",
                                                                                         "DATABASE": "Database, e.g. GRANTS table",
                                                                                         "FILE_SYSTEM": "File system, e.g. file/directory owner",
                                                                                         "ISSUE_TRACKING_SYSTEM": "Issue tracking system, e.g. Jira",
                                                                                         "MANUAL": "Manually provided by a user",
                                                                                         "OTHER": "Other sources",
@@ -671,35 +683,37 @@
                                                                                 "type": [
                                                                                     "null",
                                                                                     "string"
                                                                                 ]
                                                                             }
                                                                         ],
                                                                         "name": "OwnershipSource",
+                                                                        "namespace": "com.linkedin.pegasus2avro.common",
                                                                         "type": "record"
                                                                     }
                                                                 ]
                                                             }
                                                         ],
                                                         "name": "Owner",
+                                                        "namespace": "com.linkedin.pegasus2avro.common",
                                                         "type": "record"
                                                     },
                                                     "type": "array"
                                                 }
                                             },
                                             {
                                                 "default": {
                                                     "actor": "urn:li:corpuser:unknown",
                                                     "impersonator": null,
                                                     "message": null,
                                                     "time": 0
                                                 },
                                                 "doc": "Audit stamp containing who last modified the record and when. A value of 0 in the time field indicates missing data.",
                                                 "name": "lastModified",
-                                                "type": "AuditStamp"
+                                                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
                                             }
                                         ],
                                         "name": "Ownership",
                                         "namespace": "com.linkedin.pegasus2avro.common",
                                         "type": "record"
                                     },
                                     {
@@ -725,43 +739,41 @@
                                     {
                                         "Aspect": {
                                             "name": "globalTags"
                                         },
                                         "doc": "Tag aspect used for applying tags to an entity",
                                         "fields": [
                                             {
+                                                "Relationship": {
+                                                    "/*/tag": {
+                                                        "entityTypes": [
+                                                            "tag"
+                                                        ],
+                                                        "name": "TaggedWith"
+                                                    }
+                                                },
                                                 "Searchable": {
                                                     "/*/tag": {
                                                         "addToFilters": true,
                                                         "boostScore": 0.5,
                                                         "fieldName": "tags",
                                                         "fieldType": "URN",
+                                                        "filterNameOverride": "Tag",
+                                                        "hasValuesFieldName": "hasTags",
                                                         "queryByDefault": true
                                                     }
                                                 },
                                                 "doc": "Tags associated with a given entity",
                                                 "name": "tags",
                                                 "type": {
                                                     "items": {
                                                         "doc": "Properties of an applied tag. For now, just an Urn. In the future we can extend this with other properties, e.g.\npropagation parameters.",
                                                         "fields": [
                                                             {
-                                                                "Relationship": {
-                                                                    "entityTypes": [
-                                                                        "tag"
-                                                                    ],
-                                                                    "name": "TaggedWith"
-                                                                },
-                                                                "Searchable": {
-                                                                    "addToFilters": true,
-                                                                    "fieldName": "tags",
-                                                                    "fieldType": "URN",
-                                                                    "filterNameOverride": "Tag",
-                                                                    "hasValuesFieldName": "hasTags"
-                                                                },
+                                                                "Urn": "TagUrn",
                                                                 "doc": "Urn of the applied tag",
                                                                 "java": {
                                                                     "class": "com.linkedin.pegasus2avro.common.urn.TagUrn"
                                                                 },
                                                                 "name": "tag",
                                                                 "type": "string"
                                                             },
@@ -772,14 +784,15 @@
                                                                 "type": [
                                                                     "null",
                                                                     "string"
                                                                 ]
                                                             }
                                                         ],
                                                         "name": "TagAssociation",
+                                                        "namespace": "com.linkedin.pegasus2avro.common",
                                                         "type": "record"
                                                     },
                                                     "type": "array"
                                                 }
                                             }
                                         ],
                                         "name": "GlobalTags",
@@ -834,14 +847,15 @@
                                                                 "Searchable": {
                                                                     "addToFilters": true,
                                                                     "fieldName": "glossaryTerms",
                                                                     "fieldType": "URN",
                                                                     "filterNameOverride": "Glossary Term",
                                                                     "hasValuesFieldName": "hasGlossaryTerms"
                                                                 },
+                                                                "Urn": "GlossaryTermUrn",
                                                                 "doc": "Urn of the applied glossary term",
                                                                 "java": {
                                                                     "class": "com.linkedin.pegasus2avro.common.urn.GlossaryTermUrn"
                                                                 },
                                                                 "name": "urn",
                                                                 "type": "string"
                                                             },
@@ -852,23 +866,24 @@
                                                                 "type": [
                                                                     "null",
                                                                     "string"
                                                                 ]
                                                             }
                                                         ],
                                                         "name": "GlossaryTermAssociation",
+                                                        "namespace": "com.linkedin.pegasus2avro.common",
                                                         "type": "record"
                                                     },
                                                     "type": "array"
                                                 }
                                             },
                                             {
                                                 "doc": "Audit stamp containing who reported the related business term",
                                                 "name": "auditStamp",
-                                                "type": "AuditStamp"
+                                                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
                                             }
                                         ],
                                         "name": "GlossaryTerms",
                                         "namespace": "com.linkedin.pegasus2avro.common",
                                         "type": "record"
                                     },
                                     {
@@ -897,18 +912,19 @@
                                                                 "doc": "Description of the link.",
                                                                 "name": "description",
                                                                 "type": "string"
                                                             },
                                                             {
                                                                 "doc": "Audit stamp associated with creation of this record",
                                                                 "name": "createStamp",
-                                                                "type": "AuditStamp"
+                                                                "type": "com.linkedin.pegasus2avro.common.AuditStamp"
                                                             }
                                                         ],
                                                         "name": "InstitutionalMemoryMetadata",
+                                                        "namespace": "com.linkedin.pegasus2avro.common",
                                                         "type": "record"
                                                     },
                                                     "type": "array"
                                                 }
                                             }
                                         ],
                                         "name": "InstitutionalMemory",
@@ -923,28 +939,30 @@
                                         "fields": [
                                             {
                                                 "Searchable": {
                                                     "addToFilters": true,
                                                     "fieldType": "URN",
                                                     "filterNameOverride": "Platform"
                                                 },
+                                                "Urn": "Urn",
                                                 "doc": "Data Platform",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                                 },
                                                 "name": "platform",
                                                 "type": "string"
                                             },
                                             {
                                                 "Searchable": {
                                                     "addToFilters": true,
                                                     "fieldName": "platformInstance",
                                                     "fieldType": "URN",
                                                     "filterNameOverride": "Platform Instance"
                                                 },
+                                                "Urn": "Urn",
                                                 "default": null,
                                                 "doc": "Instance of the data platform (e.g. db instance)",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                                 },
                                                 "name": "instance",
                                                 "type": [
@@ -970,14 +988,15 @@
                     "Entity": {
                         "keyAspect": "corpGroupKey",
                         "name": "corpGroup"
                     },
                     "doc": "A metadata snapshot for a specific CorpGroup entity.",
                     "fields": [
                         {
+                            "Urn": "CorpGroupUrn",
                             "doc": "URN for the entity the metadata snapshot is associated with.",
                             "java": {
                                 "class": "com.linkedin.pegasus2avro.common.urn.CorpGroupUrn"
                             },
                             "name": "urn",
                             "type": "string"
                         },
@@ -990,15 +1009,18 @@
                                         "Aspect": {
                                             "name": "corpGroupKey"
                                         },
                                         "doc": "Key for a CorpGroup",
                                         "fields": [
                                             {
                                                 "Searchable": {
-                                                    "fieldType": "TEXT_PARTIAL"
+                                                    "boostScore": 10.0,
+                                                    "enableAutocomplete": true,
+                                                    "fieldType": "TEXT_PARTIAL",
+                                                    "queryByDefault": true
                                                 },
                                                 "doc": "The URL-encoded name of the AD/LDAP group. Serves as a globally unique identifier within DataHub.",
                                                 "name": "name",
                                                 "type": "string"
                                             }
                                         ],
                                         "name": "CorpGroupKey",
@@ -1013,14 +1035,15 @@
                                             "name": "corpGroupInfo"
                                         },
                                         "doc": "Information about a Corp Group ingested from a third party source",
                                         "fields": [
                                             {
                                                 "Searchable": {
                                                     "boostScore": 10.0,
+                                                    "enableAutocomplete": true,
                                                     "fieldType": "TEXT_PARTIAL",
                                                     "queryByDefault": true
                                                 },
                                                 "default": null,
                                                 "doc": "The name of the group.",
                                                 "name": "displayName",
                                                 "type": [
@@ -1042,55 +1065,61 @@
                                                     "/*": {
                                                         "entityTypes": [
                                                             "corpuser"
                                                         ],
                                                         "name": "OwnedBy"
                                                     }
                                                 },
+                                                "Urn": "CorpuserUrn",
                                                 "deprecated": true,
                                                 "doc": "owners of this group\nDeprecated! Replaced by Ownership aspect.",
                                                 "name": "admins",
                                                 "type": {
                                                     "items": "string",
                                                     "type": "array"
-                                                }
+                                                },
+                                                "urn_is_array": true
                                             },
                                             {
                                                 "Relationship": {
                                                     "/*": {
                                                         "entityTypes": [
                                                             "corpuser"
                                                         ],
                                                         "name": "IsPartOf"
                                                     }
                                                 },
+                                                "Urn": "CorpuserUrn",
                                                 "deprecated": true,
                                                 "doc": "List of ldap urn in this group.\nDeprecated! Replaced by GroupMembership aspect.",
                                                 "name": "members",
                                                 "type": {
                                                     "items": "string",
                                                     "type": "array"
-                                                }
+                                                },
+                                                "urn_is_array": true
                                             },
                                             {
                                                 "Relationship": {
                                                     "/*": {
                                                         "entityTypes": [
                                                             "corpGroup"
                                                         ],
                                                         "name": "IsPartOf"
                                                     }
                                                 },
+                                                "Urn": "CorpGroupUrn",
                                                 "deprecated": true,
                                                 "doc": "List of groups in this group.\nDeprecated! This field is unused.",
                                                 "name": "groups",
                                                 "type": {
                                                     "items": "string",
                                                     "type": "array"
-                                                }
+                                                },
+                                                "urn_is_array": true
                                             },
                                             {
                                                 "Searchable": {
                                                     "fieldType": "TEXT_PARTIAL"
                                                 },
                                                 "default": null,
                                                 "doc": "A description of the group.",
@@ -1144,14 +1173,15 @@
                     "Entity": {
                         "keyAspect": "corpUserKey",
                         "name": "corpuser"
                     },
                     "doc": "A metadata snapshot for a specific CorpUser entity.",
                     "fields": [
                         {
+                            "Urn": "CorpuserUrn",
                             "doc": "URN for the entity the metadata snapshot is associated with.",
                             "java": {
                                 "class": "com.linkedin.pegasus2avro.common.urn.CorpuserUrn"
                             },
                             "name": "urn",
                             "type": "string"
                         },
@@ -1215,14 +1245,15 @@
                                                 "doc": "Deprecated! Use CorpUserStatus instead. Whether the corpUser is active, ref: https://iwww.corp.linkedin.com/wiki/cf/display/GTSD/Accessing+Active+Directory+via+LDAP+tools",
                                                 "name": "active",
                                                 "type": "boolean"
                                             },
                                             {
                                                 "Searchable": {
                                                     "boostScore": 10.0,
+                                                    "enableAutocomplete": true,
                                                     "fieldType": "TEXT_PARTIAL",
                                                     "queryByDefault": true
                                                 },
                                                 "default": null,
                                                 "doc": "displayName of this user ,  e.g.  Hang Zhang(DataHQ)",
                                                 "name": "displayName",
                                                 "type": [
@@ -1264,14 +1295,15 @@
                                                     "name": "ReportsTo"
                                                 },
                                                 "Searchable": {
                                                     "fieldName": "managerLdap",
                                                     "fieldType": "URN",
                                                     "queryByDefault": true
                                                 },
+                                                "Urn": "CorpuserUrn",
                                                 "default": null,
                                                 "doc": "direct manager of this user",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.urn.CorpuserUrn"
                                                 },
                                                 "name": "managerUrn",
                                                 "type": [
@@ -1314,14 +1346,15 @@
                                                     "null",
                                                     "string"
                                                 ]
                                             },
                                             {
                                                 "Searchable": {
                                                     "boostScore": 10.0,
+                                                    "enableAutocomplete": true,
                                                     "fieldType": "TEXT_PARTIAL",
                                                     "queryByDefault": true
                                                 },
                                                 "default": null,
                                                 "doc": "Common name of this user, format is firstName + lastName (split by a whitespace)",
                                                 "name": "fullName",
                                                 "type": [
@@ -1489,19 +1522,21 @@
                                                     "/*": {
                                                         "entityTypes": [
                                                             "corpGroup"
                                                         ],
                                                         "name": "IsMemberOfGroup"
                                                     }
                                                 },
+                                                "Urn": "Urn",
                                                 "name": "groups",
                                                 "type": {
                                                     "items": "string",
                                                     "type": "array"
-                                                }
+                                                },
+                                                "urn_is_array": true
                                             }
                                         ],
                                         "name": "GroupMembership",
                                         "namespace": "com.linkedin.pegasus2avro.identity",
                                         "type": "record"
                                     },
                                     "com.linkedin.pegasus2avro.common.GlobalTags",
@@ -1519,14 +1554,15 @@
                     "Entity": {
                         "keyAspect": "dashboardKey",
                         "name": "dashboard"
                     },
                     "doc": "A metadata snapshot for a specific Dashboard entity.",
                     "fields": [
                         {
+                            "Urn": "DashboardUrn",
                             "doc": "URN for the entity the metadata snapshot is associated with.",
                             "java": {
                                 "class": "com.linkedin.pegasus2avro.common.urn.DashboardUrn"
                             },
                             "name": "urn",
                             "type": "string"
                         },
@@ -1619,22 +1655,24 @@
                                                         "entityTypes": [
                                                             "chart"
                                                         ],
                                                         "isLineage": true,
                                                         "name": "Contains"
                                                     }
                                                 },
+                                                "Urn": "ChartUrn",
                                                 "default": [],
                                                 "deprecated": true,
                                                 "doc": "Charts in a dashboard\nDeprecated! Use chartEdges instead.",
                                                 "name": "charts",
                                                 "type": {
                                                     "items": "string",
                                                     "type": "array"
-                                                }
+                                                },
+                                                "urn_is_array": true
                                             },
                                             {
                                                 "Relationship": {
                                                     "/*/destinationUrn": {
                                                         "createdActor": "chartEdges/*/created/actor",
                                                         "createdOn": "chartEdges/*/created/time",
                                                         "entityTypes": [
@@ -1664,22 +1702,24 @@
                                                         "entityTypes": [
                                                             "dataset"
                                                         ],
                                                         "isLineage": true,
                                                         "name": "Consumes"
                                                     }
                                                 },
+                                                "Urn": "Urn",
                                                 "default": [],
                                                 "deprecated": true,
                                                 "doc": "Datasets consumed by a dashboard\nDeprecated! Use datasetEdges instead.",
                                                 "name": "datasets",
                                                 "type": {
                                                     "items": "string",
                                                     "type": "array"
-                                                }
+                                                },
+                                                "urn_is_array": true
                                             },
                                             {
                                                 "Relationship": {
                                                     "/*/destinationUrn": {
                                                         "createdActor": "datasetEdges/*/created/actor",
                                                         "createdOn": "datasetEdges/*/created/time",
                                                         "entityTypes": [
@@ -1824,14 +1864,15 @@
                     "Entity": {
                         "keyAspect": "dataFlowKey",
                         "name": "dataFlow"
                     },
                     "doc": "A metadata snapshot for a specific DataFlow entity.",
                     "fields": [
                         {
+                            "Urn": "DataFlowUrn",
                             "doc": "URN for the entity the metadata snapshot is associated with.",
                             "java": {
                                 "class": "com.linkedin.pegasus2avro.common.urn.DataFlowUrn"
                             },
                             "name": "urn",
                             "type": "string"
                         },
@@ -1962,14 +2003,15 @@
                                                         "fields": [
                                                             {
                                                                 "doc": "When did the event occur",
                                                                 "name": "time",
                                                                 "type": "long"
                                                             },
                                                             {
+                                                                "Urn": "Urn",
                                                                 "default": null,
                                                                 "doc": "Optional: The actor urn involved in the event.",
                                                                 "java": {
                                                                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                                                 },
                                                                 "name": "actor",
                                                                 "type": [
@@ -2079,14 +2121,15 @@
                     "Entity": {
                         "keyAspect": "dataJobKey",
                         "name": "dataJob"
                     },
                     "doc": "A metadata snapshot for a specific DataJob entity.",
                     "fields": [
                         {
+                            "Urn": "DataJobUrn",
                             "doc": "URN for the entity the metadata snapshot is associated with.",
                             "java": {
                                 "class": "com.linkedin.pegasus2avro.common.urn.DataJobUrn"
                             },
                             "name": "urn",
                             "type": "string"
                         },
@@ -2109,14 +2152,15 @@
                                                     "name": "IsPartOf"
                                                 },
                                                 "Searchable": {
                                                     "fieldName": "dataFlow",
                                                     "fieldType": "URN_PARTIAL",
                                                     "queryByDefault": false
                                                 },
+                                                "Urn": "Urn",
                                                 "doc": "Standardized data processing flow urn representing the flow for the job",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                                 },
                                                 "name": "flow",
                                                 "type": "string"
                                             },
@@ -2218,14 +2262,15 @@
                                                         ],
                                                         "type": "enum"
                                                     },
                                                     "string"
                                                 ]
                                             },
                                             {
+                                                "Urn": "DataFlowUrn",
                                                 "default": null,
                                                 "doc": "DataFlow urn that this job is part of",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.urn.DataFlowUrn"
                                                 },
                                                 "name": "flowUrn",
                                                 "type": [
@@ -2269,14 +2314,15 @@
                                                 "doc": "Status of the job - Deprecated for Data Process Instance model.",
                                                 "name": "status",
                                                 "type": [
                                                     "null",
                                                     {
                                                         "doc": "Job statuses",
                                                         "name": "JobStatus",
+                                                        "namespace": "com.linkedin.pegasus2avro.datajob",
                                                         "symbolDocs": {
                                                             "COMPLETED": "Jobs with successful completion.",
                                                             "FAILED": "Jobs that have failed.",
                                                             "IN_PROGRESS": "Jobs currently running.",
                                                             "SKIPPED": "Jobs that have been skipped.",
                                                             "STARTING": "Jobs being initialized.",
                                                             "STOPPED": "Jobs that have stopped.",
@@ -2322,21 +2368,23 @@
                                                     "/*": {
                                                         "fieldName": "inputs",
                                                         "fieldType": "URN",
                                                         "numValuesFieldName": "numInputDatasets",
                                                         "queryByDefault": false
                                                     }
                                                 },
+                                                "Urn": "DatasetUrn",
                                                 "deprecated": true,
                                                 "doc": "Input datasets consumed by the data job during processing\nDeprecated! Use inputDatasetEdges instead.",
                                                 "name": "inputDatasets",
                                                 "type": {
                                                     "items": "string",
                                                     "type": "array"
-                                                }
+                                                },
+                                                "urn_is_array": true
                                             },
                                             {
                                                 "Relationship": {
                                                     "/*/destinationUrn": {
                                                         "createdActor": "inputDatasetEdges/*/created/actor",
                                                         "createdOn": "inputDatasetEdges/*/created/time",
                                                         "entityTypes": [
@@ -2383,21 +2431,23 @@
                                                     "/*": {
                                                         "fieldName": "outputs",
                                                         "fieldType": "URN",
                                                         "numValuesFieldName": "numOutputDatasets",
                                                         "queryByDefault": false
                                                     }
                                                 },
+                                                "Urn": "DatasetUrn",
                                                 "deprecated": true,
                                                 "doc": "Output datasets produced by the data job during processing\nDeprecated! Use outputDatasetEdges instead.",
                                                 "name": "outputDatasets",
                                                 "type": {
                                                     "items": "string",
                                                     "type": "array"
-                                                }
+                                                },
+                                                "urn_is_array": true
                                             },
                                             {
                                                 "Relationship": {
                                                     "/*/destinationUrn": {
                                                         "createdActor": "outputDatasetEdges/*/created/actor",
                                                         "createdOn": "outputDatasetEdges/*/created/time",
                                                         "entityTypes": [
@@ -2436,25 +2486,27 @@
                                                         "entityTypes": [
                                                             "dataJob"
                                                         ],
                                                         "isLineage": true,
                                                         "name": "DownstreamOf"
                                                     }
                                                 },
+                                                "Urn": "DataJobUrn",
                                                 "default": null,
                                                 "deprecated": true,
                                                 "doc": "Input datajobs that this data job depends on\nDeprecated! Use inputDatajobEdges instead.",
                                                 "name": "inputDatajobs",
                                                 "type": [
                                                     "null",
                                                     {
                                                         "items": "string",
                                                         "type": "array"
                                                     }
-                                                ]
+                                                ],
+                                                "urn_is_array": true
                                             },
                                             {
                                                 "Relationship": {
                                                     "/*/destinationUrn": {
                                                         "createdActor": "inputDatajobEdges/*/created/actor",
                                                         "createdOn": "inputDatajobEdges/*/created/time",
                                                         "entityTypes": [
@@ -2491,24 +2543,26 @@
                                                     "/*": {
                                                         "fieldName": "inputFields",
                                                         "fieldType": "URN",
                                                         "numValuesFieldName": "numInputFields",
                                                         "queryByDefault": false
                                                     }
                                                 },
+                                                "Urn": "Urn",
                                                 "default": null,
                                                 "doc": "Fields of the input datasets used by this job",
                                                 "name": "inputDatasetFields",
                                                 "type": [
                                                     "null",
                                                     {
                                                         "items": "string",
                                                         "type": "array"
                                                     }
-                                                ]
+                                                ],
+                                                "urn_is_array": true
                                             },
                                             {
                                                 "Relationship": {
                                                     "/*": {
                                                         "entityTypes": [
                                                             "schemaField"
                                                         ],
@@ -2519,24 +2573,26 @@
                                                     "/*": {
                                                         "fieldName": "outputFields",
                                                         "fieldType": "URN",
                                                         "numValuesFieldName": "numOutputFields",
                                                         "queryByDefault": false
                                                     }
                                                 },
+                                                "Urn": "Urn",
                                                 "default": null,
                                                 "doc": "Fields of the output datasets this job writes to",
                                                 "name": "outputDatasetFields",
                                                 "type": [
                                                     "null",
                                                     {
                                                         "items": "string",
                                                         "type": "array"
                                                     }
-                                                ]
+                                                ],
+                                                "urn_is_array": true
                                             },
                                             {
                                                 "default": null,
                                                 "doc": "Fine-grained column-level lineages",
                                                 "name": "fineGrainedLineages",
                                                 "type": [
                                                     "null",
@@ -2546,67 +2602,73 @@
                                                             "fields": [
                                                                 {
                                                                     "doc": "The type of upstream entity",
                                                                     "name": "upstreamType",
                                                                     "type": {
                                                                         "doc": "The type of upstream entity in a fine-grained lineage",
                                                                         "name": "FineGrainedLineageUpstreamType",
+                                                                        "namespace": "com.linkedin.pegasus2avro.dataset",
                                                                         "symbolDocs": {
                                                                             "DATASET": " Indicates that this lineage is originating from upstream dataset(s)",
                                                                             "FIELD_SET": " Indicates that this lineage is originating from upstream field(s)",
                                                                             "NONE": " Indicates that there is no upstream lineage i.e. the downstream field is not a derived field"
                                                                         },
                                                                         "symbols": [
                                                                             "FIELD_SET",
                                                                             "DATASET",
                                                                             "NONE"
                                                                         ],
                                                                         "type": "enum"
                                                                     }
                                                                 },
                                                                 {
+                                                                    "Urn": "Urn",
                                                                     "default": null,
                                                                     "doc": "Upstream entities in the lineage",
                                                                     "name": "upstreams",
                                                                     "type": [
                                                                         "null",
                                                                         {
                                                                             "items": "string",
                                                                             "type": "array"
                                                                         }
-                                                                    ]
+                                                                    ],
+                                                                    "urn_is_array": true
                                                                 },
                                                                 {
                                                                     "doc": "The type of downstream field(s)",
                                                                     "name": "downstreamType",
                                                                     "type": {
                                                                         "doc": "The type of downstream field(s) in a fine-grained lineage",
                                                                         "name": "FineGrainedLineageDownstreamType",
+                                                                        "namespace": "com.linkedin.pegasus2avro.dataset",
                                                                         "symbolDocs": {
                                                                             "FIELD": " Indicates that the lineage is for a single, specific, downstream field",
                                                                             "FIELD_SET": " Indicates that the lineage is for a set of downstream fields"
                                                                         },
                                                                         "symbols": [
                                                                             "FIELD",
                                                                             "FIELD_SET"
                                                                         ],
                                                                         "type": "enum"
                                                                     }
                                                                 },
                                                                 {
+                                                                    "Urn": "Urn",
                                                                     "default": null,
                                                                     "doc": "Downstream fields in the lineage",
                                                                     "name": "downstreams",
                                                                     "type": [
                                                                         "null",
                                                                         {
                                                                             "items": "string",
                                                                             "type": "array"
                                                                         }
-                                                                    ]
+                                                                    ],
+                                                                    "urn_is_array": true
                                                                 },
                                                                 {
                                                                     "default": null,
                                                                     "doc": "The transform operation applied to the upstream entities to produce the downstream field(s)",
                                                                     "name": "transformOperation",
                                                                     "type": [
                                                                         "null",
@@ -2708,14 +2770,15 @@
                     "Entity": {
                         "keyAspect": "datasetKey",
                         "name": "dataset"
                     },
                     "doc": "A metadata snapshot for a specific dataset entity.",
                     "fields": [
                         {
+                            "Urn": "DatasetUrn",
                             "doc": "URN for the entity the metadata snapshot is associated with.",
                             "java": {
                                 "class": "com.linkedin.pegasus2avro.common.urn.DatasetUrn"
                             },
                             "name": "urn",
                             "type": "string"
                         },
@@ -2731,14 +2794,15 @@
                                         "doc": "Key for a Dataset",
                                         "fields": [
                                             {
                                                 "Searchable": {
                                                     "enableAutocomplete": true,
                                                     "fieldType": "URN"
                                                 },
+                                                "Urn": "Urn",
                                                 "doc": "Data platform urn associated with the dataset",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                                 },
                                                 "name": "platform",
                                                 "type": "string"
                                             },
@@ -3015,14 +3079,15 @@
                                             },
                                             {
                                                 "doc": "Additional information about the dataset deprecation plan, such as the wiki, doc, RB.",
                                                 "name": "note",
                                                 "type": "string"
                                             },
                                             {
+                                                "Urn": "Urn",
                                                 "default": null,
                                                 "doc": "The corpuser URN which will be credited for modifying this deprecation content.",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                                 },
                                                 "name": "actor",
                                                 "type": [
@@ -3095,24 +3160,26 @@
                                                                     "items": [
                                                                         "string"
                                                                     ],
                                                                     "type": "array"
                                                                 }
                                                             },
                                                             {
+                                                                "Urn": "DatasetFieldUrn",
                                                                 "deprecated": "use SchemaFieldPath and represent as generic Urn instead",
                                                                 "doc": "Destination field which is derived from source fields",
                                                                 "java": {
                                                                     "class": "com.linkedin.pegasus2avro.common.urn.DatasetFieldUrn"
                                                                 },
                                                                 "name": "destinationField",
                                                                 "type": "string"
                                                             }
                                                         ],
                                                         "name": "DatasetFieldMapping",
+                                                        "namespace": "com.linkedin.pegasus2avro.dataset",
                                                         "type": "record"
                                                     },
                                                     "type": "array"
                                                 }
                                             }
                                         ],
                                         "name": "DatasetUpstreamLineage",
@@ -3166,27 +3233,29 @@
                                                                     "updatedOn": "upstreams/*/auditStamp/time"
                                                                 },
                                                                 "Searchable": {
                                                                     "fieldName": "upstreams",
                                                                     "fieldType": "URN",
                                                                     "queryByDefault": false
                                                                 },
+                                                                "Urn": "DatasetUrn",
                                                                 "doc": "The upstream dataset the lineage points to",
                                                                 "java": {
                                                                     "class": "com.linkedin.pegasus2avro.common.urn.DatasetUrn"
                                                                 },
                                                                 "name": "dataset",
                                                                 "type": "string"
                                                             },
                                                             {
                                                                 "doc": "The type of the lineage",
                                                                 "name": "type",
                                                                 "type": {
                                                                     "doc": "The various types of supported dataset lineage",
                                                                     "name": "DatasetLineageType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.dataset",
                                                                     "symbolDocs": {
                                                                         "COPY": "Direct copy without modification",
                                                                         "TRANSFORMED": "Transformed data with modification (format or content change)",
                                                                         "VIEW": "Represents a view defined on the sources e.g. Hive view defined on underlying hive tables or a Hive table pointing to a HDFS dataset or DALI view defined on multiple sources"
                                                                     },
                                                                     "symbols": [
                                                                         "COPY",
@@ -3206,14 +3275,15 @@
                                                                         "type": "map",
                                                                         "values": "string"
                                                                     }
                                                                 ]
                                                             }
                                                         ],
                                                         "name": "Upstream",
+                                                        "namespace": "com.linkedin.pegasus2avro.dataset",
                                                         "type": "record"
                                                     },
                                                     "type": "array"
                                                 }
                                             },
                                             {
                                                 "Relationship": {
@@ -3227,15 +3297,15 @@
                                                 },
                                                 "default": null,
                                                 "doc": " List of fine-grained lineage information, including field-level lineage",
                                                 "name": "fineGrainedLineages",
                                                 "type": [
                                                     "null",
                                                     {
-                                                        "items": "FineGrainedLineage",
+                                                        "items": "com.linkedin.pegasus2avro.dataset.FineGrainedLineage",
                                                         "type": "array"
                                                     }
                                                 ]
                                             }
                                         ],
                                         "name": "UpstreamLineage",
                                         "namespace": "com.linkedin.pegasus2avro.dataset",
@@ -3258,14 +3328,15 @@
                                                     "strlen": {
                                                         "max": 500,
                                                         "min": 1
                                                     }
                                                 }
                                             },
                                             {
+                                                "Urn": "DataPlatformUrn",
                                                 "doc": "Standardized platform urn where schema is defined. The data platform Urn (urn:li:platform:{platform_name})",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.urn.DataPlatformUrn"
                                                 },
                                                 "name": "platform",
                                                 "type": "string"
                                             },
@@ -3302,14 +3373,15 @@
                                                 "name": "deleted",
                                                 "type": [
                                                     "null",
                                                     "com.linkedin.pegasus2avro.common.AuditStamp"
                                                 ]
                                             },
                                             {
+                                                "Urn": "DatasetUrn",
                                                 "default": null,
                                                 "doc": "Dataset this schema metadata is associated with.",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.urn.DatasetUrn"
                                                 },
                                                 "name": "dataset",
                                                 "type": [
@@ -3346,50 +3418,54 @@
                                                             {
                                                                 "doc": "The espresso table schema definition.",
                                                                 "name": "tableSchema",
                                                                 "type": "string"
                                                             }
                                                         ],
                                                         "name": "EspressoSchema",
+                                                        "namespace": "com.linkedin.pegasus2avro.schema",
                                                         "type": "record"
                                                     },
                                                     {
                                                         "doc": "Schema holder for oracle data definition language that describes an oracle table.",
                                                         "fields": [
                                                             {
                                                                 "doc": "The native schema in the dataset's platform. This is a human readable (json blob) table schema.",
                                                                 "name": "tableSchema",
                                                                 "type": "string"
                                                             }
                                                         ],
                                                         "name": "OracleDDL",
+                                                        "namespace": "com.linkedin.pegasus2avro.schema",
                                                         "type": "record"
                                                     },
                                                     {
                                                         "doc": "Schema holder for MySql data definition language that describes an MySql table.",
                                                         "fields": [
                                                             {
                                                                 "doc": "The native schema in the dataset's platform. This is a human readable (json blob) table schema.",
                                                                 "name": "tableSchema",
                                                                 "type": "string"
                                                             }
                                                         ],
                                                         "name": "MySqlDDL",
+                                                        "namespace": "com.linkedin.pegasus2avro.schema",
                                                         "type": "record"
                                                     },
                                                     {
                                                         "doc": "Schema holder for presto data definition language that describes a presto view.",
                                                         "fields": [
                                                             {
                                                                 "doc": "The raw schema in the dataset's platform. This includes the DDL and the columns extracted from DDL.",
                                                                 "name": "rawSchema",
                                                                 "type": "string"
                                                             }
                                                         ],
                                                         "name": "PrestoDDL",
+                                                        "namespace": "com.linkedin.pegasus2avro.schema",
                                                         "type": "record"
                                                     },
                                                     {
                                                         "doc": "Schema holder for kafka schema.",
                                                         "fields": [
                                                             {
                                                                 "doc": "The native kafka document schema. This is a human readable avro document schema.",
@@ -3403,44 +3479,48 @@
                                                                 "type": [
                                                                     "null",
                                                                     "string"
                                                                 ]
                                                             }
                                                         ],
                                                         "name": "KafkaSchema",
+                                                        "namespace": "com.linkedin.pegasus2avro.schema",
                                                         "type": "record"
                                                     },
                                                     {
                                                         "doc": "Schema text of binary JSON schema.",
                                                         "fields": [
                                                             {
                                                                 "doc": "The native schema text for binary JSON file format.",
                                                                 "name": "schema",
                                                                 "type": "string"
                                                             }
                                                         ],
                                                         "name": "BinaryJsonSchema",
+                                                        "namespace": "com.linkedin.pegasus2avro.schema",
                                                         "type": "record"
                                                     },
                                                     {
                                                         "doc": "Schema text of an ORC schema.",
                                                         "fields": [
                                                             {
                                                                 "doc": "The native schema for ORC file format.",
                                                                 "name": "schema",
                                                                 "type": "string"
                                                             }
                                                         ],
                                                         "name": "OrcSchema",
+                                                        "namespace": "com.linkedin.pegasus2avro.schema",
                                                         "type": "record"
                                                     },
                                                     {
                                                         "doc": "The dataset has no specific schema associated with it",
                                                         "fields": [],
                                                         "name": "Schemaless",
+                                                        "namespace": "com.linkedin.pegasus2avro.schema",
                                                         "type": "record"
                                                     },
                                                     {
                                                         "doc": "Schema text of a key-value store schema.",
                                                         "fields": [
                                                             {
                                                                 "doc": "The raw schema for the key in the key-value store.",
@@ -3450,39 +3530,42 @@
                                                             {
                                                                 "doc": "The raw schema for the value in the key-value store.",
                                                                 "name": "valueSchema",
                                                                 "type": "string"
                                                             }
                                                         ],
                                                         "name": "KeyValueSchema",
+                                                        "namespace": "com.linkedin.pegasus2avro.schema",
                                                         "type": "record"
                                                     },
                                                     {
                                                         "doc": "Schema holder for undefined schema types.",
                                                         "fields": [
                                                             {
                                                                 "doc": "The native schema in the dataset's platform.",
                                                                 "name": "rawSchema",
                                                                 "type": "string"
                                                             }
                                                         ],
                                                         "name": "OtherSchema",
+                                                        "namespace": "com.linkedin.pegasus2avro.schema",
                                                         "type": "record"
                                                     }
                                                 ]
                                             },
                                             {
                                                 "doc": "Client provided a list of fields from document schema.",
                                                 "name": "fields",
                                                 "type": {
                                                     "items": {
                                                         "doc": "SchemaField to describe metadata related to dataset schema.",
                                                         "fields": [
                                                             {
                                                                 "Searchable": {
+                                                                    "boostScore": 5.0,
                                                                     "fieldName": "fieldPaths",
                                                                     "fieldType": "TEXT"
                                                                 },
                                                                 "doc": "Flattened name of the field. Field is computed from jsonPath field.",
                                                                 "name": "fieldPath",
                                                                 "type": "string"
                                                             },
@@ -3558,62 +3641,71 @@
                                                                             "doc": "Data platform specific types",
                                                                             "name": "type",
                                                                             "type": [
                                                                                 {
                                                                                     "doc": "Boolean field type.",
                                                                                     "fields": [],
                                                                                     "name": "BooleanType",
+                                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
                                                                                     "type": "record"
                                                                                 },
                                                                                 {
                                                                                     "doc": "Fixed field type.",
                                                                                     "fields": [],
                                                                                     "name": "FixedType",
+                                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
                                                                                     "type": "record"
                                                                                 },
                                                                                 {
                                                                                     "doc": "String field type.",
                                                                                     "fields": [],
                                                                                     "name": "StringType",
+                                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
                                                                                     "type": "record"
                                                                                 },
                                                                                 {
                                                                                     "doc": "Bytes field type.",
                                                                                     "fields": [],
                                                                                     "name": "BytesType",
+                                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
                                                                                     "type": "record"
                                                                                 },
                                                                                 {
                                                                                     "doc": "Number data type: long, integer, short, etc..",
                                                                                     "fields": [],
                                                                                     "name": "NumberType",
+                                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
                                                                                     "type": "record"
                                                                                 },
                                                                                 {
                                                                                     "doc": "Date field type.",
                                                                                     "fields": [],
                                                                                     "name": "DateType",
+                                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
                                                                                     "type": "record"
                                                                                 },
                                                                                 {
                                                                                     "doc": "Time field type. This should also be used for datetimes.",
                                                                                     "fields": [],
                                                                                     "name": "TimeType",
+                                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
                                                                                     "type": "record"
                                                                                 },
                                                                                 {
                                                                                     "doc": "Enum field type.",
                                                                                     "fields": [],
                                                                                     "name": "EnumType",
+                                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
                                                                                     "type": "record"
                                                                                 },
                                                                                 {
                                                                                     "doc": "Null field type.",
                                                                                     "fields": [],
                                                                                     "name": "NullType",
+                                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
                                                                                     "type": "record"
                                                                                 },
                                                                                 {
                                                                                     "doc": "Map field type.",
                                                                                     "fields": [
                                                                                         {
                                                                                             "default": null,
@@ -3631,14 +3723,15 @@
                                                                                             "type": [
                                                                                                 "null",
                                                                                                 "string"
                                                                                             ]
                                                                                         }
                                                                                     ],
                                                                                     "name": "MapType",
+                                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
                                                                                     "type": "record"
                                                                                 },
                                                                                 {
                                                                                     "doc": "Array field type.",
                                                                                     "fields": [
                                                                                         {
                                                                                             "default": null,
@@ -3650,14 +3743,15 @@
                                                                                                     "items": "string",
                                                                                                     "type": "array"
                                                                                                 }
                                                                                             ]
                                                                                         }
                                                                                     ],
                                                                                     "name": "ArrayType",
+                                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
                                                                                     "type": "record"
                                                                                 },
                                                                                 {
                                                                                     "doc": "Union field type.",
                                                                                     "fields": [
                                                                                         {
                                                                                             "default": null,
@@ -3669,26 +3763,29 @@
                                                                                                     "items": "string",
                                                                                                     "type": "array"
                                                                                                 }
                                                                                             ]
                                                                                         }
                                                                                     ],
                                                                                     "name": "UnionType",
+                                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
                                                                                     "type": "record"
                                                                                 },
                                                                                 {
                                                                                     "doc": "Record field type.",
                                                                                     "fields": [],
                                                                                     "name": "RecordType",
+                                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
                                                                                     "type": "record"
                                                                                 }
                                                                             ]
                                                                         }
                                                                     ],
                                                                     "name": "SchemaFieldDataType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.schema",
                                                                     "type": "record"
                                                                 }
                                                             },
                                                             {
                                                                 "doc": "The native type of the field in the dataset's platform as declared by platform schema.",
                                                                 "name": "nativeDataType",
                                                                 "type": "string"
@@ -3769,14 +3866,15 @@
                                                                 "type": [
                                                                     "null",
                                                                     "string"
                                                                 ]
                                                             }
                                                         ],
                                                         "name": "SchemaField",
+                                                        "namespace": "com.linkedin.pegasus2avro.schema",
                                                         "type": "record"
                                                     },
                                                     "type": "array"
                                                 }
                                             },
                                             {
                                                 "default": null,
@@ -3806,14 +3904,15 @@
                                                                     "doc": "Foreign key definition in metadata schema.",
                                                                     "name": "foreignKey",
                                                                     "type": [
                                                                         {
                                                                             "doc": "For non-urn based foregin keys.",
                                                                             "fields": [
                                                                                 {
+                                                                                    "Urn": "DatasetUrn",
                                                                                     "doc": "dataset that stores the resource.",
                                                                                     "java": {
                                                                                         "class": "com.linkedin.pegasus2avro.common.urn.DatasetUrn"
                                                                                     },
                                                                                     "name": "parentDataset",
                                                                                     "type": "string"
                                                                                 },
@@ -3828,32 +3927,35 @@
                                                                                 {
                                                                                     "doc": "SchemaField@fieldPath that uniquely identify field in parent dataset that this field references.",
                                                                                     "name": "parentField",
                                                                                     "type": "string"
                                                                                 }
                                                                             ],
                                                                             "name": "DatasetFieldForeignKey",
+                                                                            "namespace": "com.linkedin.pegasus2avro.schema",
                                                                             "type": "record"
                                                                         },
                                                                         {
                                                                             "doc": "If SchemaMetadata fields make any external references and references are of type com.linkedin.pegasus2avro.common.Urn or any children, this models can be used to mark it.",
                                                                             "fields": [
                                                                                 {
                                                                                     "doc": "Field in hosting(current) SchemaMetadata.",
                                                                                     "name": "currentFieldPath",
                                                                                     "type": "string"
                                                                                 }
                                                                             ],
                                                                             "name": "UrnForeignKey",
+                                                                            "namespace": "com.linkedin.pegasus2avro.schema",
                                                                             "type": "record"
                                                                         }
                                                                     ]
                                                                 }
                                                             ],
                                                             "name": "ForeignKeySpec",
+                                                            "namespace": "com.linkedin.pegasus2avro.schema",
                                                             "type": "record"
                                                         }
                                                     }
                                                 ]
                                             },
                                             {
                                                 "default": null,
@@ -3875,45 +3977,51 @@
                                                                         "/*": {
                                                                             "entityTypes": [
                                                                                 "schemaField"
                                                                             ],
                                                                             "name": "ForeignKeyTo"
                                                                         }
                                                                     },
+                                                                    "Urn": "Urn",
                                                                     "doc": "Fields the constraint maps to on the foreign dataset",
                                                                     "name": "foreignFields",
                                                                     "type": {
                                                                         "items": "string",
                                                                         "type": "array"
-                                                                    }
+                                                                    },
+                                                                    "urn_is_array": true
                                                                 },
                                                                 {
+                                                                    "Urn": "Urn",
                                                                     "doc": "Fields the constraint maps to on the source dataset",
                                                                     "name": "sourceFields",
                                                                     "type": {
                                                                         "items": "string",
                                                                         "type": "array"
-                                                                    }
+                                                                    },
+                                                                    "urn_is_array": true
                                                                 },
                                                                 {
                                                                     "Relationship": {
                                                                         "entityTypes": [
                                                                             "dataset"
                                                                         ],
                                                                         "name": "ForeignKeyToDataset"
                                                                     },
+                                                                    "Urn": "Urn",
                                                                     "doc": "Reference to the foreign dataset for ease of lookup",
                                                                     "java": {
                                                                         "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                                                     },
                                                                     "name": "foreignDataset",
                                                                     "type": "string"
                                                                 }
                                                             ],
                                                             "name": "ForeignKeyConstraint",
+                                                            "namespace": "com.linkedin.pegasus2avro.schema",
                                                             "type": "record"
                                                         },
                                                         "type": "array"
                                                     }
                                                 ]
                                             }
                                         ],
@@ -4030,14 +4138,15 @@
                                                                 "type": [
                                                                     "null",
                                                                     "com.linkedin.pegasus2avro.common.GlossaryTerms"
                                                                 ]
                                                             }
                                                         ],
                                                         "name": "EditableSchemaFieldInfo",
+                                                        "namespace": "com.linkedin.pegasus2avro.schema",
                                                         "type": "record"
                                                     },
                                                     "type": "array"
                                                 }
                                             }
                                         ],
                                         "name": "EditableSchemaMetadata",
@@ -4094,14 +4203,15 @@
                         "keyAspect": "dataProcessKey",
                         "name": "dataProcess"
                     },
                     "deprecated": "Use DataJob instead.",
                     "doc": "A metadata snapshot for a specific Data process entity.",
                     "fields": [
                         {
+                            "Urn": "DataProcessUrn",
                             "doc": "URN for the entity the metadata snapshot is associated with.",
                             "java": {
                                 "class": "com.linkedin.pegasus2avro.common.urn.DataProcessUrn"
                             },
                             "name": "urn",
                             "type": "string"
                         },
@@ -4170,24 +4280,26 @@
                                                     "/*": {
                                                         "fieldName": "inputs",
                                                         "fieldType": "URN",
                                                         "numValuesFieldName": "numInputDatasets",
                                                         "queryByDefault": false
                                                     }
                                                 },
+                                                "Urn": "DatasetUrn",
                                                 "default": null,
                                                 "doc": "the inputs of the data process",
                                                 "name": "inputs",
                                                 "type": [
                                                     "null",
                                                     {
                                                         "items": "string",
                                                         "type": "array"
                                                     }
-                                                ]
+                                                ],
+                                                "urn_is_array": true
                                             },
                                             {
                                                 "Relationship": {
                                                     "/*": {
                                                         "entityTypes": [
                                                             "dataset"
                                                         ],
@@ -4199,24 +4311,26 @@
                                                     "/*": {
                                                         "fieldName": "outputs",
                                                         "fieldType": "URN",
                                                         "numValuesFieldName": "numOutputDatasets",
                                                         "queryByDefault": false
                                                     }
                                                 },
+                                                "Urn": "DatasetUrn",
                                                 "default": null,
                                                 "doc": "the outputs of the data process",
                                                 "name": "outputs",
                                                 "type": [
                                                     "null",
                                                     {
                                                         "items": "string",
                                                         "type": "array"
                                                     }
-                                                ]
+                                                ],
+                                                "urn_is_array": true
                                             }
                                         ],
                                         "name": "DataProcessInfo",
                                         "namespace": "com.linkedin.pegasus2avro.dataprocess",
                                         "type": "record"
                                     },
                                     "com.linkedin.pegasus2avro.common.Status"
@@ -4233,14 +4347,15 @@
                     "Entity": {
                         "keyAspect": "dataPlatformKey",
                         "name": "dataPlatform"
                     },
                     "doc": "A metadata snapshot for a specific dataplatform entity.",
                     "fields": [
                         {
+                            "Urn": "DataPlatformUrn",
                             "doc": "URN for the entity the metadata snapshot is associated with.",
                             "java": {
                                 "class": "com.linkedin.pegasus2avro.common.urn.DataPlatformUrn"
                             },
                             "name": "urn",
                             "type": "string"
                         },
@@ -4302,14 +4417,15 @@
                                             },
                                             {
                                                 "doc": "Platform type this data platform describes",
                                                 "name": "type",
                                                 "type": {
                                                     "doc": "Platform types available at LinkedIn",
                                                     "name": "PlatformType",
+                                                    "namespace": "com.linkedin.pegasus2avro.dataplatform",
                                                     "symbolDocs": {
                                                         "FILE_SYSTEM": "Value for a file system, e.g. hdfs",
                                                         "KEY_VALUE_STORE": "Value for a key value store, e.g. espresso, voldemort",
                                                         "MESSAGE_BROKER": "Value for a message broker, e.g. kafka",
                                                         "OBJECT_STORE": "Value for an object store, e.g. ambry",
                                                         "OLAP_DATASTORE": "Value for an OLAP datastore, e.g. pinot",
                                                         "OTHERS": "Value for other platforms, e.g salesforce, dovetail",
@@ -4367,14 +4483,15 @@
                     "Entity": {
                         "keyAspect": "mlModelKey",
                         "name": "mlModel"
                     },
                     "doc": "MLModel Snapshot entity details.",
                     "fields": [
                         {
+                            "Urn": "MLModelUrn",
                             "doc": "URN for the entity the metadata snapshot is associated with.",
                             "java": {
                                 "class": "com.linkedin.pegasus2avro.common.urn.MLModelUrn"
                             },
                             "name": "urn",
                             "type": "string"
                         },
@@ -4386,14 +4503,15 @@
                                     {
                                         "Aspect": {
                                             "name": "mlModelKey"
                                         },
                                         "doc": "Key for an ML model",
                                         "fields": [
                                             {
+                                                "Urn": "Urn",
                                                 "doc": "Standardized platform urn for the model",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                                 },
                                                 "name": "platform",
                                                 "type": "string"
                                             },
@@ -4576,14 +4694,15 @@
                                                                     "type": [
                                                                         "null",
                                                                         "long"
                                                                     ]
                                                                 }
                                                             ],
                                                             "name": "MLHyperParam",
+                                                            "namespace": "com.linkedin.pegasus2avro.ml.metadata",
                                                             "type": "record"
                                                         },
                                                         "type": "array"
                                                     }
                                                 ]
                                             },
                                             {
@@ -4629,52 +4748,55 @@
                                                                     "type": [
                                                                         "null",
                                                                         "long"
                                                                     ]
                                                                 }
                                                             ],
                                                             "name": "MLMetric",
+                                                            "namespace": "com.linkedin.pegasus2avro.ml.metadata",
                                                             "type": "record"
                                                         },
                                                         "type": "array"
                                                     }
                                                 ]
                                             },
                                             {
                                                 "default": null,
                                                 "doc": "Metrics of the MLModel used in production",
                                                 "name": "onlineMetrics",
                                                 "type": [
                                                     "null",
                                                     {
-                                                        "items": "MLMetric",
+                                                        "items": "com.linkedin.pegasus2avro.ml.metadata.MLMetric",
                                                         "type": "array"
                                                     }
                                                 ]
                                             },
                                             {
                                                 "Relationship": {
                                                     "/*": {
                                                         "entityTypes": [
                                                             "mlFeature"
                                                         ],
                                                         "isLineage": true,
                                                         "name": "Consumes"
                                                     }
                                                 },
+                                                "Urn": "MLFeatureUrn",
                                                 "default": null,
                                                 "doc": "List of features used for MLModel training",
                                                 "name": "mlFeatures",
                                                 "type": [
                                                     "null",
                                                     {
                                                         "items": "string",
                                                         "type": "array"
                                                     }
-                                                ]
+                                                ],
+                                                "urn_is_array": true
                                             },
                                             {
                                                 "default": [],
                                                 "doc": "Tags for the MLModel",
                                                 "name": "tags",
                                                 "type": {
                                                     "items": "string",
@@ -4686,89 +4808,97 @@
                                                     "/*": {
                                                         "entityTypes": [
                                                             "mlModelDeployment"
                                                         ],
                                                         "name": "DeployedTo"
                                                     }
                                                 },
+                                                "Urn": "Urn",
                                                 "default": null,
                                                 "doc": "Deployments for the MLModel",
                                                 "name": "deployments",
                                                 "type": [
                                                     "null",
                                                     {
                                                         "items": "string",
                                                         "type": "array"
                                                     }
-                                                ]
+                                                ],
+                                                "urn_is_array": true
                                             },
                                             {
                                                 "Relationship": {
                                                     "/*": {
                                                         "entityTypes": [
                                                             "dataJob"
                                                         ],
                                                         "isLineage": true,
                                                         "name": "TrainedBy"
                                                     }
                                                 },
+                                                "Urn": "Urn",
                                                 "default": null,
                                                 "doc": "List of jobs (if any) used to train the model",
                                                 "name": "trainingJobs",
                                                 "type": [
                                                     "null",
                                                     {
                                                         "items": "string",
                                                         "type": "array"
                                                     }
-                                                ]
+                                                ],
+                                                "urn_is_array": true
                                             },
                                             {
                                                 "Relationship": {
                                                     "/*": {
                                                         "entityTypes": [
                                                             "dataJob"
                                                         ],
                                                         "isLineage": true,
                                                         "isUpstream": false,
                                                         "name": "UsedBy"
                                                     }
                                                 },
+                                                "Urn": "Urn",
                                                 "default": null,
                                                 "doc": "List of jobs (if any) that use the model",
                                                 "name": "downstreamJobs",
                                                 "type": [
                                                     "null",
                                                     {
                                                         "items": "string",
                                                         "type": "array"
                                                     }
-                                                ]
+                                                ],
+                                                "urn_is_array": true
                                             },
                                             {
                                                 "Relationship": {
                                                     "/*": {
                                                         "entityTypes": [
                                                             "mlModelGroup"
                                                         ],
                                                         "isLineage": true,
                                                         "isUpstream": false,
                                                         "name": "MemberOf"
                                                     }
                                                 },
+                                                "Urn": "Urn",
                                                 "default": null,
                                                 "doc": "Groups the model belongs to",
                                                 "name": "groups",
                                                 "type": [
                                                     "null",
                                                     {
                                                         "items": "string",
                                                         "type": "array"
                                                     }
-                                                ]
+                                                ],
+                                                "urn_is_array": true
                                             }
                                         ],
                                         "name": "MLModelProperties",
                                         "namespace": "com.linkedin.pegasus2avro.ml.metadata",
                                         "type": "record"
                                     },
                                     {
@@ -4794,14 +4924,15 @@
                                                 "doc": "Primary Intended Users - For example, was the MLModel developed for entertainment purposes, for hobbyists, or enterprise solutions?",
                                                 "name": "primaryUsers",
                                                 "type": [
                                                     "null",
                                                     {
                                                         "items": {
                                                             "name": "IntendedUserType",
+                                                            "namespace": "com.linkedin.pegasus2avro.ml.metadata",
                                                             "symbols": [
                                                                 "ENTERPRISE",
                                                                 "HOBBY",
                                                                 "ENTERTAINMENT"
                                                             ],
                                                             "type": "enum"
                                                         },
@@ -4876,28 +5007,29 @@
                                                                             "items": "string",
                                                                             "type": "array"
                                                                         }
                                                                     ]
                                                                 }
                                                             ],
                                                             "name": "MLModelFactors",
+                                                            "namespace": "com.linkedin.pegasus2avro.ml.metadata",
                                                             "type": "record"
                                                         },
                                                         "type": "array"
                                                     }
                                                 ]
                                             },
                                             {
                                                 "default": null,
                                                 "doc": "Which factors are being reported, and why were these chosen?",
                                                 "name": "evaluationFactors",
                                                 "type": [
                                                     "null",
                                                     {
-                                                        "items": "MLModelFactors",
+                                                        "items": "com.linkedin.pegasus2avro.ml.metadata.MLModelFactors",
                                                         "type": "array"
                                                     }
                                                 ]
                                             }
                                         ],
                                         "name": "MLModelFactorPrompts",
                                         "namespace": "com.linkedin.pegasus2avro.ml.metadata",
@@ -4948,14 +5080,15 @@
                                                 "doc": "Details on the dataset(s) used for the quantitative analyses in the MLModel",
                                                 "name": "evaluationData",
                                                 "type": {
                                                     "items": {
                                                         "doc": "BaseData record",
                                                         "fields": [
                                                             {
+                                                                "Urn": "DatasetUrn",
                                                                 "doc": "What dataset were used in the MLModel?",
                                                                 "java": {
                                                                     "class": "com.linkedin.pegasus2avro.common.urn.DatasetUrn"
                                                                 },
                                                                 "name": "dataset",
                                                                 "type": "string"
                                                             },
@@ -4978,14 +5111,15 @@
                                                                         "items": "string",
                                                                         "type": "array"
                                                                     }
                                                                 ]
                                                             }
                                                         ],
                                                         "name": "BaseData",
+                                                        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
                                                         "type": "record"
                                                     },
                                                     "type": "array"
                                                 }
                                             }
                                         ],
                                         "name": "EvaluationData",
@@ -4998,15 +5132,15 @@
                                         },
                                         "doc": "Ideally, the MLModel card would contain as much information about the training data as the evaluation data. However, there might be cases where it is not feasible to provide this level of detailed information about the training data. For example, the data may be proprietary, or require a non-disclosure agreement. In these cases, we advocate for basic details about the distributions over groups in the data, as well as any other details that could inform stakeholders on the kinds of biases the model may have encoded.",
                                         "fields": [
                                             {
                                                 "doc": "Details on the dataset(s) used for training the MLModel",
                                                 "name": "trainingData",
                                                 "type": {
-                                                    "items": "BaseData",
+                                                    "items": "com.linkedin.pegasus2avro.ml.metadata.BaseData",
                                                     "type": "array"
                                                 }
                                             }
                                         ],
                                         "name": "TrainingData",
                                         "namespace": "com.linkedin.pegasus2avro.ml.metadata",
                                         "type": "record"
@@ -5154,14 +5288,15 @@
                                                                         "items": "string",
                                                                         "type": "array"
                                                                     }
                                                                 ]
                                                             }
                                                         ],
                                                         "name": "CaveatDetails",
+                                                        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
                                                         "type": "record"
                                                     }
                                                 ]
                                             },
                                             {
                                                 "default": null,
                                                 "doc": "Recommendations on where this MLModel should be used.",
@@ -5203,14 +5338,15 @@
                                                         "doc": "Source Code Url Entity",
                                                         "fields": [
                                                             {
                                                                 "doc": "Source Code Url Types",
                                                                 "name": "type",
                                                                 "type": {
                                                                     "name": "SourceCodeUrlType",
+                                                                    "namespace": "com.linkedin.pegasus2avro.ml.metadata",
                                                                     "symbols": [
                                                                         "ML_MODEL_SOURCE_CODE",
                                                                         "TRAINING_PIPELINE_SOURCE_CODE",
                                                                         "EVALUATION_PIPELINE_SOURCE_CODE"
                                                                     ],
                                                                     "type": "enum"
                                                                 }
@@ -5222,14 +5358,15 @@
                                                                     "coercerClass": "com.linkedin.pegasus2avro.common.url.UrlCoercer"
                                                                 },
                                                                 "name": "sourceCodeUrl",
                                                                 "type": "string"
                                                             }
                                                         ],
                                                         "name": "SourceCodeUrl",
+                                                        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
                                                         "type": "record"
                                                     },
                                                     "type": "array"
                                                 }
                                             }
                                         ],
                                         "name": "SourceCode",
@@ -5243,14 +5380,15 @@
                                         },
                                         "fields": [
                                             {
                                                 "name": "costType",
                                                 "type": {
                                                     "doc": "Type of Cost Code",
                                                     "name": "CostType",
+                                                    "namespace": "com.linkedin.pegasus2avro.common",
                                                     "symbolDocs": {
                                                         "ORG_COST_TYPE": "Org Cost Type to which the Cost of this entity should be attributed to"
                                                     },
                                                     "symbols": [
                                                         "ORG_COST_TYPE"
                                                     ],
                                                     "type": "enum"
@@ -5277,23 +5415,25 @@
                                                             ]
                                                         },
                                                         {
                                                             "doc": "Contains the name of the field that has its value set.",
                                                             "name": "fieldDiscriminator",
                                                             "type": {
                                                                 "name": "CostCostDiscriminator",
+                                                                "namespace": "com.linkedin.pegasus2avro.common",
                                                                 "symbols": [
                                                                     "costId",
                                                                     "costCode"
                                                                 ],
                                                                 "type": "enum"
                                                             }
                                                         }
                                                     ],
                                                     "name": "CostCost",
+                                                    "namespace": "com.linkedin.pegasus2avro.common",
                                                     "type": "record"
                                                 }
                                             }
                                         ],
                                         "name": "Cost",
                                         "namespace": "com.linkedin.pegasus2avro.common",
                                         "type": "record"
@@ -5326,14 +5466,15 @@
                                             },
                                             {
                                                 "doc": "Additional information about the entity deprecation plan, such as the wiki, doc, RB.",
                                                 "name": "note",
                                                 "type": "string"
                                             },
                                             {
+                                                "Urn": "Urn",
                                                 "doc": "The user URN which will be credited for modifying this deprecation content.",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                                 },
                                                 "name": "actor",
                                                 "type": "string"
                                             }
@@ -5357,14 +5498,15 @@
                 {
                     "Entity": {
                         "keyAspect": "mlPrimaryKeyKey",
                         "name": "mlPrimaryKey"
                     },
                     "fields": [
                         {
+                            "Urn": "Urn",
                             "doc": "URN for the entity the metadata snapshot is associated with.",
                             "java": {
                                 "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                             },
                             "name": "urn",
                             "type": "string"
                         },
@@ -5484,20 +5626,22 @@
                                                         "entityTypes": [
                                                             "dataset"
                                                         ],
                                                         "isLineage": true,
                                                         "name": "DerivedFrom"
                                                     }
                                                 },
+                                                "Urn": "Urn",
                                                 "doc": "Source of the MLPrimaryKey",
                                                 "name": "sources",
                                                 "type": {
                                                     "items": "string",
                                                     "type": "array"
-                                                }
+                                                },
+                                                "urn_is_array": true
                                             }
                                         ],
                                         "name": "MLPrimaryKeyProperties",
                                         "namespace": "com.linkedin.pegasus2avro.ml.metadata",
                                         "type": "record"
                                     },
                                     "com.linkedin.pegasus2avro.common.Ownership",
@@ -5518,14 +5662,15 @@
                 {
                     "Entity": {
                         "keyAspect": "mlFeatureKey",
                         "name": "mlFeature"
                     },
                     "fields": [
                         {
+                            "Urn": "MLFeatureUrn",
                             "doc": "URN for the entity the metadata snapshot is associated with.",
                             "java": {
                                 "class": "com.linkedin.pegasus2avro.common.urn.MLFeatureUrn"
                             },
                             "name": "urn",
                             "type": "string"
                         },
@@ -5606,24 +5751,26 @@
                                                         "entityTypes": [
                                                             "dataset"
                                                         ],
                                                         "isLineage": true,
                                                         "name": "DerivedFrom"
                                                     }
                                                 },
+                                                "Urn": "Urn",
                                                 "default": null,
                                                 "doc": "Source of the MLFeature",
                                                 "name": "sources",
                                                 "type": [
                                                     "null",
                                                     {
                                                         "items": "string",
                                                         "type": "array"
                                                     }
-                                                ]
+                                                ],
+                                                "urn_is_array": true
                                             }
                                         ],
                                         "name": "MLFeatureProperties",
                                         "namespace": "com.linkedin.pegasus2avro.ml.metadata",
                                         "type": "record"
                                     },
                                     "com.linkedin.pegasus2avro.common.Ownership",
@@ -5645,14 +5792,15 @@
                 {
                     "Entity": {
                         "keyAspect": "mlFeatureTableKey",
                         "name": "mlFeatureTable"
                     },
                     "fields": [
                         {
+                            "Urn": "Urn",
                             "doc": "URN for the entity the metadata snapshot is associated with.",
                             "java": {
                                 "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                             },
                             "name": "urn",
                             "type": "string"
                         },
@@ -5670,14 +5818,15 @@
                                             {
                                                 "Relationship": {
                                                     "entityTypes": [
                                                         "dataPlatform"
                                                     ],
                                                     "name": "SourcePlatform"
                                                 },
+                                                "Urn": "Urn",
                                                 "doc": "Data platform urn associated with the feature table",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                                 },
                                                 "name": "platform",
                                                 "type": "string"
                                             },
@@ -5740,24 +5889,26 @@
                                                 },
                                                 "Searchable": {
                                                     "/*": {
                                                         "fieldName": "features",
                                                         "fieldType": "URN"
                                                     }
                                                 },
+                                                "Urn": "Urn",
                                                 "default": null,
                                                 "doc": "List of features contained in the feature table",
                                                 "name": "mlFeatures",
                                                 "type": [
                                                     "null",
                                                     {
                                                         "items": "string",
                                                         "type": "array"
                                                     }
-                                                ]
+                                                ],
+                                                "urn_is_array": true
                                             },
                                             {
                                                 "Relationship": {
                                                     "/*": {
                                                         "entityTypes": [
                                                             "mlPrimaryKey"
                                                         ],
@@ -5766,24 +5917,26 @@
                                                 },
                                                 "Searchable": {
                                                     "/*": {
                                                         "fieldName": "primaryKeys",
                                                         "fieldType": "URN"
                                                     }
                                                 },
+                                                "Urn": "Urn",
                                                 "default": null,
                                                 "doc": "List of primary keys in the feature table (if multiple, assumed to act as a composite key)",
                                                 "name": "mlPrimaryKeys",
                                                 "type": [
                                                     "null",
                                                     {
                                                         "items": "string",
                                                         "type": "array"
                                                     }
-                                                ]
+                                                ],
+                                                "urn_is_array": true
                                             }
                                         ],
                                         "name": "MLFeatureTableProperties",
                                         "namespace": "com.linkedin.pegasus2avro.ml.metadata",
                                         "type": "record"
                                     },
                                     "com.linkedin.pegasus2avro.common.Ownership",
@@ -5805,14 +5958,15 @@
                 {
                     "Entity": {
                         "keyAspect": "mlModelDeploymentKey",
                         "name": "mlModelDeployment"
                     },
                     "fields": [
                         {
+                            "Urn": "Urn",
                             "doc": "URN for the entity the metadata snapshot is associated with.",
                             "java": {
                                 "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                             },
                             "name": "urn",
                             "type": "string"
                         },
@@ -5824,14 +5978,15 @@
                                     {
                                         "Aspect": {
                                             "name": "mlModelDeploymentKey"
                                         },
                                         "doc": "Key for an ML model deployment",
                                         "fields": [
                                             {
+                                                "Urn": "Urn",
                                                 "doc": "Standardized platform urn for the model Deployment",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                                 },
                                                 "name": "platform",
                                                 "type": "string"
                                             },
@@ -5930,14 +6085,15 @@
                                                 "doc": "Status of the deployment",
                                                 "name": "status",
                                                 "type": [
                                                     "null",
                                                     {
                                                         "doc": "Model endpoint statuses",
                                                         "name": "DeploymentStatus",
+                                                        "namespace": "com.linkedin.pegasus2avro.ml.metadata",
                                                         "symbolDocs": {
                                                             "CREATING": "Deployments being created.",
                                                             "DELETING": "Deployments being deleted.",
                                                             "FAILED": "Deployments with an error state.",
                                                             "IN_SERVICE": "Deployments that are active.",
                                                             "OUT_OF_SERVICE": "Deployments out of service.",
                                                             "ROLLING_BACK": "Deployments being reverted to a previous version.",
@@ -5980,14 +6136,15 @@
                 {
                     "Entity": {
                         "keyAspect": "mlModelGroupKey",
                         "name": "mlModelGroup"
                     },
                     "fields": [
                         {
+                            "Urn": "Urn",
                             "doc": "URN for the entity the metadata snapshot is associated with.",
                             "java": {
                                 "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                             },
                             "name": "urn",
                             "type": "string"
                         },
@@ -5999,14 +6156,15 @@
                                     {
                                         "Aspect": {
                                             "name": "mlModelGroupKey"
                                         },
                                         "doc": "Key for an ML model group",
                                         "fields": [
                                             {
+                                                "Urn": "Urn",
                                                 "doc": "Standardized platform urn for the model group",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                                 },
                                                 "name": "platform",
                                                 "type": "string"
                                             },
@@ -6109,14 +6267,15 @@
                     "Entity": {
                         "keyAspect": "tagKey",
                         "name": "tag"
                     },
                     "doc": "A metadata snapshot for a specific dataset entity.",
                     "fields": [
                         {
+                            "Urn": "TagUrn",
                             "doc": "URN for the entity the metadata snapshot is associated with.",
                             "java": {
                                 "class": "com.linkedin.pegasus2avro.common.urn.TagUrn"
                             },
                             "name": "urn",
                             "type": "string"
                         },
@@ -6202,14 +6361,15 @@
                     "Entity": {
                         "keyAspect": "glossaryTermKey",
                         "name": "glossaryTerm"
                     },
                     "doc": "A metadata snapshot for a specific GlossaryTerm entity.",
                     "fields": [
                         {
+                            "Urn": "GlossaryTermUrn",
                             "doc": "URN for the entity the metadata snapshot is associated with.",
                             "java": {
                                 "class": "com.linkedin.pegasus2avro.common.urn.GlossaryTermUrn"
                             },
                             "name": "urn",
                             "type": "string"
                         },
@@ -6299,14 +6459,15 @@
                                                     "name": "IsPartOf"
                                                 },
                                                 "Searchable": {
                                                     "fieldName": "parentNode",
                                                     "fieldType": "URN",
                                                     "hasValuesFieldName": "hasParentNode"
                                                 },
+                                                "Urn": "GlossaryNodeUrn",
                                                 "default": null,
                                                 "doc": "Parent node of the glossary term",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.urn.GlossaryNodeUrn"
                                                 },
                                                 "name": "parentNode",
                                                 "type": [
@@ -6383,24 +6544,26 @@
                                                 "Searchable": {
                                                     "/*": {
                                                         "boostScore": 2.0,
                                                         "fieldName": "isRelatedTerms",
                                                         "fieldType": "URN"
                                                     }
                                                 },
+                                                "Urn": "GlossaryTermUrn",
                                                 "default": null,
                                                 "doc": "The relationship Is A with glossary term",
                                                 "name": "isRelatedTerms",
                                                 "type": [
                                                     "null",
                                                     {
                                                         "items": "string",
                                                         "type": "array"
                                                     }
-                                                ]
+                                                ],
+                                                "urn_is_array": true
                                             },
                                             {
                                                 "Relationship": {
                                                     "/*": {
                                                         "entityTypes": [
                                                             "glossaryTerm"
                                                         ],
@@ -6410,24 +6573,26 @@
                                                 "Searchable": {
                                                     "/*": {
                                                         "boostScore": 2.0,
                                                         "fieldName": "hasRelatedTerms",
                                                         "fieldType": "URN"
                                                     }
                                                 },
+                                                "Urn": "GlossaryTermUrn",
                                                 "default": null,
                                                 "doc": "The relationship Has A with glossary term",
                                                 "name": "hasRelatedTerms",
                                                 "type": [
                                                     "null",
                                                     {
                                                         "items": "string",
                                                         "type": "array"
                                                     }
-                                                ]
+                                                ],
+                                                "urn_is_array": true
                                             },
                                             {
                                                 "Relationship": {
                                                     "/*": {
                                                         "entityTypes": [
                                                             "glossaryTerm"
                                                         ],
@@ -6436,24 +6601,26 @@
                                                 },
                                                 "Searchable": {
                                                     "/*": {
                                                         "fieldName": "values",
                                                         "fieldType": "URN"
                                                     }
                                                 },
+                                                "Urn": "GlossaryTermUrn",
                                                 "default": null,
                                                 "doc": "The relationship Has Value with glossary term.\nThese are fixed value a term has. For example a ColorEnum where RED, GREEN and YELLOW are fixed values.",
                                                 "name": "values",
                                                 "type": [
                                                     "null",
                                                     {
                                                         "items": "string",
                                                         "type": "array"
                                                     }
-                                                ]
+                                                ],
+                                                "urn_is_array": true
                                             },
                                             {
                                                 "Relationship": {
                                                     "/*": {
                                                         "entityTypes": [
                                                             "glossaryTerm"
                                                         ],
@@ -6462,24 +6629,26 @@
                                                 },
                                                 "Searchable": {
                                                     "/*": {
                                                         "fieldName": "relatedTerms",
                                                         "fieldType": "URN"
                                                     }
                                                 },
+                                                "Urn": "GlossaryTermUrn",
                                                 "default": null,
                                                 "doc": "The relationship isRelatedTo with glossary term",
                                                 "name": "relatedTerms",
                                                 "type": [
                                                     "null",
                                                     {
                                                         "items": "string",
                                                         "type": "array"
                                                     }
-                                                ]
+                                                ],
+                                                "urn_is_array": true
                                             }
                                         ],
                                         "name": "GlossaryRelatedTerms",
                                         "namespace": "com.linkedin.pegasus2avro.glossary",
                                         "type": "record"
                                     }
                                 ],
@@ -6495,14 +6664,15 @@
                     "Entity": {
                         "keyAspect": "glossaryNodeKey",
                         "name": "glossaryNode"
                     },
                     "doc": "A metadata snapshot for a specific GlossaryNode entity.",
                     "fields": [
                         {
+                            "Urn": "GlossaryNodeUrn",
                             "doc": "URN for the entity the metadata snapshot is associated with.",
                             "java": {
                                 "class": "com.linkedin.pegasus2avro.common.urn.GlossaryNodeUrn"
                             },
                             "name": "urn",
                             "type": "string"
                         },
@@ -6550,14 +6720,15 @@
                                                     "name": "IsPartOf"
                                                 },
                                                 "Searchable": {
                                                     "fieldName": "parentNode",
                                                     "fieldType": "URN",
                                                     "hasValuesFieldName": "hasParentNode"
                                                 },
+                                                "Urn": "GlossaryNodeUrn",
                                                 "default": null,
                                                 "doc": "Parent node of the glossary term",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.urn.GlossaryNodeUrn"
                                                 },
                                                 "name": "parentNode",
                                                 "type": [
@@ -6612,14 +6783,15 @@
                     "Entity": {
                         "keyAspect": "dataHubPolicyKey",
                         "name": "dataHubPolicy"
                     },
                     "doc": "A metadata snapshot for DataHub Access Policy data.",
                     "fields": [
                         {
+                            "Urn": "Urn",
                             "doc": "URN for the entity the metadata snapshot is associated with.",
                             "java": {
                                 "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                             },
                             "name": "urn",
                             "type": "string"
                         },
@@ -6747,38 +6919,42 @@
                                                                                             {
                                                                                                 "default": "EQUALS",
                                                                                                 "doc": "The condition for the criterion",
                                                                                                 "name": "condition",
                                                                                                 "type": {
                                                                                                     "doc": "The matching condition in a filter criterion",
                                                                                                     "name": "PolicyMatchCondition",
+                                                                                                    "namespace": "com.linkedin.pegasus2avro.policy",
                                                                                                     "symbolDocs": {
                                                                                                         "EQUALS": "Whether the field matches the value"
                                                                                                     },
                                                                                                     "symbols": [
                                                                                                         "EQUALS"
                                                                                                     ],
                                                                                                     "type": "enum"
                                                                                                 }
                                                                                             }
                                                                                         ],
                                                                                         "name": "PolicyMatchCriterion",
+                                                                                        "namespace": "com.linkedin.pegasus2avro.policy",
                                                                                         "type": "record"
                                                                                     },
                                                                                     "type": "array"
                                                                                 }
                                                                             }
                                                                         ],
                                                                         "name": "PolicyMatchFilter",
+                                                                        "namespace": "com.linkedin.pegasus2avro.policy",
                                                                         "type": "record"
                                                                     }
                                                                 ]
                                                             }
                                                         ],
                                                         "name": "DataHubResourceFilter",
+                                                        "namespace": "com.linkedin.pegasus2avro.policy",
                                                         "type": "record"
                                                     }
                                                 ]
                                             },
                                             {
                                                 "doc": "The privileges that the policy grants.",
                                                 "name": "privileges",
@@ -6790,36 +6966,40 @@
                                             {
                                                 "doc": "The actors that the policy applies to.",
                                                 "name": "actors",
                                                 "type": {
                                                     "doc": "Information used to filter DataHub actors.",
                                                     "fields": [
                                                         {
+                                                            "Urn": "Urn",
                                                             "default": null,
                                                             "doc": "A specific set of users to apply the policy to (disjunctive)",
                                                             "name": "users",
                                                             "type": [
                                                                 "null",
                                                                 {
                                                                     "items": "string",
                                                                     "type": "array"
                                                                 }
-                                                            ]
+                                                            ],
+                                                            "urn_is_array": true
                                                         },
                                                         {
+                                                            "Urn": "Urn",
                                                             "default": null,
                                                             "doc": "A specific set of groups to apply the policy to (disjunctive)",
                                                             "name": "groups",
                                                             "type": [
                                                                 "null",
                                                                 {
                                                                     "items": "string",
                                                                     "type": "array"
                                                                 }
-                                                            ]
+                                                            ],
+                                                            "urn_is_array": true
                                                         },
                                                         {
                                                             "default": false,
                                                             "doc": "Whether the filter should return true for owners of a particular resource.\nOnly applies to policies of type 'Metadata', which have a resource associated with them.",
                                                             "name": "resourceOwners",
                                                             "type": "boolean"
                                                         },
@@ -6840,27 +7020,30 @@
                                                                 "/*": {
                                                                     "entityTypes": [
                                                                         "dataHubRole"
                                                                     ],
                                                                     "name": "IsAssociatedWithRole"
                                                                 }
                                                             },
+                                                            "Urn": "Urn",
                                                             "default": null,
                                                             "doc": "A specific set of roles to apply the policy to (disjunctive).",
                                                             "name": "roles",
                                                             "type": [
                                                                 "null",
                                                                 {
                                                                     "items": "string",
                                                                     "type": "array"
                                                                 }
-                                                            ]
+                                                            ],
+                                                            "urn_is_array": true
                                                         }
                                                     ],
                                                     "name": "DataHubActorFilter",
+                                                    "namespace": "com.linkedin.pegasus2avro.policy",
                                                     "type": "record"
                                                 }
                                             },
                                             {
                                                 "default": true,
                                                 "doc": "Whether the policy should be editable via the UI",
                                                 "name": "editable",
@@ -6896,14 +7079,15 @@
                     "Entity": {
                         "keyAspect": "schemaFieldKey",
                         "name": "schemaField"
                     },
                     "doc": "A metadata snapshot for a specific schema field entity.",
                     "fields": [
                         {
+                            "Urn": "Urn",
                             "doc": "URN for the entity the metadata snapshot is associated with.",
                             "java": {
                                 "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                             },
                             "name": "urn",
                             "type": "string"
                         },
@@ -6918,14 +7102,15 @@
                                         },
                                         "doc": "Key for a SchemaField",
                                         "fields": [
                                             {
                                                 "Searchable": {
                                                     "fieldType": "URN"
                                                 },
+                                                "Urn": "Urn",
                                                 "doc": "Parent associated with the schema field",
                                                 "java": {
                                                     "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                                                 },
                                                 "name": "parent",
                                                 "type": "string"
                                             },
@@ -6955,14 +7140,15 @@
                     "Entity": {
                         "keyAspect": "dataHubRetentionKey",
                         "name": "dataHubRetention"
                     },
                     "doc": "A metadata snapshot for DataHub Access Policy data.",
                     "fields": [
                         {
+                            "Urn": "Urn",
                             "doc": "URN for the entity the metadata snapshot is associated with.",
                             "java": {
                                 "class": "com.linkedin.pegasus2avro.common.urn.Urn"
                             },
                             "name": "urn",
                             "type": "string"
                         },
@@ -7012,14 +7198,15 @@
                                                                     "fields": [
                                                                         {
                                                                             "name": "maxVersions",
                                                                             "type": "int"
                                                                         }
                                                                     ],
                                                                     "name": "VersionBasedRetention",
+                                                                    "namespace": "com.linkedin.pegasus2avro.retention",
                                                                     "type": "record"
                                                                 }
                                                             ]
                                                         },
                                                         {
                                                             "default": null,
                                                             "name": "time",
@@ -7030,20 +7217,22 @@
                                                                     "fields": [
                                                                         {
                                                                             "name": "maxAgeInSeconds",
                                                                             "type": "int"
                                                                         }
                                                                     ],
                                                                     "name": "TimeBasedRetention",
+                                                                    "namespace": "com.linkedin.pegasus2avro.retention",
                                                                     "type": "record"
                                                                 }
                                                             ]
                                                         }
                                                     ],
                                                     "name": "Retention",
+                                                    "namespace": "com.linkedin.pegasus2avro.retention",
                                                     "type": "record"
                                                 }
                                             }
                                         ],
                                         "name": "DataHubRetentionConfig",
                                         "namespace": "com.linkedin.pegasus2avro.retention",
                                         "type": "record"
@@ -7122,14 +7311,15 @@
                                     "type": "map",
                                     "values": "string"
                                 }
                             ]
                         }
                     ],
                     "name": "SystemMetadata",
+                    "namespace": "com.linkedin.pegasus2avro.mxe",
                     "type": "record"
                 }
             ]
         }
     ],
     "name": "MetadataChangeEvent",
     "namespace": "com.linkedin.pegasus2avro.mxe",
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/metadata/schemas/TrainingData.avsc` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/mysql_sample_dag.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,124 +1,121 @@
-00000000: 7b0a 2020 2274 7970 6522 3a20 2272 6563  {.  "type": "rec
-00000010: 6f72 6422 2c0a 2020 226e 616d 6522 3a20  ord",.  "name": 
-00000020: 2254 7261 696e 696e 6744 6174 6122 2c0a  "TrainingData",.
-00000030: 2020 226e 616d 6573 7061 6365 223a 2022    "namespace": "
-00000040: 636f 6d2e 6c69 6e6b 6564 696e 2e70 6567  com.linkedin.peg
-00000050: 6173 7573 3261 7672 6f2e 6d6c 2e6d 6574  asus2avro.ml.met
-00000060: 6164 6174 6122 2c0a 2020 2264 6f63 223a  adata",.  "doc":
-00000070: 2022 4964 6561 6c6c 792c 2074 6865 204d   "Ideally, the M
-00000080: 4c4d 6f64 656c 2063 6172 6420 776f 756c  LModel card woul
-00000090: 6420 636f 6e74 6169 6e20 6173 206d 7563  d contain as muc
-000000a0: 6820 696e 666f 726d 6174 696f 6e20 6162  h information ab
-000000b0: 6f75 7420 7468 6520 7472 6169 6e69 6e67  out the training
-000000c0: 2064 6174 6120 6173 2074 6865 2065 7661   data as the eva
-000000d0: 6c75 6174 696f 6e20 6461 7461 2e20 486f  luation data. Ho
-000000e0: 7765 7665 722c 2074 6865 7265 206d 6967  wever, there mig
-000000f0: 6874 2062 6520 6361 7365 7320 7768 6572  ht be cases wher
-00000100: 6520 6974 2069 7320 6e6f 7420 6665 6173  e it is not feas
-00000110: 6962 6c65 2074 6f20 7072 6f76 6964 6520  ible to provide 
-00000120: 7468 6973 206c 6576 656c 206f 6620 6465  this level of de
-00000130: 7461 696c 6564 2069 6e66 6f72 6d61 7469  tailed informati
-00000140: 6f6e 2061 626f 7574 2074 6865 2074 7261  on about the tra
-00000150: 696e 696e 6720 6461 7461 2e20 466f 7220  ining data. For 
-00000160: 6578 616d 706c 652c 2074 6865 2064 6174  example, the dat
-00000170: 6120 6d61 7920 6265 2070 726f 7072 6965  a may be proprie
-00000180: 7461 7279 2c20 6f72 2072 6571 7569 7265  tary, or require
-00000190: 2061 206e 6f6e 2d64 6973 636c 6f73 7572   a non-disclosur
-000001a0: 6520 6167 7265 656d 656e 742e 2049 6e20  e agreement. In 
-000001b0: 7468 6573 6520 6361 7365 732c 2077 6520  these cases, we 
-000001c0: 6164 766f 6361 7465 2066 6f72 2062 6173  advocate for bas
-000001d0: 6963 2064 6574 6169 6c73 2061 626f 7574  ic details about
-000001e0: 2074 6865 2064 6973 7472 6962 7574 696f   the distributio
-000001f0: 6e73 206f 7665 7220 6772 6f75 7073 2069  ns over groups i
-00000200: 6e20 7468 6520 6461 7461 2c20 6173 2077  n the data, as w
-00000210: 656c 6c20 6173 2061 6e79 206f 7468 6572  ell as any other
-00000220: 2064 6574 6169 6c73 2074 6861 7420 636f   details that co
-00000230: 756c 6420 696e 666f 726d 2073 7461 6b65  uld inform stake
-00000240: 686f 6c64 6572 7320 6f6e 2074 6865 206b  holders on the k
-00000250: 696e 6473 206f 6620 6269 6173 6573 2074  inds of biases t
-00000260: 6865 206d 6f64 656c 206d 6179 2068 6176  he model may hav
-00000270: 6520 656e 636f 6465 642e 222c 0a20 2022  e encoded.",.  "
-00000280: 6669 656c 6473 223a 205b 0a20 2020 207b  fields": [.    {
-00000290: 0a20 2020 2020 2022 6e61 6d65 223a 2022  .      "name": "
-000002a0: 7472 6169 6e69 6e67 4461 7461 222c 0a20  trainingData",. 
-000002b0: 2020 2020 2022 7479 7065 223a 207b 0a20       "type": {. 
-000002c0: 2020 2020 2020 2022 7479 7065 223a 2022         "type": "
-000002d0: 6172 7261 7922 2c0a 2020 2020 2020 2020  array",.        
-000002e0: 2269 7465 6d73 223a 207b 0a20 2020 2020  "items": {.     
-000002f0: 2020 2020 2022 7479 7065 223a 2022 7265       "type": "re
-00000300: 636f 7264 222c 0a20 2020 2020 2020 2020  cord",.         
-00000310: 2022 6e61 6d65 223a 2022 4261 7365 4461   "name": "BaseDa
-00000320: 7461 222c 0a20 2020 2020 2020 2020 2022  ta",.          "
-00000330: 646f 6322 3a20 2242 6173 6544 6174 6120  doc": "BaseData 
-00000340: 7265 636f 7264 222c 0a20 2020 2020 2020  record",.       
-00000350: 2020 2022 6669 656c 6473 223a 205b 0a20     "fields": [. 
-00000360: 2020 2020 2020 2020 2020 207b 0a20 2020             {.   
-00000370: 2020 2020 2020 2020 2020 2022 6e61 6d65             "name
-00000380: 223a 2022 6461 7461 7365 7422 2c0a 2020  ": "dataset",.  
-00000390: 2020 2020 2020 2020 2020 2020 2274 7970              "typ
-000003a0: 6522 3a20 2273 7472 696e 6722 2c0a 2020  e": "string",.  
-000003b0: 2020 2020 2020 2020 2020 2020 2264 6f63              "doc
-000003c0: 223a 2022 5768 6174 2064 6174 6173 6574  ": "What dataset
-000003d0: 2077 6572 6520 7573 6564 2069 6e20 7468   were used in th
-000003e0: 6520 4d4c 4d6f 6465 6c3f 222c 0a20 2020  e MLModel?",.   
-000003f0: 2020 2020 2020 2020 2020 2022 6a61 7661             "java
-00000400: 223a 207b 0a20 2020 2020 2020 2020 2020  ": {.           
-00000410: 2020 2020 2022 636c 6173 7322 3a20 2263       "class": "c
-00000420: 6f6d 2e6c 696e 6b65 6469 6e2e 7065 6761  om.linkedin.pega
-00000430: 7375 7332 6176 726f 2e63 6f6d 6d6f 6e2e  sus2avro.common.
-00000440: 7572 6e2e 4461 7461 7365 7455 726e 220a  urn.DatasetUrn".
-00000450: 2020 2020 2020 2020 2020 2020 2020 7d0a                }.
-00000460: 2020 2020 2020 2020 2020 2020 7d2c 0a20              },. 
-00000470: 2020 2020 2020 2020 2020 207b 0a20 2020             {.   
-00000480: 2020 2020 2020 2020 2020 2022 6e61 6d65             "name
-00000490: 223a 2022 6d6f 7469 7661 7469 6f6e 222c  ": "motivation",
-000004a0: 0a20 2020 2020 2020 2020 2020 2020 2022  .              "
-000004b0: 7479 7065 223a 205b 0a20 2020 2020 2020  type": [.       
-000004c0: 2020 2020 2020 2020 2022 6e75 6c6c 222c           "null",
-000004d0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-000004e0: 2022 7374 7269 6e67 220a 2020 2020 2020   "string".      
-000004f0: 2020 2020 2020 2020 5d2c 0a20 2020 2020          ],.     
-00000500: 2020 2020 2020 2020 2022 646f 6322 3a20           "doc": 
-00000510: 2257 6879 2077 6173 2074 6869 7320 6461  "Why was this da
-00000520: 7461 7365 7420 6368 6f73 656e 3f22 2c0a  taset chosen?",.
-00000530: 2020 2020 2020 2020 2020 2020 2020 2264                "d
-00000540: 6566 6175 6c74 223a 206e 756c 6c0a 2020  efault": null.  
-00000550: 2020 2020 2020 2020 2020 7d2c 0a20 2020            },.   
-00000560: 2020 2020 2020 2020 207b 0a20 2020 2020           {.     
-00000570: 2020 2020 2020 2020 2022 6e61 6d65 223a           "name":
-00000580: 2022 7072 6550 726f 6365 7373 696e 6722   "preProcessing"
-00000590: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-000005a0: 2274 7970 6522 3a20 5b0a 2020 2020 2020  "type": [.      
-000005b0: 2020 2020 2020 2020 2020 226e 756c 6c22            "null"
-000005c0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-000005d0: 2020 7b0a 2020 2020 2020 2020 2020 2020    {.            
-000005e0: 2020 2020 2020 2274 7970 6522 3a20 2261        "type": "a
-000005f0: 7272 6179 222c 0a20 2020 2020 2020 2020  rray",.         
-00000600: 2020 2020 2020 2020 2022 6974 656d 7322           "items"
-00000610: 3a20 2273 7472 696e 6722 0a20 2020 2020  : "string".     
-00000620: 2020 2020 2020 2020 2020 207d 0a20 2020             }.   
-00000630: 2020 2020 2020 2020 2020 205d 2c0a 2020             ],.  
-00000640: 2020 2020 2020 2020 2020 2020 2264 6f63              "doc
-00000650: 223a 2022 486f 7720 7761 7320 7468 6520  ": "How was the 
-00000660: 6461 7461 2070 7265 7072 6f63 6573 7365  data preprocesse
-00000670: 6420 2865 2e67 2e2c 2074 6f6b 656e 697a  d (e.g., tokeniz
-00000680: 6174 696f 6e20 6f66 2073 656e 7465 6e63  ation of sentenc
-00000690: 6573 2c20 6372 6f70 7069 6e67 206f 6620  es, cropping of 
-000006a0: 696d 6167 6573 2c20 616e 7920 6669 6c74  images, any filt
-000006b0: 6572 696e 6720 7375 6368 2061 7320 6472  ering such as dr
-000006c0: 6f70 7069 6e67 2069 6d61 6765 7320 7769  opping images wi
-000006d0: 7468 6f75 7420 6661 6365 7329 3f22 2c0a  thout faces)?",.
-000006e0: 2020 2020 2020 2020 2020 2020 2020 2264                "d
-000006f0: 6566 6175 6c74 223a 206e 756c 6c0a 2020  efault": null.  
-00000700: 2020 2020 2020 2020 2020 7d0a 2020 2020            }.    
-00000710: 2020 2020 2020 5d0a 2020 2020 2020 2020        ].        
-00000720: 7d0a 2020 2020 2020 7d2c 0a20 2020 2020  }.      },.     
-00000730: 2022 646f 6322 3a20 2244 6574 6169 6c73   "doc": "Details
-00000740: 206f 6e20 7468 6520 6461 7461 7365 7428   on the dataset(
-00000750: 7329 2075 7365 6420 666f 7220 7472 6169  s) used for trai
-00000760: 6e69 6e67 2074 6865 204d 4c4d 6f64 656c  ning the MLModel
-00000770: 220a 2020 2020 7d0a 2020 5d2c 0a20 2022  ".    }.  ],.  "
-00000780: 4173 7065 6374 223a 207b 0a20 2020 2022  Aspect": {.    "
-00000790: 6e61 6d65 223a 2022 6d6c 4d6f 6465 6c54  name": "mlModelT
-000007a0: 7261 696e 696e 6744 6174 6122 0a20 207d  rainingData".  }
-000007b0: 0a7d                                     .}
+00000000: 2222 224d 7953 514c 2044 6174 6148 7562  """MySQL DataHub
+00000010: 2049 6e67 6573 7420 4441 470a 0a54 6869   Ingest DAG..Thi
+00000020: 7320 6578 616d 706c 6520 6465 6d6f 6e73  s example demons
+00000030: 7472 6174 6573 2068 6f77 2074 6f20 696e  trates how to in
+00000040: 6765 7374 206d 6574 6164 6174 6120 6672  gest metadata fr
+00000050: 6f6d 204d 7953 514c 2069 6e74 6f20 4461  om MySQL into Da
+00000060: 7461 4875 620a 6672 6f6d 2077 6974 6869  taHub.from withi
+00000070: 6e20 616e 2041 6972 666c 6f77 2044 4147  n an Airflow DAG
+00000080: 2e20 4e6f 7465 2074 6861 7420 7468 6520  . Note that the 
+00000090: 4442 2063 6f6e 6e65 6374 696f 6e20 636f  DB connection co
+000000a0: 6e66 6967 7572 6174 696f 6e20 6973 0a65  nfiguration is.e
+000000b0: 6d62 6564 6465 6420 7769 7468 696e 2074  mbedded within t
+000000c0: 6865 2063 6f64 652e 0a22 2222 0a0a 6672  he code.."""..fr
+000000d0: 6f6d 2064 6174 6574 696d 6520 696d 706f  om datetime impo
+000000e0: 7274 2064 6174 6574 696d 652c 2074 696d  rt datetime, tim
+000000f0: 6564 656c 7461 0a0a 6672 6f6d 2061 6972  edelta..from air
+00000100: 666c 6f77 2069 6d70 6f72 7420 4441 470a  flow import DAG.
+00000110: 6672 6f6d 2061 6972 666c 6f77 2e6f 7065  from airflow.ope
+00000120: 7261 746f 7273 2e70 7974 686f 6e20 696d  rators.python im
+00000130: 706f 7274 2050 7974 686f 6e56 6972 7475  port PythonVirtu
+00000140: 616c 656e 764f 7065 7261 746f 720a 0a0a  alenvOperator...
+00000150: 6465 6620 696e 6765 7374 5f66 726f 6d5f  def ingest_from_
+00000160: 6d79 7371 6c28 293a 0a20 2020 2066 726f  mysql():.    fro
+00000170: 6d20 6461 7461 6875 622e 696e 6765 7374  m datahub.ingest
+00000180: 696f 6e2e 7275 6e2e 7069 7065 6c69 6e65  ion.run.pipeline
+00000190: 2069 6d70 6f72 7420 5069 7065 6c69 6e65   import Pipeline
+000001a0: 0a0a 2020 2020 7069 7065 6c69 6e65 203d  ..    pipeline =
+000001b0: 2050 6970 656c 696e 652e 6372 6561 7465   Pipeline.create
+000001c0: 280a 2020 2020 2020 2020 2320 5468 6973  (.        # This
+000001d0: 2063 6f6e 6669 6775 7261 7469 6f6e 2069   configuration i
+000001e0: 7320 616e 616c 6f67 6f75 7320 746f 2061  s analogous to a
+000001f0: 2072 6563 6970 6520 636f 6e66 6967 7572   recipe configur
+00000200: 6174 696f 6e2e 0a20 2020 2020 2020 207b  ation..        {
+00000210: 0a20 2020 2020 2020 2020 2020 2022 736f  .            "so
+00000220: 7572 6365 223a 207b 0a20 2020 2020 2020  urce": {.       
+00000230: 2020 2020 2020 2020 2022 7479 7065 223a           "type":
+00000240: 2022 6d79 7371 6c22 2c0a 2020 2020 2020   "mysql",.      
+00000250: 2020 2020 2020 2020 2020 2263 6f6e 6669            "confi
+00000260: 6722 3a20 7b0a 2020 2020 2020 2020 2020  g": {.          
+00000270: 2020 2020 2020 2020 2020 2320 4966 2079            # If y
+00000280: 6f75 2077 616e 7420 746f 2075 7365 2041  ou want to use A
+00000290: 6972 666c 6f77 2063 6f6e 6e65 6374 696f  irflow connectio
+000002a0: 6e73 2c20 7461 6b65 2061 206c 6f6f 6b20  ns, take a look 
+000002b0: 6174 2074 6865 2073 6e6f 7766 6c61 6b65  at the snowflake
+000002c0: 5f73 616d 706c 655f 6461 672e 7079 2065  _sample_dag.py e
+000002d0: 7861 6d70 6c65 2e0a 2020 2020 2020 2020  xample..        
+000002e0: 2020 2020 2020 2020 2020 2020 2275 7365              "use
+000002f0: 726e 616d 6522 3a20 2275 7365 7222 2c0a  rname": "user",.
+00000300: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000310: 2020 2020 2270 6173 7377 6f72 6422 3a20      "password": 
+00000320: 2270 6173 7322 2c0a 2020 2020 2020 2020  "pass",.        
+00000330: 2020 2020 2020 2020 2020 2020 2264 6174              "dat
+00000340: 6162 6173 6522 3a20 2264 625f 6e61 6d65  abase": "db_name
+00000350: 222c 0a20 2020 2020 2020 2020 2020 2020  ",.             
+00000360: 2020 2020 2020 2022 686f 7374 5f70 6f72         "host_por
+00000370: 7422 3a20 226c 6f63 616c 686f 7374 3a33  t": "localhost:3
+00000380: 3330 3622 2c0a 2020 2020 2020 2020 2020  306",.          
+00000390: 2020 2020 2020 7d2c 0a20 2020 2020 2020        },.       
+000003a0: 2020 2020 207d 2c0a 2020 2020 2020 2020       },.        
+000003b0: 2020 2020 2273 696e 6b22 3a20 7b0a 2020      "sink": {.  
+000003c0: 2020 2020 2020 2020 2020 2020 2020 2274                "t
+000003d0: 7970 6522 3a20 2264 6174 6168 7562 2d72  ype": "datahub-r
+000003e0: 6573 7422 2c0a 2020 2020 2020 2020 2020  est",.          
+000003f0: 2020 2020 2020 2263 6f6e 6669 6722 3a20        "config": 
+00000400: 7b22 7365 7276 6572 223a 2022 6874 7470  {"server": "http
+00000410: 3a2f 2f6c 6f63 616c 686f 7374 3a38 3038  ://localhost:808
+00000420: 3022 7d2c 0a20 2020 2020 2020 2020 2020  0"},.           
+00000430: 207d 2c0a 2020 2020 2020 2020 7d0a 2020   },.        }.  
+00000440: 2020 290a 2020 2020 7069 7065 6c69 6e65    ).    pipeline
+00000450: 2e72 756e 2829 0a20 2020 2070 6970 656c  .run().    pipel
+00000460: 696e 652e 7072 6574 7479 5f70 7269 6e74  ine.pretty_print
+00000470: 5f73 756d 6d61 7279 2829 0a20 2020 2070  _summary().    p
+00000480: 6970 656c 696e 652e 7261 6973 655f 6672  ipeline.raise_fr
+00000490: 6f6d 5f73 7461 7475 7328 290a 0a0a 7769  om_status()...wi
+000004a0: 7468 2044 4147 280a 2020 2020 2264 6174  th DAG(.    "dat
+000004b0: 6168 7562 5f6d 7973 716c 5f69 6e67 6573  ahub_mysql_inges
+000004c0: 7422 2c0a 2020 2020 6465 6661 756c 745f  t",.    default_
+000004d0: 6172 6773 3d7b 0a20 2020 2020 2020 2022  args={.        "
+000004e0: 6f77 6e65 7222 3a20 2261 6972 666c 6f77  owner": "airflow
+000004f0: 222c 0a20 2020 207d 2c0a 2020 2020 6465  ",.    },.    de
+00000500: 7363 7269 7074 696f 6e3d 2241 6e20 6578  scription="An ex
+00000510: 616d 706c 6520 4441 4720 7768 6963 6820  ample DAG which 
+00000520: 696e 6765 7374 7320 6d65 7461 6461 7461  ingests metadata
+00000530: 2066 726f 6d20 4d79 5351 4c20 746f 2044   from MySQL to D
+00000540: 6174 6148 7562 222c 0a20 2020 2073 7461  ataHub",.    sta
+00000550: 7274 5f64 6174 653d 6461 7465 7469 6d65  rt_date=datetime
+00000560: 2832 3032 322c 2031 2c20 3129 2c0a 2020  (2022, 1, 1),.  
+00000570: 2020 7363 6865 6475 6c65 5f69 6e74 6572    schedule_inter
+00000580: 7661 6c3d 7469 6d65 6465 6c74 6128 6461  val=timedelta(da
+00000590: 7973 3d31 292c 0a20 2020 2063 6174 6368  ys=1),.    catch
+000005a0: 7570 3d46 616c 7365 2c0a 2920 6173 2064  up=False,.) as d
+000005b0: 6167 3a0a 2020 2020 2320 5768 696c 6520  ag:.    # While 
+000005c0: 6974 2069 7320 616c 736f 2070 6f73 7369  it is also possi
+000005d0: 626c 6520 746f 2075 7365 2074 6865 2050  ble to use the P
+000005e0: 7974 686f 6e4f 7065 7261 746f 722c 2077  ythonOperator, w
+000005f0: 6520 7265 636f 6d6d 656e 6420 7573 696e  e recommend usin
+00000600: 670a 2020 2020 2320 7468 6520 5079 7468  g.    # the Pyth
+00000610: 6f6e 5669 7274 7561 6c65 6e76 4f70 6572  onVirtualenvOper
+00000620: 6174 6f72 2074 6f20 656e 7375 7265 2074  ator to ensure t
+00000630: 6861 7420 7468 6572 6520 6172 6520 6e6f  hat there are no
+00000640: 2064 6570 656e 6465 6e63 790a 2020 2020   dependency.    
+00000650: 2320 636f 6e66 6c69 6374 7320 6265 7477  # conflicts betw
+00000660: 6565 6e20 4461 7461 4875 6220 616e 6420  een DataHub and 
+00000670: 7468 6520 7265 7374 206f 6620 796f 7572  the rest of your
+00000680: 2041 6972 666c 6f77 2065 6e76 6972 6f6e   Airflow environ
+00000690: 6d65 6e74 2e0a 2020 2020 696e 6765 7374  ment..    ingest
+000006a0: 5f74 6173 6b20 3d20 5079 7468 6f6e 5669  _task = PythonVi
+000006b0: 7274 7561 6c65 6e76 4f70 6572 6174 6f72  rtualenvOperator
+000006c0: 280a 2020 2020 2020 2020 7461 736b 5f69  (.        task_i
+000006d0: 643d 2269 6e67 6573 745f 6672 6f6d 5f6d  d="ingest_from_m
+000006e0: 7973 716c 222c 0a20 2020 2020 2020 2072  ysql",.        r
+000006f0: 6571 7569 7265 6d65 6e74 733d 5b0a 2020  equirements=[.  
+00000700: 2020 2020 2020 2020 2020 2261 6372 796c            "acryl
+00000710: 2d64 6174 6168 7562 5b6d 7973 716c 5d22  -datahub[mysql]"
+00000720: 2c0a 2020 2020 2020 2020 5d2c 0a20 2020  ,.        ],.   
+00000730: 2020 2020 2073 7973 7465 6d5f 7369 7465       system_site
+00000740: 5f70 6163 6b61 6765 733d 4661 6c73 652c  _packages=False,
+00000750: 0a20 2020 2020 2020 2070 7974 686f 6e5f  .        python_
+00000760: 6361 6c6c 6162 6c65 3d69 6e67 6573 745f  callable=ingest_
+00000770: 6672 6f6d 5f6d 7973 716c 2c0a 2020 2020  from_mysql,.    
+00000780: 290a                                     ).
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/specific/dataset.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/specific/dataset.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,173 +1,227 @@
-from typing import List, Optional, Union
+from typing import Dict, Generic, List, Optional, TypeVar, Union
+from urllib.parse import quote
 
 from datahub.emitter.mcp_patch_builder import MetadataPatchProposal
 from datahub.metadata.schema_classes import (
+    DatasetPropertiesClass as DatasetProperties,
+    EditableDatasetPropertiesClass as EditableDatasetProperties,
+    EditableSchemaMetadataClass as EditableSchemaMetadata,
+    GlobalTagsClass as GlobalTags,
     GlossaryTermAssociationClass as Term,
+    GlossaryTermsClass as GlossaryTerms,
     KafkaAuditHeaderClass,
     OwnerClass as Owner,
+    OwnershipClass as Ownership,
     OwnershipTypeClass,
+    SchemaMetadataClass as SchemaMetadata,
     SystemMetadataClass,
     TagAssociationClass as Tag,
     UpstreamClass as Upstream,
+    UpstreamLineageClass as UpstreamLineage,
 )
+from datahub.specific.custom_properties import CustomPropertiesPatchHelper
 from datahub.utilities.urns.tag_urn import TagUrn
 from datahub.utilities.urns.urn import Urn
 
+T = TypeVar("T", bound=MetadataPatchProposal)
 
-class FieldPatchBuilder(MetadataPatchProposal):
+
+class FieldPatchHelper(Generic[T]):
     def __init__(
         self,
-        dataset_urn: str,
+        parent: T,
         field_path: str,
-        system_metadata: Optional[SystemMetadataClass] = None,
-        audit_header: Optional[KafkaAuditHeaderClass] = None,
         editable: bool = True,
     ) -> None:
-        super().__init__(
-            dataset_urn,
-            "dataset",
-            system_metadata=system_metadata,
-            audit_header=audit_header,
-        )
+        self._parent: T = parent
         self.field_path = field_path
-        self.aspect_name = "editableSchemaMetadata" if editable else "schemaMetadata"
+        self.aspect_name = (
+            EditableSchemaMetadata.ASPECT_NAME
+            if editable
+            else SchemaMetadata.ASPECT_NAME
+        )
         self.aspect_field = "editableSchemaFieldInfo" if editable else "schemaFieldInfo"
 
-    def add_tag(self, tag: Tag) -> "FieldPatchBuilder":
-        self._add_patch(
+    def add_tag(self, tag: Tag) -> "FieldPatchHelper":
+        self._parent._add_patch(
             self.aspect_name,
             "add",
             path=f"/{self.aspect_field}/{self.field_path}/globalTags/tags/{tag.tag}",
             value=tag,
         )
         return self
 
-    def remove_tag(self, tag: Union[str, Urn]) -> "FieldPatchBuilder":
+    def remove_tag(self, tag: Union[str, Urn]) -> "FieldPatchHelper":
         if isinstance(tag, str) and not tag.startswith("urn:li:tag:"):
             tag = TagUrn.create_from_id(tag)
-        self._add_patch(
+        self._parent._add_patch(
             self.aspect_name,
             "remove",
             path=f"/{self.aspect_field}/{self.field_path}/globalTags/tags/{tag}",
             value={},
         )
         return self
 
-    def add_term(self, term: Term) -> "FieldPatchBuilder":
-        self._add_patch(
+    def add_term(self, term: Term) -> "FieldPatchHelper":
+        self._parent._add_patch(
             self.aspect_name,
             "add",
             path=f"/{self.aspect_field}/{self.field_path}/glossaryTerms/terms/{term.urn}",
             value=term,
         )
         return self
 
-    def remove_term(self, term: Union[str, Urn]) -> "FieldPatchBuilder":
+    def remove_term(self, term: Union[str, Urn]) -> "FieldPatchHelper":
         if isinstance(term, str) and not term.startswith("urn:li:glossaryTerm:"):
             term = "urn:li:glossaryTerm:" + term
-        self._add_patch(
+        self._parent._add_patch(
             self.aspect_name,
             "remove",
             path=f"/{self.aspect_field}/{self.field_path}/glossaryTerms/terms/{term}",
             value={},
         )
         return self
 
+    def parent(self) -> T:
+        return self._parent
+
 
 class DatasetPatchBuilder(MetadataPatchProposal):
     def __init__(
         self,
         urn: str,
         system_metadata: Optional[SystemMetadataClass] = None,
         audit_header: Optional[KafkaAuditHeaderClass] = None,
     ) -> None:
         super().__init__(
             urn, "dataset", system_metadata=system_metadata, audit_header=audit_header
         )
+        self.custom_properties_patch_helper = CustomPropertiesPatchHelper(
+            self, DatasetProperties.ASPECT_NAME
+        )
 
     def add_owner(self, owner: Owner) -> "DatasetPatchBuilder":
         self._add_patch(
-            "ownership", "add", path=f"/owners/{owner.owner}/{owner.type}", value=owner
+            Ownership.ASPECT_NAME,
+            "add",
+            path=f"/owners/{owner.owner}/{owner.type}",
+            value=owner,
         )
         return self
 
     def remove_owner(
         self, owner: Urn, owner_type: Optional[OwnershipTypeClass] = None
     ) -> "DatasetPatchBuilder":
         """
         param: owner_type is optional
         """
         self._add_patch(
-            "ownership",
+            Ownership.ASPECT_NAME,
             "remove",
             path=f"/owners/{owner}" + (f"/{owner_type}" if owner_type else ""),
             value=owner,
         )
         return self
 
     def set_owners(self, owners: List[Owner]) -> "DatasetPatchBuilder":
-        self._add_patch("ownership", "replace", path="/owners", value=owners)
+        self._add_patch(Ownership.ASPECT_NAME, "replace", path="/owners", value=owners)
         return self
 
     def add_upstream_lineage(self, upstream: Upstream) -> "DatasetPatchBuilder":
         self._add_patch(
-            "upstreamLineage",
+            UpstreamLineage.ASPECT_NAME,
             "add",
-            path=f"/upstreams/{upstream.dataset}",
+            path=f"/upstreams/{quote(upstream.dataset, safe='')}",
             value=upstream,
         )
         return self
 
     def remove_upstream_lineage(
         self, dataset: Union[str, Urn]
     ) -> "DatasetPatchBuilder":
         self._add_patch(
-            "upstreamLineage",
+            UpstreamLineage.ASPECT_NAME,
             "remove",
             path=f"/upstreams/{dataset}",
             value={},
         )
         return self
 
     def set_upstream_lineages(self, upstreams: List[Upstream]) -> "DatasetPatchBuilder":
         self._add_patch(
-            "upstreamLineage", "replace", path="/upstreams", value=upstreams
+            UpstreamLineage.ASPECT_NAME, "replace", path="/upstreams", value=upstreams
         )
         return self
 
     def add_tag(self, tag: Tag) -> "DatasetPatchBuilder":
-        self._add_patch("globalTags", "add", path=f"/tags/{tag.tag}", value=tag)
+        self._add_patch(
+            GlobalTags.ASPECT_NAME, "add", path=f"/tags/{tag.tag}", value=tag
+        )
         return self
 
     def remove_tag(self, tag: Union[str, Urn]) -> "DatasetPatchBuilder":
         if isinstance(tag, str) and not tag.startswith("urn:li:tag:"):
             tag = TagUrn.create_from_id(tag)
-        self._add_patch("globalTags", "remove", path=f"/tags/{tag}", value={})
+        self._add_patch(GlobalTags.ASPECT_NAME, "remove", path=f"/tags/{tag}", value={})
         return self
 
     def add_term(self, term: Term) -> "DatasetPatchBuilder":
-        self._add_patch("glossaryTerms", "add", path=f"/terms/{term.urn}", value=term)
+        self._add_patch(
+            GlossaryTerms.ASPECT_NAME, "add", path=f"/terms/{term.urn}", value=term
+        )
         return self
 
     def remove_term(self, term: Union[str, Urn]) -> "DatasetPatchBuilder":
         if isinstance(term, str) and not term.startswith("urn:li:glossaryTerm:"):
             term = "urn:li:glossaryTerm:" + term
-        self._add_patch("glossaryTerms", "remove", path=f"/terms/{term}", value={})
+        self._add_patch(
+            GlossaryTerms.ASPECT_NAME, "remove", path=f"/terms/{term}", value={}
+        )
         return self
 
-    def field_patch_builder(
+    def for_field(
         self, field_path: str, editable: bool = True
-    ) -> "FieldPatchBuilder":
+    ) -> FieldPatchHelper["DatasetPatchBuilder"]:
         """
-        Get a builder that can perform patches against fields in the dataset
+        Get a helper that can perform patches against fields in the dataset
 
         :param field_path: The field path in datahub format
         :param editable: Whether patches should apply to the editable section of schema metadata or not
         """
-        return FieldPatchBuilder(
-            self.urn,
+        return FieldPatchHelper(
+            self,
             field_path,
-            system_metadata=self.system_metadata,
-            audit_header=self.audit_header,
             editable=editable,
         )
+
+    def set_description(
+        self, description: str, editable: bool = False
+    ) -> "DatasetPatchBuilder":
+        self._add_patch(
+            DatasetProperties.ASPECT_NAME
+            if not editable
+            else EditableDatasetProperties.ASPECT_NAME,
+            "replace",
+            path="/description",
+            value=description,
+        )
+        return self
+
+    def set_custom_properties(
+        self, custom_properties: Dict[str, str]
+    ) -> "DatasetPatchBuilder":
+        self._add_patch(
+            DatasetProperties.ASPECT_NAME,
+            "replace",
+            path="/customProperties",
+            value=custom_properties,
+        )
+        return self
+
+    def add_custom_property(self, key: str, value: str) -> "DatasetPatchBuilder":
+        self.custom_properties_patch_helper.add_property(key, value)
+        return self
+
+    def remove_custom_property(self, key: str) -> "DatasetPatchBuilder":
+        self.custom_properties_patch_helper.remove_property(key)
+        return self
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/telemetry/stats.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/telemetry/stats.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/telemetry/telemetry.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/telemetry/telemetry.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/upgrade/upgrade.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/upgrade/upgrade.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,15 +3,14 @@
 import functools
 import logging
 import sys
 from datetime import datetime, timedelta, timezone
 from functools import wraps
 from typing import Any, Callable, Optional, Tuple, TypeVar
 
-import aiohttp
 import humanfriendly
 from packaging.version import Version
 from pydantic import BaseModel
 from termcolor import colored
 
 from datahub import __version__
 from datahub.cli import cli_utils
@@ -41,14 +40,16 @@
 
 class DataHubVersionStats(BaseModel):
     server: ServerVersionStats
     client: ClientVersionStats
 
 
 async def get_client_version_stats():
+    import aiohttp
+
     current_version_string = __version__
     current_version = Version(current_version_string)
     client_version_stats: ClientVersionStats = ClientVersionStats(
         current=VersionStats(version=current_version, release_date=None), latest=None
     )
     async with aiohttp.ClientSession() as session:
         pypi_url = "https://pypi.org/pypi/acryl_datahub/json"
@@ -84,26 +85,30 @@
             except Exception as e:
                 log.debug(f"Failed to determine cli releases from pypi due to {e}")
                 pass
     return client_version_stats
 
 
 async def get_github_stats():
+    import aiohttp
+
     async with aiohttp.ClientSession(
         headers={"Accept": "application/vnd.github.v3+json"}
     ) as session:
         gh_url = "https://api.github.com/repos/datahub-project/datahub/releases"
         async with session.get(gh_url) as gh_response:
             gh_response_json = await gh_response.json()
             latest_server_version = Version(gh_response_json[0].get("tag_name"))
             latest_server_date = gh_response_json[0].get("published_at")
             return (latest_server_version, latest_server_date)
 
 
 async def get_server_config(gms_url: str, token: str) -> dict:
+    import aiohttp
+
     async with aiohttp.ClientSession(
         headers={
             "X-RestLi-Protocol-Version": "2.0.0",
             "Content-Type": "application/json",
             "Authorization": f"Bearer {token}",
         }
     ) as session:
@@ -112,14 +117,16 @@
             dh_response_json = await dh_response.json()
             return dh_response_json
 
 
 async def get_server_version_stats(
     server: Optional[DataHubGraph] = None,
 ) -> Tuple[Optional[str], Optional[Version], Optional[datetime]]:
+    import aiohttp
+
     server_config = None
     if not server:
         try:
             # let's get the server from the cli config
             host, token = cli_utils.get_url_and_token()
             server_config = await get_server_config(host, token)
             log.debug(f"server_config:{server_config}")
@@ -219,36 +226,41 @@
     if version.major == 0 and version.minor in [8, 9, 10, 11]:
         return True
 
     return False
 
 
 def valid_server_version(version: Version) -> bool:
-    """Only version strings like 0.8.x or 0.9.x are valid. 0.1.x is not"""
+    """Only version strings like 0.8.x, 0.9.x or 0.10.x are valid. 0.1.x is not"""
     if version.is_prerelease or version.is_postrelease or version.is_devrelease:
         return False
 
-    if version.major == 0 and version.minor in [8, 9]:
+    if version.major == 0 and version.minor in [8, 9, 10]:
         return True
 
     return False
 
 
 def is_client_server_compatible(client: VersionStats, server: VersionStats) -> int:
     """
-    -ve implies client is behind server
+    -ve implies server is behind client
     0 implies client and server are aligned
     +ve implies server is ahead of client
     """
     if not valid_client_version(client.version) or not valid_server_version(
         server.version
     ):
         # we cannot evaluate compatibility, choose True as default
         return 0
-    return server.version.micro - client.version.micro
+    if server.version.major != client.version.major:
+        return server.version.major - client.version.major
+    elif server.version.minor != client.version.minor:
+        return server.version.minor - client.version.minor
+    else:
+        return server.version.micro - client.version.micro
 
 
 def maybe_print_upgrade_message(  # noqa: C901
     version_stats: Optional[DataHubVersionStats],
 ) -> None:  # noqa: C901
     days_before_cli_stale = 7
     days_before_quickstart_stale = 7
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/bigquery_sql_parser.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/bigquery_sql_parser.py`

 * *Files 1% similar despite different names*

```diff
@@ -75,15 +75,15 @@
     @staticmethod
     def _escape_object_name_after_keyword_from(sql_query: str) -> str:
         """
         Reason: in case table name contains hyphens which breaks sqllineage later on
         Note: ignore cases of having keyword FROM as part of datetime function EXTRACT
         """
         return re.sub(
-            r"(?<!day\s)(?<!(date|time|hour|week|year)\s)(?<!month\s)(?<!(second|minute)\s)(?<!quarter\s)(?<!\.)(from\s)([^`\s()]+)",
+            r"(?<!day\s)(?<!(date|time|hour|week|year)\s)(?<!month\s)(?<!(second|minute)\s)(?<!quarter\s)(?<!\.)(from\s)([^`\s();]+)",
             r"\3`\4`",
             sql_query,
             flags=re.IGNORECASE,
         )
 
     def get_tables(self) -> List[str]:
         return self.parser.get_tables()
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/checkpoint_state_util.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/checkpoint_state_util.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/delayed_iter.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/delayed_iter.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/hive_schema_to_avro.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/hive_schema_to_avro.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/logging_manager.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/logging_manager.py`

 * *Files 5% similar despite different names*

```diff
@@ -107,15 +107,19 @@
 
 class _BufferLogHandler(logging.Handler):
     def __init__(self, storage: _LogBuffer) -> None:
         super().__init__()
         self._storage = storage
 
     def emit(self, record: logging.LogRecord) -> None:
-        self._storage.write(self.format(record))
+        try:
+            message = self.format(record)
+        except TypeError as e:
+            message = f"Error formatting log message: {e}\nMessage: {record.msg}, Args: {record.args}"
+        self._storage.write(message)
 
 
 def _remove_all_handlers(logger: logging.Logger) -> None:
     for handler in logger.handlers[:]:
         logger.removeHandler(handler)
         handler.close()
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/lossy_collections.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/lossy_collections.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/mapping.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/mapping.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,20 +1,44 @@
+import contextlib
 import logging
 import re
 from typing import Any, Dict, List, Match, Optional, Union
 
 from datahub.emitter import mce_builder
 from datahub.emitter.mce_builder import OwnerType
 from datahub.metadata.schema_classes import (
     OwnerClass,
     OwnershipClass,
     OwnershipSourceClass,
     OwnershipTypeClass,
 )
 
+logger = logging.getLogger(__name__)
+
+
+def _get_best_match(the_match: Match, group_name: str) -> str:
+    with contextlib.suppress(IndexError):
+        return the_match.group(group_name)
+
+    with contextlib.suppress(IndexError):
+        return the_match.group(1)
+
+    return the_match.group(0)
+
+
+_match_regexp = re.compile(r"{{\s*\$match\s*}}", flags=re.MULTILINE)
+
+
+def _insert_match_value(original_value: str, match_value: str) -> str:
+    """
+    If the original value is something like "foo{{ $match }}bar", then we insert the match value
+    e.g. "foo<match_value>bar". Otherwise, it will leave the original value unchanged.
+    """
+    return _match_regexp.sub(match_value, original_value)
+
 
 class Constants:
     ADD_TAG_OPERATION = "add_tag"
     ADD_TERM_OPERATION = "add_term"
     ADD_TERMS_OPERATION = "add_terms"
     ADD_OWNER_OPERATION = "add_owner"
     OPERATION = "operation"
@@ -24,14 +48,16 @@
     OWNER_TYPE = "owner_type"
     OWNER_CATEGORY = "owner_category"
     MATCH = "match"
     USER_OWNER = "user"
     GROUP_OWNER = "group"
     OPERAND_DATATYPE_SUPPORTED = [int, bool, str, float]
     TAG_PARTITION_KEY = "PARTITION_KEY"
+    TAG_DIST_KEY = "DIST_KEY"
+    TAG_SORT_KEY = "SORT_KEY"
     SEPARATOR = "separator"
 
 
 class OperationProcessor:
     """
     A general class that processes a dictionary of properties and operations defined on it.
     An action is defined over a property with a specific match condition. If the condition is satisfied then the
@@ -56,15 +82,14 @@
         "has_pii": true,
       }
     If the match clause of both operations are satisfied on the raw properties a tag and a term aspect
     will be returned for further processing.
     """
 
     operation_defs: Dict[str, Dict] = {}
-    logger = logging.getLogger(__name__)
     tag_prefix: str = ""
 
     def __init__(
         self,
         operation_defs: Dict[str, Dict],
         tag_prefix: str = "",
         owner_source_type: Optional[str] = None,
@@ -124,15 +149,15 @@
                                 operation_type, list()
                             )
                             operations_value_list.append(operation)  # type: ignore
                             operations_map[operation_type] = operations_value_list
 
             aspect_map = self.convert_to_aspects(operations_map)
         except Exception as e:
-            self.logger.error("Error while processing operation defs over raw_props", e)
+            logger.error(f"Error while processing operation defs over raw_props: {e}")
         return aspect_map
 
     def convert_to_aspects(
         self, operation_map: Dict[str, Union[set, list]]
     ) -> Dict[str, Any]:
         aspect_map: Dict[str, Any] = {}
         if Constants.ADD_TAG_OPERATION in operation_map:
@@ -167,38 +192,20 @@
     def get_operation_value(
         self,
         operation_key: str,
         operation_type: str,
         operation_config: Dict,
         match: Match,
     ) -> Optional[Union[str, Dict, List[str]]]:
-        def _get_best_match(the_match: Match, group_name: str) -> str:
-            result = the_match.group(0)
-            try:
-                result = the_match.group(group_name)
-                return result
-            except IndexError:
-                pass
-            try:
-                result = the_match.group(1)
-                return result
-            except IndexError:
-                pass
-            return result
-
-        match_regexp = r"{{\s*\$match\s*}}"
-
         if (
             operation_type == Constants.ADD_TAG_OPERATION
             and operation_config[Constants.TAG]
         ):
             tag = operation_config[Constants.TAG]
-            tag_id = _get_best_match(match, "tag")
-            if isinstance(tag_id, str):
-                tag = re.sub(match_regexp, tag_id, tag, 0, re.MULTILINE)
+            tag = _insert_match_value(tag, _get_best_match(match, "tag"))
 
             if self.tag_prefix:
                 tag = self.tag_prefix + tag
             return tag
         elif (
             operation_type == Constants.ADD_OWNER_OPERATION
             and operation_config[Constants.OWNER_TYPE]
@@ -222,17 +229,15 @@
                     "category": owner_category,
                 }
         elif (
             operation_type == Constants.ADD_TERM_OPERATION
             and operation_config[Constants.TERM]
         ):
             term = operation_config[Constants.TERM]
-            captured_term_id = _get_best_match(match, "term")
-            if isinstance(captured_term_id, str):
-                term = re.sub(match_regexp, captured_term_id, term, 0, re.MULTILINE)
+            term = _insert_match_value(term, _get_best_match(match, "term"))
             return mce_builder.make_term_urn(term)
         elif operation_type == Constants.ADD_TERMS_OPERATION:
             separator = operation_config.get(Constants.SEPARATOR, ",")
             captured_terms = match.group(0)
             return [
                 mce_builder.make_term_urn(term.strip())
                 for term in captured_terms.split(separator)
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/memory_footprint.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/memory_footprint.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/memory_leak_detector.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/memory_leak_detector.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/parsing_util.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/parsing_util.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/perf_timer.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/perf_timer.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/registries/domain_registry.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/registries/domain_registry.py`

 * *Files 1% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 
 
 class DomainRegistry:
     """A class that makes it easy to resolve domains using DataHub"""
 
     def __init__(
         self,
-        cached_domains: Optional[List[str]] = [],
+        cached_domains: Optional[List[str]] = None,
         graph: Optional[DataHubGraph] = None,
     ):
         self.domain_registry = {}
         if cached_domains:
             # isolate the domains that don't seem fully specified
             domains_needing_resolution = [
                 d
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/sample_data.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sample_data.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/server_config_util.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/server_config_util.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/source_helpers.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/source_helpers.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,26 +1,32 @@
-from typing import Callable, Iterable, Optional, Set, Union
+from typing import Callable, Iterable, Optional, Set, TypeVar, Union
 
 from datahub.emitter.mcp import MetadataChangeProposalWrapper
+from datahub.ingestion.api.common import WorkUnit
 from datahub.ingestion.api.source import SourceReport
 from datahub.ingestion.api.workunit import MetadataWorkUnit
 from datahub.ingestion.source.state.stale_entity_removal_handler import (
     StaleEntityRemovalHandler,
 )
 from datahub.metadata.schema_classes import (
     MetadataChangeEventClass,
     MetadataChangeProposalClass,
     StatusClass,
+    TagKeyClass,
 )
+from datahub.utilities.urns.tag_urn import TagUrn
 from datahub.utilities.urns.urn import guess_entity_type
+from datahub.utilities.urns.urn_iter import list_urns
 
 
 def auto_workunit(
     stream: Iterable[Union[MetadataChangeEventClass, MetadataChangeProposalWrapper]]
 ) -> Iterable[MetadataWorkUnit]:
+    """Convert a stream of MCEs and MCPs to a stream of :class:`MetadataWorkUnit`s."""
+
     for item in stream:
         if isinstance(item, MetadataChangeEventClass):
             yield MetadataWorkUnit(id=f"{item.proposedSnapshot.urn}/mce", mce=item)
         else:
             yield item.as_workunit()
 
 
@@ -94,18 +100,51 @@
 
         yield wu
 
     # Clean up stale entities.
     yield from stale_entity_removal_handler.gen_removed_entity_workunits()
 
 
-def auto_workunit_reporter(
-    report: SourceReport,
-    stream: Iterable[MetadataWorkUnit],
-) -> Iterable[MetadataWorkUnit]:
+T = TypeVar("T", bound=WorkUnit)
+
+
+def auto_workunit_reporter(report: SourceReport, stream: Iterable[T]) -> Iterable[T]:
     """
     Calls report.report_workunit() on each workunit.
     """
 
     for wu in stream:
         report.report_workunit(wu)
         yield wu
+
+
+def auto_materialize_referenced_tags(
+    stream: Iterable[MetadataWorkUnit],
+    active: bool = True,
+) -> Iterable[MetadataWorkUnit]:
+    """For all references to tags, emit a tag key aspect to ensure that the tag exists in our backend."""
+
+    if not active:
+        yield from stream
+        return
+
+    referenced_tags = set()
+    tags_with_aspects = set()
+
+    for wu in stream:
+        for urn in list_urns(wu.metadata):
+            if guess_entity_type(urn) == "tag":
+                referenced_tags.add(urn)
+
+        urn = wu.get_urn()
+        if guess_entity_type(urn) == "tag":
+            tags_with_aspects.add(urn)
+
+        yield wu
+
+    for urn in sorted(referenced_tags - tags_with_aspects):
+        tag_urn = TagUrn.create_from_string(urn)
+
+        yield MetadataChangeProposalWrapper(
+            entityUrn=urn,
+            aspect=TagKeyClass(name=tag_urn.get_entity_id()[0]),
+        ).as_workunit()
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/sql_formatter.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sql_formatter.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/sql_lineage_parser_impl.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sql_lineage_parser_impl.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/sql_parser.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sql_parser.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 import contextlib
 import logging
 import multiprocessing
 import re
-import sys
 import traceback
 from multiprocessing import Process, Queue
-from typing import Any, List, Optional, Tuple, Type
+from typing import Any, List, Optional, Tuple
 
 from datahub.utilities.sql_lineage_parser_impl import SqlLineageSQLParserImpl
 from datahub.utilities.sql_parser_base import SQLParser
 
 with contextlib.suppress(ImportError):
     from sql_metadata import Parser as MetadataSQLParser
 logger = logging.getLogger(__name__)
@@ -76,32 +75,31 @@
     functionality in a separate process, and hence protect our sources from memory leaks originating in
     the sqllineage module.
     :param queue: The shared IPC queue on to which the results will be put.
     :param sql_query: The SQL query to extract the tables & columns from.
     :param use_raw_names: Parameter used to ignore sqllineage's default lowercasing.
     :return: None.
     """
-    exception_details: Optional[Tuple[Optional[Type[BaseException]], str]] = None
+    exception_details: Optional[Tuple[BaseException, str]] = None
     tables: List[str] = []
     columns: List[str] = []
     try:
         parser = SqlLineageSQLParserImpl(sql_query, use_raw_names)
         tables = parser.get_tables()
         columns = parser.get_columns()
-    except BaseException:
-        exc_info = sys.exc_info()
-        exc_msg: str = str(exc_info[1]) + "".join(traceback.format_tb(exc_info[2]))
-        exception_details = (exc_info[0], exc_msg)
+    except BaseException as e:
+        exc_msg = traceback.format_exc()
+        exception_details = (e, exc_msg)
         logger.debug(exc_msg)
-    finally:
-        if queue is not None:
-            queue.put((tables, columns, exception_details))
-            return None
-        else:
-            return (tables, columns, exception_details)
+
+    if queue is not None:
+        queue.put((tables, columns, exception_details))
+        return None
+    else:
+        return (tables, columns, exception_details)
 
 
 class SqlLineageSQLParser(SQLParser):
     def __init__(
         self,
         sql_query: str,
         use_external_process: bool = True,
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/sqlalchemy_query_combiner.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sqlalchemy_query_combiner.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/sqllineage_patch.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/sqllineage_patch.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/tee_io.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/tee_io.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/type_annotations.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/type_annotations.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urn_encoder.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urn_encoder.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/corp_group_urn.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/corp_group_urn.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/corpuser_urn.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/corpuser_urn.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/data_flow_urn.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/data_flow_urn.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import List
+from typing import List, Optional
 
 from datahub.configuration.source_common import ALL_ENV_TYPES
 from datahub.utilities.urns.error import InvalidUrnError
 from datahub.utilities.urns.urn import Urn
 
 
 class DataFlowUrn(Urn):
@@ -45,21 +45,29 @@
         """
         :return: the environment where the DataFlow is run
         """
         return self.get_entity_id()[2]
 
     @classmethod
     def create_from_ids(
-        cls, orchestrator: str, flow_id: str, env: str
+        cls,
+        orchestrator: str,
+        flow_id: str,
+        env: str,
+        platform_instance: Optional[str] = None,
     ) -> "DataFlowUrn":
-        entity_id: List[str] = [
-            orchestrator,
-            flow_id,
-            env,
-        ]
+        entity_id: List[str]
+        if platform_instance:
+            entity_id = [
+                orchestrator,
+                f"{platform_instance}.{flow_id}",
+                env,
+            ]
+        else:
+            entity_id = [orchestrator, flow_id, env]
         return cls(DataFlowUrn.ENTITY_TYPE, entity_id)
 
     @staticmethod
     def _validate_entity_type(entity_type: str) -> None:
         if entity_type != DataFlowUrn.ENTITY_TYPE:
             raise InvalidUrnError(
                 f"Entity type should be {DataFlowUrn.ENTITY_TYPE} but found {entity_type}"
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/data_job_urn.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/data_job_urn.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/data_platform_urn.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/data_platform_urn.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/data_process_instance_urn.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/data_process_instance_urn.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/dataset_urn.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/dataset_urn.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/domain_urn.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/domain_urn.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/notebook_urn.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/notebook_urn.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/tag_urn.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/tag_urn.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub/utilities/urns/urn.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub/utilities/urns/urn.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub_provider/__init__.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/__init__.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub_provider/_airflow_shims.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/_airflow_shims.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub_provider/_lineage_core.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/_lineage_core.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub_provider/_plugin.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/_plugin.py`

 * *Files 2% similar despite different names*

```diff
@@ -331,15 +331,16 @@
     if policy and hasattr(policy, "_task_policy_patched_by"):
         return policy
 
     def custom_task_policy(task):
         policy(task)
         task_policy(task)
 
-    setattr(custom_task_policy, "_task_policy_patched_by", "datahub_plugin")
+    # Add a flag to the policy to indicate that we've patched it.
+    custom_task_policy._task_policy_patched_by = "datahub_plugin"  # type: ignore[attr-defined]
     return custom_task_policy
 
 
 def _patch_policy(settings):
     if hasattr(settings, "task_policy"):
         datahub_task_policy = _wrap_task_policy(settings.task_policy)
         settings.task_policy = datahub_task_policy
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub_provider/client/airflow_generator.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/client/airflow_generator.py`

 * *Files 1% similar despite different names*

```diff
@@ -74,17 +74,15 @@
             and len(task.upstream_task_ids) == 0
         ):
             # filter through the parent dag's tasks and find the subdag trigger(s)
             subdags = [
                 x for x in dag.parent_dag.task_dict.values() if x.subdag is not None
             ]
             matched_subdags = [
-                x
-                for x in subdags
-                if getattr(getattr(x, "subdag"), "dag_id") == dag.dag_id
+                x for x in subdags if x.subdag and x.subdag.dag_id == dag.dag_id
             ]
 
             # id of the task containing the subdag
             subdag_task_id = matched_subdags[0].task_id
 
             # iterate through the parent dag's tasks and find the ones that trigger the subdag
             for upstream_task_id in dag.parent_dag.task_dict:
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub_provider/entities.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/entities.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub_provider/example_dags/generic_recipe_sample_dag.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/generic_recipe_sample_dag.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub_provider/example_dags/lineage_backend_demo.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/lineage_backend_demo.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub_provider/example_dags/lineage_backend_taskflow_demo.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/lineage_backend_taskflow_demo.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub_provider/example_dags/lineage_emission_dag.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/lineage_emission_dag.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub_provider/example_dags/mysql_sample_dag.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/example_dags/snowflake_sample_dag.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,61 +1,87 @@
-"""MySQL DataHub Ingest DAG
+"""Snowflake DataHub Ingest DAG
 
-This example demonstrates how to ingest metadata from MySQL into DataHub
-from within an Airflow DAG. Note that the DB connection configuration is
-embedded within the code.
+This example demonstrates how to ingest metadata from Snowflake into DataHub
+from within an Airflow DAG. In contrast to the MySQL example, this DAG
+pulls the DB connection configuration from Airflow's connection store.
 """
 
 from datetime import datetime, timedelta
 
 from airflow import DAG
+from airflow.hooks.base import BaseHook
 from airflow.operators.python import PythonVirtualenvOperator
 
 
-def ingest_from_mysql():
+def ingest_from_snowflake(snowflake_credentials, datahub_gms_server):
     from datahub.ingestion.run.pipeline import Pipeline
 
     pipeline = Pipeline.create(
         # This configuration is analogous to a recipe configuration.
         {
             "source": {
-                "type": "mysql",
+                "type": "snowflake",
                 "config": {
-                    # If you want to use Airflow connections, take a look at the snowflake_sample_dag.py example.
-                    "username": "user",
-                    "password": "pass",
-                    "database": "db_name",
-                    "host_port": "localhost:3306",
+                    **snowflake_credentials,
+                    # Other Snowflake config can be added here.
+                    "profiling": {"enabled": False},
                 },
             },
+            # Other ingestion features, like transformers, are also supported.
+            # "transformers": [
+            #     {
+            #         "type": "simple_add_dataset_ownership",
+            #         "config": {
+            #             "owner_urns": [
+            #                 "urn:li:corpuser:example",
+            #             ]
+            #         },
+            #     }
+            # ],
             "sink": {
                 "type": "datahub-rest",
-                "config": {"server": "http://localhost:8080"},
+                "config": {"server": datahub_gms_server},
             },
         }
     )
     pipeline.run()
     pipeline.pretty_print_summary()
     pipeline.raise_from_status()
 
 
 with DAG(
-    "datahub_mysql_ingest",
+    "datahub_snowflake_ingest",
     default_args={
         "owner": "airflow",
     },
-    description="An example DAG which ingests metadata from MySQL to DataHub",
+    description="An example DAG which ingests metadata from Snowflake to DataHub",
     start_date=datetime(2022, 1, 1),
     schedule_interval=timedelta(days=1),
     catchup=False,
 ) as dag:
+    # This example pulls credentials from Airflow's connection store.
+    # For this to work, you must have previously configured these connections in Airflow.
+    # See the Airflow docs for details: https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html
+    snowflake_conn = BaseHook.get_connection("snowflake_admin_default")
+    datahub_conn = BaseHook.get_connection("datahub_rest_default")
+
     # While it is also possible to use the PythonOperator, we recommend using
     # the PythonVirtualenvOperator to ensure that there are no dependency
     # conflicts between DataHub and the rest of your Airflow environment.
     ingest_task = PythonVirtualenvOperator(
-        task_id="ingest_from_mysql",
+        task_id="ingest_from_snowflake",
         requirements=[
-            "acryl-datahub[mysql]",
+            "acryl-datahub[snowflake]",
         ],
         system_site_packages=False,
-        python_callable=ingest_from_mysql,
+        python_callable=ingest_from_snowflake,
+        op_kwargs={
+            "snowflake_credentials": {
+                "username": snowflake_conn.login,
+                "password": snowflake_conn.password,
+                "account_id": snowflake_conn.extra_dejson["account"],
+                "warehouse": snowflake_conn.extra_dejson.get("warehouse"),
+                "role": snowflake_conn.extra_dejson.get("role"),
+            },
+            "datahub_gms_server": datahub_conn.host,
+        },
     )
```

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub_provider/hooks/datahub.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/hooks/datahub.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub_provider/lineage/datahub.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/lineage/datahub.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub_provider/operators/datahub.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/operators/datahub.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub_provider/operators/datahub_assertion_operator.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/operators/datahub_assertion_operator.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub_provider/operators/datahub_assertion_sensor.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/operators/datahub_assertion_sensor.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub_provider/operators/datahub_operation_operator.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/operators/datahub_operation_operator.py`

 * *Files identical despite different names*

### Comparing `acryl-datahub-tc-0.10.0.3/src/datahub_provider/operators/datahub_operation_sensor.py` & `acryl-datahub-tc-0.10.2rc1/src/datahub_provider/operators/datahub_operation_sensor.py`

 * *Files identical despite different names*

