# Comparing `tmp/spark_rapids_user_tools-23.4.0-127_7804c08-py3-none-any.whl.zip` & `tmp/spark_rapids_user_tools-23.4.1-137_b601490-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,70 +1,73 @@
-Zip file size: 241734 bytes, number of entries: 68
--rw-r--r--  2.0 unx      648 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/__init__.py
--rw-r--r--  2.0 unx    10120 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/cost_estimator.py
--rw-r--r--  2.0 unx    27693 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/dataproc_utils.py
--rw-r--r--  2.0 unx    15306 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/dataproc_wrapper.py
--rw-r--r--  2.0 unx    10514 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/diag.py
--rw-r--r--  2.0 unx     5344 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/diag_dataproc.py
--rw-r--r--  2.0 unx    63046 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/rapids_models.py
--rw-r--r--  2.0 unx     8782 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/utilities.py
--rw-r--r--  2.0 unx      921 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/csp/__init__.py
--rw-r--r--  2.0 unx     1671 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/csp/csp.py
--rw-r--r--  2.0 unx     5196 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/csp/dataproc.py
--rw-r--r--  2.0 unx     1154 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/diag_scripts/hello_world.py
--rw-r--r--  2.0 unx     1276 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/diag_scripts/perf.py
--rw-r--r--  2.0 unx     1273 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/resources/bootstrap-conf.yaml
--rw-r--r--  2.0 unx   684199 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/resources/gcloud-catalog.json
--rw-r--r--  2.0 unx     1416 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/resources/profiling-conf.yaml
--rw-r--r--  2.0 unx     2554 b- defN 23-Apr-26 15:42 spark_rapids_dataproc_tools/resources/qualification-conf.yaml
--rw-r--r--  2.0 unx      750 b- defN 23-Apr-26 15:42 spark_rapids_pytools/__init__.py
--rw-r--r--  2.0 unx      992 b- defN 23-Apr-26 15:42 spark_rapids_pytools/build.py
--rw-r--r--  2.0 unx     1094 b- defN 23-Apr-26 15:42 spark_rapids_pytools/wrapper.py
--rw-r--r--  2.0 unx      646 b- defN 23-Apr-26 15:42 spark_rapids_pytools/cloud_api/__init__.py
--rw-r--r--  2.0 unx    12837 b- defN 23-Apr-26 15:42 spark_rapids_pytools/cloud_api/databricks_aws.py
--rw-r--r--  2.0 unx     1268 b- defN 23-Apr-26 15:42 spark_rapids_pytools/cloud_api/databricks_aws_job.py
--rw-r--r--  2.0 unx    24216 b- defN 23-Apr-26 15:42 spark_rapids_pytools/cloud_api/dataproc.py
--rw-r--r--  2.0 unx     1426 b- defN 23-Apr-26 15:42 spark_rapids_pytools/cloud_api/dataproc_job.py
--rw-r--r--  2.0 unx    21270 b- defN 23-Apr-26 15:42 spark_rapids_pytools/cloud_api/emr.py
--rw-r--r--  2.0 unx    11269 b- defN 23-Apr-26 15:42 spark_rapids_pytools/cloud_api/emr_job.py
--rw-r--r--  2.0 unx     4939 b- defN 23-Apr-26 15:42 spark_rapids_pytools/cloud_api/gstorage.py
--rw-r--r--  2.0 unx     4055 b- defN 23-Apr-26 15:42 spark_rapids_pytools/cloud_api/s3storage.py
--rw-r--r--  2.0 unx    47222 b- defN 23-Apr-26 15:42 spark_rapids_pytools/cloud_api/sp_types.py
--rw-r--r--  2.0 unx      658 b- defN 23-Apr-26 15:42 spark_rapids_pytools/common/__init__.py
--rw-r--r--  2.0 unx      978 b- defN 23-Apr-26 15:42 spark_rapids_pytools/common/exceptions.py
--rw-r--r--  2.0 unx     5432 b- defN 23-Apr-26 15:42 spark_rapids_pytools/common/prop_manager.py
--rw-r--r--  2.0 unx    14712 b- defN 23-Apr-26 15:42 spark_rapids_pytools/common/sys_storage.py
--rw-r--r--  2.0 unx    13037 b- defN 23-Apr-26 15:42 spark_rapids_pytools/common/utilities.py
--rw-r--r--  2.0 unx      659 b- defN 23-Apr-26 15:42 spark_rapids_pytools/pricing/__init__.py
--rw-r--r--  2.0 unx     3522 b- defN 23-Apr-26 15:42 spark_rapids_pytools/pricing/databricks_pricing.py
--rw-r--r--  2.0 unx     4247 b- defN 23-Apr-26 15:42 spark_rapids_pytools/pricing/dataproc_pricing.py
--rw-r--r--  2.0 unx     4717 b- defN 23-Apr-26 15:42 spark_rapids_pytools/pricing/emr_pricing.py
--rw-r--r--  2.0 unx     6400 b- defN 23-Apr-26 15:42 spark_rapids_pytools/pricing/price_provider.py
--rw-r--r--  2.0 unx      666 b- defN 23-Apr-26 15:42 spark_rapids_pytools/rapids/__init__.py
--rw-r--r--  2.0 unx     8004 b- defN 23-Apr-26 15:42 spark_rapids_pytools/rapids/bootstrap.py
--rw-r--r--  2.0 unx    12902 b- defN 23-Apr-26 15:42 spark_rapids_pytools/rapids/profiling.py
--rw-r--r--  2.0 unx    41708 b- defN 23-Apr-26 15:42 spark_rapids_pytools/rapids/qualification.py
--rw-r--r--  2.0 unx     6864 b- defN 23-Apr-26 15:42 spark_rapids_pytools/rapids/rapids_job.py
--rw-r--r--  2.0 unx    33530 b- defN 23-Apr-26 15:42 spark_rapids_pytools/rapids/rapids_tool.py
--rw-r--r--  2.0 unx     6576 b- defN 23-Apr-26 15:42 spark_rapids_pytools/rapids/tool_ctxt.py
--rw-r--r--  2.0 unx     1295 b- defN 23-Apr-26 15:42 spark_rapids_pytools/resources/bootstrap-conf.yaml
--rw-r--r--  2.0 unx    30566 b- defN 23-Apr-26 15:42 spark_rapids_pytools/resources/databricks-premium-catalog.json
--rw-r--r--  2.0 unx    10448 b- defN 23-Apr-26 15:42 spark_rapids_pytools/resources/databricks_aws-configs.json
--rw-r--r--  2.0 unx     8047 b- defN 23-Apr-26 15:42 spark_rapids_pytools/resources/dataproc-configs.json
--rw-r--r--  2.0 unx     8994 b- defN 23-Apr-26 15:42 spark_rapids_pytools/resources/emr-configs.json
--rw-r--r--  2.0 unx     1460 b- defN 23-Apr-26 15:42 spark_rapids_pytools/resources/profiling-conf.yaml
--rw-r--r--  2.0 unx     4252 b- defN 23-Apr-26 15:42 spark_rapids_pytools/resources/qualification-conf.yaml
--rw-r--r--  2.0 unx      671 b- defN 23-Apr-26 15:42 spark_rapids_pytools/resources/templates/dataproc-create_gpu_cluster_script.ms
--rw-r--r--  2.0 unx      518 b- defN 23-Apr-26 15:42 spark_rapids_pytools/resources/templates/dataproc-run_bootstrap.ms
--rw-r--r--  2.0 unx      855 b- defN 23-Apr-26 15:42 spark_rapids_pytools/resources/templates/emr-create_gpu_cluster_script.ms
--rw-r--r--  2.0 unx      537 b- defN 23-Apr-26 15:42 spark_rapids_pytools/resources/templates/emr-run_bootstrap.ms
--rw-r--r--  2.0 unx      630 b- defN 23-Apr-26 15:42 spark_rapids_pytools/wrappers/__init__.py
--rw-r--r--  2.0 unx     8104 b- defN 23-Apr-26 15:42 spark_rapids_pytools/wrappers/databricks_aws_wrapper.py
--rw-r--r--  2.0 unx    14685 b- defN 23-Apr-26 15:42 spark_rapids_pytools/wrappers/dataproc_wrapper.py
--rw-r--r--  2.0 unx    15570 b- defN 23-Apr-26 15:42 spark_rapids_pytools/wrappers/emr_wrapper.py
--rw-r--r--  2.0 unx    18745 b- defN 23-Apr-26 15:42 spark_rapids_user_tools-23.4.0.dist-info/LICENSE
--rw-r--r--  2.0 unx     2939 b- defN 23-Apr-26 15:42 spark_rapids_user_tools-23.4.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Apr-26 15:42 spark_rapids_user_tools-23.4.0.dist-info/WHEEL
--rw-r--r--  2.0 unx      152 b- defN 23-Apr-26 15:42 spark_rapids_user_tools-23.4.0.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       49 b- defN 23-Apr-26 15:42 spark_rapids_user_tools-23.4.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     7009 b- defN 23-Apr-26 15:42 spark_rapids_user_tools-23.4.0.dist-info/RECORD
-68 files, 1273327 bytes uncompressed, 230176 bytes compressed:  81.9%
+Zip file size: 245837 bytes, number of entries: 71
+-rw-r--r--  2.0 unx      648 b- defN 23-May-31 14:16 spark_rapids_dataproc_tools/__init__.py
+-rw-r--r--  2.0 unx    10120 b- defN 23-May-31 14:16 spark_rapids_dataproc_tools/cost_estimator.py
+-rw-r--r--  2.0 unx    27693 b- defN 23-May-31 14:16 spark_rapids_dataproc_tools/dataproc_utils.py
+-rw-r--r--  2.0 unx    15306 b- defN 23-May-31 14:16 spark_rapids_dataproc_tools/dataproc_wrapper.py
+-rw-r--r--  2.0 unx    10514 b- defN 23-May-31 14:16 spark_rapids_dataproc_tools/diag.py
+-rw-r--r--  2.0 unx     5344 b- defN 23-May-31 14:16 spark_rapids_dataproc_tools/diag_dataproc.py
+-rw-r--r--  2.0 unx    63046 b- defN 23-May-31 14:16 spark_rapids_dataproc_tools/rapids_models.py
+-rw-r--r--  2.0 unx     8782 b- defN 23-May-31 14:16 spark_rapids_dataproc_tools/utilities.py
+-rw-r--r--  2.0 unx      921 b- defN 23-May-31 14:16 spark_rapids_dataproc_tools/csp/__init__.py
+-rw-r--r--  2.0 unx     1671 b- defN 23-May-31 14:16 spark_rapids_dataproc_tools/csp/csp.py
+-rw-r--r--  2.0 unx     5196 b- defN 23-May-31 14:16 spark_rapids_dataproc_tools/csp/dataproc.py
+-rw-r--r--  2.0 unx     1154 b- defN 23-May-31 14:16 spark_rapids_dataproc_tools/diag_scripts/hello_world.py
+-rw-r--r--  2.0 unx     1276 b- defN 23-May-31 14:16 spark_rapids_dataproc_tools/diag_scripts/perf.py
+-rw-r--r--  2.0 unx     1273 b- defN 23-May-31 14:16 spark_rapids_dataproc_tools/resources/bootstrap-conf.yaml
+-rw-r--r--  2.0 unx   684199 b- defN 23-May-31 14:16 spark_rapids_dataproc_tools/resources/gcloud-catalog.json
+-rw-r--r--  2.0 unx     1416 b- defN 23-May-31 14:16 spark_rapids_dataproc_tools/resources/profiling-conf.yaml
+-rw-r--r--  2.0 unx     2575 b- defN 23-May-31 14:16 spark_rapids_dataproc_tools/resources/qualification-conf.yaml
+-rw-r--r--  2.0 unx      750 b- defN 23-May-31 14:16 spark_rapids_pytools/__init__.py
+-rw-r--r--  2.0 unx      992 b- defN 23-May-31 14:16 spark_rapids_pytools/build.py
+-rw-r--r--  2.0 unx     1198 b- defN 23-May-31 14:16 spark_rapids_pytools/wrapper.py
+-rw-r--r--  2.0 unx      646 b- defN 23-May-31 14:16 spark_rapids_pytools/cloud_api/__init__.py
+-rw-r--r--  2.0 unx    12837 b- defN 23-May-31 14:16 spark_rapids_pytools/cloud_api/databricks_aws.py
+-rw-r--r--  2.0 unx     1268 b- defN 23-May-31 14:16 spark_rapids_pytools/cloud_api/databricks_aws_job.py
+-rw-r--r--  2.0 unx    24216 b- defN 23-May-31 14:16 spark_rapids_pytools/cloud_api/dataproc.py
+-rw-r--r--  2.0 unx     1426 b- defN 23-May-31 14:16 spark_rapids_pytools/cloud_api/dataproc_job.py
+-rw-r--r--  2.0 unx    21270 b- defN 23-May-31 14:16 spark_rapids_pytools/cloud_api/emr.py
+-rw-r--r--  2.0 unx    11269 b- defN 23-May-31 14:16 spark_rapids_pytools/cloud_api/emr_job.py
+-rw-r--r--  2.0 unx     4939 b- defN 23-May-31 14:16 spark_rapids_pytools/cloud_api/gstorage.py
+-rw-r--r--  2.0 unx     2261 b- defN 23-May-31 14:16 spark_rapids_pytools/cloud_api/onprem.py
+-rw-r--r--  2.0 unx     4055 b- defN 23-May-31 14:16 spark_rapids_pytools/cloud_api/s3storage.py
+-rw-r--r--  2.0 unx    47363 b- defN 23-May-31 14:16 spark_rapids_pytools/cloud_api/sp_types.py
+-rw-r--r--  2.0 unx      658 b- defN 23-May-31 14:16 spark_rapids_pytools/common/__init__.py
+-rw-r--r--  2.0 unx      978 b- defN 23-May-31 14:16 spark_rapids_pytools/common/exceptions.py
+-rw-r--r--  2.0 unx     5432 b- defN 23-May-31 14:16 spark_rapids_pytools/common/prop_manager.py
+-rw-r--r--  2.0 unx    14712 b- defN 23-May-31 14:16 spark_rapids_pytools/common/sys_storage.py
+-rw-r--r--  2.0 unx    13037 b- defN 23-May-31 14:16 spark_rapids_pytools/common/utilities.py
+-rw-r--r--  2.0 unx      659 b- defN 23-May-31 14:16 spark_rapids_pytools/pricing/__init__.py
+-rw-r--r--  2.0 unx     3522 b- defN 23-May-31 14:16 spark_rapids_pytools/pricing/databricks_pricing.py
+-rw-r--r--  2.0 unx     4247 b- defN 23-May-31 14:16 spark_rapids_pytools/pricing/dataproc_pricing.py
+-rw-r--r--  2.0 unx     4717 b- defN 23-May-31 14:16 spark_rapids_pytools/pricing/emr_pricing.py
+-rw-r--r--  2.0 unx     6400 b- defN 23-May-31 14:16 spark_rapids_pytools/pricing/price_provider.py
+-rw-r--r--  2.0 unx      666 b- defN 23-May-31 14:16 spark_rapids_pytools/rapids/__init__.py
+-rw-r--r--  2.0 unx     8165 b- defN 23-May-31 14:16 spark_rapids_pytools/rapids/bootstrap.py
+-rw-r--r--  2.0 unx    13009 b- defN 23-May-31 14:16 spark_rapids_pytools/rapids/profiling.py
+-rw-r--r--  2.0 unx    43271 b- defN 23-May-31 14:16 spark_rapids_pytools/rapids/qualification.py
+-rw-r--r--  2.0 unx     6864 b- defN 23-May-31 14:16 spark_rapids_pytools/rapids/rapids_job.py
+-rw-r--r--  2.0 unx    33590 b- defN 23-May-31 14:16 spark_rapids_pytools/rapids/rapids_tool.py
+-rw-r--r--  2.0 unx     6576 b- defN 23-May-31 14:16 spark_rapids_pytools/rapids/tool_ctxt.py
+-rw-r--r--  2.0 unx     1390 b- defN 23-May-31 14:16 spark_rapids_pytools/resources/bootstrap-conf.yaml
+-rw-r--r--  2.0 unx    30566 b- defN 23-May-31 14:16 spark_rapids_pytools/resources/databricks-premium-catalog.json
+-rw-r--r--  2.0 unx    10448 b- defN 23-May-31 14:16 spark_rapids_pytools/resources/databricks_aws-configs.json
+-rw-r--r--  2.0 unx     8047 b- defN 23-May-31 14:16 spark_rapids_pytools/resources/dataproc-configs.json
+-rw-r--r--  2.0 unx     8994 b- defN 23-May-31 14:16 spark_rapids_pytools/resources/emr-configs.json
+-rw-r--r--  2.0 unx     1684 b- defN 23-May-31 14:16 spark_rapids_pytools/resources/onprem-configs.json
+-rw-r--r--  2.0 unx     1460 b- defN 23-May-31 14:16 spark_rapids_pytools/resources/profiling-conf.yaml
+-rw-r--r--  2.0 unx     4273 b- defN 23-May-31 14:16 spark_rapids_pytools/resources/qualification-conf.yaml
+-rw-r--r--  2.0 unx      671 b- defN 23-May-31 14:16 spark_rapids_pytools/resources/templates/dataproc-create_gpu_cluster_script.ms
+-rw-r--r--  2.0 unx      518 b- defN 23-May-31 14:16 spark_rapids_pytools/resources/templates/dataproc-run_bootstrap.ms
+-rw-r--r--  2.0 unx      855 b- defN 23-May-31 14:16 spark_rapids_pytools/resources/templates/emr-create_gpu_cluster_script.ms
+-rw-r--r--  2.0 unx      537 b- defN 23-May-31 14:16 spark_rapids_pytools/resources/templates/emr-run_bootstrap.ms
+-rw-r--r--  2.0 unx      630 b- defN 23-May-31 14:16 spark_rapids_pytools/wrappers/__init__.py
+-rw-r--r--  2.0 unx     8104 b- defN 23-May-31 14:16 spark_rapids_pytools/wrappers/databricks_aws_wrapper.py
+-rw-r--r--  2.0 unx    14685 b- defN 23-May-31 14:16 spark_rapids_pytools/wrappers/dataproc_wrapper.py
+-rw-r--r--  2.0 unx    15570 b- defN 23-May-31 14:16 spark_rapids_pytools/wrappers/emr_wrapper.py
+-rw-r--r--  2.0 unx     3463 b- defN 23-May-31 14:16 spark_rapids_pytools/wrappers/onprem_wrapper.py
+-rw-r--r--  2.0 unx    21086 b- defN 23-May-31 14:16 spark_rapids_user_tools-23.4.1.dist-info/LICENSE
+-rw-r--r--  2.0 unx     2939 b- defN 23-May-31 14:16 spark_rapids_user_tools-23.4.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-May-31 14:16 spark_rapids_user_tools-23.4.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx      152 b- defN 23-May-31 14:16 spark_rapids_user_tools-23.4.1.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       49 b- defN 23-May-31 14:16 spark_rapids_user_tools-23.4.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     7317 b- defN 23-May-31 14:16 spark_rapids_user_tools-23.4.1.dist-info/RECORD
+71 files, 1285657 bytes uncompressed, 233777 bytes compressed:  81.8%
```

## zipnote {}

```diff
@@ -78,14 +78,17 @@
 
 Filename: spark_rapids_pytools/cloud_api/emr_job.py
 Comment: 
 
 Filename: spark_rapids_pytools/cloud_api/gstorage.py
 Comment: 
 
+Filename: spark_rapids_pytools/cloud_api/onprem.py
+Comment: 
+
 Filename: spark_rapids_pytools/cloud_api/s3storage.py
 Comment: 
 
 Filename: spark_rapids_pytools/cloud_api/sp_types.py
 Comment: 
 
 Filename: spark_rapids_pytools/common/__init__.py
@@ -150,14 +153,17 @@
 
 Filename: spark_rapids_pytools/resources/dataproc-configs.json
 Comment: 
 
 Filename: spark_rapids_pytools/resources/emr-configs.json
 Comment: 
 
+Filename: spark_rapids_pytools/resources/onprem-configs.json
+Comment: 
+
 Filename: spark_rapids_pytools/resources/profiling-conf.yaml
 Comment: 
 
 Filename: spark_rapids_pytools/resources/qualification-conf.yaml
 Comment: 
 
 Filename: spark_rapids_pytools/resources/templates/dataproc-create_gpu_cluster_script.ms
@@ -180,26 +186,29 @@
 
 Filename: spark_rapids_pytools/wrappers/dataproc_wrapper.py
 Comment: 
 
 Filename: spark_rapids_pytools/wrappers/emr_wrapper.py
 Comment: 
 
-Filename: spark_rapids_user_tools-23.4.0.dist-info/LICENSE
+Filename: spark_rapids_pytools/wrappers/onprem_wrapper.py
+Comment: 
+
+Filename: spark_rapids_user_tools-23.4.1.dist-info/LICENSE
 Comment: 
 
-Filename: spark_rapids_user_tools-23.4.0.dist-info/METADATA
+Filename: spark_rapids_user_tools-23.4.1.dist-info/METADATA
 Comment: 
 
-Filename: spark_rapids_user_tools-23.4.0.dist-info/WHEEL
+Filename: spark_rapids_user_tools-23.4.1.dist-info/WHEEL
 Comment: 
 
-Filename: spark_rapids_user_tools-23.4.0.dist-info/entry_points.txt
+Filename: spark_rapids_user_tools-23.4.1.dist-info/entry_points.txt
 Comment: 
 
-Filename: spark_rapids_user_tools-23.4.0.dist-info/top_level.txt
+Filename: spark_rapids_user_tools-23.4.1.dist-info/top_level.txt
 Comment: 
 
-Filename: spark_rapids_user_tools-23.4.0.dist-info/RECORD
+Filename: spark_rapids_user_tools-23.4.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## spark_rapids_dataproc_tools/resources/qualification-conf.yaml

```diff
@@ -48,14 +48,15 @@
       - filter-criteria
       - h
       - html-report
       - no-html-report
       - m
       - match-event-logs
       - max-sql-desc-length
+      - ml-functions
       - n
       - num-output-rows
       - num-threads
       - order
       - p
       - per-sql
       - r
```

## spark_rapids_pytools/__init__.py

```diff
@@ -12,9 +12,9 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """init file of the spark_rapids_pytools package."""
 
 from spark_rapids_pytools.build import get_version
 
-VERSION = '23.04.0'
+VERSION = '23.04.1'
 __version__ = get_version(VERSION)
```

## spark_rapids_pytools/wrapper.py

```diff
@@ -15,19 +15,21 @@
 """Wrapper class to run tools associated with RAPIDS Accelerator for Apache Spark plugin."""
 
 import fire
 
 from spark_rapids_pytools.wrappers.databricks_aws_wrapper import DBAWSWrapper
 from spark_rapids_pytools.wrappers.dataproc_wrapper import DataprocWrapper
 from spark_rapids_pytools.wrappers.emr_wrapper import EMRWrapper
+from spark_rapids_pytools.wrappers.onprem_wrapper import OnPremWrapper
 
 
 def main():
     fire.Fire({
         'emr': EMRWrapper,
         'dataproc': DataprocWrapper,
-        'databricks_aws': DBAWSWrapper
+        'databricks_aws': DBAWSWrapper,
+        'onprem': OnPremWrapper
     })
 
 
 if __name__ == '__main__':
     main()
```

## spark_rapids_pytools/cloud_api/sp_types.py

```diff
@@ -119,14 +119,15 @@
 
 
 class CloudPlatform(EnumeratedType):
     """symbolic names (members) bound to supported cloud platforms."""
     DATABRICKS_AWS = 'databricks_aws'
     DATAPROC = 'dataproc'
     EMR = 'emr'
+    ONPREM = 'onprem'
     LOCAL = 'local'
     NONE = 'NONE'
 
 
 class SparkNodeType(EnumeratedType):
     """
     Node type from Spark perspective. We either have a master node or a worker node.
@@ -354,15 +355,15 @@
         cmd_runner_props = self.get_cmd_run_configs()
         if cmd_runner_props:
             res = cmd_runner_props.get('cliPiggyBackArgs')['definedArgs']
         return res
 
     def get_rapids_job_configs(self, deploy_mode: DeployMode) -> dict:
         cmd_runner_props = self.get_cmd_run_configs()
-        if cmd_runner_props:
+        if cmd_runner_props and deploy_mode is not None:
             deploy_mode_configs = get_elem_non_safe(cmd_runner_props,
                                                     ['rapidsJobs', DeployMode.tostring(deploy_mode)])
             return deploy_mode_configs
         return None
 
     def get_and_set_env_vars(self):
         """For that driver, try to get all the available system environment for the system."""
@@ -1120,13 +1121,14 @@
 
 
 def get_platform(platform_id: Enum) -> Type[PlatformBase]:
     platform_hash = {
         CloudPlatform.DATABRICKS_AWS: ('spark_rapids_pytools.cloud_api.databricks_aws', 'DBAWSPlatform'),
         CloudPlatform.DATAPROC: ('spark_rapids_pytools.cloud_api.dataproc', 'DataprocPlatform'),
         CloudPlatform.EMR: ('spark_rapids_pytools.cloud_api.emr', 'EMRPlatform'),
+        CloudPlatform.ONPREM: ('spark_rapids_pytools.cloud_api.onprem', 'OnPremPlatform'),
     }
     if platform_id in platform_hash:
         mod_name, clz_name = platform_hash[platform_id]
         imported_mod = __import__(mod_name, globals(), locals(), [clz_name])
         return getattr(imported_mod, clz_name)
     raise AttributeError(f'Provider {platform_id} does not exist')
```

## spark_rapids_pytools/rapids/bootstrap.py

```diff
@@ -57,18 +57,20 @@
         usable_worker_mem = max(0, cpu_mem - constants.get('systemReserveMB'))
         executor_container_mem = usable_worker_mem // executors_per_node
         # reserve 10% of heap as memory overhead
         max_executor_heap = max(0, int(executor_container_mem * (1 - constants.get('heapOverheadFraction'))))
         # give up to 2GB of heap to each executor core
         executor_heap = min(max_executor_heap, constants.get('heapPerCoreMB') * num_executor_cores)
         executor_mem_overhead = int(executor_heap * constants.get('heapOverheadFraction'))
+        # use default for pageable_pool to add to memory overhead
+        pageable_pool = constants.get('defaultPageablePoolMB')
         # pinned memory uses any unused space up to 4GB
         pinned_mem = min(constants.get('maxPinnedMemoryMB'),
-                         executor_container_mem - executor_heap - executor_mem_overhead)
-        executor_mem_overhead += pinned_mem
+                         executor_container_mem - executor_heap - executor_mem_overhead - pageable_pool)
+        executor_mem_overhead += pinned_mem + pageable_pool
         res = {
             'spark.executor.cores': num_executor_cores,
             'spark.executor.memory': f'{executor_heap}m',
             'spark.executor.memoryOverhead': f'{executor_mem_overhead}m',
             'spark.rapids.sql.concurrentGpuTasks': gpu_concurrent_tasks,
             'spark.rapids.memory.pinnedPool.size': f'{pinned_mem}m',
             'spark.sql.files.maxPartitionBytes': f'{constants.get("maxSqlFilesPartitionsMB")}m',
```

## spark_rapids_pytools/rapids/profiling.py

```diff
@@ -183,17 +183,19 @@
                     comments_list = recom_section[begin_comm_ind: last_comm_ind]
         except OSError:
             self.logger.error('Could not open output of profiler %s', file_path)
         if len(props_list) == 0:
             props_list = ['- No recommendations']
         if len(comments_list) == 0:
             comments_list = ['- No comments']
-        # sort the comments and the recommendations so that the two values are aligned
+        # Note that sorting the comments is disabled because it will change the order
+        # of multiline entries
+        # Recommendations can be sorted so that the two values are aligned
+        # comments_list.sort()
         props_list.sort()
-        comments_list.sort()
         return app_name, props_list, comments_list
 
     def _write_summary(self):
         print(Utils.gen_multiline_str(self._report_tool_full_location(),
                                       self.ctxt.get_ctxt('wrapperOutputContent')))
 
     def _process_output(self):
```

## spark_rapids_pytools/rapids/qualification.py

```diff
@@ -54,14 +54,15 @@
     Encapsulates the logic to organize Qualification report.
     """
     comments: Any = None
     all_apps: pd.DataFrame = None
     recommended_apps: pd.DataFrame = None
     df_result: pd.DataFrame = None
     irrelevant_speedups: bool = False
+    pricing_config: Any = None
     sections_generators: List[Callable] = field(default_factory=lambda: [])
 
     def _get_total_durations(self) -> int:
         if not self.is_empty():
             return self.all_apps['App Duration'].sum()
         return 0
 
@@ -136,31 +137,40 @@
             if pretty_df.empty:
                 # the results were reduced to no rows because of the filters
                 report_content.append(
                     f'{app_name} tool found no qualified applications after applying the filters.\n'
                     f'See the CSV file for full report or disable the filters.')
             else:
                 report_content.append(tabulate(pretty_df, headers='keys', tablefmt='psql', floatfmt='.2f'))
+        elif self.pricing_config is None:
+            report_content.append(f'pricing information not found for ${app_name}')
         else:
             report_content.append(f'{app_name} tool found no records to show.')
 
-        total_app_cost = self._get_stats_total_cost()
-        total_gpu_cost = self._get_stats_total_gpu_cost()
-        estimated_gpu_savings = 0.0
-        if total_app_cost > 0.0:
-            estimated_gpu_savings = 100.0 - (100.0 * total_gpu_cost / total_app_cost)
         overall_speedup = 0.0
         total_apps_durations = 1.0 * self._get_total_durations()
         total_gpu_durations = self._get_total_gpu_durations()
         if total_gpu_durations > 0:
             overall_speedup = total_apps_durations / total_gpu_durations
-        report_content.append(Utils.gen_report_sec_header('Report Summary', hrule=False))
-        report_summary = [['Total applications', self._get_stats_total_apps()],
-                          ['Overall estimated speedup', format_float(overall_speedup)],
-                          ['Overall estimated cost savings', f'{format_float(estimated_gpu_savings)}%']]
+
+        if self.pricing_config is None:
+            report_content.append(Utils.gen_report_sec_header('Report Summary', hrule=False))
+            report_summary = [['Total applications', self._get_stats_total_apps()],
+                              ['Overall estimated speedup', format_float(overall_speedup)]]
+        else:
+            total_app_cost = self._get_stats_total_cost()
+            total_gpu_cost = self._get_stats_total_gpu_cost()
+            estimated_gpu_savings = 0.0
+            if total_app_cost > 0.0:
+                estimated_gpu_savings = 100.0 - (100.0 * total_gpu_cost / total_app_cost)
+
+            report_content.append(Utils.gen_report_sec_header('Report Summary', hrule=False))
+            report_summary = [['Total applications', self._get_stats_total_apps()],
+                              ['Overall estimated speedup', format_float(overall_speedup)],
+                              ['Overall estimated cost savings', f'{format_float(estimated_gpu_savings)}%']]
         if not self.irrelevant_speedups:
             # do not display speedups stats if the speedup is being overriden by the shape recommendations
             report_summary.insert(1, ['RAPIDS candidates', self._get_stats_recommended_apps()])
         report_content.append(tabulate(report_summary, colalign=('left', 'right')))
         if self.comments:
             report_content.append(Utils.gen_report_sec_header('Notes'))
             report_content.extend(f' - {line}' for line in self.comments)
@@ -190,16 +200,17 @@
         """
         self.logger.info('Qualification tool processing the arguments')
         super()._process_rapids_args()
 
     def _process_cpu_cluster_args(self, offline_cluster_opts: dict = None):
         # get the name of the cpu_cluster
         cpu_cluster_arg = offline_cluster_opts.get('cpuCluster')
-        cpu_cluster_obj = self._create_migration_cluster('CPU', cpu_cluster_arg)
-        self.ctxt.set_ctxt('cpuClusterProxy', cpu_cluster_obj)
+        if cpu_cluster_arg is not None:
+            cpu_cluster_obj = self._create_migration_cluster('CPU', cpu_cluster_arg)
+            self.ctxt.set_ctxt('cpuClusterProxy', cpu_cluster_obj)
 
     def _process_gpu_cluster_args(self, offline_cluster_opts: dict = None):
         gpu_cluster_arg = offline_cluster_opts.get('gpuCluster')
         if gpu_cluster_arg is None:
             self.logger.info('Creating GPU cluster by converting the CPU cluster instances to GPU supported types')
             # Convert the CPU instances to support gpu
             orig_cluster = self.ctxt.get_ctxt('cpuClusterProxy')
@@ -591,43 +602,56 @@
         return app_df_set
 
     def __build_global_report_summary(self,
                                       all_apps: pd.DataFrame,
                                       csv_out: str) -> QualificationSummary:
         if all_apps.empty:
             # No need to run saving estimator or process the data frames.
-            return QualificationSummary(comments=[self.__generate_mc_types_conversion_report])
+            return QualificationSummary(comments=self.__generate_mc_types_conversion_report())
 
-        reshape_col = self.ctxt.get_value('local', 'output', 'processDFProps',
-                                          'clusterShapeCols', 'columnName')
-        speed_recommendation_col = self.ctxt.get_value('local', 'output', 'speedupRecommendColumn')
         apps_pruned_df, prune_notes = self.__remap_columns_and_prune(all_apps)
         recommended_apps = self.__get_recommended_apps(apps_pruned_df)
-        apps_reshaped_df, per_row_flag = self.__apply_gpu_cluster_reshape(apps_pruned_df)
-        # Now, the dataframe is ready to calculate the cost and the savings
-        apps_working_set = self.__calc_apps_cost(apps_reshaped_df,
-                                                 reshape_col,
-                                                 speed_recommendation_col,
-                                                 per_row_flag)
-
-        if not apps_working_set.empty:
-            self.logger.info('Generating GPU Estimated Speedup and Savings as %s', csv_out)
-            # we can use the general format as well but this will transform numbers to E+. So, stick with %f
-            apps_working_set.to_csv(csv_out, float_format='%.2f')
-
         # if the gpu_reshape_type is set to JOB then, then we should ignore recommended apps
         speedups_irrelevant_flag = self.__recommendation_is_non_standard()
         reshaped_notes = self.__generate_cluster_shape_report()
         report_comments = [prune_notes] if prune_notes else []
         if reshaped_notes:
             report_comments.append(reshaped_notes)
+
+        pricing_config = self.ctxt.platform.configs.get_value_silent('pricing')
+        reshape_col = self.ctxt.get_value('local', 'output', 'processDFProps',
+                                          'clusterShapeCols', 'columnName')
+        speed_recommendation_col = self.ctxt.get_value('local', 'output', 'speedupRecommendColumn')
+        apps_reshaped_df, per_row_flag = self.__apply_gpu_cluster_reshape(apps_pruned_df)
+        # OnPrem platform doesn't have pricing information. We do not calculate cost savings for
+        # OnPrem platform.
+        if pricing_config is None:
+            df_final_result = apps_reshaped_df
+            if not apps_reshaped_df.empty:
+                # Do not include estimated job frequency in csv file
+                apps_reshaped_df = apps_reshaped_df.drop(columns=['Estimated Job Frequency (monthly)'])
+                self.logger.info('Generating GPU Estimated Speedup as %s', csv_out)
+                apps_reshaped_df.to_csv(csv_out, float_format='%.2f')
+        else:
+            # Now, the dataframe is ready to calculate the cost and the savings
+            apps_working_set = self.__calc_apps_cost(apps_reshaped_df,
+                                                     reshape_col,
+                                                     speed_recommendation_col,
+                                                     per_row_flag)
+            df_final_result = apps_working_set
+            if not apps_working_set.empty:
+                self.logger.info('Generating GPU Estimated Speedup and Savings as %s', csv_out)
+                # we can use the general format as well but this will transform numbers to E+. So, stick with %f
+                apps_working_set.to_csv(csv_out, float_format='%.2f')
+
         return QualificationSummary(comments=report_comments,
                                     all_apps=apps_pruned_df,
                                     recommended_apps=recommended_apps,
-                                    df_result=apps_working_set,
+                                    pricing_config=pricing_config,
+                                    df_result=df_final_result,
                                     irrelevant_speedups=speedups_irrelevant_flag,
                                     sections_generators=[self.__generate_mc_types_conversion_report])
 
     def _process_output(self):
         def process_df_for_stdout(raw_df):
             """
             process the dataframe to be more readable on the stdout
@@ -642,14 +666,18 @@
                 # During processing of arguments phase, we verified that the filter does not conflict
                 # with the shape recommendation
                 raw_df = self.__remap_cols_for_shape_type(raw_df,
                                                           selected_cols,
                                                           self.ctxt.get_ctxt('gpuClusterShapeRecommendation'))
                 # update the selected columns
                 selected_cols = list(raw_df.columns)
+
+            pricing_config = self.ctxt.platform.configs.get_value_silent('pricing')
+            if pricing_config is None:
+                selected_cols = list(raw_df.columns)
             # filter by recommendations if enabled
             if filter_recommendation_enabled:
                 df_row = self.__get_recommended_apps(raw_df, selected_cols)
             else:
                 df_row = raw_df.loc[:, selected_cols]
             if df_row.empty:
                 return df_row
```

## spark_rapids_pytools/rapids/rapids_tool.py

```diff
@@ -10,14 +10,16 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Abstract class representing wrapper around the RAPIDS acceleration tools."""
 
+import concurrent
+import logging
 import os
 import re
 import sys
 import tarfile
 import time
 from concurrent.futures import ThreadPoolExecutor
 from dataclasses import dataclass, field
@@ -100,19 +102,18 @@
             def wrapper(self, *args, **kwargs):
                 try:
                     if enable_prologue:
                         self.logger.info('******* [%s]: Starting *******', phase_name)
                     func_cb(self, *args, **kwargs)  # pylint: disable=not-callable
                     if enable_epilogue:
                         self.logger.info('======= [%s]: Finished =======', phase_name)
-                except Exception as ex:    # pylint: disable=broad-except
-                    self.logger.error('%s. Raised an error in phase [%s]\n%s',
+                except Exception:    # pylint: disable=broad-except
+                    logging.exception('%s. Raised an error in phase [%s]\n',
                                       self.pretty_name(),
-                                      phase_name,
-                                      ex)
+                                      phase_name)
                     sys.exit(1)
             return wrapper
         return decorator
 
     def __post_init__(self):
         # when debug is set to true set it in the environment.
         self.logger = ToolLogging.get_and_setup_logger(f'rapids.tools.{self.name}')
@@ -469,21 +470,22 @@
             futures_list = []
             results = []
             with ThreadPoolExecutor(max_workers=4) as executor:
                 for dep in dep_arr:
                     futures = executor.submit(cache_single_dependency, dep)
                     futures.add_done_callback(exception_handler)
                     futures_list.append(futures)
-                for future in futures_list:
-                    try:
-                        result = future.result(timeout=360)
+                try:
+                    # set the timeout to 30 minutes.
+                    for future in concurrent.futures.as_completed(futures_list, timeout=1800):
+                        result = future.result()
                         results.append(result)
-                    except Exception as ex:    # pylint: disable=broad-except
-                        self.logger.error('Failed to download dependencies %s', ex)
-                        raise ex
+                except Exception as ex:    # pylint: disable=broad-except
+                    self.logger.error('Failed to download dependencies %s', ex)
+                    raise ex
             return results
 
         # TODO: Verify the downloaded file by checking their MD5
         deploy_mode = DeployMode.tostring(self.ctxt.get_deploy_mode())
         depend_arr = self.ctxt.platform.configs.get_value_silent('dependencies',
                                                                  'deployMode',
                                                                  deploy_mode)
```

## spark_rapids_pytools/resources/bootstrap-conf.yaml

```diff
@@ -8,14 +8,16 @@
     cleanUp: true
     # Name of the file where the final result is going to show
     fileName: rapids_4_dataproc_bootstrap_output.log
   clusterConfigs:
     constants:
       # Maximum amount of pinned memory to use per executor in megabytes
       maxPinnedMemoryMB: 4096
+      # Default pageable pool size per executor in megabytes
+      defaultPageablePoolMB: 1024
       # Maximum number of concurrent tasks to run on the GPU
       maxGpuConcurrent: 4
       # Amount of GPU memory to use per concurrent task in megabytes
       # Using a bit less than 8GB here since Dataproc clusters advertise
       # T4s as only having around 14.75 GB and we want to run with
       # 2 concurrent by default on T4s.
       gpuMemPerTaskMB: 7500
```

## spark_rapids_pytools/resources/qualification-conf.yaml

```diff
@@ -59,14 +59,15 @@
       - filter-criteria
       - h
       - html-report
       - no-html-report
       - m
       - match-event-logs
       - max-sql-desc-length
+      - ml-functions
       - n
       - num-output-rows
       - num-threads
       - order
       - p
       - per-sql
       - r
```

## Comparing `spark_rapids_user_tools-23.4.0.dist-info/LICENSE` & `spark_rapids_user_tools-23.4.1.dist-info/LICENSE`

 * *Files 4% similar despite different names*

```diff
@@ -349,7 +349,49 @@
     substantial portions of the Software.
 
     THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING
     BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
     NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,
     DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
     OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+jsoup License
+
+    The MIT License
+
+    Copyright (c) 2009 - 2023 Jonathan Hedley (https://jsoup.org/)
+
+    Permission is hereby granted, free of charge, to any person obtaining a copy of this software
+    and associated documentation files (the "Software"), to deal in the Software without
+    restriction, including without limitation the rights to use, copy, modify, merge, publish,
+    distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the
+    Software is furnished to do so, subject to the following conditions:
+
+    The above copyright notice and this permission notice shall be included in all copies or
+    substantial portions of the Software.
+
+    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING
+    BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+    NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,
+    DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+scallop License
+
+    The MIT License
+
+    Copyright (C) 2012 Platon Pronko and Chris Hodapp
+
+    Permission is hereby granted, free of charge, to any person obtaining a copy of this software
+    and associated documentation files (the "Software"), to deal in the Software without
+    restriction, including without limitation the rights to use, copy, modify, merge, publish,
+    distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the
+    Software is furnished to do so, subject to the following conditions:
+
+    The above copyright notice and this permission notice shall be included in all copies or
+    substantial portions of the Software.
+
+    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING
+    BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+    NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,
+    DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
```

## Comparing `spark_rapids_user_tools-23.4.0.dist-info/METADATA` & `spark_rapids_user_tools-23.4.1.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: spark-rapids-user-tools
-Version: 23.4.0
+Version: 23.4.1
 Summary: A simple wrapper process around cloud service providers to run tools for the RAPIDS Accelerator for Apache Spark.
 Author-email: Raza Jafri <raza.jafri@gmail.com>, Ahmed Hussein <a@ahussein.me>
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
 Requires-Python: >=3.8
 Description-Content-Type: text/markdown
@@ -13,15 +13,15 @@
 Requires-Dist: fastprogress (==1.0.3)
 Requires-Dist: fastcore (==1.5.29)
 Requires-Dist: fire (==0.4.0)
 Requires-Dist: pandas (==1.4.3)
 Requires-Dist: pyYAML (==6.0)
 Requires-Dist: tabulate (==0.8.10)
 Requires-Dist: importlib-resources (==5.10.2)
-Requires-Dist: requests (==2.28.2)
+Requires-Dist: requests (==2.31.0)
 Requires-Dist: packaging (==23.0)
 Requires-Dist: certifi (==2022.12.7)
 Requires-Dist: idna (==3.4)
 Requires-Dist: urllib3 (==1.26.14)
 Requires-Dist: beautifulsoup4 (==4.11.2)
 Requires-Dist: pygments (==2.15.0)
```

## Comparing `spark_rapids_user_tools-23.4.0.dist-info/RECORD` & `spark_rapids_user_tools-23.4.1.dist-info/RECORD`

 * *Files 7% similar despite different names*

```diff
@@ -10,59 +10,62 @@
 spark_rapids_dataproc_tools/csp/csp.py,sha256=sNDKuJMZ4LwfKuCQWOyzyavb-gdhNa-lz_CuIbf9L7Y,1671
 spark_rapids_dataproc_tools/csp/dataproc.py,sha256=NX1VvXqMTCCSw4whxD_6KoHWVSPQadXazjTtlRj6qGM,5196
 spark_rapids_dataproc_tools/diag_scripts/hello_world.py,sha256=d7OPcIZfQ_as5qJWkwno77kzsryU0_f4t8A6z-kIZP4,1154
 spark_rapids_dataproc_tools/diag_scripts/perf.py,sha256=7vUsfw7hWe2NoD8_37_f6c2AbocBIgAQVP2uSDj3H4I,1276
 spark_rapids_dataproc_tools/resources/bootstrap-conf.yaml,sha256=FfMyJqOt8VwK-FuVAASCenxBCk3EWkZbZ5Hwd9N8WlQ,1273
 spark_rapids_dataproc_tools/resources/gcloud-catalog.json,sha256=gsaT3fho6yLJGdpAMi6oRQjJtombuKa4vZaRPKqxMaM,684199
 spark_rapids_dataproc_tools/resources/profiling-conf.yaml,sha256=xluVs7e1AcbDXWUYt3_2FPZSiwGPCOJlsiwnnZT59nM,1416
-spark_rapids_dataproc_tools/resources/qualification-conf.yaml,sha256=FLC0kHTPNakz6GWtVA5IeolR3golZB1LqRc_rdPY0qc,2554
-spark_rapids_pytools/__init__.py,sha256=VnkBArGJEnTlfFUuFWuNeF6AMkK2l8uCnwwGfH5UKjM,750
+spark_rapids_dataproc_tools/resources/qualification-conf.yaml,sha256=ABoda4ZHAAE8uywI4vjjHgJ2pyzrShsL1zr7ksMbucA,2575
+spark_rapids_pytools/__init__.py,sha256=OKBChLKzXThChVe7nIGm7UVKH3H24OKA5nKsE-q_204,750
 spark_rapids_pytools/build.py,sha256=Ej4Pc2jPIyeVUUOyC67ZMc6Tj90NKj0DrXyniJc0FnY,992
-spark_rapids_pytools/wrapper.py,sha256=Cl2u6zebY-gMuCJ8jeqeGJP5XSVQToi0spa6563BB_4,1094
+spark_rapids_pytools/wrapper.py,sha256=WPqjnSq5sik4eyB4G4R1HqhOGRSbFkThdpOxca5R9AQ,1198
 spark_rapids_pytools/cloud_api/__init__.py,sha256=NQSbmxhLzvnZrf4ltpgCLMjCEERPv5QoYo62LEdDBs8,646
 spark_rapids_pytools/cloud_api/databricks_aws.py,sha256=g0XXBVeQwtVAjsm6Po3R3dUA-7CUqaRp9W7iFqkuU4I,12837
 spark_rapids_pytools/cloud_api/databricks_aws_job.py,sha256=ZNYM2pa8fObRhc9c9VcqCT6HxGJNfHeFhdoVYNek51E,1268
 spark_rapids_pytools/cloud_api/dataproc.py,sha256=ibU7ThJvJ6tnEuyG5dDedHApZGYUILGz6gVOOCDl8aI,24216
 spark_rapids_pytools/cloud_api/dataproc_job.py,sha256=gwqDpa_pmwKB5TAFCJqWs8mOFENcMhwfgCJwOh3TMVg,1426
 spark_rapids_pytools/cloud_api/emr.py,sha256=sjnaYwg-n-Fc5Q_rNgWaTd99g7tG6791MyuzlsXo778,21270
 spark_rapids_pytools/cloud_api/emr_job.py,sha256=EoVrjwSL9pYTJk6QDfokQOudla8KLwGAbR0AAZhk2bA,11269
 spark_rapids_pytools/cloud_api/gstorage.py,sha256=88IzLWOzEphCETGQaVN3_U7wUD0M2doDy759WQmNoJI,4939
+spark_rapids_pytools/cloud_api/onprem.py,sha256=MYSOu7VF_KiY6KIVhn_ZZn8qZCUnqlriQH4hhBKRBiw,2261
 spark_rapids_pytools/cloud_api/s3storage.py,sha256=cFqZETxWt_3Yagk3UlQt65pM4A0qb6idpbi8kUOK5pE,4055
-spark_rapids_pytools/cloud_api/sp_types.py,sha256=XtncEANi3jGbQ-xnEmw6i2jR4ItSBkpHed48mLerL2k,47222
+spark_rapids_pytools/cloud_api/sp_types.py,sha256=mbhbT_Io-DOMTXKVDNCD-SiuNQH3DC-9ttRwpT4VhJw,47363
 spark_rapids_pytools/common/__init__.py,sha256=A8h0t211p8t_aATYiwCLWTwTy564kCcZp-HTWKiO4u8,658
 spark_rapids_pytools/common/exceptions.py,sha256=CK9PF75hSrRh6qaz0aKoiNqaU78E8OfiNydZ4i6WgoM,978
 spark_rapids_pytools/common/prop_manager.py,sha256=ijCqsnrLHn8Ntb1dCEDHqITrxjb3LoEP8Eqhqsb0PS8,5432
 spark_rapids_pytools/common/sys_storage.py,sha256=-La2Oc5EemnRH2x5B24MHlrtEZh6nL2FjqAyrV953Os,14712
 spark_rapids_pytools/common/utilities.py,sha256=nJ6YAdAhVw0f-d6scwb5mpXQz_CNcySCzN4i4KqRSfM,13037
 spark_rapids_pytools/pricing/__init__.py,sha256=Y_IWTtiKulkkoC84SLS8LkRWTi393zeUGHaNogZfOSg,659
 spark_rapids_pytools/pricing/databricks_pricing.py,sha256=q7pRBs2YGJAJnxItBUDYB00f3hdWsFKgK4sXZocjMC4,3522
 spark_rapids_pytools/pricing/dataproc_pricing.py,sha256=4-RuYxW9V0K0mqj2mqCJsKEUbvxPXHCwF4AsK1z6UXE,4247
 spark_rapids_pytools/pricing/emr_pricing.py,sha256=vat14Df8_1By-McNl2ot_75RAwjJkyTfXA24-iefrM4,4717
 spark_rapids_pytools/pricing/price_provider.py,sha256=zZt2ay-BN1vM0cZXkiEzY5kpnYx1brRCJcyfotU71lo,6400
 spark_rapids_pytools/rapids/__init__.py,sha256=xiYk9b76AV_v3fEELcYzXCUPvXQhCD687eJ5RCYQW7M,666
-spark_rapids_pytools/rapids/bootstrap.py,sha256=sUnTfoutyQ4ml7c7jTstMj2x0YqY7VeQzEIF8wuSdbE,8004
-spark_rapids_pytools/rapids/profiling.py,sha256=KYiAf6lDiDADXBjPLRrCYFbc77l-xqEvWUrxnBzK7Fk,12902
-spark_rapids_pytools/rapids/qualification.py,sha256=SlBVzXioVqzDhJAGnkvpsPs0TDpk-bFv10kvJHUArhU,41708
+spark_rapids_pytools/rapids/bootstrap.py,sha256=1q9wq9GzVV9havj-TK0T2U0GYY-WLnpk9qBF5VOPSH4,8165
+spark_rapids_pytools/rapids/profiling.py,sha256=D3Muau5k89CTAg7EA2q757OQsNsEboS6sgJJO_nQIwM,13009
+spark_rapids_pytools/rapids/qualification.py,sha256=0EiSvSbYh2fIqQps83pdBLnDt8z6BhvL90wIxvrisUY,43271
 spark_rapids_pytools/rapids/rapids_job.py,sha256=w7D5jZPCsyb7PqupPfQ02bNnOYAwld7RLMHXqoovUTs,6864
-spark_rapids_pytools/rapids/rapids_tool.py,sha256=TyZxCPaGakzWXzN71S9UyFTIidUhB7SAVSJNXEKMymE,33530
+spark_rapids_pytools/rapids/rapids_tool.py,sha256=3USqFAQRUDKDtsBHUqQSP-EQHp-71USzcYKlmciZh3w,33590
 spark_rapids_pytools/rapids/tool_ctxt.py,sha256=8bWqooTKcIH5ShxTFrXPuSr1KEJeHXU5LdDq8F6ienU,6576
-spark_rapids_pytools/resources/bootstrap-conf.yaml,sha256=IlY7nmhSDlqH0Ahcc_PLP4v6zyU9DQ943LYAv-QbDbU,1295
+spark_rapids_pytools/resources/bootstrap-conf.yaml,sha256=xATXdInBA2NFR2Le9uqioRwyqYB7TnFBYvoaT8NEDZQ,1390
 spark_rapids_pytools/resources/databricks-premium-catalog.json,sha256=XBptMDeu7Abbrv6xC4C1oDuMuEIqnrGEqwrhJFutLJM,30566
 spark_rapids_pytools/resources/databricks_aws-configs.json,sha256=I_VqLdvavbQGUqCNcUqMpNrH5aF0xBZDdN7kuncNKQk,10448
 spark_rapids_pytools/resources/dataproc-configs.json,sha256=9rEXtuCKsLUMyoRsKAeW2OWckM1AXYsE3JuNjFAYmWA,8047
 spark_rapids_pytools/resources/emr-configs.json,sha256=_F5XxM5kFen_iCAWW5Z_jdD6G626h4hH_Uf2dq2Dn6U,8994
+spark_rapids_pytools/resources/onprem-configs.json,sha256=IZkqd_oZAwg2ax528vOuH2_a3eO0apOGh1bQP51RvEo,1684
 spark_rapids_pytools/resources/profiling-conf.yaml,sha256=s7TR1agVORA4g28bYYK7jSMpgMT6s-awMySK0jT4kaA,1460
-spark_rapids_pytools/resources/qualification-conf.yaml,sha256=QOtc8bq0PxPK5LvXFFUd9oeCoy3BEe_gKNx0OGrvrDo,4252
+spark_rapids_pytools/resources/qualification-conf.yaml,sha256=FwQGa9cFXQbEutN9aGb9IEzgRFdHTC7fP_QYTrFWmyc,4273
 spark_rapids_pytools/resources/templates/dataproc-create_gpu_cluster_script.ms,sha256=pigL0kAsg1VaP9Jn9-5oNblJRj0IbeZLh1T5Op8CQqA,671
 spark_rapids_pytools/resources/templates/dataproc-run_bootstrap.ms,sha256=lmnyouNCpbytpRVZ_YbQ6d6hCl3XaYxqzNomxSJwSSk,518
 spark_rapids_pytools/resources/templates/emr-create_gpu_cluster_script.ms,sha256=UIlW0GcYn2fovtcDNYPUmg99h0zKy-DxXGO-1Lr_xQs,855
 spark_rapids_pytools/resources/templates/emr-run_bootstrap.ms,sha256=7xqXBMIu5Tg02KCq_YN9oLLiE3c8jgWl8BUvFQZODhs,537
 spark_rapids_pytools/wrappers/__init__.py,sha256=CQ7Mf-YyBFNl6xmQGsPARv1w5GU3jGliByn5IHCcLkE,630
 spark_rapids_pytools/wrappers/databricks_aws_wrapper.py,sha256=FPuw9pnU8huyRfeGYGc9xljYe9xf9cSsuEg0iaHBYtE,8104
 spark_rapids_pytools/wrappers/dataproc_wrapper.py,sha256=0WTdCPfNoB9s3QCJGhv2t28fbrI7Ovv1esiaVSJiQhw,14685
 spark_rapids_pytools/wrappers/emr_wrapper.py,sha256=vWg2hV1MEyjVFXgtsTvTnaSJYNo9eZ_UVM0XHZJhDTo,15570
-spark_rapids_user_tools-23.4.0.dist-info/LICENSE,sha256=Q4cfQjX201ldPCukEkWCu_m9gqQOGraxXCLYHsrBRbU,18745
-spark_rapids_user_tools-23.4.0.dist-info/METADATA,sha256=05Vvl4hIzAfj43LQfWAvtWxmzci2EPI6kQSPB0SWQNw,2939
-spark_rapids_user_tools-23.4.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-spark_rapids_user_tools-23.4.0.dist-info/entry_points.txt,sha256=OQT0O5JpYFfCtexjgM31__xoCugCT1eHZ9PDT-xq054,152
-spark_rapids_user_tools-23.4.0.dist-info/top_level.txt,sha256=u2CXaBsgoZoiL8XafMeC-8ITnOTUn7Bwrmz8fPjwjio,49
-spark_rapids_user_tools-23.4.0.dist-info/RECORD,,
+spark_rapids_pytools/wrappers/onprem_wrapper.py,sha256=SPRH-hFlVSdLhgUpxgXDKKXEQtvmWkSf2oY4dPsZxbg,3463
+spark_rapids_user_tools-23.4.1.dist-info/LICENSE,sha256=RnI8IUCDrXfGVHFfYTsT8OKEuaOtkMGDQOn8foh7D00,21086
+spark_rapids_user_tools-23.4.1.dist-info/METADATA,sha256=9INd0_jhiD6rB7zmDhlkdBpydLEl_8T1xfswSmGtQTY,2939
+spark_rapids_user_tools-23.4.1.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+spark_rapids_user_tools-23.4.1.dist-info/entry_points.txt,sha256=OQT0O5JpYFfCtexjgM31__xoCugCT1eHZ9PDT-xq054,152
+spark_rapids_user_tools-23.4.1.dist-info/top_level.txt,sha256=u2CXaBsgoZoiL8XafMeC-8ITnOTUn7Bwrmz8fPjwjio,49
+spark_rapids_user_tools-23.4.1.dist-info/RECORD,,
```

