# Comparing `tmp/mmdet3d-1.1.0rc3.tar.gz` & `tmp/mmdet3d-1.1.1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist/mmdet3d-1.1.0rc3.tar", last modified: Tue Jan 10 03:25:08 2023, max compression
+gzip compressed data, was "dist/mmdet3d-1.1.1.tar", last modified: Wed May 31 07:58:38 2023, max compression
```

## Comparing `mmdet3d-1.1.0rc3.tar` & `mmdet3d-1.1.1.tar`

### file list

```diff
@@ -1,612 +1,661 @@
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/
--rw-r--r--   0 runner    (1001) docker     (116)      216 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (116)    19584 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (116)    16455 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/README.md
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/3dssd/
--rw-r--r--   0 runner    (1001) docker     (116)     3642 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/3dssd/3dssd_4xb4_kitti-3d-car.py
--rw-r--r--   0 runner    (1001) docker     (116)      914 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/3dssd/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/
--rw-r--r--   0 runner    (1001) docker     (116)     4516 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/kitti-3d-3class.py
--rw-r--r--   0 runner    (1001) docker     (116)     4422 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/kitti-3d-car.py
--rw-r--r--   0 runner    (1001) docker     (116)     2800 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/kitti-mono3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     3879 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/lyft-3d-range100.py
--rw-r--r--   0 runner    (1001) docker     (116)     4570 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/lyft-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     1960 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/nuim-instance.py
--rw-r--r--   0 runner    (1001) docker     (116)     5007 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     3772 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/nus-mono3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     3460 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/s3dis-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     4656 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/s3dis-seg.py
--rw-r--r--   0 runner    (1001) docker     (116)     3965 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/scannet-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     4551 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/scannet-seg.py
--rw-r--r--   0 runner    (1001) docker     (116)     3455 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/sunrgbd-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     5563 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/waymoD5-3d-3class.py
--rw-r--r--   0 runner    (1001) docker     (116)     5020 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/waymoD5-3d-car.py
--rw-r--r--   0 runner    (1001) docker     (116)     4906 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/waymoD5-fov-mono3d-3class.py
--rw-r--r--   0 runner    (1001) docker     (116)     4902 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/waymoD5-mv-mono3d-3class.py
--rw-r--r--   0 runner    (1001) docker     (116)     4712 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/waymoD5-mv3d-3class.py
--rw-r--r--   0 runner    (1001) docker     (116)      655 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/default_runtime.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/
--rw-r--r--   0 runner    (1001) docker     (116)     3077 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/3dssd.py
--rw-r--r--   0 runner    (1001) docker     (116)     6987 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/cascade-mask-rcnn_r50_fpn.py
--rw-r--r--   0 runner    (1001) docker     (116)     3323 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/centerpoint_pillar02_second_secfpn_nus.py
--rw-r--r--   0 runner    (1001) docker     (116)     3415 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/centerpoint_voxel01_second_secfpn_nus.py
--rw-r--r--   0 runner    (1001) docker     (116)     1044 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/dgcnn.py
--rw-r--r--   0 runner    (1001) docker     (116)      723 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/fcaf3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     2702 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/fcos3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     2737 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/groupfree3d.py
--rw-r--r--   0 runner    (1001) docker     (116)    11349 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/h3dnet.py
--rw-r--r--   0 runner    (1001) docker     (116)     3872 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/imvotenet.py
--rw-r--r--   0 runner    (1001) docker     (116)     4165 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/mask-rcnn_r50_fpn.py
--rw-r--r--   0 runner    (1001) docker     (116)     3572 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/multiview_dfm.py
--rw-r--r--   0 runner    (1001) docker     (116)      180 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/paconv_ssg-cuda.py
--rw-r--r--   0 runner    (1001) docker     (116)     2069 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/paconv_ssg.py
--rw-r--r--   0 runner    (1001) docker     (116)     7191 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/parta2.py
--rw-r--r--   0 runner    (1001) docker     (116)     1830 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/pgd.py
--rw-r--r--   0 runner    (1001) docker     (116)     5661 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/point_rcnn.py
--rw-r--r--   0 runner    (1001) docker     (116)     1223 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/pointnet2_msg.py
--rw-r--r--   0 runner    (1001) docker     (116)     1330 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/pointnet2_ssg.py
--rw-r--r--   0 runner    (1001) docker     (116)     1004 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/pointpillars_hv_fpn_lyft.py
--rw-r--r--   0 runner    (1001) docker     (116)     3448 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/pointpillars_hv_fpn_nus.py
--rw-r--r--   0 runner    (1001) docker     (116)     1016 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/pointpillars_hv_fpn_range100_lyft.py
--rw-r--r--   0 runner    (1001) docker     (116)     3318 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/pointpillars_hv_secfpn_kitti.py
--rw-r--r--   0 runner    (1001) docker     (116)     4115 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/pointpillars_hv_secfpn_waymo.py
--rw-r--r--   0 runner    (1001) docker     (116)     3065 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/second_hv_secfpn_kitti.py
--rw-r--r--   0 runner    (1001) docker     (116)     3859 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/second_hv_secfpn_waymo.py
--rw-r--r--   0 runner    (1001) docker     (116)     1983 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/smoke.py
--rw-r--r--   0 runner    (1001) docker     (116)     2610 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/votenet.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/schedules/
--rw-r--r--   0 runner    (1001) docker     (116)      947 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/schedules/cosine.py
--rw-r--r--   0 runner    (1001) docker     (116)     2015 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/schedules/cyclic-20e.py
--rw-r--r--   0 runner    (1001) docker     (116)     2284 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/schedules/cyclic-40e.py
--rw-r--r--   0 runner    (1001) docker     (116)      814 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/schedules/mmdet-schedule-1x.py
--rw-r--r--   0 runner    (1001) docker     (116)      995 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/schedules/schedule-2x.py
--rw-r--r--   0 runner    (1001) docker     (116)      888 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/schedules/schedule-3x.py
--rw-r--r--   0 runner    (1001) docker     (116)      749 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/schedules/seg-cosine-100e.py
--rw-r--r--   0 runner    (1001) docker     (116)      749 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/schedules/seg-cosine-150e.py
--rw-r--r--   0 runner    (1001) docker     (116)      736 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/schedules/seg-cosine-200e.py
--rw-r--r--   0 runner    (1001) docker     (116)      734 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/schedules/seg-cosine-50e.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/benchmark/
--rw-r--r--   0 runner    (1001) docker     (116)    12862 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/benchmark/hv_PartA2_secfpn_4x8_cyclic_80e_pcdet_kitti-3d-3class.py
--rw-r--r--   0 runner    (1001) docker     (116)     7830 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/benchmark/hv_pointpillars_secfpn_3x8_100e_det3d_kitti-3d-car.py
--rw-r--r--   0 runner    (1001) docker     (116)     8950 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/benchmark/hv_pointpillars_secfpn_4x8_80e_pcdet_kitti-3d-3class.py
--rw-r--r--   0 runner    (1001) docker     (116)     8750 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/benchmark/hv_second_secfpn_4x8_80e_pcdet_kitti-3d-3class.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/centerpoint/
--rw-r--r--   0 runner    (1001) docker     (116)     4869 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/centerpoint/centerpoint_pillar02_second_secfpn_8xb4-cyclic-20e_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)      134 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/centerpoint/centerpoint_pillar02_second_secfpn_head-circlenms_8xb4-cyclic-20e_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)      493 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/centerpoint/centerpoint_pillar02_second_secfpn_head-dcn-circlenms_8xb4-cyclic-20e_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)      445 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/centerpoint/centerpoint_pillar02_second_secfpn_head-dcn_8xb4-cyclic-20e_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     4447 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel0075_second_secfpn_8xb4-cyclic-20e_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)      135 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel0075_second_secfpn_head-circlenms_8xb4-cyclic-20e_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)      494 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel0075_second_secfpn_head-dcn-circlenms_8xb4-cyclic-20e_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     1563 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel0075_second_secfpn_head-dcn-circlenms_8xb4-flip-tta-cyclic-20e_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)      446 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel0075_second_secfpn_head-dcn_8xb4-cyclic-20e_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     1552 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel0075_second_secfpn_head-dcn_8xb4-flip-tta-cyclic-20e_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     1641 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel0075_second_secfpn_head-dcn_8xb4-tta-cyclic-20e_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     4901 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel01_second_secfpn_8xb4-cyclic-20e_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)      133 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel01_second_secfpn_head-circlenms_8xb4-cyclic-20e_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)      492 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel01_second_secfpn_head-dcn-circlenms_8xb4-cyclic-20e_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)      444 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel01_second_secfpn_head-dcn_8xb4-cyclic-20e_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     4378 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/centerpoint/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dfm/
--rw-r--r--   0 runner    (1001) docker     (116)     1298 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dfm/multiview-dfm_r101-dcn_16xb2_waymoD5-3d-3class.py
--rw-r--r--   0 runner    (1001) docker     (116)     1840 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dfm/multiview-dfm_r101-dcn_centerhead_16xb2_waymoD5-3d-3class.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dgcnn/
--rw-r--r--   0 runner    (1001) docker     (116)      568 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area1.py
--rw-r--r--   0 runner    (1001) docker     (116)      568 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area2.py
--rw-r--r--   0 runner    (1001) docker     (116)      568 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area3.py
--rw-r--r--   0 runner    (1001) docker     (116)      568 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area4.py
--rw-r--r--   0 runner    (1001) docker     (116)      695 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area5.py
--rw-r--r--   0 runner    (1001) docker     (116)      568 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area6.py
--rw-r--r--   0 runner    (1001) docker     (116)     3575 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dgcnn/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dynamic_voxelization/
--rw-r--r--   0 runner    (1001) docker     (116)     2210 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dynamic_voxelization/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (116)      657 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dynamic_voxelization/pointpillars_dv_secfpn_8xb6-160e_kitti-3d-car.py
--rw-r--r--   0 runner    (1001) docker     (116)      715 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dynamic_voxelization/second_dv_secfpn_8xb2-cosine-80e_kitti-3d-3class.py
--rw-r--r--   0 runner    (1001) docker     (116)      601 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dynamic_voxelization/second_dv_secfpn_8xb6-80e_kitti-3d-car.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/fcaf3d/
--rw-r--r--   0 runner    (1001) docker     (116)      700 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/fcaf3d/fcaf3d_2xb8_s3dis-3d-5class.py
--rw-r--r--   0 runner    (1001) docker     (116)     2702 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/fcaf3d/fcaf3d_2xb8_scannet-3d-18class.py
--rw-r--r--   0 runner    (1001) docker     (116)     2523 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/fcaf3d/fcaf3d_2xb8_sunrgbd-3d-10class.py
--rw-r--r--   0 runner    (1001) docker     (116)     1981 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/fcaf3d/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/fcos3d/
--rw-r--r--   0 runner    (1001) docker     (116)     2450 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/fcos3d/fcos3d_r101-caffe-dcn_fpn_head-gn_8xb2-1x_nus-mono3d.py
--rw-r--r--   0 runner    (1001) docker     (116)      295 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/fcos3d/fcos3d_r101-caffe-dcn_fpn_head-gn_8xb2-1x_nus-mono3d_finetune.py
--rw-r--r--   0 runner    (1001) docker     (116)     1691 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/fcos3d/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/free_anchor/
--rw-r--r--   0 runner    (1001) docker     (116)     5576 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/free_anchor/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (116)     1628 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/free_anchor/pointpillars_hv_fpn_head-free-anchor_sbn-all_8xb4-2x_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)      600 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/free_anchor/pointpillars_hv_regnet-1.6gf_fpn_head-free-anchor_sbn-all_8xb4-2x_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     2595 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/free_anchor/pointpillars_hv_regnet-1.6gf_fpn_head-free-anchor_sbn-all_8xb4-strong-aug-3x_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)      601 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/free_anchor/pointpillars_hv_regnet-3.2gf_fpn_head-free-anchor_sbn-all_8xb4-2x_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     2724 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/free_anchor/pointpillars_hv_regnet-3.2gf_fpn_head-free-anchor_sbn-all_8xb4-strong-aug-3x_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)      599 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/free_anchor/pointpillars_hv_regnet-400mf_fpn_head-free-anchor_sbn-all_8xb4-2x_nus-3d.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/groupfree3d/
--rw-r--r--   0 runner    (1001) docker     (116)     7360 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/groupfree3d/groupfree3d_head-L12-O256_4xb8_scannet-seg.py
--rw-r--r--   0 runner    (1001) docker     (116)     7329 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/groupfree3d/groupfree3d_head-L6-O256_4xb8_scannet-seg.py
--rw-r--r--   0 runner    (1001) docker     (116)     7887 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/groupfree3d/groupfree3d_w2x-head-L12-O256_4xb8_scannet-seg.py
--rw-r--r--   0 runner    (1001) docker     (116)     7913 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/groupfree3d/groupfree3d_w2x-head-L12-O512_4xb8_scannet-seg.py
--rw-r--r--   0 runner    (1001) docker     (116)     2847 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/groupfree3d/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/h3dnet/
--rw-r--r--   0 runner    (1001) docker     (116)     3270 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/h3dnet/h3dnet_8xb3_scannet-seg.py
--rw-r--r--   0 runner    (1001) docker     (116)      969 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/h3dnet/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/imvotenet/
--rw-r--r--   0 runner    (1001) docker     (116)     2253 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/imvotenet/imvotenet_faster-rcnn-r50_fpn_4xb2_sunrgbd-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     8035 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/imvotenet/imvotenet_stage2_8xb16_sunrgbd-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     1637 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/imvotenet/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/imvoxelnet/
--rw-r--r--   0 runner    (1001) docker     (116)     4427 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/imvoxelnet/imvoxelnet_2xb4_sunrgbd-3d-10class.py
--rw-r--r--   0 runner    (1001) docker     (116)     5511 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/imvoxelnet/imvoxelnet_8xb4_kitti-3d-car.py
--rw-r--r--   0 runner    (1001) docker     (116)     1017 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/imvoxelnet/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/monoflex/
--rw-r--r--   0 runner    (1001) docker     (116)     1044 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/monoflex/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/mvxnet/
--rw-r--r--   0 runner    (1001) docker     (116)     1100 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/mvxnet/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (116)     8651 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/mvxnet/mvxnet_fpn_dv_second_secfpn_8xb2-80e_kitti-3d-3class.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/
--rw-r--r--   0 runner    (1001) docker     (116)      343 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/cascade-mask-rcnn-r50-fpn_coco-20e_nuim.py
--rw-r--r--   0 runner    (1001) docker     (116)      127 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/cascade-mask-rcnn_r101_fpn_1x_nuim.py
--rw-r--r--   0 runner    (1001) docker     (116)     2335 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/cascade-mask-rcnn_r50_fpn_1x_nuim.py
--rw-r--r--   0 runner    (1001) docker     (116)      263 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/cascade-mask-rcnn_r50_fpn_coco-20e-1x_nuim.py
--rw-r--r--   0 runner    (1001) docker     (116)      376 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/cascade-mask-rcnn_x101_32x4d_fpn_1x_nuim.py
--rw-r--r--   0 runner    (1001) docker     (116)     1164 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/htc_r50_fpn_1x_nuim.py
--rw-r--r--   0 runner    (1001) docker     (116)      174 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/htc_r50_fpn_coco-20e-1x_nuim.py
--rw-r--r--   0 runner    (1001) docker     (116)      124 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/htc_r50_fpn_coco-20e_nuim.py
--rw-r--r--   0 runner    (1001) docker     (116)     7778 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/htc_r50_fpn_head-without-semantic_1x_nuim.py
--rw-r--r--   0 runner    (1001) docker     (116)      859 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/htc_x101_64x4d_fpn_dconv_c3-c5_coco-20e-1xb16_nuim.py
--rw-r--r--   0 runner    (1001) docker     (116)      119 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/mask-rcnn_r101_fpn_1x_nuim.py
--rw-r--r--   0 runner    (1001) docker     (116)     1298 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/mask-rcnn_r50_caffe_fpn_1x_nuim.py
--rw-r--r--   0 runner    (1001) docker     (116)     1528 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/mask-rcnn_r50_caffe_fpn_coco-3x_1x_nuim.py
--rw-r--r--   0 runner    (1001) docker     (116)     1607 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/mask-rcnn_r50_caffe_fpn_coco-3x_20e_nuim.py
--rw-r--r--   0 runner    (1001) docker     (116)      286 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/mask-rcnn_r50_fpn_1x_nuim.py
--rw-r--r--   0 runner    (1001) docker     (116)      478 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/mask-rcnn_r50_fpn_coco-2x_1x_nuim.py
--rw-r--r--   0 runner    (1001) docker     (116)     1108 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/mask-rcnn_r50_fpn_coco-2x_1x_nus-2d.py
--rw-r--r--   0 runner    (1001) docker     (116)      368 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/mask-rcnn_x101_32x4d_fpn_1x_nuim.py
--rw-r--r--   0 runner    (1001) docker     (116)    10039 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/paconv/
--rw-r--r--   0 runner    (1001) docker     (116)     1590 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/paconv/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (116)     1818 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/paconv/paconv_ssg-cuda_8xb8-cosine-200e_s3dis-seg.py
--rw-r--r--   0 runner    (1001) docker     (116)     1729 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/paconv/paconv_ssg_8xb8-cosine-150e_s3dis-seg.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/parta2/
--rw-r--r--   0 runner    (1001) docker     (116)     1593 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/parta2/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (116)     4746 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/parta2/parta2_hv_secfpn_8xb2-cyclic-80e_kitti-3d-3class.py
--rw-r--r--   0 runner    (1001) docker     (116)     4736 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/parta2/parta2_hv_secfpn_8xb2-cyclic-80e_kitti-3d-car.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pgd/
--rw-r--r--   0 runner    (1001) docker     (116)     3205 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pgd/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (116)     3526 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pgd/pgd_r101-caffe_fpn_head-gn_16xb2-1x_nus-mono3d.py
--rw-r--r--   0 runner    (1001) docker     (116)      324 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pgd/pgd_r101-caffe_fpn_head-gn_16xb2-1x_nus-mono3d_finetune.py
--rw-r--r--   0 runner    (1001) docker     (116)      403 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pgd/pgd_r101-caffe_fpn_head-gn_16xb2-2x_nus-mono3d.py
--rw-r--r--   0 runner    (1001) docker     (116)      324 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pgd/pgd_r101-caffe_fpn_head-gn_16xb2-2x_nus-mono3d_finetune.py
--rw-r--r--   0 runner    (1001) docker     (116)     4508 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pgd/pgd_r101-caffe_fpn_head-gn_4xb3-4x_kitti-mono3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     3660 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pgd/pgd_r101_fpn-head_dcn_16xb3_waymoD5-fov-mono3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     3652 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pgd/pgd_r101_fpn-head_dcn_16xb3_waymoD5-mono3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     3659 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pgd/pgd_r101_fpn-head_dcn_16xb3_waymoD5-mv-mono3d.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/point_rcnn/
--rw-r--r--   0 runner    (1001) docker     (116)      966 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/point_rcnn/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (116)     4447 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/point_rcnn/point-rcnn_8xb2_kitti-3d-3class.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointnet2/
--rw-r--r--   0 runner    (1001) docker     (116)     3986 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointnet2/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (116)     3615 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointnet2/pointnet2_msg_2xb16-cosine-250e_scannet-seg-xyz-only.py
--rw-r--r--   0 runner    (1001) docker     (116)     1280 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointnet2/pointnet2_msg_2xb16-cosine-250e_scannet-seg.py
--rw-r--r--   0 runner    (1001) docker     (116)      831 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointnet2/pointnet2_msg_2xb16-cosine-80e_s3dis-seg.py
--rw-r--r--   0 runner    (1001) docker     (116)     3521 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointnet2/pointnet2_ssg_2xb16-cosine-200e_scannet-seg-xyz-only.py
--rw-r--r--   0 runner    (1001) docker     (116)     1186 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointnet2/pointnet2_ssg_2xb16-cosine-200e_scannet-seg.py
--rw-r--r--   0 runner    (1001) docker     (116)      738 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointnet2/pointnet2_ssg_2xb16-cosine-50e_s3dis-seg.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointpillars/
--rw-r--r--   0 runner    (1001) docker     (116)     8125 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointpillars/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (116)      433 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_fpn_sbn-all_8xb2-2x_lyft-3d-range100.py
--rw-r--r--   0 runner    (1001) docker     (116)      415 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_fpn_sbn-all_8xb2-2x_lyft-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)      460 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_fpn_sbn-all_8xb4-2x_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)      140 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_fpn_sbn-all_8xb4-amp-2x_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     4061 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_8xb6-160e_kitti-3d-3class.py
--rw-r--r--   0 runner    (1001) docker     (116)     2993 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_8xb6-160e_kitti-3d-car.py
--rw-r--r--   0 runner    (1001) docker     (116)      521 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_16xb2-2x_waymo-3d-3class.py
--rw-r--r--   0 runner    (1001) docker     (116)     1428 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_16xb2-2x_waymo-3d-car.py
--rw-r--r--   0 runner    (1001) docker     (116)      194 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_16xb2-2x_waymoD5-3d-3class.py
--rw-r--r--   0 runner    (1001) docker     (116)     1342 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_16xb2-2x_waymoD5-3d-car.py
--rw-r--r--   0 runner    (1001) docker     (116)     2045 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_8xb2-2x_lyft-3d-range100.py
--rw-r--r--   0 runner    (1001) docker     (116)     1996 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_8xb2-2x_lyft-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     2041 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_8xb4-2x_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)      143 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_8xb4-amp-2x_nus-3d.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pv_rcnn/
--rw-r--r--   0 runner    (1001) docker     (116)     1001 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pv_rcnn/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (116)    12065 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pv_rcnn/pv_rcnn_8xb2-80e_kitti-3d-3class.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/regnet/
--rw-r--r--   0 runner    (1001) docker     (116)     3436 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/regnet/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (116)      744 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-1.6gf_fpn_sbn-all_8xb4-2x_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     1035 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_fpn_sbn-all_8xb2-2x_lyft-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)      793 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_fpn_sbn-all_8xb4-2x_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     1053 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_fpn_sbn-all_range100_8xb2-2x_lyft-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     1677 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_secfpn_sbn-all_8xb2-2x_lyft-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     1676 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_secfpn_sbn-all_8xb4-2x_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     1728 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_secfpn_sbn-all_range100_8xb2-2x_lyft-3d.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/sassd/
--rw-r--r--   0 runner    (1001) docker     (116)     3198 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/sassd/sassd_8xb6-80e_kitti-3d-3class.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/second/
--rw-r--r--   0 runner    (1001) docker     (116)     3391 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/second/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (116)      180 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/second/second_hv_secfpn_8xb6-80e_kitti-3d-3class.py
--rw-r--r--   0 runner    (1001) docker     (116)      972 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/second/second_hv_secfpn_8xb6-80e_kitti-3d-car.py
--rw-r--r--   0 runner    (1001) docker     (116)      139 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/second/second_hv_secfpn_8xb6-amp-80e_kitti-3d-3class.py
--rw-r--r--   0 runner    (1001) docker     (116)      136 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/second/second_hv_secfpn_8xb6-amp-80e_kitti-3d-car.py
--rw-r--r--   0 runner    (1001) docker     (116)     4664 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/second/second_hv_secfpn_sbn-all_16xb2-2x_waymoD5-3d-3class.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/smoke/
--rw-r--r--   0 runner    (1001) docker     (116)     1039 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/smoke/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (116)     2151 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/smoke/smoke_dla34_dlaneck_gn-all_4xb8-6x_kitti-mono3d.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/ssn/
--rw-r--r--   0 runner    (1001) docker     (116)     2751 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/ssn/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (116)      742 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/ssn/ssn_hv_regnet-400mf_secfpn_sbn-all_16xb1-2x_lyft-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)      727 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/ssn/ssn_hv_regnet-400mf_secfpn_sbn-all_16xb2-2x_nus-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     9147 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/ssn/ssn_hv_secfpn_sbn-all_16xb2-2x_lyft-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     9774 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/ssn/ssn_hv_secfpn_sbn-all_16xb2-2x_nus-3d.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/votenet/
--rw-r--r--   0 runner    (1001) docker     (116)     1890 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/votenet/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (116)     1108 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/votenet/votenet_8xb16_sunrgbd-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     1773 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/votenet/votenet_8xb8_scannet-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)      225 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/votenet/votenet_head-iouloss_8xb8_scannet-3d.py
--rw-r--r--   0 runner    (1001) docker     (116)      821 2023-01-10 03:25:07.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/model-index.yml
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/analysis_tools/
--rw-r--r--   0 runner    (1001) docker     (116)     7535 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/analysis_tools/analyze_logs.py
--rw-r--r--   0 runner    (1001) docker     (116)     2883 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/analysis_tools/benchmark.py
--rw-r--r--   0 runner    (1001) docker     (116)     2694 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/analysis_tools/get_flops.py
--rw-r--r--   0 runner    (1001) docker     (116)    13299 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/create_data.py
--rwxr-xr-x   0 runner    (1001) docker     (116)      565 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/create_data.sh
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/
--rw-r--r--   0 runner    (1001) docker     (116)    25037 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/create_gt_database.py
--rw-r--r--   0 runner    (1001) docker     (116)     5097 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/indoor_converter.py
--rw-r--r--   0 runner    (1001) docker     (116)    24425 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/kitti_converter.py
--rw-r--r--   0 runner    (1001) docker     (116)    24980 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/kitti_data_utils.py
--rw-r--r--   0 runner    (1001) docker     (116)    10712 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/lyft_converter.py
--rw-r--r--   0 runner    (1001) docker     (116)     1411 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/lyft_data_fixer.py
--rw-r--r--   0 runner    (1001) docker     (116)     7644 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/nuimage_converter.py
--rw-r--r--   0 runner    (1001) docker     (116)    24485 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/nuscenes_converter.py
--rw-r--r--   0 runner    (1001) docker     (116)    10109 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/s3dis_data_utils.py
--rw-r--r--   0 runner    (1001) docker     (116)    12779 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/scannet_data_utils.py
--rw-r--r--   0 runner    (1001) docker     (116)     9109 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/sunrgbd_data_utils.py
--rw-r--r--   0 runner    (1001) docker     (116)    48219 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/update_infos_to_v2.py
--rw-r--r--   0 runner    (1001) docker     (116)    24687 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/waymo_converter.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/deployment/
--rw-r--r--   0 runner    (1001) docker     (116)     3820 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/deployment/mmdet3d2torchserve.py
--rw-r--r--   0 runner    (1001) docker     (116)     4246 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/deployment/mmdet3d_handler.py
--rw-r--r--   0 runner    (1001) docker     (116)     1930 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/deployment/test_torchserver.py
--rwxr-xr-x   0 runner    (1001) docker     (116)      479 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dist_test.sh
--rwxr-xr-x   0 runner    (1001) docker     (116)      442 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dist_train.sh
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/misc/
--rw-r--r--   0 runner    (1001) docker     (116)     5207 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/misc/browse_dataset.py
--rw-r--r--   0 runner    (1001) docker     (116)     2246 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/misc/fuse_conv_bn.py
--rw-r--r--   0 runner    (1001) docker     (116)      643 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/misc/print_config.py
--rw-r--r--   0 runner    (1001) docker     (116)     1456 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/misc/visualize_results.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/model_converters/
--rw-r--r--   0 runner    (1001) docker     (116)     6149 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/model_converters/convert_h3dnet_checkpoints.py
--rw-r--r--   0 runner    (1001) docker     (116)     5091 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/model_converters/convert_votenet_checkpoints.py
--rw-r--r--   0 runner    (1001) docker     (116)     1076 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/model_converters/publish_model.py
--rw-r--r--   0 runner    (1001) docker     (116)     3063 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/model_converters/regnet2mmdet.py
--rwxr-xr-x   0 runner    (1001) docker     (116)      566 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/slurm_test.sh
--rwxr-xr-x   0 runner    (1001) docker     (116)      574 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/slurm_train.sh
--rw-r--r--   0 runner    (1001) docker     (116)     4581 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/test.py
--rw-r--r--   0 runner    (1001) docker     (116)     4558 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/train.py
--rw-r--r--   0 runner    (1001) docker     (116)     6566 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/update_data_coords.py
--rw-r--r--   0 runner    (1001) docker     (116)      529 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/update_data_coords.sh
--rw-r--r--   0 runner    (1001) docker     (116)     1457 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/apis/
--rw-r--r--   0 runner    (1001) docker     (116)      459 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/apis/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)    12476 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/apis/inference.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/datasets/
--rw-r--r--   0 runner    (1001) docker     (116)     2253 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/datasets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)    16257 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/datasets/convert_utils.py
--rw-r--r--   0 runner    (1001) docker     (116)     2843 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/datasets/dataset_wrappers.py
--rw-r--r--   0 runner    (1001) docker     (116)    17076 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/datasets/det3d_dataset.py
--rw-r--r--   0 runner    (1001) docker     (116)     8812 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/datasets/kitti2d_dataset.py
--rw-r--r--   0 runner    (1001) docker     (116)     7288 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/datasets/kitti_dataset.py
--rw-r--r--   0 runner    (1001) docker     (116)     3916 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/datasets/lyft_dataset.py
--rw-r--r--   0 runner    (1001) docker     (116)     9827 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/datasets/nuscenes_dataset.py
--rw-r--r--   0 runner    (1001) docker     (116)    15383 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/datasets/s3dis_dataset.py
--rw-r--r--   0 runner    (1001) docker     (116)    13254 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/datasets/scannet_dataset.py
--rw-r--r--   0 runner    (1001) docker     (116)    12382 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/datasets/seg3d_dataset.py
--rw-r--r--   0 runner    (1001) docker     (116)     3319 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/datasets/semantickitti_dataset.py
--rw-r--r--   0 runner    (1001) docker     (116)     5470 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/datasets/sunrgbd_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/datasets/transforms/
--rw-r--r--   0 runner    (1001) docker     (116)     1870 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/datasets/transforms/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)    17093 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/datasets/transforms/data_augment_utils.py
--rw-r--r--   0 runner    (1001) docker     (116)    13086 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/datasets/transforms/dbsampler.py
--rw-r--r--   0 runner    (1001) docker     (116)     8582 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/datasets/transforms/formating.py
--rw-r--r--   0 runner    (1001) docker     (116)    37640 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/datasets/transforms/loading.py
--rw-r--r--   0 runner    (1001) docker     (116)     5608 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/datasets/transforms/test_time_aug.py
--rw-r--r--   0 runner    (1001) docker     (116)    90137 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/datasets/transforms/transforms_3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     5206 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/datasets/utils.py
--rw-r--r--   0 runner    (1001) docker     (116)    10773 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/datasets/waymo_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/engine/
--rw-r--r--   0 runner    (1001) docker     (116)      160 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/engine/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/engine/hooks/
--rw-r--r--   0 runner    (1001) docker     (116)      297 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/engine/hooks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     1339 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/engine/hooks/benchmark_hook.py
--rw-r--r--   0 runner    (1001) docker     (116)     2152 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/engine/hooks/disable_object_sample_hook.py
--rw-r--r--   0 runner    (1001) docker     (116)     7512 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/engine/hooks/visualization_hook.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/
--rw-r--r--   0 runner    (1001) docker     (116)     1406 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/
--rw-r--r--   0 runner    (1001) docker     (116)     1045 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)    10707 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/indoor_eval.py
--rw-r--r--   0 runner    (1001) docker     (116)     5191 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/instance_seg_eval.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/kitti_utils/
--rw-r--r--   0 runner    (1001) docker     (116)      197 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/kitti_utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)    37841 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/kitti_utils/eval.py
--rw-r--r--   0 runner    (1001) docker     (116)    13153 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/kitti_utils/rotate_iou.py
--rw-r--r--   0 runner    (1001) docker     (116)    10382 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/lyft_eval.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/scannet_utils/
--rw-r--r--   0 runner    (1001) docker     (116)      167 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/scannet_utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)    15624 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/scannet_utils/evaluate_semantic_instance.py
--rw-r--r--   0 runner    (1001) docker     (116)     2607 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/scannet_utils/util_3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     3720 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/seg_eval.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/waymo_utils/
--rw-r--r--   0 runner    (1001) docker     (116)      131 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/waymo_utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)    16481 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/waymo_utils/prediction_to_waymo.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/metrics/
--rw-r--r--   0 runner    (1001) docker     (116)      596 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/metrics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     6790 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/metrics/indoor_metric.py
--rw-r--r--   0 runner    (1001) docker     (116)     3556 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/metrics/instance_seg_metric.py
--rw-r--r--   0 runner    (1001) docker     (116)    28781 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/metrics/kitti_metric.py
--rw-r--r--   0 runner    (1001) docker     (116)    15980 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/metrics/lyft_metric.py
--rw-r--r--   0 runner    (1001) docker     (116)    30914 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/metrics/nuscenes_metric.py
--rw-r--r--   0 runner    (1001) docker     (116)     5343 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/metrics/seg_metric.py
--rw-r--r--   0 runner    (1001) docker     (116)    31095 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/evaluation/metrics/waymo_metric.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/
--rw-r--r--   0 runner    (1001) docker     (116)      750 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/backbones/
--rw-r--r--   0 runner    (1001) docker     (116)      617 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/backbones/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     1339 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/backbones/base_pointnet.py
--rw-r--r--   0 runner    (1001) docker     (116)     3688 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/backbones/dgcnn.py
--rw-r--r--   0 runner    (1001) docker     (116)    14804 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/backbones/dla.py
--rw-r--r--   0 runner    (1001) docker     (116)     4202 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/backbones/mink_resnet.py
--rw-r--r--   0 runner    (1001) docker     (116)     4702 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/backbones/multi_backbone.py
--rw-r--r--   0 runner    (1001) docker     (116)     3354 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/backbones/nostem_regnet.py
--rw-r--r--   0 runner    (1001) docker     (116)     7245 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/backbones/pointnet2_sa_msg.py
--rw-r--r--   0 runner    (1001) docker     (116)     5491 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/backbones/pointnet2_sa_ssg.py
--rw-r--r--   0 runner    (1001) docker     (116)     3249 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/backbones/second.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/data_preprocessors/
--rw-r--r--   0 runner    (1001) docker     (116)      138 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/data_preprocessors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)    16582 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/data_preprocessors/data_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (116)     2827 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/data_preprocessors/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/decode_heads/
--rw-r--r--   0 runner    (1001) docker     (116)      216 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/decode_heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     6235 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/decode_heads/decode_head.py
--rw-r--r--   0 runner    (1001) docker     (116)     2096 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/decode_heads/dgcnn_head.py
--rw-r--r--   0 runner    (1001) docker     (116)     2568 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/decode_heads/paconv_head.py
--rw-r--r--   0 runner    (1001) docker     (116)     3222 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/decode_heads/pointnet2_head.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/
--rw-r--r--   0 runner    (1001) docker     (116)     1226 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)    18494 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/anchor3d_head.py
--rw-r--r--   0 runner    (1001) docker     (116)    20566 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/anchor_free_mono3d_head.py
--rw-r--r--   0 runner    (1001) docker     (116)    16518 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/base_3d_dense_head.py
--rw-r--r--   0 runner    (1001) docker     (116)     4407 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/base_conv_bbox_head.py
--rw-r--r--   0 runner    (1001) docker     (116)     7535 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/base_mono3d_dense_head.py
--rw-r--r--   0 runner    (1001) docker     (116)    37205 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/centerpoint_head.py
--rw-r--r--   0 runner    (1001) docker     (116)    29985 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/fcaf3d_head.py
--rw-r--r--   0 runner    (1001) docker     (116)    43730 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/fcos_mono3d_head.py
--rw-r--r--   0 runner    (1001) docker     (116)    12085 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/free_anchor3d_head.py
--rw-r--r--   0 runner    (1001) docker     (116)    47843 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/groupfree3d_head.py
--rw-r--r--   0 runner    (1001) docker     (116)    29871 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/imvoxel_head.py
--rw-r--r--   0 runner    (1001) docker     (116)    36543 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/monoflex_head.py
--rw-r--r--   0 runner    (1001) docker     (116)    18097 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/parta2_rpn_head.py
--rw-r--r--   0 runner    (1001) docker     (116)    59511 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/pgd_head.py
--rw-r--r--   0 runner    (1001) docker     (116)    21560 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/point_rpn_head.py
--rw-r--r--   0 runner    (1001) docker     (116)    22790 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/shape_aware_head.py
--rw-r--r--   0 runner    (1001) docker     (116)    23223 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/smoke_mono3d_head.py
--rw-r--r--   0 runner    (1001) docker     (116)    25624 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/ssd_3d_head.py
--rw-r--r--   0 runner    (1001) docker     (116)    17338 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/train_mixins.py
--rw-r--r--   0 runner    (1001) docker     (116)    36207 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/vote_head.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/
--rw-r--r--   0 runner    (1001) docker     (116)     1282 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     6614 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/base.py
--rw-r--r--   0 runner    (1001) docker     (116)     2936 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/centerpoint.py
--rw-r--r--   0 runner    (1001) docker     (116)    10030 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/dfm.py
--rw-r--r--   0 runner    (1001) docker     (116)     1710 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/dynamic_voxelnet.py
--rw-r--r--   0 runner    (1001) docker     (116)     4213 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/fcos_mono3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     3436 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/groupfree3dnet.py
--rw-r--r--   0 runner    (1001) docker     (116)     5825 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/h3dnet.py
--rw-r--r--   0 runner    (1001) docker     (116)    23036 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/imvotenet.py
--rw-r--r--   0 runner    (1001) docker     (116)    11896 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/imvoxelnet.py
--rw-r--r--   0 runner    (1001) docker     (116)     5766 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/mink_single_stage.py
--rw-r--r--   0 runner    (1001) docker     (116)    18515 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/multiview_dfm.py
--rw-r--r--   0 runner    (1001) docker     (116)     2001 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/mvx_faster_rcnn.py
--rw-r--r--   0 runner    (1001) docker     (116)    16714 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/mvx_two_stage.py
--rw-r--r--   0 runner    (1001) docker     (116)     2563 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/parta2.py
--rw-r--r--   0 runner    (1001) docker     (116)     2443 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/point_rcnn.py
--rw-r--r--   0 runner    (1001) docker     (116)     9948 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/pv_rcnn.py
--rw-r--r--   0 runner    (1001) docker     (116)     4016 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/sassd.py
--rw-r--r--   0 runner    (1001) docker     (116)     6800 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/single_stage.py
--rw-r--r--   0 runner    (1001) docker     (116)     3693 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/single_stage_mono3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     1774 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/smoke_mono3d.py
--rw-r--r--   0 runner    (1001) docker     (116)      669 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/ssd3dnet.py
--rw-r--r--   0 runner    (1001) docker     (116)     8191 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/two_stage.py
--rw-r--r--   0 runner    (1001) docker     (116)     6356 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/votenet.py
--rw-r--r--   0 runner    (1001) docker     (116)     1870 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/detectors/voxelnet.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/
--rw-r--r--   0 runner    (1001) docker     (116)     1655 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)    10877 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/box3d_nms.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/dgcnn_modules/
--rw-r--r--   0 runner    (1001) docker     (116)      240 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/dgcnn_modules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     2505 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/dgcnn_modules/dgcnn_fa_module.py
--rw-r--r--   0 runner    (1001) docker     (116)     2163 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/dgcnn_modules/dgcnn_fp_module.py
--rw-r--r--   0 runner    (1001) docker     (116)     8928 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/dgcnn_modules/dgcnn_gf_module.py
--rw-r--r--   0 runner    (1001) docker     (116)     3112 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/edge_fusion_module.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/fusion_layers/
--rw-r--r--   0 runner    (1001) docker     (116)      367 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/fusion_layers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     8001 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/fusion_layers/coord_transform.py
--rw-r--r--   0 runner    (1001) docker     (116)    17816 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/fusion_layers/point_fusion.py
--rw-r--r--   0 runner    (1001) docker     (116)     9135 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/fusion_layers/vote_fusion.py
--rw-r--r--   0 runner    (1001) docker     (116)     2162 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/mlp.py
--rw-r--r--   0 runner    (1001) docker     (116)     5617 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/norm.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/paconv/
--rw-r--r--   0 runner    (1001) docker     (116)      123 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/paconv/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)    16190 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/paconv/paconv.py
--rw-r--r--   0 runner    (1001) docker     (116)     3824 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/paconv/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/pointnet_modules/
--rw-r--r--   0 runner    (1001) docker     (116)      590 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/pointnet_modules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     1359 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/pointnet_modules/builder.py
--rw-r--r--   0 runner    (1001) docker     (116)    15034 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/pointnet_modules/paconv_sa_module.py
--rw-r--r--   0 runner    (1001) docker     (116)     3011 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/pointnet_modules/point_fp_module.py
--rw-r--r--   0 runner    (1001) docker     (116)    14492 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/pointnet_modules/point_sa_module.py
--rw-r--r--   0 runner    (1001) docker     (116)     7976 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/pointnet_modules/stack_point_sa_module.py
--rw-r--r--   0 runner    (1001) docker     (116)     7231 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/sparse_block.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/spconv/
--rw-r--r--   0 runner    (1001) docker     (116)      384 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/spconv/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/spconv/overwrite_spconv/
--rw-r--r--   0 runner    (1001) docker     (116)      124 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/spconv/overwrite_spconv/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     4632 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/spconv/overwrite_spconv/write_spconv2.py
--rw-r--r--   0 runner    (1001) docker     (116)     6086 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/transformer.py
--rw-r--r--   0 runner    (1001) docker     (116)     7840 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/layers/vote_module.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/losses/
--rw-r--r--   0 runner    (1001) docker     (116)      818 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/losses/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     2810 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/losses/axis_aligned_iou_loss.py
--rw-r--r--   0 runner    (1001) docker     (116)     5524 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/losses/chamfer_distance.py
--rw-r--r--   0 runner    (1001) docker     (116)     3473 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/losses/multibin_loss.py
--rw-r--r--   0 runner    (1001) docker     (116)     4078 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/losses/paconv_regularization_loss.py
--rw-r--r--   0 runner    (1001) docker     (116)     3150 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/losses/rotated_iou_loss.py
--rw-r--r--   0 runner    (1001) docker     (116)     6355 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/losses/uncertain_smooth_l1_loss.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/middle_encoders/
--rw-r--r--   0 runner    (1001) docker     (116)      370 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/middle_encoders/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     3651 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/middle_encoders/pillar_scatter.py
--rw-r--r--   0 runner    (1001) docker     (116)    20530 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/middle_encoders/sparse_encoder.py
--rw-r--r--   0 runner    (1001) docker     (116)    11897 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/middle_encoders/sparse_unet.py
--rw-r--r--   0 runner    (1001) docker     (116)    13989 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/middle_encoders/voxel_set_abstraction.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/necks/
--rw-r--r--   0 runner    (1001) docker     (116)      376 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/necks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     8291 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/necks/dla_neck.py
--rw-r--r--   0 runner    (1001) docker     (116)     7722 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/necks/imvoxel_neck.py
--rw-r--r--   0 runner    (1001) docker     (116)     3204 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/necks/pointnet2_fp_neck.py
--rw-r--r--   0 runner    (1001) docker     (116)     3438 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/necks/second_fpn.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/
--rw-r--r--   0 runner    (1001) docker     (116)      689 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     1997 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/base_3droi_head.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/bbox_heads/
--rw-r--r--   0 runner    (1001) docker     (116)      709 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/bbox_heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)    42934 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/bbox_heads/h3d_bbox_head.py
--rw-r--r--   0 runner    (1001) docker     (116)    26660 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/bbox_heads/parta2_bbox_head.py
--rw-r--r--   0 runner    (1001) docker     (116)    25475 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/bbox_heads/point_rcnn_bbox_head.py
--rw-r--r--   0 runner    (1001) docker     (116)    20899 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/bbox_heads/pv_rcnn_bbox_head.py
--rw-r--r--   0 runner    (1001) docker     (116)     4447 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/h3d_roi_head.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/mask_heads/
--rw-r--r--   0 runner    (1001) docker     (116)      308 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/mask_heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     6402 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/mask_heads/foreground_segmentation_head.py
--rw-r--r--   0 runner    (1001) docker     (116)     8432 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/mask_heads/pointwise_semantic_head.py
--rw-r--r--   0 runner    (1001) docker     (116)    46441 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/mask_heads/primitive_head.py
--rw-r--r--   0 runner    (1001) docker     (116)    17145 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/part_aggregation_roi_head.py
--rw-r--r--   0 runner    (1001) docker     (116)    13326 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/point_rcnn_roi_head.py
--rw-r--r--   0 runner    (1001) docker     (116)    13976 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/pv_rcnn_roi_head.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/roi_extractors/
--rw-r--r--   0 runner    (1001) docker     (116)      443 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/roi_extractors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     3674 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/roi_extractors/batch_roigridpoint_extractor.py
--rw-r--r--   0 runner    (1001) docker     (116)     2204 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/roi_extractors/single_roiaware_extractor.py
--rw-r--r--   0 runner    (1001) docker     (116)     2460 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/roi_extractors/single_roipoint_extractor.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/segmentors/
--rw-r--r--   0 runner    (1001) docker     (116)      179 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/segmentors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     6456 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/segmentors/base.py
--rw-r--r--   0 runner    (1001) docker     (116)    22249 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/segmentors/encoder_decoder.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/
--rw-r--r--   0 runner    (1001) docker     (116)     1736 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/anchor/
--rw-r--r--   0 runner    (1001) docker     (116)      586 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/anchor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)    17850 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/anchor/anchor_3d_generator.py
--rw-r--r--   0 runner    (1001) docker     (116)      686 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/anchor/builder.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/assigners/
--rw-r--r--   0 runner    (1001) docker     (116)      130 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/assigners/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     7574 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/assigners/max_3d_iou_assigner.py
--rw-r--r--   0 runner    (1001) docker     (116)     1015 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/builder.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/coders/
--rw-r--r--   0 runner    (1001) docker     (116)      827 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/coders/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     4356 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/coders/anchor_free_bbox_coder.py
--rw-r--r--   0 runner    (1001) docker     (116)     8688 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/coders/centerpoint_bbox_coders.py
--rw-r--r--   0 runner    (1001) docker     (116)     3168 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/coders/delta_xyzwhlr_bbox_coder.py
--rw-r--r--   0 runner    (1001) docker     (116)     5148 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/coders/fcos3d_bbox_coder.py
--rw-r--r--   0 runner    (1001) docker     (116)     7243 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/coders/groupfree3d_bbox_coder.py
--rw-r--r--   0 runner    (1001) docker     (116)    20307 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/coders/monoflex_bbox_coder.py
--rw-r--r--   0 runner    (1001) docker     (116)     9145 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/coders/partial_bin_based_bbox_coder.py
--rw-r--r--   0 runner    (1001) docker     (116)     5363 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/coders/pgd_bbox_coder.py
--rw-r--r--   0 runner    (1001) docker     (116)     4542 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/coders/point_xyzwhlr_bbox_coder.py
--rw-r--r--   0 runner    (1001) docker     (116)     8291 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/coders/smoke_bbox_coder.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/samplers/
--rw-r--r--   0 runner    (1001) docker     (116)      724 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/samplers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     8352 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/samplers/iou_neg_piecewise_sampler.py
--rw-r--r--   0 runner    (1001) docker     (116)     2180 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/samplers/pseudosample.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/voxel/
--rw-r--r--   0 runner    (1001) docker     (116)      122 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/voxel/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)    11493 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/voxel/voxel_generator.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/test_time_augs/
--rw-r--r--   0 runner    (1001) docker     (116)      127 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/test_time_augs/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     3570 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/test_time_augs/merge_augs.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/utils/
--rw-r--r--   0 runner    (1001) docker     (116)      653 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)      419 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/utils/add_prefix.py
--rw-r--r--   0 runner    (1001) docker     (116)      468 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/utils/clip_sigmoid.py
--rw-r--r--   0 runner    (1001) docker     (116)     3090 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/utils/edge_indices.py
--rw-r--r--   0 runner    (1001) docker     (116)     5252 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/utils/gaussian.py
--rw-r--r--   0 runner    (1001) docker     (116)     3152 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/utils/gen_keypoints.py
--rw-r--r--   0 runner    (1001) docker     (116)     5493 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/utils/handle_objs.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/models/voxel_encoders/
--rw-r--r--   0 runner    (1001) docker     (116)      329 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/voxel_encoders/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)    13784 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/voxel_encoders/pillar_encoder.py
--rw-r--r--   0 runner    (1001) docker     (116)     6778 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/voxel_encoders/utils.py
--rw-r--r--   0 runner    (1001) docker     (116)    20760 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/models/voxel_encoders/voxel_encoder.py
--rw-r--r--   0 runner    (1001) docker     (116)     3975 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/structures/
--rw-r--r--   0 runner    (1001) docker     (116)     2931 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/structures/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/structures/bbox_3d/
--rw-r--r--   0 runner    (1001) docker     (116)      817 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/structures/bbox_3d/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)    21679 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/structures/bbox_3d/base_box3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     9735 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/structures/bbox_3d/box_3d_mode.py
--rw-r--r--   0 runner    (1001) docker     (116)    14014 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/structures/bbox_3d/cam_box3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     9153 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/structures/bbox_3d/coord_3d_mode.py
--rw-r--r--   0 runner    (1001) docker     (116)    10806 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/structures/bbox_3d/depth_box3d.py
--rw-r--r--   0 runner    (1001) docker     (116)     8330 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/structures/bbox_3d/lidar_box3d.py
--rw-r--r--   0 runner    (1001) docker     (116)    12393 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/structures/bbox_3d/utils.py
--rw-r--r--   0 runner    (1001) docker     (116)     9073 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/structures/det3d_data_sample.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/structures/ops/
--rw-r--r--   0 runner    (1001) docker     (116)     2207 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/structures/ops/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)    30923 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/structures/ops/box_np_ops.py
--rw-r--r--   0 runner    (1001) docker     (116)    12785 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/structures/ops/iou3d_calculator.py
--rw-r--r--   0 runner    (1001) docker     (116)     2421 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/structures/ops/transforms.py
--rw-r--r--   0 runner    (1001) docker     (116)     6293 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/structures/point_data.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/structures/points/
--rw-r--r--   0 runner    (1001) docker     (116)      921 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/structures/points/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)    16593 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/structures/points/base_points.py
--rw-r--r--   0 runner    (1001) docker     (116)     2476 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/structures/points/cam_points.py
--rw-r--r--   0 runner    (1001) docker     (116)     2343 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/structures/points/depth_points.py
--rw-r--r--   0 runner    (1001) docker     (116)     2343 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/structures/points/lidar_points.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/testing/
--rw-r--r--   0 runner    (1001) docker     (116)      543 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/testing/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)     6782 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/testing/data_utils.py
--rw-r--r--   0 runner    (1001) docker     (116)     5365 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/testing/model_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/utils/
--rw-r--r--   0 runner    (1001) docker     (116)      771 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)    13217 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/utils/array_converter.py
--rw-r--r--   0 runner    (1001) docker     (116)      671 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/utils/collect_env.py
--rw-r--r--   0 runner    (1001) docker     (116)     5970 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/utils/compat_cfg.py
--rw-r--r--   0 runner    (1001) docker     (116)     3753 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/utils/misc.py
--rw-r--r--   0 runner    (1001) docker     (116)     4319 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/utils/setup_env.py
--rw-r--r--   0 runner    (1001) docker     (116)      805 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/utils/typing_utils.py
--rw-r--r--   0 runner    (1001) docker     (116)      800 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/version.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d/visualization/
--rw-r--r--   0 runner    (1001) docker     (116)      485 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/visualization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (116)    36078 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/visualization/local_visualizer.py
--rw-r--r--   0 runner    (1001) docker     (116)     6186 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/mmdet3d/visualization/vis_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d.egg-info/
--rw-r--r--   0 runner    (1001) docker     (116)    19584 2023-01-10 03:25:07.000000 mmdet3d-1.1.0rc3/mmdet3d.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (116)    27734 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/mmdet3d.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (116)        1 2023-01-10 03:25:07.000000 mmdet3d-1.1.0rc3/mmdet3d.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (116)        1 2023-01-10 03:25:07.000000 mmdet3d-1.1.0rc3/mmdet3d.egg-info/not-zip-safe
--rw-r--r--   0 runner    (1001) docker     (116)      720 2023-01-10 03:25:07.000000 mmdet3d-1.1.0rc3/mmdet3d.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (116)        8 2023-01-10 03:25:07.000000 mmdet3d-1.1.0rc3/mmdet3d.egg-info/top_level.txt
-drwxr-xr-x   0 runner    (1001) docker     (116)        0 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/requirements/
--rw-r--r--   0 runner    (1001) docker     (116)        0 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/requirements/build.txt
--rw-r--r--   0 runner    (1001) docker     (116)      197 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/requirements/docs.txt
--rw-r--r--   0 runner    (1001) docker     (116)       68 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/requirements/mminstall.txt
--rw-r--r--   0 runner    (1001) docker     (116)      158 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/requirements/optional.txt
--rw-r--r--   0 runner    (1001) docker     (116)       65 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/requirements/readthedocs.txt
--rw-r--r--   0 runner    (1001) docker     (116)      185 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/requirements/runtime.txt
--rw-r--r--   0 runner    (1001) docker     (116)      193 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/requirements/tests.txt
--rw-r--r--   0 runner    (1001) docker     (116)      795 2023-01-10 03:25:08.000000 mmdet3d-1.1.0rc3/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (116)     8183 2023-01-10 03:23:30.000000 mmdet3d-1.1.0rc3/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/
+-rw-r--r--   0 runner    (1001) docker     (123)      216 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (123)    23107 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    19598 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/3dssd/
+-rw-r--r--   0 runner    (1001) docker     (123)     3596 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/3dssd/3dssd_4xb4_kitti-3d-car.py
+-rw-r--r--   0 runner    (1001) docker     (123)      914 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/3dssd/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/
+-rw-r--r--   0 runner    (1001) docker     (123)     5449 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/kitti-3d-3class.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5355 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/kitti-3d-car.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3190 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/kitti-mono3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4760 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/lyft-3d-range100.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5159 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/lyft-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2259 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/nuim-instance.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5658 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4002 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/nus-mono3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4179 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/s3dis-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5282 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/s3dis-seg.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4370 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/scannet-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5175 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/scannet-seg.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7512 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/semantickitti.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3827 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/sunrgbd-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5853 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/waymoD5-3d-3class.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5683 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/waymoD5-3d-car.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5458 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/waymoD5-fov-mono3d-3class.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5454 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/waymoD5-mv-mono3d-3class.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5304 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/waymoD5-mv3d-3class.py
+-rw-r--r--   0 runner    (1001) docker     (123)      655 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/default_runtime.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/
+-rw-r--r--   0 runner    (1001) docker     (123)     3077 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/3dssd.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6987 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/cascade-mask-rcnn_r50_fpn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3323 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/centerpoint_pillar02_second_secfpn_nus.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3415 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/centerpoint_voxel01_second_secfpn_nus.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1199 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/cylinder3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1044 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/dgcnn.py
+-rw-r--r--   0 runner    (1001) docker     (123)      723 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/fcaf3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2702 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/fcos3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2737 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/groupfree3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11349 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/h3dnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3872 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/imvotenet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4165 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/mask-rcnn_r50_fpn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1029 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/minkunet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3572 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/multiview_dfm.py
+-rw-r--r--   0 runner    (1001) docker     (123)      180 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/paconv_ssg-cuda.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2069 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/paconv_ssg.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7191 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/parta2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1830 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/pgd.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5661 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/point_rcnn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1223 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/pointnet2_msg.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1330 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/pointnet2_ssg.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1004 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/pointpillars_hv_fpn_lyft.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3448 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/pointpillars_hv_fpn_nus.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1016 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/pointpillars_hv_fpn_range100_lyft.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3318 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/pointpillars_hv_secfpn_kitti.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4115 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/pointpillars_hv_secfpn_waymo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3065 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/second_hv_secfpn_kitti.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3859 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/second_hv_secfpn_waymo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1983 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/smoke.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1051 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/spvcnn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2610 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/votenet.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/schedules/
+-rw-r--r--   0 runner    (1001) docker     (123)      947 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/schedules/cosine.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2015 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/schedules/cyclic-20e.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2284 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/schedules/cyclic-40e.py
+-rw-r--r--   0 runner    (1001) docker     (123)      814 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/schedules/mmdet-schedule-1x.py
+-rw-r--r--   0 runner    (1001) docker     (123)      995 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/schedules/schedule-2x.py
+-rw-r--r--   0 runner    (1001) docker     (123)      888 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/schedules/schedule-3x.py
+-rw-r--r--   0 runner    (1001) docker     (123)      749 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/schedules/seg-cosine-100e.py
+-rw-r--r--   0 runner    (1001) docker     (123)      749 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/schedules/seg-cosine-150e.py
+-rw-r--r--   0 runner    (1001) docker     (123)      736 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/schedules/seg-cosine-200e.py
+-rw-r--r--   0 runner    (1001) docker     (123)      734 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/schedules/seg-cosine-50e.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/benchmark/
+-rw-r--r--   0 runner    (1001) docker     (123)    12862 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/benchmark/hv_PartA2_secfpn_4x8_cyclic_80e_pcdet_kitti-3d-3class.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7830 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/benchmark/hv_pointpillars_secfpn_3x8_100e_det3d_kitti-3d-car.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8950 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/benchmark/hv_pointpillars_secfpn_4x8_80e_pcdet_kitti-3d-3class.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8750 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/benchmark/hv_second_secfpn_4x8_80e_pcdet_kitti-3d-3class.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/centerpoint/
+-rw-r--r--   0 runner    (1001) docker     (123)     5160 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/centerpoint/centerpoint_pillar02_second_secfpn_8xb4-cyclic-20e_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      134 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/centerpoint/centerpoint_pillar02_second_secfpn_head-circlenms_8xb4-cyclic-20e_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      493 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/centerpoint/centerpoint_pillar02_second_secfpn_head-dcn-circlenms_8xb4-cyclic-20e_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      445 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/centerpoint/centerpoint_pillar02_second_secfpn_head-dcn_8xb4-cyclic-20e_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4699 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel0075_second_secfpn_8xb4-cyclic-20e_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      135 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel0075_second_secfpn_head-circlenms_8xb4-cyclic-20e_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      494 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel0075_second_secfpn_head-dcn-circlenms_8xb4-cyclic-20e_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1646 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel0075_second_secfpn_head-dcn-circlenms_8xb4-flip-tta-cyclic-20e_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      446 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel0075_second_secfpn_head-dcn_8xb4-cyclic-20e_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1627 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel0075_second_secfpn_head-dcn_8xb4-flip-tta-cyclic-20e_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1716 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel0075_second_secfpn_head-dcn_8xb4-tta-cyclic-20e_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5192 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel01_second_secfpn_8xb4-cyclic-20e_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      133 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel01_second_secfpn_head-circlenms_8xb4-cyclic-20e_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      492 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel01_second_secfpn_head-dcn-circlenms_8xb4-cyclic-20e_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      444 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel01_second_secfpn_head-dcn_8xb4-cyclic-20e_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4378 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/centerpoint/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/cylinder3d/
+-rw-r--r--   0 runner    (1001) docker     (123)     1031 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/cylinder3d/cylinder3d_4xb4-3x_semantickitti.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2891 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/cylinder3d/cylinder3d_8xb2-laser-polar-mix-3x_semantickitti.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1586 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/cylinder3d/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/dfm/
+-rw-r--r--   0 runner    (1001) docker     (123)     1298 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/dfm/multiview-dfm_r101-dcn_16xb2_waymoD5-3d-3class.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1841 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/dfm/multiview-dfm_r101-dcn_centerhead_16xb2_waymoD5-3d-3class.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/dgcnn/
+-rw-r--r--   0 runner    (1001) docker     (123)      568 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area1.py
+-rw-r--r--   0 runner    (1001) docker     (123)      568 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area2.py
+-rw-r--r--   0 runner    (1001) docker     (123)      568 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area3.py
+-rw-r--r--   0 runner    (1001) docker     (123)      568 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area4.py
+-rw-r--r--   0 runner    (1001) docker     (123)      695 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area5.py
+-rw-r--r--   0 runner    (1001) docker     (123)      568 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area6.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3575 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/dgcnn/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/dynamic_voxelization/
+-rw-r--r--   0 runner    (1001) docker     (123)     2210 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/dynamic_voxelization/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      657 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/dynamic_voxelization/pointpillars_dv_secfpn_8xb6-160e_kitti-3d-car.py
+-rw-r--r--   0 runner    (1001) docker     (123)      715 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/dynamic_voxelization/second_dv_secfpn_8xb2-cosine-80e_kitti-3d-3class.py
+-rw-r--r--   0 runner    (1001) docker     (123)      602 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/dynamic_voxelization/second_dv_secfpn_8xb6-80e_kitti-3d-car.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/fcaf3d/
+-rw-r--r--   0 runner    (1001) docker     (123)      700 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/fcaf3d/fcaf3d_2xb8_s3dis-3d-5class.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2792 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/fcaf3d/fcaf3d_2xb8_scannet-3d-18class.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2613 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/fcaf3d/fcaf3d_2xb8_sunrgbd-3d-10class.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1981 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/fcaf3d/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/fcos3d/
+-rw-r--r--   0 runner    (1001) docker     (123)     2065 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/fcos3d/fcos3d_r101-caffe-dcn_fpn_head-gn_8xb2-1x_nus-mono3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      295 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/fcos3d/fcos3d_r101-caffe-dcn_fpn_head-gn_8xb2-1x_nus-mono3d_finetune.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1702 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/fcos3d/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/free_anchor/
+-rw-r--r--   0 runner    (1001) docker     (123)     5576 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/free_anchor/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1628 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/free_anchor/pointpillars_hv_fpn_head-free-anchor_sbn-all_8xb4-2x_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      600 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/free_anchor/pointpillars_hv_regnet-1.6gf_fpn_head-free-anchor_sbn-all_8xb4-2x_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2341 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/free_anchor/pointpillars_hv_regnet-1.6gf_fpn_head-free-anchor_sbn-all_8xb4-strong-aug-3x_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      601 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/free_anchor/pointpillars_hv_regnet-3.2gf_fpn_head-free-anchor_sbn-all_8xb4-2x_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2334 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/free_anchor/pointpillars_hv_regnet-3.2gf_fpn_head-free-anchor_sbn-all_8xb4-strong-aug-3x_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      599 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/free_anchor/pointpillars_hv_regnet-400mf_fpn_head-free-anchor_sbn-all_8xb4-2x_nus-3d.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/groupfree3d/
+-rw-r--r--   0 runner    (1001) docker     (123)     7594 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/groupfree3d/groupfree3d_head-L12-O256_4xb8_scannet-seg.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7589 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/groupfree3d/groupfree3d_head-L6-O256_4xb8_scannet-seg.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8121 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/groupfree3d/groupfree3d_w2x-head-L12-O256_4xb8_scannet-seg.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8147 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/groupfree3d/groupfree3d_w2x-head-L12-O512_4xb8_scannet-seg.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2847 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/groupfree3d/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/h3dnet/
+-rw-r--r--   0 runner    (1001) docker     (123)     3270 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/h3dnet/h3dnet_8xb3_scannet-seg.py
+-rw-r--r--   0 runner    (1001) docker     (123)      973 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/h3dnet/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/imvotenet/
+-rw-r--r--   0 runner    (1001) docker     (123)     2184 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/imvotenet/imvotenet_faster-rcnn-r50_fpn_4xb2_sunrgbd-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8205 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/imvotenet/imvotenet_stage2_8xb16_sunrgbd-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1637 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/imvotenet/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/imvoxelnet/
+-rw-r--r--   0 runner    (1001) docker     (123)     4057 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/imvoxelnet/imvoxelnet_2xb4_sunrgbd-3d-10class.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5491 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/imvoxelnet/imvoxelnet_8xb4_kitti-3d-car.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1017 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/imvoxelnet/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/minkunet/
+-rw-r--r--   0 runner    (1001) docker     (123)     5969 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/minkunet/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      470 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/minkunet/minkunet18_w16_torchsparse_8xb2-amp-15e_semantickitti.py
+-rw-r--r--   0 runner    (1001) docker     (123)      238 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/minkunet/minkunet18_w20_torchsparse_8xb2-amp-15e_semantickitti.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1593 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/minkunet/minkunet18_w32_torchsparse_8xb2-amp-15e_semantickitti.py
+-rw-r--r--   0 runner    (1001) docker     (123)      201 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/minkunet/minkunet34_w32_minkowski_8xb2-laser-polar-mix-3x_semantickitti.py
+-rw-r--r--   0 runner    (1001) docker     (123)      266 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/minkunet/minkunet34_w32_spconv_8xb2-amp-laser-polar-mix-3x_semantickitti.py
+-rw-r--r--   0 runner    (1001) docker     (123)      198 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/minkunet/minkunet34_w32_spconv_8xb2-laser-polar-mix-3x_semantickitti.py
+-rw-r--r--   0 runner    (1001) docker     (123)      157 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/minkunet/minkunet34_w32_torchsparse_8xb2-amp-laser-polar-mix-3x_semantickitti.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2997 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/minkunet/minkunet34_w32_torchsparse_8xb2-laser-polar-mix-3x_semantickitti.py
+-rw-r--r--   0 runner    (1001) docker     (123)      310 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/minkunet/minkunet34v2_w32_torchsparse_8xb2-amp-laser-polar-mix-3x_semantickitti.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/monoflex/
+-rw-r--r--   0 runner    (1001) docker     (123)     1044 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/monoflex/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/mvxnet/
+-rw-r--r--   0 runner    (1001) docker     (123)     1110 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/mvxnet/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     9246 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/mvxnet/mvxnet_fpn_dv_second_secfpn_8xb2-80e_kitti-3d-3class.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/
+-rw-r--r--   0 runner    (1001) docker     (123)      343 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/cascade-mask-rcnn-r50-fpn_coco-20e_nuim.py
+-rw-r--r--   0 runner    (1001) docker     (123)      127 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/cascade-mask-rcnn_r101_fpn_1x_nuim.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2335 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/cascade-mask-rcnn_r50_fpn_1x_nuim.py
+-rw-r--r--   0 runner    (1001) docker     (123)      263 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/cascade-mask-rcnn_r50_fpn_coco-20e-1x_nuim.py
+-rw-r--r--   0 runner    (1001) docker     (123)      376 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/cascade-mask-rcnn_x101_32x4d_fpn_1x_nuim.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1216 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/htc_r50_fpn_1x_nuim.py
+-rw-r--r--   0 runner    (1001) docker     (123)      174 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/htc_r50_fpn_coco-20e-1x_nuim.py
+-rw-r--r--   0 runner    (1001) docker     (123)      124 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/htc_r50_fpn_coco-20e_nuim.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7778 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/htc_r50_fpn_head-without-semantic_1x_nuim.py
+-rw-r--r--   0 runner    (1001) docker     (123)      859 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/htc_x101_64x4d_fpn_dconv_c3-c5_coco-20e-1xb16_nuim.py
+-rw-r--r--   0 runner    (1001) docker     (123)      119 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/mask-rcnn_r101_fpn_1x_nuim.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1372 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/mask-rcnn_r50_caffe_fpn_1x_nuim.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1602 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/mask-rcnn_r50_caffe_fpn_coco-3x_1x_nuim.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1681 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/mask-rcnn_r50_caffe_fpn_coco-3x_20e_nuim.py
+-rw-r--r--   0 runner    (1001) docker     (123)      286 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/mask-rcnn_r50_fpn_1x_nuim.py
+-rw-r--r--   0 runner    (1001) docker     (123)      478 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/mask-rcnn_r50_fpn_coco-2x_1x_nuim.py
+-rw-r--r--   0 runner    (1001) docker     (123)      969 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/mask-rcnn_r50_fpn_coco-2x_1x_nus-2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      368 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/mask-rcnn_x101_32x4d_fpn_1x_nuim.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10039 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/paconv/
+-rw-r--r--   0 runner    (1001) docker     (123)     1590 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/paconv/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1908 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/paconv/paconv_ssg-cuda_8xb8-cosine-200e_s3dis-seg.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1819 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/paconv/paconv_ssg_8xb8-cosine-150e_s3dis-seg.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/parta2/
+-rw-r--r--   0 runner    (1001) docker     (123)     1593 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/parta2/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     5200 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/parta2/parta2_hv_secfpn_8xb2-cyclic-80e_kitti-3d-3class.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4982 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/parta2/parta2_hv_secfpn_8xb2-cyclic-80e_kitti-3d-car.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pgd/
+-rw-r--r--   0 runner    (1001) docker     (123)     3235 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pgd/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3601 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pgd/pgd_r101-caffe_fpn_head-gn_16xb2-1x_nus-mono3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      324 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pgd/pgd_r101-caffe_fpn_head-gn_16xb2-1x_nus-mono3d_finetune.py
+-rw-r--r--   0 runner    (1001) docker     (123)      403 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pgd/pgd_r101-caffe_fpn_head-gn_16xb2-2x_nus-mono3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      324 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pgd/pgd_r101-caffe_fpn_head-gn_16xb2-2x_nus-mono3d_finetune.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4135 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pgd/pgd_r101-caffe_fpn_head-gn_4xb3-4x_kitti-mono3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3660 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pgd/pgd_r101_fpn-head_dcn_16xb3_waymoD5-fov-mono3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3659 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pgd/pgd_r101_fpn-head_dcn_16xb3_waymoD5-mv-mono3d.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/point_rcnn/
+-rw-r--r--   0 runner    (1001) docker     (123)      966 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/point_rcnn/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4693 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/point_rcnn/point-rcnn_8xb2_kitti-3d-3class.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pointnet2/
+-rw-r--r--   0 runner    (1001) docker     (123)     4027 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pointnet2/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3775 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pointnet2/pointnet2_msg_2xb16-cosine-250e_scannet-seg-xyz-only.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1280 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pointnet2/pointnet2_msg_2xb16-cosine-250e_scannet-seg.py
+-rw-r--r--   0 runner    (1001) docker     (123)      831 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pointnet2/pointnet2_msg_2xb16-cosine-80e_s3dis-seg.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3681 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pointnet2/pointnet2_ssg_2xb16-cosine-200e_scannet-seg-xyz-only.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1186 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pointnet2/pointnet2_ssg_2xb16-cosine-200e_scannet-seg.py
+-rw-r--r--   0 runner    (1001) docker     (123)      738 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pointnet2/pointnet2_ssg_2xb16-cosine-50e_s3dis-seg.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pointpillars/
+-rw-r--r--   0 runner    (1001) docker     (123)     8206 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pointpillars/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      433 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_fpn_sbn-all_8xb2-2x_lyft-3d-range100.py
+-rw-r--r--   0 runner    (1001) docker     (123)      415 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_fpn_sbn-all_8xb2-2x_lyft-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      195 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_fpn_sbn-all_8xb2-amp-2x_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      460 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_fpn_sbn-all_8xb4-2x_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4307 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_8xb6-160e_kitti-3d-3class.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3239 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_8xb6-160e_kitti-3d-car.py
+-rw-r--r--   0 runner    (1001) docker     (123)      521 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_16xb2-2x_waymo-3d-3class.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1428 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_16xb2-2x_waymo-3d-car.py
+-rw-r--r--   0 runner    (1001) docker     (123)      194 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_16xb2-2x_waymoD5-3d-3class.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1342 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_16xb2-2x_waymoD5-3d-car.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2045 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_8xb2-2x_lyft-3d-range100.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1996 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_8xb2-2x_lyft-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      198 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_8xb2-amp-2x_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2041 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_8xb4-2x_nus-3d.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pv_rcnn/
+-rw-r--r--   0 runner    (1001) docker     (123)     1000 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pv_rcnn/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)    12311 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/pv_rcnn/pv_rcnn_8xb2-80e_kitti-3d-3class.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/regnet/
+-rw-r--r--   0 runner    (1001) docker     (123)     3436 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/regnet/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      744 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-1.6gf_fpn_sbn-all_8xb4-2x_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1035 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_fpn_sbn-all_8xb2-2x_lyft-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      793 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_fpn_sbn-all_8xb4-2x_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1053 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_fpn_sbn-all_range100_8xb2-2x_lyft-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1678 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_secfpn_sbn-all_8xb2-2x_lyft-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1677 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_secfpn_sbn-all_8xb4-2x_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1729 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_secfpn_sbn-all_range100_8xb2-2x_lyft-3d.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/sassd/
+-rw-r--r--   0 runner    (1001) docker     (123)     3198 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/sassd/sassd_8xb6-80e_kitti-3d-3class.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/second/
+-rw-r--r--   0 runner    (1001) docker     (123)     3361 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/second/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      180 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/second/second_hv_secfpn_8xb6-80e_kitti-3d-3class.py
+-rw-r--r--   0 runner    (1001) docker     (123)      972 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/second/second_hv_secfpn_8xb6-80e_kitti-3d-car.py
+-rw-r--r--   0 runner    (1001) docker     (123)      140 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/second/second_hv_secfpn_8xb6-amp-80e_kitti-3d-3class.py
+-rw-r--r--   0 runner    (1001) docker     (123)      137 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/second/second_hv_secfpn_8xb6-amp-80e_kitti-3d-car.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4609 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/second/second_hv_secfpn_sbn-all_16xb2-2x_waymoD5-3d-3class.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/smoke/
+-rw-r--r--   0 runner    (1001) docker     (123)     1023 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/smoke/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1792 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/smoke/smoke_dla34_dlaneck_gn-all_4xb8-6x_kitti-mono3d.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/spvcnn/
+-rw-r--r--   0 runner    (1001) docker     (123)     2710 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/spvcnn/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      272 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/spvcnn/spvcnn_w16_8xb2-amp-15e_semantickitti.py
+-rw-r--r--   0 runner    (1001) docker     (123)      236 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/spvcnn/spvcnn_w20_8xb2-amp-15e_semantickitti.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1591 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/spvcnn/spvcnn_w32_8xb2-amp-15e_semantickitti.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3115 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/spvcnn/spvcnn_w32_8xb2-amp-laser-polar-mix-3x_semantickitti.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/ssn/
+-rw-r--r--   0 runner    (1001) docker     (123)     2751 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/ssn/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      742 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/ssn/ssn_hv_regnet-400mf_secfpn_sbn-all_16xb1-2x_lyft-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      727 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/ssn/ssn_hv_regnet-400mf_secfpn_sbn-all_16xb2-2x_nus-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9407 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/ssn/ssn_hv_secfpn_sbn-all_16xb2-2x_lyft-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10034 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/ssn/ssn_hv_secfpn_sbn-all_16xb2-2x_nus-3d.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/votenet/
+-rw-r--r--   0 runner    (1001) docker     (123)     1890 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/votenet/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1108 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/votenet/votenet_8xb16_sunrgbd-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1773 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/votenet/votenet_8xb8_scannet-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      225 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/configs/votenet/votenet_head-iouloss_8xb8_scannet-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      988 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/model-index.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/analysis_tools/
+-rw-r--r--   0 runner    (1001) docker     (123)     7486 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/analysis_tools/analyze_logs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2677 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/analysis_tools/benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2729 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/analysis_tools/get_flops.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15296 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/create_data.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      565 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/create_data.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/
+-rw-r--r--   0 runner    (1001) docker     (123)    24865 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/create_gt_database.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5097 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/indoor_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24425 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/kitti_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24939 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/kitti_data_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10724 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/lyft_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1411 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/lyft_data_fixer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7644 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/nuimage_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24694 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/nuscenes_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10149 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/s3dis_data_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12781 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/scannet_data_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3083 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/semantickitti_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9109 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/sunrgbd_data_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    49931 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/update_infos_to_v2.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25658 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/waymo_converter.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/deployment/
+-rw-r--r--   0 runner    (1001) docker     (123)     3820 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/deployment/mmdet3d2torchserve.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4232 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/deployment/mmdet3d_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1916 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/deployment/test_torchserver.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      479 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/dist_test.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      442 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/dist_train.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/misc/
+-rw-r--r--   0 runner    (1001) docker     (123)     5348 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/misc/browse_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2246 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/misc/fuse_conv_bn.py
+-rw-r--r--   0 runner    (1001) docker     (123)      643 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/misc/print_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1456 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/misc/visualize_results.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/model_converters/
+-rw-r--r--   0 runner    (1001) docker     (123)     6149 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/model_converters/convert_h3dnet_checkpoints.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5091 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/model_converters/convert_votenet_checkpoints.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1076 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/model_converters/publish_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3063 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/model_converters/regnet2mmdet.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      566 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/slurm_test.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      574 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/slurm_train.sh
+-rw-r--r--   0 runner    (1001) docker     (123)     5549 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4883 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/train.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6566 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/update_data_coords.py
+-rw-r--r--   0 runner    (1001) docker     (123)      529 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/.mim/tools/update_data_coords.sh
+-rw-r--r--   0 runner    (1001) docker     (123)     1454 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/apis/
+-rw-r--r--   0 runner    (1001) docker     (123)      768 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/apis/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14829 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/apis/inference.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/apis/inferencers/
+-rw-r--r--   0 runner    (1001) docker     (123)      485 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/apis/inferencers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12668 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/apis/inferencers/base_3d_inferencer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7573 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/apis/inferencers/lidar_det3d_inferencer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7861 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/apis/inferencers/lidar_seg3d_inferencer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7078 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/apis/inferencers/mono_det3d_inferencer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9547 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/apis/inferencers/multi_modality_det3d_inferencer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/datasets/
+-rw-r--r--   0 runner    (1001) docker     (123)     2253 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/datasets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16500 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/datasets/convert_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6358 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/datasets/dataset_wrappers.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17108 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/datasets/det3d_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8812 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/datasets/kitti2d_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7447 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/datasets/kitti_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4109 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/datasets/lyft_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10243 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/datasets/nuscenes_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15514 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/datasets/s3dis_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13653 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/datasets/scannet_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13867 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/datasets/seg3d_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3988 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/datasets/semantickitti_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5715 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/datasets/sunrgbd_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/datasets/transforms/
+-rw-r--r--   0 runner    (1001) docker     (123)     2154 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/datasets/transforms/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17093 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/datasets/transforms/data_augment_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13197 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/datasets/transforms/dbsampler.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10106 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/datasets/transforms/formating.py
+-rw-r--r--   0 runner    (1001) docker     (123)    51869 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/datasets/transforms/loading.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5608 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/datasets/transforms/test_time_aug.py
+-rw-r--r--   0 runner    (1001) docker     (123)   102784 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/datasets/transforms/transforms_3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5206 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/datasets/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10934 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/datasets/waymo_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/engine/
+-rw-r--r--   0 runner    (1001) docker     (123)      160 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/engine/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/engine/hooks/
+-rw-r--r--   0 runner    (1001) docker     (123)      297 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/engine/hooks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1339 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/engine/hooks/benchmark_hook.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2355 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/engine/hooks/disable_object_sample_hook.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9284 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/engine/hooks/visualization_hook.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/evaluation/
+-rw-r--r--   0 runner    (1001) docker     (123)     1512 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/evaluation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/evaluation/functional/
+-rw-r--r--   0 runner    (1001) docker     (123)     1115 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/evaluation/functional/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10707 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/evaluation/functional/indoor_eval.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5193 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/evaluation/functional/instance_seg_eval.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/evaluation/functional/kitti_utils/
+-rw-r--r--   0 runner    (1001) docker     (123)      197 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/evaluation/functional/kitti_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    37841 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/evaluation/functional/kitti_utils/eval.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13153 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/evaluation/functional/kitti_utils/rotate_iou.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10382 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/evaluation/functional/lyft_eval.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16234 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/evaluation/functional/panoptic_seg_eval.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/evaluation/functional/scannet_utils/
+-rw-r--r--   0 runner    (1001) docker     (123)      167 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/evaluation/functional/scannet_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15593 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/evaluation/functional/scannet_utils/evaluate_semantic_instance.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2607 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/evaluation/functional/scannet_utils/util_3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3844 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/evaluation/functional/seg_eval.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/evaluation/functional/waymo_utils/
+-rw-r--r--   0 runner    (1001) docker     (123)      131 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/evaluation/functional/waymo_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16446 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/evaluation/functional/waymo_utils/prediction_to_waymo.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/evaluation/metrics/
+-rw-r--r--   0 runner    (1001) docker     (123)      687 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/evaluation/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6695 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/evaluation/metrics/indoor_metric.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3509 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/evaluation/metrics/instance_seg_metric.py
+-rw-r--r--   0 runner    (1001) docker     (123)    28517 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/evaluation/metrics/kitti_metric.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17017 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/evaluation/metrics/lyft_metric.py
+-rw-r--r--   0 runner    (1001) docker     (123)    32405 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/evaluation/metrics/nuscenes_metric.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3867 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/evaluation/metrics/panoptic_seg_metric.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5347 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/evaluation/metrics/seg_metric.py
+-rw-r--r--   0 runner    (1001) docker     (123)    31670 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/evaluation/metrics/waymo_metric.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/
+-rw-r--r--   0 runner    (1001) docker     (123)      750 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/backbones/
+-rw-r--r--   0 runner    (1001) docker     (123)      847 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1519 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/backbones/base_pointnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16662 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/backbones/cylinder3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4022 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/backbones/dgcnn.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15720 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/backbones/dla.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4921 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/backbones/mink_resnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10450 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/backbones/minkunet_backbone.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5015 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/backbones/multi_backbone.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3629 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/backbones/nostem_regnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8218 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/backbones/pointnet2_sa_msg.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5991 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/backbones/pointnet2_sa_ssg.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3553 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/backbones/second.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11624 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/backbones/spvcnn_backone.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/data_preprocessors/
+-rw-r--r--   0 runner    (1001) docker     (123)      138 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/data_preprocessors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23948 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/data_preprocessors/data_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2854 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/data_preprocessors/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13979 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/data_preprocessors/voxelize.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/decode_heads/
+-rw-r--r--   0 runner    (1001) docker     (123)      406 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/decode_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6320 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/decode_heads/cylinder3d_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7039 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/decode_heads/decode_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2111 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/decode_heads/dgcnn_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2427 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/decode_heads/minkunet_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2788 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/decode_heads/paconv_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3398 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/decode_heads/pointnet2_head.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/dense_heads/
+-rw-r--r--   0 runner    (1001) docker     (123)     1226 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/dense_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18722 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/dense_heads/anchor3d_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20532 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/dense_heads/anchor_free_mono3d_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16518 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/dense_heads/base_3d_dense_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4407 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/dense_heads/base_conv_bbox_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7535 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/dense_heads/base_mono3d_dense_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    37182 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/dense_heads/centerpoint_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29966 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/dense_heads/fcaf3d_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    43730 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/dense_heads/fcos_mono3d_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12085 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/dense_heads/free_anchor3d_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    47808 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/dense_heads/groupfree3d_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29871 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/dense_heads/imvoxel_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    36543 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/dense_heads/monoflex_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18097 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/dense_heads/parta2_rpn_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    59511 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/dense_heads/pgd_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21560 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/dense_heads/point_rpn_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22790 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/dense_heads/shape_aware_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23223 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/dense_heads/smoke_mono3d_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25624 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/dense_heads/ssd_3d_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17338 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/dense_heads/train_mixins.py
+-rw-r--r--   0 runner    (1001) docker     (123)    36173 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/dense_heads/vote_head.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/
+-rw-r--r--   0 runner    (1001) docker     (123)     1282 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6614 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2936 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/centerpoint.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10030 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/dfm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1710 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/dynamic_voxelnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4213 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/fcos_mono3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3436 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/groupfree3dnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5825 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/h3dnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23036 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/imvotenet.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11896 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/imvoxelnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5731 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/mink_single_stage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18515 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/multiview_dfm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2001 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/mvx_faster_rcnn.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16714 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/mvx_two_stage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2563 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/parta2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2439 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/point_rcnn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9948 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/pv_rcnn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4016 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/sassd.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6800 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/single_stage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3693 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/single_stage_mono3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1774 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/smoke_mono3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      669 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/ssd3dnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8191 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/two_stage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6356 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/votenet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1870 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/detectors/voxelnet.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/layers/
+-rw-r--r--   0 runner    (1001) docker     (123)     2077 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10877 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/box3d_nms.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/layers/dgcnn_modules/
+-rw-r--r--   0 runner    (1001) docker     (123)      240 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/dgcnn_modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2505 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/dgcnn_modules/dgcnn_fa_module.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2163 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/dgcnn_modules/dgcnn_fp_module.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8928 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/dgcnn_modules/dgcnn_gf_module.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3112 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/edge_fusion_module.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/layers/fusion_layers/
+-rw-r--r--   0 runner    (1001) docker     (123)      367 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/fusion_layers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8001 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/fusion_layers/coord_transform.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17816 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/fusion_layers/point_fusion.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9135 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/fusion_layers/vote_fusion.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6530 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/minkowski_engine_block.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2162 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/mlp.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5504 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/norm.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/layers/paconv/
+-rw-r--r--   0 runner    (1001) docker     (123)      123 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/paconv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16190 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/paconv/paconv.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3824 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/paconv/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/layers/pointnet_modules/
+-rw-r--r--   0 runner    (1001) docker     (123)      590 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/pointnet_modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1427 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/pointnet_modules/builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15034 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/pointnet_modules/paconv_sa_module.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3011 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/pointnet_modules/point_fp_module.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14492 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/pointnet_modules/point_sa_module.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7976 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/pointnet_modules/stack_point_sa_module.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8170 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/sparse_block.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/layers/spconv/
+-rw-r--r--   0 runner    (1001) docker     (123)      384 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/spconv/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/layers/spconv/overwrite_spconv/
+-rw-r--r--   0 runner    (1001) docker     (123)      124 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/spconv/overwrite_spconv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4632 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/spconv/overwrite_spconv/write_spconv2.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/layers/torchsparse/
+-rw-r--r--   0 runner    (1001) docker     (123)      296 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/torchsparse/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1044 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/torchsparse/torchsparse_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7384 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/torchsparse_block.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6086 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7840 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/layers/vote_module.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/losses/
+-rw-r--r--   0 runner    (1001) docker     (123)      868 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/losses/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3010 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/losses/axis_aligned_iou_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5895 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/losses/chamfer_distance.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13480 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/losses/lovasz_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3760 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/losses/multibin_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4388 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/losses/paconv_regularization_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3140 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/losses/rotated_iou_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6976 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/losses/uncertain_smooth_l1_loss.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/middle_encoders/
+-rw-r--r--   0 runner    (1001) docker     (123)      370 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/middle_encoders/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3826 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/middle_encoders/pillar_scatter.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21205 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/middle_encoders/sparse_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12837 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/middle_encoders/sparse_unet.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14048 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/middle_encoders/voxel_set_abstraction.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/necks/
+-rw-r--r--   0 runner    (1001) docker     (123)      376 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/necks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8291 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/necks/dla_neck.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7722 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/necks/imvoxel_neck.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3204 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/necks/pointnet2_fp_neck.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3404 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/necks/second_fpn.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/roi_heads/
+-rw-r--r--   0 runner    (1001) docker     (123)      689 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/roi_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1997 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/roi_heads/base_3droi_head.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/roi_heads/bbox_heads/
+-rw-r--r--   0 runner    (1001) docker     (123)      709 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/roi_heads/bbox_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    42934 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/roi_heads/bbox_heads/h3d_bbox_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26660 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/roi_heads/bbox_heads/parta2_bbox_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25475 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/roi_heads/bbox_heads/point_rcnn_bbox_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20899 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/roi_heads/bbox_heads/pv_rcnn_bbox_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4447 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/roi_heads/h3d_roi_head.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/roi_heads/mask_heads/
+-rw-r--r--   0 runner    (1001) docker     (123)      308 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/roi_heads/mask_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6402 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/roi_heads/mask_heads/foreground_segmentation_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8432 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/roi_heads/mask_heads/pointwise_semantic_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    46441 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/roi_heads/mask_heads/primitive_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17145 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/roi_heads/part_aggregation_roi_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13326 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/roi_heads/point_rcnn_roi_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13976 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/roi_heads/pv_rcnn_roi_head.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/roi_heads/roi_extractors/
+-rw-r--r--   0 runner    (1001) docker     (123)      443 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/roi_heads/roi_extractors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3674 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/roi_heads/roi_extractors/batch_roigridpoint_extractor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2204 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/roi_heads/roi_extractors/single_roiaware_extractor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2460 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/roi_heads/roi_extractors/single_roipoint_extractor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/segmentors/
+-rw-r--r--   0 runner    (1001) docker     (123)      335 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/segmentors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6595 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/segmentors/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6212 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/segmentors/cylinder3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23331 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/segmentors/encoder_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4324 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/segmentors/minkunet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1266 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/segmentors/seg3d_tta.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/
+-rw-r--r--   0 runner    (1001) docker     (123)     1736 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/anchor/
+-rw-r--r--   0 runner    (1001) docker     (123)      586 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/anchor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18571 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/anchor/anchor_3d_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)      784 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/anchor/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/assigners/
+-rw-r--r--   0 runner    (1001) docker     (123)      130 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/assigners/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7515 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/assigners/max_3d_iou_assigner.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1145 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/coders/
+-rw-r--r--   0 runner    (1001) docker     (123)      827 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/coders/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4611 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/coders/anchor_free_bbox_coder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9127 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/coders/centerpoint_bbox_coders.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3260 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/coders/delta_xyzwhlr_bbox_coder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5497 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/coders/fcos3d_bbox_coder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7613 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/coders/groupfree3d_bbox_coder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20991 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/coders/monoflex_bbox_coder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9671 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/coders/partial_bin_based_bbox_coder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5544 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/coders/pgd_bbox_coder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4935 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/coders/point_xyzwhlr_bbox_coder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8822 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/coders/smoke_bbox_coder.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/samplers/
+-rw-r--r--   0 runner    (1001) docker     (123)      724 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/samplers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8764 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/samplers/iou_neg_piecewise_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2198 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/samplers/pseudosample.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/voxel/
+-rw-r--r--   0 runner    (1001) docker     (123)      122 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/voxel/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12316 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/task_modules/voxel/voxel_generator.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/test_time_augs/
+-rw-r--r--   0 runner    (1001) docker     (123)      127 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/test_time_augs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3769 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/test_time_augs/merge_augs.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)      653 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      438 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/utils/add_prefix.py
+-rw-r--r--   0 runner    (1001) docker     (123)      498 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/utils/clip_sigmoid.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3167 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/utils/edge_indices.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5658 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/utils/gaussian.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3325 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/utils/gen_keypoints.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6088 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/utils/handle_objs.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/models/voxel_encoders/
+-rw-r--r--   0 runner    (1001) docker     (123)      377 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/voxel_encoders/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14462 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/voxel_encoders/pillar_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7129 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/voxel_encoders/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    28039 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/models/voxel_encoders/voxel_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5612 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/registry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/structures/
+-rw-r--r--   0 runner    (1001) docker     (123)     2931 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/structures/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/structures/bbox_3d/
+-rw-r--r--   0 runner    (1001) docker     (123)      817 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/structures/bbox_3d/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26127 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/structures/bbox_3d/base_box3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10058 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/structures/bbox_3d/box_3d_mode.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15616 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/structures/bbox_3d/cam_box3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10557 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/structures/bbox_3d/coord_3d_mode.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11028 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/structures/bbox_3d/depth_box3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8599 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/structures/bbox_3d/lidar_box3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13035 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/structures/bbox_3d/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9032 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/structures/det3d_data_sample.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/structures/ops/
+-rw-r--r--   0 runner    (1001) docker     (123)     2207 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/structures/ops/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    30923 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/structures/ops/box_np_ops.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12785 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/structures/ops/iou3d_calculator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2421 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/structures/ops/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6777 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/structures/point_data.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/structures/points/
+-rw-r--r--   0 runner    (1001) docker     (123)      995 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/structures/points/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19825 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/structures/points/base_points.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3022 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/structures/points/cam_points.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2885 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/structures/points/depth_points.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2885 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/structures/points/lidar_points.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/testing/
+-rw-r--r--   0 runner    (1001) docker     (123)      543 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/testing/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6989 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/testing/data_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5539 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/testing/model_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)      807 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14155 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/utils/array_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      671 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/utils/collect_env.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5970 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/utils/compat_cfg.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4297 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/utils/misc.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4328 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/utils/setup_env.py
+-rw-r--r--   0 runner    (1001) docker     (123)      842 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/utils/typing_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)      796 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/version.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d/visualization/
+-rw-r--r--   0 runner    (1001) docker     (123)      485 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/visualization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    44177 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/visualization/local_visualizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6275 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/mmdet3d/visualization/vis_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)    23107 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    30357 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d.egg-info/not-zip-safe
+-rw-r--r--   0 runner    (1001) docker     (123)      673 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        8 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/mmdet3d.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/requirements/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/requirements/build.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      230 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/requirements/docs.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       65 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/requirements/mminstall.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      158 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/requirements/optional.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       62 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/requirements/readthedocs.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      226 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/requirements/runtime.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      197 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/requirements/tests.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      795 2023-05-31 07:58:38.000000 mmdet3d-1.1.1/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)     8150 2023-05-31 07:57:16.000000 mmdet3d-1.1.1/setup.py
```

### Comparing `mmdet3d-1.1.0rc3/PKG-INFO` & `mmdet3d-1.1.1/PKG-INFO`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: mmdet3d
-Version: 1.1.0rc3
+Version: 1.1.1
 Summary: OpenMMLab's next-generation platformfor general 3D object detection.
 Home-page: https://github.com/open-mmlab/mmdetection3d
 Author: MMDetection3D Contributors
 Author-email: zwwdev@gmail.com
 License: Apache License 2.0
 Description: <div align="center">
           <img src="resources/mmdet3d-logo.png" width="600"/>
@@ -21,34 +21,58 @@
             <sup>
               <a href="https://platform.openmmlab.com">
                 <i><font size="4">TRY IT OUT</font></i>
               </a>
             </sup>
           </div>
           <div>&nbsp;</div>
-        </div>
         
-        [![docs](https://img.shields.io/badge/docs-latest-blue)](https://mmdetection3d.readthedocs.io/en/1.1/)
+        [![docs](https://img.shields.io/badge/docs-latest-blue)](https://mmdetection3d.readthedocs.io/en/latest/)
         [![badge](https://github.com/open-mmlab/mmdetection3d/workflows/build/badge.svg)](https://github.com/open-mmlab/mmdetection3d/actions)
         [![codecov](https://codecov.io/gh/open-mmlab/mmdetection3d/branch/master/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmdetection3d)
         [![license](https://img.shields.io/github/license/open-mmlab/mmdetection3d.svg)](https://github.com/open-mmlab/mmdetection3d/blob/master/LICENSE)
         
+        </div>
+        
+        </div>
+        
+        <div align="center">
+          <a href="https://openmmlab.medium.com/" style="text-decoration:none;">
+            <img src="https://user-images.githubusercontent.com/25839884/219255827-67c1a27f-f8c5-46a9-811d-5e57448c61d1.png" width="3%" alt="" /></a>
+          <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+          <a href="https://discord.com/channels/1037617289144569886/1046608014234370059" style="text-decoration:none;">
+            <img src="https://user-images.githubusercontent.com/25839884/218347213-c080267f-cbb6-443e-8532-8e1ed9a58ea9.png" width="3%" alt="" /></a>
+          <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+          <a href="https://twitter.com/OpenMMLab" style="text-decoration:none;">
+            <img src="https://user-images.githubusercontent.com/25839884/218346637-d30c8a0f-3eba-4699-8131-512fb06d46db.png" width="3%" alt="" /></a>
+          <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+          <a href="https://www.youtube.com/openmmlab" style="text-decoration:none;">
+            <img src="https://user-images.githubusercontent.com/25839884/218346691-ceb2116a-465a-40af-8424-9f30d2348ca9.png" width="3%" alt="" /></a>
+          <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+          <a href="https://space.bilibili.com/1293512903" style="text-decoration:none;">
+            <img src="https://user-images.githubusercontent.com/25839884/219026751-d7d14cce-a7c9-4e82-9942-8375fca65b99.png" width="3%" alt="" /></a>
+          <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+          <a href="https://www.zhihu.com/people/openmmlab" style="text-decoration:none;">
+            <img src="https://user-images.githubusercontent.com/25839884/219026120-ba71e48b-6e94-4bd4-b4e9-b7d175b5e362.png" width="3%" alt="" /></a>
+        </div>
+        
         **News**:
         
-        **v1.1.0rc3** was released in 7/1/2023
+        **We have renamed the branch `1.1`  to `main` and switched the default branch from `master` to `main`. We encourage
+        users to migrate to the latest version, though it comes with some cost. Please refer to [Migration Guide](docs/en/migration.md) for more details.**
         
-        The compatibilities of models are broken due to the unification and simplification of coordinate systems after v1.0.0rc0. For now, most models are benchmarked with similar performance, though few models are still being benchmarked. In the following release, we will update all the model checkpoints and benchmarks. See more details in the [Changelog](docs/en/notes/changelog.md) and [Changelog-v1.0.x](docs/en/notes/changelog_v1.0.x.md).
+        **v1.1.1** was released in 30/5/2023
         
-        Documentation: https://mmdetection3d.readthedocs.io/
+        We have constructed a comprehensive LiDAR semantic segmentation benchmark on SemanticKITTI, including Cylinder3D, MinkUNet and SPVCNN methods. Noteworthy, the improved MinkUNetv2 can achieve 70.3 mIoU on the validation set of SemanticKITTI. We have also supported the training of BEVFusion and an occupancy prediction method, TPVFomrer, in our `projects`. More new features about 3D perception are on the way. Please stay tuned!
         
         ## Introduction
         
         English | [](README_zh-CN.md)
         
-        The master branch works with **PyTorch 1.6+**.
+        The main branch works with **PyTorch 1.8+**.
         
         MMDetection3D is an open source object detection toolbox based on PyTorch, towards the next-generation platform for general 3D detection. It is
         a part of the OpenMMLab project developed by [MMLab](http://mmlab.ie.cuhk.edu.hk/).
         
         ![demo image](resources/mmdet3d_outdoor_demo.gif)
         
         ### Major features
@@ -56,41 +80,41 @@
         - **Support multi-modality/single-modality detectors out of box**
         
           It directly supports multi-modality/single-modality detectors including MVXNet, VoteNet, PointPillars, etc.
         
         - **Support indoor/outdoor 3D detection out of box**
         
           It directly supports popular indoor and outdoor 3D detection datasets, including ScanNet, SUNRGB-D, Waymo, nuScenes, Lyft, and KITTI.
-          For nuScenes dataset, we also support [nuImages dataset](https://github.com/open-mmlab/mmdetection3d/tree/1.1/configs/nuimages).
+          For nuScenes dataset, we also support [nuImages dataset](https://github.com/open-mmlab/mmdetection3d/tree/main/configs/nuimages).
         
         - **Natural integration with 2D detection**
         
           All the about **300+ models, methods of 40+ papers**, and modules supported in [MMDetection](https://github.com/open-mmlab/mmdetection/blob/3.x/docs/en/model_zoo.md) can be trained or used in this codebase.
         
         - **High efficiency**
         
-          It trains faster than other codebases. The main results are as below. Details can be found in [benchmark.md](./docs/en/notes/benchmarks.md). We compare the number of samples trained per second (the higher, the better). The models that are not supported by other codebases are marked by ``.
+          It trains faster than other codebases. The main results are as below. Details can be found in [benchmark.md](./docs/en/notes/benchmarks.md). We compare the number of samples trained per second (the higher, the better). The models that are not supported by other codebases are marked by ``.
         
           |       Methods       | MMDetection3D | [OpenPCDet](https://github.com/open-mmlab/OpenPCDet) | [votenet](https://github.com/facebookresearch/votenet) | [Det3D](https://github.com/poodarchu/Det3D) |
           | :-----------------: | :-----------: | :--------------------------------------------------: | :----------------------------------------------------: | :-----------------------------------------: |
-          |       VoteNet       |      358      |                                                     |                           77                           |                                            |
-          |  PointPillars-car   |      141      |                                                     |                                                       |                     140                     |
-          | PointPillars-3class |      107      |                          44                          |                                                       |                                            |
-          |       SECOND        |      40       |                          30                          |                                                       |                                            |
-          |       Part-A2       |      17       |                          14                          |                                                       |                                            |
+          |       VoteNet       |      358      |                                                     |                           77                           |                                            |
+          |  PointPillars-car   |      141      |                                                     |                                                       |                     140                     |
+          | PointPillars-3class |      107      |                          44                          |                                                       |                                            |
+          |       SECOND        |      40       |                          30                          |                                                       |                                            |
+          |       Part-A2       |      17       |                          14                          |                                                       |                                            |
         
         Like [MMDetection](https://github.com/open-mmlab/mmdetection) and [MMCV](https://github.com/open-mmlab/mmcv), MMDetection3D can also be used as a library to support different projects on top of it.
         
         ## License
         
         This project is released under the [Apache 2.0 license](LICENSE).
         
         ## Changelog
         
-        **1.1.0rc3** was released in 7/1/2023.
+        **1.1.0** was released in 6/4/2023.
         
         Please refer to [changelog.md](docs/en/notes/changelog.md) for details and release history.
         
         ## Benchmark and model zoo
         
         Results and models are available in the [model zoo](docs/en/model_zoo.md).
         
@@ -114,14 +138,17 @@
               <td>
               <ul>
                 <li><a href="configs/pointnet2">PointNet (CVPR'2017)</a></li>
                 <li><a href="configs/pointnet2">PointNet++ (NeurIPS'2017)</a></li>
                 <li><a href="configs/regnet">RegNet (CVPR'2020)</a></li>
                 <li><a href="configs/dgcnn">DGCNN (TOG'2019)</a></li>
                 <li>DLA (CVPR'2018)</li>
+                <li>MinkResNet (CVPR'2019)</li>
+                <li><a href="configs/minkunet">MinkUNet (CVPR'2019)</a></li>
+                <li><a href="configs/cylinder3d">Cylinder3D (CVPR'2021)</a></li>
               </ul>
               </td>
               <td>
               <ul>
                 <li><a href="configs/free_anchor">FreeAnchor (NeurIPS'2019)</a></li>
               </ul>
               </td>
@@ -182,75 +209,88 @@
                 <ul>
                   <li><a href="configs/imvoxelnet">ImVoxelNet (WACV'2022)</a></li>
                   <li><a href="configs/smoke">SMOKE (CVPRW'2020)</a></li>
                   <li><a href="configs/fcos3d">FCOS3D (ICCVW'2021)</a></li>
                   <li><a href="configs/pgd">PGD (CoRL'2021)</a></li>
                   <li><a href="configs/monoflex">MonoFlex (CVPR'2021)</a></li>
                 </ul>
+                <li><b>Indoor</b></li>
+                <ul>
+                  <li><a href="configs/imvoxelnet">ImVoxelNet (WACV'2022)</a></li>
+                </ul>
               </td>
               <td>
                 <li><b>Outdoor</b></li>
                 <ul>
                   <li><a href="configs/mvxnet">MVXNet (ICRA'2019)</a></li>
                 </ul>
                 <li><b>Indoor</b></li>
                 <ul>
                   <li><a href="configs/imvotenet">ImVoteNet (CVPR'2020)</a></li>
                 </ul>
               </td>
               <td>
+                <li><b>Outdoor</b></li>
+                <ul>
+                  <li><a href="configs/minkunet">MinkUNet (CVPR'2019)</a></li>
+                  <li><a href="configs/spvcnn">SPVCNN (ECCV'2020)</a></li>
+                  <li><a href="configs/cylinder3d">Cylinder3D (CVPR'2021)</a></li>
+                </ul>
                 <li><b>Indoor</b></li>
                 <ul>
                   <li><a href="configs/pointnet2">PointNet++ (NeurIPS'2017)</a></li>
                   <li><a href="configs/paconv">PAConv (CVPR'2021)</a></li>
                   <li><a href="configs/dgcnn">DGCNN (TOG'2019)</a></li>
                 </ul>
               </ul>
               </td>
             </tr>
         </td>
             </tr>
           </tbody>
         </table>
         
-        |               | ResNet | ResNeXt | SENet | PointNet++ | DGCNN | HRNet | RegNetX | Res2Net | DLA | MinkResNet |
-        | ------------- | :----: | :-----: | :---: | :--------: | :---: | :---: | :-----: | :-----: | :-: | :--------: |
-        | SECOND        |       |        |      |           |      |      |        |        |    |           |
-        | PointPillars  |       |        |      |           |      |      |        |        |    |           |
-        | FreeAnchor    |       |        |      |           |      |      |        |        |    |           |
-        | VoteNet       |       |        |      |           |      |      |        |        |    |           |
-        | H3DNet        |       |        |      |           |      |      |        |        |    |           |
-        | 3DSSD         |       |        |      |           |      |      |        |        |    |           |
-        | Part-A2       |       |        |      |           |      |      |        |        |    |           |
-        | MVXNet        |       |        |      |           |      |      |        |        |    |           |
-        | CenterPoint   |       |        |      |           |      |      |        |        |    |           |
-        | SSN           |       |        |      |           |      |      |        |        |    |           |
-        | ImVoteNet     |       |        |      |           |      |      |        |        |    |           |
-        | FCOS3D        |       |        |      |           |      |      |        |        |    |           |
-        | PointNet++    |       |        |      |           |      |      |        |        |    |           |
-        | Group-Free-3D |       |        |      |           |      |      |        |        |    |           |
-        | ImVoxelNet    |       |        |      |           |      |      |        |        |    |           |
-        | PAConv        |       |        |      |           |      |      |        |        |    |           |
-        | DGCNN         |       |        |      |           |      |      |        |        |    |           |
-        | SMOKE         |       |        |      |           |      |      |        |        |    |           |
-        | PGD           |       |        |      |           |      |      |        |        |    |           |
-        | MonoFlex      |       |        |      |           |      |      |        |        |    |           |
-        | SA-SSD        |       |        |      |           |      |      |        |        |    |           |
-        | FCAF3D        |       |        |      |           |      |      |        |        |    |           |
-        | PV-RCNN       |       |        |      |           |      |      |        |        |    |           |
+        |               | ResNet | PointNet++ | SECOND | DGCNN | RegNetX | DLA | MinkResNet | Cylinder3D | MinkUNet |
+        | :-----------: | :----: | :--------: | :----: | :---: | :-----: | :-: | :--------: | :--------: | :------: |
+        |    SECOND     |       |           |       |      |        |    |           |           |         |
+        | PointPillars  |       |           |       |      |        |    |           |           |         |
+        |  FreeAnchor   |       |           |       |      |        |    |           |           |         |
+        |    VoteNet    |       |           |       |      |        |    |           |           |         |
+        |    H3DNet     |       |           |       |      |        |    |           |           |         |
+        |     3DSSD     |       |           |       |      |        |    |           |           |         |
+        |    Part-A2    |       |           |       |      |        |    |           |           |         |
+        |    MVXNet     |       |           |       |      |        |    |           |           |         |
+        |  CenterPoint  |       |           |       |      |        |    |           |           |         |
+        |      SSN      |       |           |       |      |        |    |           |           |         |
+        |   ImVoteNet   |       |           |       |      |        |    |           |           |         |
+        |    FCOS3D     |       |           |       |      |        |    |           |           |         |
+        |  PointNet++   |       |           |       |      |        |    |           |           |         |
+        | Group-Free-3D |       |           |       |      |        |    |           |           |         |
+        |  ImVoxelNet   |       |           |       |      |        |    |           |           |         |
+        |    PAConv     |       |           |       |      |        |    |           |           |         |
+        |     DGCNN     |       |           |       |      |        |    |           |           |         |
+        |     SMOKE     |       |           |       |      |        |    |           |           |         |
+        |      PGD      |       |           |       |      |        |    |           |           |         |
+        |   MonoFlex    |       |           |       |      |        |    |           |           |         |
+        |    SA-SSD     |       |           |       |      |        |    |           |           |         |
+        |    FCAF3D     |       |           |       |      |        |    |           |           |         |
+        |    PV-RCNN    |       |           |       |      |        |    |           |           |         |
+        |  Cylinder3D   |       |           |       |      |        |    |           |           |         |
+        |   MinkUNet    |       |           |       |      |        |    |           |           |         |
+        |    SPVCNN     |       |           |       |      |        |    |           |           |         |
         
         **Note:** All the about **300+ models, methods of 40+ papers** in 2D detection supported by [MMDetection](https://github.com/open-mmlab/mmdetection/blob/3.x/docs/en/model_zoo.md) can be trained or used in this codebase.
         
         ## Installation
         
-        Please refer to [getting_started.md](docs/en/getting_started.md) for installation.
+        Please refer to [get_started.md](docs/en/get_started.md) for installation.
         
         ## Get Started
         
-        Please see [getting_started.md](docs/en/getting_started.md) for the basic usage of MMDetection3D. We provide guidance for quick run [with existing dataset](docs/en/user_guides/train_test.md) and [with new dataset](docs/en/user_guides/2_new_data_model.md) for beginners. There are also tutorials for [learning configuration systems](docs/en/user_guides/config.md), [customizing dataset](docs/en/advanced_guides/customize_dataset.md), [designing data pipeline](docs/en/user_guides/data_pipeline.md), [customizing models](docs/en/advanced_guides/customize_models.md), [customizing runtime settings](docs/en/advanced_guides/customize_runtime.md) and [Waymo dataset](docs/en/advanced_guides/datasets/waymo_det.md).
+        Please see [get_started.md](docs/en/get_started.md) for the basic usage of MMDetection3D. We provide guidance for quick run [with existing dataset](docs/en/user_guides/train_test.md) and [with new dataset](docs/en/user_guides/2_new_data_model.md) for beginners. There are also tutorials for [learning configuration systems](docs/en/user_guides/config.md), [customizing dataset](docs/en/advanced_guides/customize_dataset.md), [designing data pipeline](docs/en/user_guides/data_pipeline.md), [customizing models](docs/en/advanced_guides/customize_models.md), [customizing runtime settings](docs/en/advanced_guides/customize_runtime.md) and [Waymo dataset](docs/en/advanced_guides/datasets/waymo_det.md).
         
         Please refer to [FAQ](docs/en/notes/faq.md) for frequently asked questions. When updating the version of MMDetection3D, please also check the [compatibility doc](docs/en/notes/compatibility.md) to be aware of the BC-breaking updates introduced in each version.
         
         ## Citation
         
         If you find this project useful in your research, please consider cite:
         
@@ -261,53 +301,54 @@
             howpublished = {\url{https://github.com/open-mmlab/mmdetection3d}},
             year={2020}
         }
         ```
         
         ## Contributing
         
-        We appreciate all contributions to improve MMDetection3D. Please refer to [CONTRIBUTING.md](.github/CONTRIBUTING.md) for the contributing guideline.
+        We appreciate all contributions to improve MMDetection3D. Please refer to [CONTRIBUTING.md](./docs/en/notes/contribution_guides.md) for the contributing guideline.
         
         ## Acknowledgement
         
         MMDetection3D is an open source project that is contributed by researchers and engineers from various colleges and companies. We appreciate all the contributors as well as users who give valuable feedbacks.
         We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new 3D detectors.
         
         ## Projects in OpenMMLab
         
         - [MMEngine](https://github.com/open-mmlab/mmengine): OpenMMLab foundational library for training deep learning models.
         - [MMCV](https://github.com/open-mmlab/mmcv): OpenMMLab foundational library for computer vision.
         - [MMEval](https://github.com/open-mmlab/mmeval): A unified evaluation library for multiple machine learning libraries.
         - [MIM](https://github.com/open-mmlab/mim): MIM installs OpenMMLab packages.
-        - [MMClassification](https://github.com/open-mmlab/mmclassification): OpenMMLab image classification toolbox and benchmark.
+        - [MMPreTrain](https://github.com/open-mmlab/mmpretrain): OpenMMLab pre-training toolbox and benchmark.
         - [MMDetection](https://github.com/open-mmlab/mmdetection): OpenMMLab detection toolbox and benchmark.
         - [MMDetection3D](https://github.com/open-mmlab/mmdetection3d): OpenMMLab's next-generation platform for general 3D object detection.
         - [MMRotate](https://github.com/open-mmlab/mmrotate): OpenMMLab rotated object detection toolbox and benchmark.
         - [MMYOLO](https://github.com/open-mmlab/mmyolo): OpenMMLab YOLO series toolbox and benchmark.
         - [MMSegmentation](https://github.com/open-mmlab/mmsegmentation): OpenMMLab semantic segmentation toolbox and benchmark.
         - [MMOCR](https://github.com/open-mmlab/mmocr): OpenMMLab text detection, recognition, and understanding toolbox.
         - [MMPose](https://github.com/open-mmlab/mmpose): OpenMMLab pose estimation toolbox and benchmark.
         - [MMHuman3D](https://github.com/open-mmlab/mmhuman3d): OpenMMLab 3D human parametric model toolbox and benchmark.
         - [MMSelfSup](https://github.com/open-mmlab/mmselfsup): OpenMMLab self-supervised learning toolbox and benchmark.
         - [MMRazor](https://github.com/open-mmlab/mmrazor): OpenMMLab model compression toolbox and benchmark.
         - [MMFewShot](https://github.com/open-mmlab/mmfewshot): OpenMMLab fewshot learning toolbox and benchmark.
         - [MMAction2](https://github.com/open-mmlab/mmaction2): OpenMMLab's next-generation action understanding toolbox and benchmark.
         - [MMTracking](https://github.com/open-mmlab/mmtracking): OpenMMLab video perception toolbox and benchmark.
         - [MMFlow](https://github.com/open-mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark.
-        - [MMEditing](https://github.com/open-mmlab/mmediting): OpenMMLab image and video editing toolbox.
+        - [MMagic](https://github.com/open-mmlab/mmagic): Open**MM**Lab **A**dvanced, **G**enerative and **I**ntelligent **C**reation toolbox.
         - [MMGeneration](https://github.com/open-mmlab/mmgeneration): OpenMMLab image and video generative models toolbox.
         - [MMDeploy](https://github.com/open-mmlab/mmdeploy): OpenMMLab model deployment framework.
         
 Keywords: computer vision,3D object detection
 Platform: UNKNOWN
-Classifier: Development Status :: 4 - Beta
+Classifier: Development Status :: 5 - Production/Stable
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.6
 Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
 Description-Content-Type: text/markdown
 Provides-Extra: all
 Provides-Extra: tests
 Provides-Extra: build
 Provides-Extra: optional
 Provides-Extra: mim
```

#### html2text {}

```diff
@@ -1,187 +1,197 @@
-Metadata-Version: 2.1 Name: mmdet3d Version: 1.1.0rc3 Summary: OpenMMLab's
-next-generation platformfor general 3D object detection. Home-page: https://
+Metadata-Version: 2.1 Name: mmdet3d Version: 1.1.1 Summary: OpenMMLab's next-
+generation platformfor general 3D object detection. Home-page: https://
 github.com/open-mmlab/mmdetection3d Author: MMDetection3D Contributors Author-
 email: zwwdev@gmail.com License: Apache License 2.0 Description:
                          [resources/mmdet3d-logo.png]
                                        
            OpenMMLab website HOT  OpenMMLab platform TRY_IT_OUT
                                        
-[![docs](https://img.shields.io/badge/docs-latest-blue)](https://
-mmdetection3d.readthedocs.io/en/1.1/) [![badge](https://github.com/open-mmlab/
-mmdetection3d/workflows/build/badge.svg)](https://github.com/open-mmlab/
-mmdetection3d/actions) [![codecov](https://codecov.io/gh/open-mmlab/
+       [![docs](https://img.shields.io/badge/docs-latest-blue)](https://
+  mmdetection3d.readthedocs.io/en/latest/) [![badge](https://github.com/open-
+mmlab/mmdetection3d/workflows/build/badge.svg)](https://github.com/open-mmlab/
+     mmdetection3d/actions) [![codecov](https://codecov.io/gh/open-mmlab/
 mmdetection3d/branch/master/graph/badge.svg)](https://codecov.io/gh/open-mmlab/
-mmdetection3d) [![license](https://img.shields.io/github/license/open-mmlab/
-mmdetection3d.svg)](https://github.com/open-mmlab/mmdetection3d/blob/master/
-LICENSE) **News**: **v1.1.0rc3** was released in 7/1/2023 The compatibilities
-of models are broken due to the unification and simplification of coordinate
-systems after v1.0.0rc0. For now, most models are benchmarked with similar
-performance, though few models are still being benchmarked. In the following
-release, we will update all the model checkpoints and benchmarks. See more
-details in the [Changelog](docs/en/notes/changelog.md) and [Changelog-v1.0.x]
-(docs/en/notes/changelog_v1.0.x.md). Documentation: https://
-mmdetection3d.readthedocs.io/ ## Introduction English | []
-(README_zh-CN.md) The master branch works with **PyTorch 1.6+**. MMDetection3D
-is an open source object detection toolbox based on PyTorch, towards the next-
-generation platform for general 3D detection. It is a part of the OpenMMLab
-project developed by [MMLab](http://mmlab.ie.cuhk.edu.hk/). ![demo image]
-(resources/mmdet3d_outdoor_demo.gif) ### Major features - **Support multi-
-modality/single-modality detectors out of box** It directly supports multi-
-modality/single-modality detectors including MVXNet, VoteNet, PointPillars,
-etc. - **Support indoor/outdoor 3D detection out of box** It directly supports
-popular indoor and outdoor 3D detection datasets, including ScanNet, SUNRGB-D,
-Waymo, nuScenes, Lyft, and KITTI. For nuScenes dataset, we also support
-[nuImages dataset](https://github.com/open-mmlab/mmdetection3d/tree/1.1/
-configs/nuimages). - **Natural integration with 2D detection** All the about
-**300+ models, methods of 40+ papers**, and modules supported in [MMDetection]
-(https://github.com/open-mmlab/mmdetection/blob/3.x/docs/en/model_zoo.md) can
-be trained or used in this codebase. - **High efficiency** It trains faster
-than other codebases. The main results are as below. Details can be found in
-[benchmark.md](./docs/en/notes/benchmarks.md). We compare the number of samples
-trained per second (the higher, the better). The models that are not supported
-by other codebases are marked by ``. | Methods | MMDetection3D | [OpenPCDet]
-(https://github.com/open-mmlab/OpenPCDet) | [votenet](https://github.com/
-facebookresearch/votenet) | [Det3D](https://github.com/poodarchu/Det3D) | | :--
----------------: | :-----------: | :-------------------------------------------
--------: | :----------------------------------------------------: | :----------
--------------------------------: | | VoteNet | 358 |  | 77 |  | |
-PointPillars-car | 141 |  |  | 140 | | PointPillars-3class | 107 | 44 | 
-|  | | SECOND | 40 | 30 |  |  | | Part-A2 | 17 | 14 |  |  | Like
-[MMDetection](https://github.com/open-mmlab/mmdetection) and [MMCV](https://
-github.com/open-mmlab/mmcv), MMDetection3D can also be used as a library to
-support different projects on top of it. ## License This project is released
-under the [Apache 2.0 license](LICENSE). ## Changelog **1.1.0rc3** was released
-in 7/1/2023. Please refer to [changelog.md](docs/en/notes/changelog.md) for
-details and release history. ## Benchmark and model zoo Results and models are
-available in the [model zoo](docs/en/model_zoo.md).
+ mmdetection3d) [![license](https://img.shields.io/github/license/open-mmlab/
+ mmdetection3d.svg)](https://github.com/open-mmlab/mmdetection3d/blob/master/
+                                   LICENSE)
+
+**News**: **We have renamed the branch `1.1` to `main` and switched the default
+branch from `master` to `main`. We encourage users to migrate to the latest
+version, though it comes with some cost. Please refer to [Migration Guide]
+(docs/en/migration.md) for more details.** **v1.1.1** was released in 30/5/2023
+We have constructed a comprehensive LiDAR semantic segmentation benchmark on
+SemanticKITTI, including Cylinder3D, MinkUNet and SPVCNN methods. Noteworthy,
+the improved MinkUNetv2 can achieve 70.3 mIoU on the validation set of
+SemanticKITTI. We have also supported the training of BEVFusion and an
+occupancy prediction method, TPVFomrer, in our `projects`. More new features
+about 3D perception are on the way. Please stay tuned! ## Introduction English
+| [](README_zh-CN.md) The main branch works with **PyTorch 1.8+**.
+MMDetection3D is an open source object detection toolbox based on PyTorch,
+towards the next-generation platform for general 3D detection. It is a part of
+the OpenMMLab project developed by [MMLab](http://mmlab.ie.cuhk.edu.hk/). !
+[demo image](resources/mmdet3d_outdoor_demo.gif) ### Major features - **Support
+multi-modality/single-modality detectors out of box** It directly supports
+multi-modality/single-modality detectors including MVXNet, VoteNet,
+PointPillars, etc. - **Support indoor/outdoor 3D detection out of box** It
+directly supports popular indoor and outdoor 3D detection datasets, including
+ScanNet, SUNRGB-D, Waymo, nuScenes, Lyft, and KITTI. For nuScenes dataset, we
+also support [nuImages dataset](https://github.com/open-mmlab/mmdetection3d/
+tree/main/configs/nuimages). - **Natural integration with 2D detection** All
+the about **300+ models, methods of 40+ papers**, and modules supported in
+[MMDetection](https://github.com/open-mmlab/mmdetection/blob/3.x/docs/en/
+model_zoo.md) can be trained or used in this codebase. - **High efficiency** It
+trains faster than other codebases. The main results are as below. Details can
+be found in [benchmark.md](./docs/en/notes/benchmarks.md). We compare the
+number of samples trained per second (the higher, the better). The models that
+are not supported by other codebases are marked by ``. | Methods |
+MMDetection3D | [OpenPCDet](https://github.com/open-mmlab/OpenPCDet) |
+[votenet](https://github.com/facebookresearch/votenet) | [Det3D](https://
+github.com/poodarchu/Det3D) | | :-----------------: | :-----------: | :--------
+------------------------------------------: | :--------------------------------
+--------------------: | :-----------------------------------------: | | VoteNet
+| 358 |  | 77 |  | | PointPillars-car | 141 |  |  | 140 | |
+PointPillars-3class | 107 | 44 |  |  | | SECOND | 40 | 30 |  |  | |
+Part-A2 | 17 | 14 |  |  | Like [MMDetection](https://github.com/open-
+mmlab/mmdetection) and [MMCV](https://github.com/open-mmlab/mmcv),
+MMDetection3D can also be used as a library to support different projects on
+top of it. ## License This project is released under the [Apache 2.0 license]
+(LICENSE). ## Changelog **1.1.0** was released in 6/4/2023. Please refer to
+[changelog.md](docs/en/notes/changelog.md) for details and release history. ##
+Benchmark and model zoo Results and models are available in the [model zoo]
+(docs/en/model_zoo.md).
                                   Components
-              Backbones                 Heads                   Features
-      * PointNet_(CVPR'2017)     * FreeAnchor_        * Dynamic_Voxelization_
-      * PointNet++_                (NeurIPS'2019)       (CoRL'2019)
-        (NeurIPS'2017)
-      * RegNet_(CVPR'2020)
-      * DGCNN_(TOG'2019)
-      * DLA (CVPR'2018)
+               Backbones                 Heads                   Features
+     * PointNet_(CVPR'2017)       * FreeAnchor_        * Dynamic_Voxelization_
+     * PointNet++_                  (NeurIPS'2019)       (CoRL'2019)
+       (NeurIPS'2017)
+     * RegNet_(CVPR'2020)
+     * DGCNN_(TOG'2019)
+     * DLA (CVPR'2018)
+     * MinkResNet (CVPR'2019)
+     * MinkUNet_(CVPR'2019)
+     * Cylinder3D_(CVPR'2021)
                                  Architectures
 3D Object Detection  Monocular 3D Object  Multi-modal 3D         3D Semantic
                           Detection      Object Detection        Segmentation
-Outdoor              Outdoor             Outdoor           Indoor
-    * SECOND_            * ImVoxelNet_       * MVXNet_         * PointNet++_
-      (Sensor'2018)        (WACV'2022)         (ICRA'2019)       (NeurIPS'2017)
-    * PointPillars_      * SMOKE_        Indoor                * PAConv_
-      (CVPR'2019)          (CVPRW'2020)      * ImVoteNet_        (CVPR'2021)
-    * SSN_               * FCOS3D_             (CVPR'2020)     * DGCNN_
-      (ECCV'2020)          (ICCVW'2021)                          (TOG'2019)
-    * 3DSSD_             * PGD_
-      (CVPR'2020)          (CoRL'2021)
-    * SA-SSD_            * MonoFlex_
-      (CVPR'2020)          (CVPR'2021)
-    * PointRCNN_
-      (CVPR'2019)
-    * Part-A2_
+Outdoor              Outdoor             Outdoor           Outdoor
+    * SECOND_            * ImVoxelNet_       * MVXNet_         * MinkUNet_
+      (Sensor'2018)        (WACV'2022)         (ICRA'2019)       (CVPR'2019)
+    * PointPillars_      * SMOKE_        Indoor                * SPVCNN_
+      (CVPR'2019)          (CVPRW'2020)      * ImVoteNet_        (ECCV'2020)
+    * SSN_               * FCOS3D_             (CVPR'2020)     * Cylinder3D_
+      (ECCV'2020)          (ICCVW'2021)                          (CVPR'2021)
+    * 3DSSD_             * PGD_                            Indoor
+      (CVPR'2020)          (CoRL'2021)                         * PointNet++_
+    * SA-SSD_            * MonoFlex_                             (NeurIPS'2017)
+      (CVPR'2020)          (CVPR'2021)                         * PAConv_
+    * PointRCNN_     Indoor                                      (CVPR'2021)
+      (CVPR'2019)        * ImVoxelNet_                         * DGCNN_
+    * Part-A2_             (WACV'2022)                           (TOG'2019)
       (TPAMI'2020)
     * CenterPoint_
       (CVPR'2021)
     * PV-RCNN_
       (CVPR'2020)
 Indoor
     * VoteNet_
       (ICCV'2019)
     * H3DNet_
       (ECCV'2020)
     * Group-Free-3D_
       (ICCV'2021)
     * FCAF3D_
       (ECCV'2022)
-| | ResNet | ResNeXt | SENet | PointNet++ | DGCNN | HRNet | RegNetX | Res2Net |
-DLA | MinkResNet | | ------------- | :----: | :-----: | :---: | :--------: | :-
---: | :---: | :-----: | :-----: | :-: | :--------: | | SECOND |  |  | 
-|  |  |  |  |  |  |  | | PointPillars |  |  |  |
- |  |  |  |  |  |  | | FreeAnchor |  |  |  | 
-|  |  |  |  |  |  | | VoteNet |  |  |  |  |  |
- |  |  |  |  | | H3DNet |  |  |  |  |  |  |
- |  |  |  | | 3DSSD |  |  |  |  |  |  |  | 
-|  |  | | Part-A2 |  |  |  |  |  |  |  |  |  |
- | | MVXNet |  |  |  |  |  |  |  |  |  |  | |
-CenterPoint |  |  |  |  |  |  |  |  |  |  | | SSN
-|  |  |  |  |  |  |  |  |  |  | | ImVoteNet | 
-|  |  |  |  |  |  |  |  |  | | FCOS3D |  |  |
- |  |  |  |  |  |  |  | | PointNet++ |  |  | 
-|  |  |  |  |  |  |  | | Group-Free-3D |  |  |  |
- |  |  |  |  |  |  | | ImVoxelNet |  |  |  | 
-|  |  |  |  |  |  | | PAConv |  |  |  |  |  |
- |  |  |  |  | | DGCNN |  |  |  |  |  |  | 
-|  |  |  | | SMOKE |  |  |  |  |  |  |  |  |
- |  | | PGD |  |  |  |  |  |  |  |  |  |  |
-| MonoFlex |  |  |  |  |  |  |  |  |  |  | | SA-
-SSD |  |  |  |  |  |  |  |  |  |  | | FCAF3D |
- |  |  |  |  |  |  |  |  |  | | PV-RCNN |  |
- |  |  |  |  |  |  |  |  | **Note:** All the about
+| | ResNet | PointNet++ | SECOND | DGCNN | RegNetX | DLA | MinkResNet |
+Cylinder3D | MinkUNet | | :-----------: | :----: | :--------: | :----: | :---:
+| :-----: | :-: | :--------: | :--------: | :------: | | SECOND |  |  |
+ |  |  |  |  |  |  | | PointPillars |  |  |  |
+ |  |  |  |  |  | | FreeAnchor |  |  |  |  | 
+|  |  |  |  | | VoteNet |  |  |  |  |  |  |  |
+ |  | | H3DNet |  |  |  |  |  |  |  |  |  | |
+3DSSD |  |  |  |  |  |  |  |  |  | | Part-A2 |  |
+ |  |  |  |  |  |  |  | | MVXNet |  |  |  |
+ |  |  |  |  |  | | CenterPoint |  |  |  |  | 
+|  |  |  |  | | SSN |  |  |  |  |  |  |  | 
+|  | | ImVoteNet |  |  |  |  |  |  |  |  |  | |
+FCOS3D |  |  |  |  |  |  |  |  |  | | PointNet++ |
+ |  |  |  |  |  |  |  |  | | Group-Free-3D |  |
+ |  |  |  |  |  |  |  | | ImVoxelNet |  |  | 
+|  |  |  |  |  |  | | PAConv |  |  |  |  |  |
+ |  |  |  | | DGCNN |  |  |  |  |  |  |  | 
+|  | | SMOKE |  |  |  |  |  |  |  |  |  | | PGD |
+ |  |  |  |  |  |  |  |  | | MonoFlex |  |  |
+ |  |  |  |  |  |  | | SA-SSD |  |  |  |  |
+ |  |  |  |  | | FCAF3D |  |  |  |  |  |  |
+ |  |  | | PV-RCNN |  |  |  |  |  |  |  |  |
+ | | Cylinder3D |  |  |  |  |  |  |  |  |  | |
+MinkUNet |  |  |  |  |  |  |  |  |  | | SPVCNN | 
+|  |  |  |  |  |  |  |  | **Note:** All the about
 **300+ models, methods of 40+ papers** in 2D detection supported by
 [MMDetection](https://github.com/open-mmlab/mmdetection/blob/3.x/docs/en/
 model_zoo.md) can be trained or used in this codebase. ## Installation Please
-refer to [getting_started.md](docs/en/getting_started.md) for installation. ##
-Get Started Please see [getting_started.md](docs/en/getting_started.md) for the
-basic usage of MMDetection3D. We provide guidance for quick run [with existing
-dataset](docs/en/user_guides/train_test.md) and [with new dataset](docs/en/
-user_guides/2_new_data_model.md) for beginners. There are also tutorials for
-[learning configuration systems](docs/en/user_guides/config.md), [customizing
-dataset](docs/en/advanced_guides/customize_dataset.md), [designing data
-pipeline](docs/en/user_guides/data_pipeline.md), [customizing models](docs/en/
-advanced_guides/customize_models.md), [customizing runtime settings](docs/en/
-advanced_guides/customize_runtime.md) and [Waymo dataset](docs/en/
-advanced_guides/datasets/waymo_det.md). Please refer to [FAQ](docs/en/notes/
-faq.md) for frequently asked questions. When updating the version of
-MMDetection3D, please also check the [compatibility doc](docs/en/notes/
-compatibility.md) to be aware of the BC-breaking updates introduced in each
-version. ## Citation If you find this project useful in your research, please
-consider cite: ```latex @misc{mmdet3d2020, title={{MMDetection3D: OpenMMLab}
-next-generation platform for general {3D} object detection}, author=
-{MMDetection3D Contributors}, howpublished = {\url{https://github.com/open-
-mmlab/mmdetection3d}}, year={2020} } ``` ## Contributing We appreciate all
-contributions to improve MMDetection3D. Please refer to [CONTRIBUTING.md]
-(.github/CONTRIBUTING.md) for the contributing guideline. ## Acknowledgement
-MMDetection3D is an open source project that is contributed by researchers and
-engineers from various colleges and companies. We appreciate all the
-contributors as well as users who give valuable feedbacks. We wish that the
-toolbox and benchmark could serve the growing research community by providing a
-flexible toolkit to reimplement existing methods and develop their own new 3D
-detectors. ## Projects in OpenMMLab - [MMEngine](https://github.com/open-mmlab/
-mmengine): OpenMMLab foundational library for training deep learning models. -
-[MMCV](https://github.com/open-mmlab/mmcv): OpenMMLab foundational library for
-computer vision. - [MMEval](https://github.com/open-mmlab/mmeval): A unified
-evaluation library for multiple machine learning libraries. - [MIM](https://
-github.com/open-mmlab/mim): MIM installs OpenMMLab packages. -
-[MMClassification](https://github.com/open-mmlab/mmclassification): OpenMMLab
-image classification toolbox and benchmark. - [MMDetection](https://github.com/
-open-mmlab/mmdetection): OpenMMLab detection toolbox and benchmark. -
-[MMDetection3D](https://github.com/open-mmlab/mmdetection3d): OpenMMLab's next-
-generation platform for general 3D object detection. - [MMRotate](https://
-github.com/open-mmlab/mmrotate): OpenMMLab rotated object detection toolbox and
-benchmark. - [MMYOLO](https://github.com/open-mmlab/mmyolo): OpenMMLab YOLO
-series toolbox and benchmark. - [MMSegmentation](https://github.com/open-mmlab/
-mmsegmentation): OpenMMLab semantic segmentation toolbox and benchmark. -
-[MMOCR](https://github.com/open-mmlab/mmocr): OpenMMLab text detection,
-recognition, and understanding toolbox. - [MMPose](https://github.com/open-
-mmlab/mmpose): OpenMMLab pose estimation toolbox and benchmark. - [MMHuman3D]
-(https://github.com/open-mmlab/mmhuman3d): OpenMMLab 3D human parametric model
-toolbox and benchmark. - [MMSelfSup](https://github.com/open-mmlab/mmselfsup):
+refer to [get_started.md](docs/en/get_started.md) for installation. ## Get
+Started Please see [get_started.md](docs/en/get_started.md) for the basic usage
+of MMDetection3D. We provide guidance for quick run [with existing dataset]
+(docs/en/user_guides/train_test.md) and [with new dataset](docs/en/user_guides/
+2_new_data_model.md) for beginners. There are also tutorials for [learning
+configuration systems](docs/en/user_guides/config.md), [customizing dataset]
+(docs/en/advanced_guides/customize_dataset.md), [designing data pipeline](docs/
+en/user_guides/data_pipeline.md), [customizing models](docs/en/advanced_guides/
+customize_models.md), [customizing runtime settings](docs/en/advanced_guides/
+customize_runtime.md) and [Waymo dataset](docs/en/advanced_guides/datasets/
+waymo_det.md). Please refer to [FAQ](docs/en/notes/faq.md) for frequently asked
+questions. When updating the version of MMDetection3D, please also check the
+[compatibility doc](docs/en/notes/compatibility.md) to be aware of the BC-
+breaking updates introduced in each version. ## Citation If you find this
+project useful in your research, please consider cite: ```latex @misc
+{mmdet3d2020, title={{MMDetection3D: OpenMMLab} next-generation platform for
+general {3D} object detection}, author={MMDetection3D Contributors},
+howpublished = {\url{https://github.com/open-mmlab/mmdetection3d}}, year={2020}
+} ``` ## Contributing We appreciate all contributions to improve MMDetection3D.
+Please refer to [CONTRIBUTING.md](./docs/en/notes/contribution_guides.md) for
+the contributing guideline. ## Acknowledgement MMDetection3D is an open source
+project that is contributed by researchers and engineers from various colleges
+and companies. We appreciate all the contributors as well as users who give
+valuable feedbacks. We wish that the toolbox and benchmark could serve the
+growing research community by providing a flexible toolkit to reimplement
+existing methods and develop their own new 3D detectors. ## Projects in
+OpenMMLab - [MMEngine](https://github.com/open-mmlab/mmengine): OpenMMLab
+foundational library for training deep learning models. - [MMCV](https://
+github.com/open-mmlab/mmcv): OpenMMLab foundational library for computer
+vision. - [MMEval](https://github.com/open-mmlab/mmeval): A unified evaluation
+library for multiple machine learning libraries. - [MIM](https://github.com/
+open-mmlab/mim): MIM installs OpenMMLab packages. - [MMPreTrain](https://
+github.com/open-mmlab/mmpretrain): OpenMMLab pre-training toolbox and
+benchmark. - [MMDetection](https://github.com/open-mmlab/mmdetection):
+OpenMMLab detection toolbox and benchmark. - [MMDetection3D](https://
+github.com/open-mmlab/mmdetection3d): OpenMMLab's next-generation platform for
+general 3D object detection. - [MMRotate](https://github.com/open-mmlab/
+mmrotate): OpenMMLab rotated object detection toolbox and benchmark. - [MMYOLO]
+(https://github.com/open-mmlab/mmyolo): OpenMMLab YOLO series toolbox and
+benchmark. - [MMSegmentation](https://github.com/open-mmlab/mmsegmentation):
+OpenMMLab semantic segmentation toolbox and benchmark. - [MMOCR](https://
+github.com/open-mmlab/mmocr): OpenMMLab text detection, recognition, and
+understanding toolbox. - [MMPose](https://github.com/open-mmlab/mmpose):
+OpenMMLab pose estimation toolbox and benchmark. - [MMHuman3D](https://
+github.com/open-mmlab/mmhuman3d): OpenMMLab 3D human parametric model toolbox
+and benchmark. - [MMSelfSup](https://github.com/open-mmlab/mmselfsup):
 OpenMMLab self-supervised learning toolbox and benchmark. - [MMRazor](https://
 github.com/open-mmlab/mmrazor): OpenMMLab model compression toolbox and
 benchmark. - [MMFewShot](https://github.com/open-mmlab/mmfewshot): OpenMMLab
 fewshot learning toolbox and benchmark. - [MMAction2](https://github.com/open-
 mmlab/mmaction2): OpenMMLab's next-generation action understanding toolbox and
 benchmark. - [MMTracking](https://github.com/open-mmlab/mmtracking): OpenMMLab
 video perception toolbox and benchmark. - [MMFlow](https://github.com/open-
-mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark. - [MMEditing]
-(https://github.com/open-mmlab/mmediting): OpenMMLab image and video editing
-toolbox. - [MMGeneration](https://github.com/open-mmlab/mmgeneration):
-OpenMMLab image and video generative models toolbox. - [MMDeploy](https://
-github.com/open-mmlab/mmdeploy): OpenMMLab model deployment framework.
-Keywords: computer vision,3D object detection Platform: UNKNOWN Classifier:
-Development Status :: 4 - Beta Classifier: License :: OSI Approved :: Apache
-Software License Classifier: Operating System :: OS Independent Classifier:
-Programming Language :: Python :: 3 Classifier: Programming Language :: Python
-:: 3.6 Classifier: Programming Language :: Python :: 3.7 Description-Content-
-Type: text/markdown Provides-Extra: all Provides-Extra: tests Provides-Extra:
-build Provides-Extra: optional Provides-Extra: mim
+mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark. - [MMagic](https:/
+/github.com/open-mmlab/mmagic): Open**MM**Lab **A**dvanced, **G**enerative and
+**I**ntelligent **C**reation toolbox. - [MMGeneration](https://github.com/open-
+mmlab/mmgeneration): OpenMMLab image and video generative models toolbox. -
+[MMDeploy](https://github.com/open-mmlab/mmdeploy): OpenMMLab model deployment
+framework. Keywords: computer vision,3D object detection Platform: UNKNOWN
+Classifier: Development Status :: 5 - Production/Stable Classifier: License ::
+OSI Approved :: Apache Software License Classifier: Operating System :: OS
+Independent Classifier: Programming Language :: Python :: 3 Classifier:
+Programming Language :: Python :: 3.7 Classifier: Programming Language ::
+Python :: 3.8 Classifier: Programming Language :: Python :: 3.9 Description-
+Content-Type: text/markdown Provides-Extra: all Provides-Extra: tests Provides-
+Extra: build Provides-Extra: optional Provides-Extra: mim
```

### Comparing `mmdet3d-1.1.0rc3/README.md` & `mmdet3d-1.1.1/README.md`

 * *Files 18% similar despite different names*

```diff
@@ -13,34 +13,58 @@
     <sup>
       <a href="https://platform.openmmlab.com">
         <i><font size="4">TRY IT OUT</font></i>
       </a>
     </sup>
   </div>
   <div>&nbsp;</div>
-</div>
 
-[![docs](https://img.shields.io/badge/docs-latest-blue)](https://mmdetection3d.readthedocs.io/en/1.1/)
+[![docs](https://img.shields.io/badge/docs-latest-blue)](https://mmdetection3d.readthedocs.io/en/latest/)
 [![badge](https://github.com/open-mmlab/mmdetection3d/workflows/build/badge.svg)](https://github.com/open-mmlab/mmdetection3d/actions)
 [![codecov](https://codecov.io/gh/open-mmlab/mmdetection3d/branch/master/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmdetection3d)
 [![license](https://img.shields.io/github/license/open-mmlab/mmdetection3d.svg)](https://github.com/open-mmlab/mmdetection3d/blob/master/LICENSE)
 
+</div>
+
+</div>
+
+<div align="center">
+  <a href="https://openmmlab.medium.com/" style="text-decoration:none;">
+    <img src="https://user-images.githubusercontent.com/25839884/219255827-67c1a27f-f8c5-46a9-811d-5e57448c61d1.png" width="3%" alt="" /></a>
+  <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+  <a href="https://discord.com/channels/1037617289144569886/1046608014234370059" style="text-decoration:none;">
+    <img src="https://user-images.githubusercontent.com/25839884/218347213-c080267f-cbb6-443e-8532-8e1ed9a58ea9.png" width="3%" alt="" /></a>
+  <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+  <a href="https://twitter.com/OpenMMLab" style="text-decoration:none;">
+    <img src="https://user-images.githubusercontent.com/25839884/218346637-d30c8a0f-3eba-4699-8131-512fb06d46db.png" width="3%" alt="" /></a>
+  <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+  <a href="https://www.youtube.com/openmmlab" style="text-decoration:none;">
+    <img src="https://user-images.githubusercontent.com/25839884/218346691-ceb2116a-465a-40af-8424-9f30d2348ca9.png" width="3%" alt="" /></a>
+  <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+  <a href="https://space.bilibili.com/1293512903" style="text-decoration:none;">
+    <img src="https://user-images.githubusercontent.com/25839884/219026751-d7d14cce-a7c9-4e82-9942-8375fca65b99.png" width="3%" alt="" /></a>
+  <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+  <a href="https://www.zhihu.com/people/openmmlab" style="text-decoration:none;">
+    <img src="https://user-images.githubusercontent.com/25839884/219026120-ba71e48b-6e94-4bd4-b4e9-b7d175b5e362.png" width="3%" alt="" /></a>
+</div>
+
 **News**:
 
-**v1.1.0rc3** was released in 7/1/2023
+**We have renamed the branch `1.1`  to `main` and switched the default branch from `master` to `main`. We encourage
+users to migrate to the latest version, though it comes with some cost. Please refer to [Migration Guide](docs/en/migration.md) for more details.**
 
-The compatibilities of models are broken due to the unification and simplification of coordinate systems after v1.0.0rc0. For now, most models are benchmarked with similar performance, though few models are still being benchmarked. In the following release, we will update all the model checkpoints and benchmarks. See more details in the [Changelog](docs/en/notes/changelog.md) and [Changelog-v1.0.x](docs/en/notes/changelog_v1.0.x.md).
+**v1.1.1** was released in 30/5/2023
 
-Documentation: https://mmdetection3d.readthedocs.io/
+We have constructed a comprehensive LiDAR semantic segmentation benchmark on SemanticKITTI, including Cylinder3D, MinkUNet and SPVCNN methods. Noteworthy, the improved MinkUNetv2 can achieve 70.3 mIoU on the validation set of SemanticKITTI. We have also supported the training of BEVFusion and an occupancy prediction method, TPVFomrer, in our `projects`. More new features about 3D perception are on the way. Please stay tuned!
 
 ## Introduction
 
 English | [](README_zh-CN.md)
 
-The master branch works with **PyTorch 1.6+**.
+The main branch works with **PyTorch 1.8+**.
 
 MMDetection3D is an open source object detection toolbox based on PyTorch, towards the next-generation platform for general 3D detection. It is
 a part of the OpenMMLab project developed by [MMLab](http://mmlab.ie.cuhk.edu.hk/).
 
 ![demo image](resources/mmdet3d_outdoor_demo.gif)
 
 ### Major features
@@ -48,41 +72,41 @@
 - **Support multi-modality/single-modality detectors out of box**
 
   It directly supports multi-modality/single-modality detectors including MVXNet, VoteNet, PointPillars, etc.
 
 - **Support indoor/outdoor 3D detection out of box**
 
   It directly supports popular indoor and outdoor 3D detection datasets, including ScanNet, SUNRGB-D, Waymo, nuScenes, Lyft, and KITTI.
-  For nuScenes dataset, we also support [nuImages dataset](https://github.com/open-mmlab/mmdetection3d/tree/1.1/configs/nuimages).
+  For nuScenes dataset, we also support [nuImages dataset](https://github.com/open-mmlab/mmdetection3d/tree/main/configs/nuimages).
 
 - **Natural integration with 2D detection**
 
   All the about **300+ models, methods of 40+ papers**, and modules supported in [MMDetection](https://github.com/open-mmlab/mmdetection/blob/3.x/docs/en/model_zoo.md) can be trained or used in this codebase.
 
 - **High efficiency**
 
-  It trains faster than other codebases. The main results are as below. Details can be found in [benchmark.md](./docs/en/notes/benchmarks.md). We compare the number of samples trained per second (the higher, the better). The models that are not supported by other codebases are marked by ``.
+  It trains faster than other codebases. The main results are as below. Details can be found in [benchmark.md](./docs/en/notes/benchmarks.md). We compare the number of samples trained per second (the higher, the better). The models that are not supported by other codebases are marked by ``.
 
   |       Methods       | MMDetection3D | [OpenPCDet](https://github.com/open-mmlab/OpenPCDet) | [votenet](https://github.com/facebookresearch/votenet) | [Det3D](https://github.com/poodarchu/Det3D) |
   | :-----------------: | :-----------: | :--------------------------------------------------: | :----------------------------------------------------: | :-----------------------------------------: |
-  |       VoteNet       |      358      |                                                     |                           77                           |                                            |
-  |  PointPillars-car   |      141      |                                                     |                                                       |                     140                     |
-  | PointPillars-3class |      107      |                          44                          |                                                       |                                            |
-  |       SECOND        |      40       |                          30                          |                                                       |                                            |
-  |       Part-A2       |      17       |                          14                          |                                                       |                                            |
+  |       VoteNet       |      358      |                                                     |                           77                           |                                            |
+  |  PointPillars-car   |      141      |                                                     |                                                       |                     140                     |
+  | PointPillars-3class |      107      |                          44                          |                                                       |                                            |
+  |       SECOND        |      40       |                          30                          |                                                       |                                            |
+  |       Part-A2       |      17       |                          14                          |                                                       |                                            |
 
 Like [MMDetection](https://github.com/open-mmlab/mmdetection) and [MMCV](https://github.com/open-mmlab/mmcv), MMDetection3D can also be used as a library to support different projects on top of it.
 
 ## License
 
 This project is released under the [Apache 2.0 license](LICENSE).
 
 ## Changelog
 
-**1.1.0rc3** was released in 7/1/2023.
+**1.1.0** was released in 6/4/2023.
 
 Please refer to [changelog.md](docs/en/notes/changelog.md) for details and release history.
 
 ## Benchmark and model zoo
 
 Results and models are available in the [model zoo](docs/en/model_zoo.md).
 
@@ -106,14 +130,17 @@
       <td>
       <ul>
         <li><a href="configs/pointnet2">PointNet (CVPR'2017)</a></li>
         <li><a href="configs/pointnet2">PointNet++ (NeurIPS'2017)</a></li>
         <li><a href="configs/regnet">RegNet (CVPR'2020)</a></li>
         <li><a href="configs/dgcnn">DGCNN (TOG'2019)</a></li>
         <li>DLA (CVPR'2018)</li>
+        <li>MinkResNet (CVPR'2019)</li>
+        <li><a href="configs/minkunet">MinkUNet (CVPR'2019)</a></li>
+        <li><a href="configs/cylinder3d">Cylinder3D (CVPR'2021)</a></li>
       </ul>
       </td>
       <td>
       <ul>
         <li><a href="configs/free_anchor">FreeAnchor (NeurIPS'2019)</a></li>
       </ul>
       </td>
@@ -174,75 +201,88 @@
         <ul>
           <li><a href="configs/imvoxelnet">ImVoxelNet (WACV'2022)</a></li>
           <li><a href="configs/smoke">SMOKE (CVPRW'2020)</a></li>
           <li><a href="configs/fcos3d">FCOS3D (ICCVW'2021)</a></li>
           <li><a href="configs/pgd">PGD (CoRL'2021)</a></li>
           <li><a href="configs/monoflex">MonoFlex (CVPR'2021)</a></li>
         </ul>
+        <li><b>Indoor</b></li>
+        <ul>
+          <li><a href="configs/imvoxelnet">ImVoxelNet (WACV'2022)</a></li>
+        </ul>
       </td>
       <td>
         <li><b>Outdoor</b></li>
         <ul>
           <li><a href="configs/mvxnet">MVXNet (ICRA'2019)</a></li>
         </ul>
         <li><b>Indoor</b></li>
         <ul>
           <li><a href="configs/imvotenet">ImVoteNet (CVPR'2020)</a></li>
         </ul>
       </td>
       <td>
+        <li><b>Outdoor</b></li>
+        <ul>
+          <li><a href="configs/minkunet">MinkUNet (CVPR'2019)</a></li>
+          <li><a href="configs/spvcnn">SPVCNN (ECCV'2020)</a></li>
+          <li><a href="configs/cylinder3d">Cylinder3D (CVPR'2021)</a></li>
+        </ul>
         <li><b>Indoor</b></li>
         <ul>
           <li><a href="configs/pointnet2">PointNet++ (NeurIPS'2017)</a></li>
           <li><a href="configs/paconv">PAConv (CVPR'2021)</a></li>
           <li><a href="configs/dgcnn">DGCNN (TOG'2019)</a></li>
         </ul>
       </ul>
       </td>
     </tr>
 </td>
     </tr>
   </tbody>
 </table>
 
-|               | ResNet | ResNeXt | SENet | PointNet++ | DGCNN | HRNet | RegNetX | Res2Net | DLA | MinkResNet |
-| ------------- | :----: | :-----: | :---: | :--------: | :---: | :---: | :-----: | :-----: | :-: | :--------: |
-| SECOND        |       |        |      |           |      |      |        |        |    |           |
-| PointPillars  |       |        |      |           |      |      |        |        |    |           |
-| FreeAnchor    |       |        |      |           |      |      |        |        |    |           |
-| VoteNet       |       |        |      |           |      |      |        |        |    |           |
-| H3DNet        |       |        |      |           |      |      |        |        |    |           |
-| 3DSSD         |       |        |      |           |      |      |        |        |    |           |
-| Part-A2       |       |        |      |           |      |      |        |        |    |           |
-| MVXNet        |       |        |      |           |      |      |        |        |    |           |
-| CenterPoint   |       |        |      |           |      |      |        |        |    |           |
-| SSN           |       |        |      |           |      |      |        |        |    |           |
-| ImVoteNet     |       |        |      |           |      |      |        |        |    |           |
-| FCOS3D        |       |        |      |           |      |      |        |        |    |           |
-| PointNet++    |       |        |      |           |      |      |        |        |    |           |
-| Group-Free-3D |       |        |      |           |      |      |        |        |    |           |
-| ImVoxelNet    |       |        |      |           |      |      |        |        |    |           |
-| PAConv        |       |        |      |           |      |      |        |        |    |           |
-| DGCNN         |       |        |      |           |      |      |        |        |    |           |
-| SMOKE         |       |        |      |           |      |      |        |        |    |           |
-| PGD           |       |        |      |           |      |      |        |        |    |           |
-| MonoFlex      |       |        |      |           |      |      |        |        |    |           |
-| SA-SSD        |       |        |      |           |      |      |        |        |    |           |
-| FCAF3D        |       |        |      |           |      |      |        |        |    |           |
-| PV-RCNN       |       |        |      |           |      |      |        |        |    |           |
+|               | ResNet | PointNet++ | SECOND | DGCNN | RegNetX | DLA | MinkResNet | Cylinder3D | MinkUNet |
+| :-----------: | :----: | :--------: | :----: | :---: | :-----: | :-: | :--------: | :--------: | :------: |
+|    SECOND     |       |           |       |      |        |    |           |           |         |
+| PointPillars  |       |           |       |      |        |    |           |           |         |
+|  FreeAnchor   |       |           |       |      |        |    |           |           |         |
+|    VoteNet    |       |           |       |      |        |    |           |           |         |
+|    H3DNet     |       |           |       |      |        |    |           |           |         |
+|     3DSSD     |       |           |       |      |        |    |           |           |         |
+|    Part-A2    |       |           |       |      |        |    |           |           |         |
+|    MVXNet     |       |           |       |      |        |    |           |           |         |
+|  CenterPoint  |       |           |       |      |        |    |           |           |         |
+|      SSN      |       |           |       |      |        |    |           |           |         |
+|   ImVoteNet   |       |           |       |      |        |    |           |           |         |
+|    FCOS3D     |       |           |       |      |        |    |           |           |         |
+|  PointNet++   |       |           |       |      |        |    |           |           |         |
+| Group-Free-3D |       |           |       |      |        |    |           |           |         |
+|  ImVoxelNet   |       |           |       |      |        |    |           |           |         |
+|    PAConv     |       |           |       |      |        |    |           |           |         |
+|     DGCNN     |       |           |       |      |        |    |           |           |         |
+|     SMOKE     |       |           |       |      |        |    |           |           |         |
+|      PGD      |       |           |       |      |        |    |           |           |         |
+|   MonoFlex    |       |           |       |      |        |    |           |           |         |
+|    SA-SSD     |       |           |       |      |        |    |           |           |         |
+|    FCAF3D     |       |           |       |      |        |    |           |           |         |
+|    PV-RCNN    |       |           |       |      |        |    |           |           |         |
+|  Cylinder3D   |       |           |       |      |        |    |           |           |         |
+|   MinkUNet    |       |           |       |      |        |    |           |           |         |
+|    SPVCNN     |       |           |       |      |        |    |           |           |         |
 
 **Note:** All the about **300+ models, methods of 40+ papers** in 2D detection supported by [MMDetection](https://github.com/open-mmlab/mmdetection/blob/3.x/docs/en/model_zoo.md) can be trained or used in this codebase.
 
 ## Installation
 
-Please refer to [getting_started.md](docs/en/getting_started.md) for installation.
+Please refer to [get_started.md](docs/en/get_started.md) for installation.
 
 ## Get Started
 
-Please see [getting_started.md](docs/en/getting_started.md) for the basic usage of MMDetection3D. We provide guidance for quick run [with existing dataset](docs/en/user_guides/train_test.md) and [with new dataset](docs/en/user_guides/2_new_data_model.md) for beginners. There are also tutorials for [learning configuration systems](docs/en/user_guides/config.md), [customizing dataset](docs/en/advanced_guides/customize_dataset.md), [designing data pipeline](docs/en/user_guides/data_pipeline.md), [customizing models](docs/en/advanced_guides/customize_models.md), [customizing runtime settings](docs/en/advanced_guides/customize_runtime.md) and [Waymo dataset](docs/en/advanced_guides/datasets/waymo_det.md).
+Please see [get_started.md](docs/en/get_started.md) for the basic usage of MMDetection3D. We provide guidance for quick run [with existing dataset](docs/en/user_guides/train_test.md) and [with new dataset](docs/en/user_guides/2_new_data_model.md) for beginners. There are also tutorials for [learning configuration systems](docs/en/user_guides/config.md), [customizing dataset](docs/en/advanced_guides/customize_dataset.md), [designing data pipeline](docs/en/user_guides/data_pipeline.md), [customizing models](docs/en/advanced_guides/customize_models.md), [customizing runtime settings](docs/en/advanced_guides/customize_runtime.md) and [Waymo dataset](docs/en/advanced_guides/datasets/waymo_det.md).
 
 Please refer to [FAQ](docs/en/notes/faq.md) for frequently asked questions. When updating the version of MMDetection3D, please also check the [compatibility doc](docs/en/notes/compatibility.md) to be aware of the BC-breaking updates introduced in each version.
 
 ## Citation
 
 If you find this project useful in your research, please consider cite:
 
@@ -253,38 +293,38 @@
     howpublished = {\url{https://github.com/open-mmlab/mmdetection3d}},
     year={2020}
 }
 ```
 
 ## Contributing
 
-We appreciate all contributions to improve MMDetection3D. Please refer to [CONTRIBUTING.md](.github/CONTRIBUTING.md) for the contributing guideline.
+We appreciate all contributions to improve MMDetection3D. Please refer to [CONTRIBUTING.md](./docs/en/notes/contribution_guides.md) for the contributing guideline.
 
 ## Acknowledgement
 
 MMDetection3D is an open source project that is contributed by researchers and engineers from various colleges and companies. We appreciate all the contributors as well as users who give valuable feedbacks.
 We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new 3D detectors.
 
 ## Projects in OpenMMLab
 
 - [MMEngine](https://github.com/open-mmlab/mmengine): OpenMMLab foundational library for training deep learning models.
 - [MMCV](https://github.com/open-mmlab/mmcv): OpenMMLab foundational library for computer vision.
 - [MMEval](https://github.com/open-mmlab/mmeval): A unified evaluation library for multiple machine learning libraries.
 - [MIM](https://github.com/open-mmlab/mim): MIM installs OpenMMLab packages.
-- [MMClassification](https://github.com/open-mmlab/mmclassification): OpenMMLab image classification toolbox and benchmark.
+- [MMPreTrain](https://github.com/open-mmlab/mmpretrain): OpenMMLab pre-training toolbox and benchmark.
 - [MMDetection](https://github.com/open-mmlab/mmdetection): OpenMMLab detection toolbox and benchmark.
 - [MMDetection3D](https://github.com/open-mmlab/mmdetection3d): OpenMMLab's next-generation platform for general 3D object detection.
 - [MMRotate](https://github.com/open-mmlab/mmrotate): OpenMMLab rotated object detection toolbox and benchmark.
 - [MMYOLO](https://github.com/open-mmlab/mmyolo): OpenMMLab YOLO series toolbox and benchmark.
 - [MMSegmentation](https://github.com/open-mmlab/mmsegmentation): OpenMMLab semantic segmentation toolbox and benchmark.
 - [MMOCR](https://github.com/open-mmlab/mmocr): OpenMMLab text detection, recognition, and understanding toolbox.
 - [MMPose](https://github.com/open-mmlab/mmpose): OpenMMLab pose estimation toolbox and benchmark.
 - [MMHuman3D](https://github.com/open-mmlab/mmhuman3d): OpenMMLab 3D human parametric model toolbox and benchmark.
 - [MMSelfSup](https://github.com/open-mmlab/mmselfsup): OpenMMLab self-supervised learning toolbox and benchmark.
 - [MMRazor](https://github.com/open-mmlab/mmrazor): OpenMMLab model compression toolbox and benchmark.
 - [MMFewShot](https://github.com/open-mmlab/mmfewshot): OpenMMLab fewshot learning toolbox and benchmark.
 - [MMAction2](https://github.com/open-mmlab/mmaction2): OpenMMLab's next-generation action understanding toolbox and benchmark.
 - [MMTracking](https://github.com/open-mmlab/mmtracking): OpenMMLab video perception toolbox and benchmark.
 - [MMFlow](https://github.com/open-mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark.
-- [MMEditing](https://github.com/open-mmlab/mmediting): OpenMMLab image and video editing toolbox.
+- [MMagic](https://github.com/open-mmlab/mmagic): Open**MM**Lab **A**dvanced, **G**enerative and **I**ntelligent **C**reation toolbox.
 - [MMGeneration](https://github.com/open-mmlab/mmgeneration): OpenMMLab image and video generative models toolbox.
 - [MMDeploy](https://github.com/open-mmlab/mmdeploy): OpenMMLab model deployment framework.
```

#### html2text {}

```diff
@@ -1,176 +1,186 @@
                          [resources/mmdet3d-logo.png]
                                        
            OpenMMLab website HOT  OpenMMLab platform TRY_IT_OUT
                                        
-[![docs](https://img.shields.io/badge/docs-latest-blue)](https://
-mmdetection3d.readthedocs.io/en/1.1/) [![badge](https://github.com/open-mmlab/
-mmdetection3d/workflows/build/badge.svg)](https://github.com/open-mmlab/
-mmdetection3d/actions) [![codecov](https://codecov.io/gh/open-mmlab/
+       [![docs](https://img.shields.io/badge/docs-latest-blue)](https://
+  mmdetection3d.readthedocs.io/en/latest/) [![badge](https://github.com/open-
+mmlab/mmdetection3d/workflows/build/badge.svg)](https://github.com/open-mmlab/
+     mmdetection3d/actions) [![codecov](https://codecov.io/gh/open-mmlab/
 mmdetection3d/branch/master/graph/badge.svg)](https://codecov.io/gh/open-mmlab/
-mmdetection3d) [![license](https://img.shields.io/github/license/open-mmlab/
-mmdetection3d.svg)](https://github.com/open-mmlab/mmdetection3d/blob/master/
-LICENSE) **News**: **v1.1.0rc3** was released in 7/1/2023 The compatibilities
-of models are broken due to the unification and simplification of coordinate
-systems after v1.0.0rc0. For now, most models are benchmarked with similar
-performance, though few models are still being benchmarked. In the following
-release, we will update all the model checkpoints and benchmarks. See more
-details in the [Changelog](docs/en/notes/changelog.md) and [Changelog-v1.0.x]
-(docs/en/notes/changelog_v1.0.x.md). Documentation: https://
-mmdetection3d.readthedocs.io/ ## Introduction English | []
-(README_zh-CN.md) The master branch works with **PyTorch 1.6+**. MMDetection3D
-is an open source object detection toolbox based on PyTorch, towards the next-
-generation platform for general 3D detection. It is a part of the OpenMMLab
-project developed by [MMLab](http://mmlab.ie.cuhk.edu.hk/). ![demo image]
-(resources/mmdet3d_outdoor_demo.gif) ### Major features - **Support multi-
-modality/single-modality detectors out of box** It directly supports multi-
-modality/single-modality detectors including MVXNet, VoteNet, PointPillars,
-etc. - **Support indoor/outdoor 3D detection out of box** It directly supports
-popular indoor and outdoor 3D detection datasets, including ScanNet, SUNRGB-D,
-Waymo, nuScenes, Lyft, and KITTI. For nuScenes dataset, we also support
-[nuImages dataset](https://github.com/open-mmlab/mmdetection3d/tree/1.1/
-configs/nuimages). - **Natural integration with 2D detection** All the about
-**300+ models, methods of 40+ papers**, and modules supported in [MMDetection]
-(https://github.com/open-mmlab/mmdetection/blob/3.x/docs/en/model_zoo.md) can
-be trained or used in this codebase. - **High efficiency** It trains faster
-than other codebases. The main results are as below. Details can be found in
-[benchmark.md](./docs/en/notes/benchmarks.md). We compare the number of samples
-trained per second (the higher, the better). The models that are not supported
-by other codebases are marked by ``. | Methods | MMDetection3D | [OpenPCDet]
-(https://github.com/open-mmlab/OpenPCDet) | [votenet](https://github.com/
-facebookresearch/votenet) | [Det3D](https://github.com/poodarchu/Det3D) | | :--
----------------: | :-----------: | :-------------------------------------------
--------: | :----------------------------------------------------: | :----------
--------------------------------: | | VoteNet | 358 |  | 77 |  | |
-PointPillars-car | 141 |  |  | 140 | | PointPillars-3class | 107 | 44 | 
-|  | | SECOND | 40 | 30 |  |  | | Part-A2 | 17 | 14 |  |  | Like
-[MMDetection](https://github.com/open-mmlab/mmdetection) and [MMCV](https://
-github.com/open-mmlab/mmcv), MMDetection3D can also be used as a library to
-support different projects on top of it. ## License This project is released
-under the [Apache 2.0 license](LICENSE). ## Changelog **1.1.0rc3** was released
-in 7/1/2023. Please refer to [changelog.md](docs/en/notes/changelog.md) for
-details and release history. ## Benchmark and model zoo Results and models are
-available in the [model zoo](docs/en/model_zoo.md).
+ mmdetection3d) [![license](https://img.shields.io/github/license/open-mmlab/
+ mmdetection3d.svg)](https://github.com/open-mmlab/mmdetection3d/blob/master/
+                                   LICENSE)
+
+**News**: **We have renamed the branch `1.1` to `main` and switched the default
+branch from `master` to `main`. We encourage users to migrate to the latest
+version, though it comes with some cost. Please refer to [Migration Guide]
+(docs/en/migration.md) for more details.** **v1.1.1** was released in 30/5/2023
+We have constructed a comprehensive LiDAR semantic segmentation benchmark on
+SemanticKITTI, including Cylinder3D, MinkUNet and SPVCNN methods. Noteworthy,
+the improved MinkUNetv2 can achieve 70.3 mIoU on the validation set of
+SemanticKITTI. We have also supported the training of BEVFusion and an
+occupancy prediction method, TPVFomrer, in our `projects`. More new features
+about 3D perception are on the way. Please stay tuned! ## Introduction English
+| [](README_zh-CN.md) The main branch works with **PyTorch 1.8+**.
+MMDetection3D is an open source object detection toolbox based on PyTorch,
+towards the next-generation platform for general 3D detection. It is a part of
+the OpenMMLab project developed by [MMLab](http://mmlab.ie.cuhk.edu.hk/). !
+[demo image](resources/mmdet3d_outdoor_demo.gif) ### Major features - **Support
+multi-modality/single-modality detectors out of box** It directly supports
+multi-modality/single-modality detectors including MVXNet, VoteNet,
+PointPillars, etc. - **Support indoor/outdoor 3D detection out of box** It
+directly supports popular indoor and outdoor 3D detection datasets, including
+ScanNet, SUNRGB-D, Waymo, nuScenes, Lyft, and KITTI. For nuScenes dataset, we
+also support [nuImages dataset](https://github.com/open-mmlab/mmdetection3d/
+tree/main/configs/nuimages). - **Natural integration with 2D detection** All
+the about **300+ models, methods of 40+ papers**, and modules supported in
+[MMDetection](https://github.com/open-mmlab/mmdetection/blob/3.x/docs/en/
+model_zoo.md) can be trained or used in this codebase. - **High efficiency** It
+trains faster than other codebases. The main results are as below. Details can
+be found in [benchmark.md](./docs/en/notes/benchmarks.md). We compare the
+number of samples trained per second (the higher, the better). The models that
+are not supported by other codebases are marked by ``. | Methods |
+MMDetection3D | [OpenPCDet](https://github.com/open-mmlab/OpenPCDet) |
+[votenet](https://github.com/facebookresearch/votenet) | [Det3D](https://
+github.com/poodarchu/Det3D) | | :-----------------: | :-----------: | :--------
+------------------------------------------: | :--------------------------------
+--------------------: | :-----------------------------------------: | | VoteNet
+| 358 |  | 77 |  | | PointPillars-car | 141 |  |  | 140 | |
+PointPillars-3class | 107 | 44 |  |  | | SECOND | 40 | 30 |  |  | |
+Part-A2 | 17 | 14 |  |  | Like [MMDetection](https://github.com/open-
+mmlab/mmdetection) and [MMCV](https://github.com/open-mmlab/mmcv),
+MMDetection3D can also be used as a library to support different projects on
+top of it. ## License This project is released under the [Apache 2.0 license]
+(LICENSE). ## Changelog **1.1.0** was released in 6/4/2023. Please refer to
+[changelog.md](docs/en/notes/changelog.md) for details and release history. ##
+Benchmark and model zoo Results and models are available in the [model zoo]
+(docs/en/model_zoo.md).
                                   Components
-              Backbones                 Heads                   Features
-      * PointNet_(CVPR'2017)     * FreeAnchor_        * Dynamic_Voxelization_
-      * PointNet++_                (NeurIPS'2019)       (CoRL'2019)
-        (NeurIPS'2017)
-      * RegNet_(CVPR'2020)
-      * DGCNN_(TOG'2019)
-      * DLA (CVPR'2018)
+               Backbones                 Heads                   Features
+     * PointNet_(CVPR'2017)       * FreeAnchor_        * Dynamic_Voxelization_
+     * PointNet++_                  (NeurIPS'2019)       (CoRL'2019)
+       (NeurIPS'2017)
+     * RegNet_(CVPR'2020)
+     * DGCNN_(TOG'2019)
+     * DLA (CVPR'2018)
+     * MinkResNet (CVPR'2019)
+     * MinkUNet_(CVPR'2019)
+     * Cylinder3D_(CVPR'2021)
                                  Architectures
 3D Object Detection  Monocular 3D Object  Multi-modal 3D         3D Semantic
                           Detection      Object Detection        Segmentation
-Outdoor              Outdoor             Outdoor           Indoor
-    * SECOND_            * ImVoxelNet_       * MVXNet_         * PointNet++_
-      (Sensor'2018)        (WACV'2022)         (ICRA'2019)       (NeurIPS'2017)
-    * PointPillars_      * SMOKE_        Indoor                * PAConv_
-      (CVPR'2019)          (CVPRW'2020)      * ImVoteNet_        (CVPR'2021)
-    * SSN_               * FCOS3D_             (CVPR'2020)     * DGCNN_
-      (ECCV'2020)          (ICCVW'2021)                          (TOG'2019)
-    * 3DSSD_             * PGD_
-      (CVPR'2020)          (CoRL'2021)
-    * SA-SSD_            * MonoFlex_
-      (CVPR'2020)          (CVPR'2021)
-    * PointRCNN_
-      (CVPR'2019)
-    * Part-A2_
+Outdoor              Outdoor             Outdoor           Outdoor
+    * SECOND_            * ImVoxelNet_       * MVXNet_         * MinkUNet_
+      (Sensor'2018)        (WACV'2022)         (ICRA'2019)       (CVPR'2019)
+    * PointPillars_      * SMOKE_        Indoor                * SPVCNN_
+      (CVPR'2019)          (CVPRW'2020)      * ImVoteNet_        (ECCV'2020)
+    * SSN_               * FCOS3D_             (CVPR'2020)     * Cylinder3D_
+      (ECCV'2020)          (ICCVW'2021)                          (CVPR'2021)
+    * 3DSSD_             * PGD_                            Indoor
+      (CVPR'2020)          (CoRL'2021)                         * PointNet++_
+    * SA-SSD_            * MonoFlex_                             (NeurIPS'2017)
+      (CVPR'2020)          (CVPR'2021)                         * PAConv_
+    * PointRCNN_     Indoor                                      (CVPR'2021)
+      (CVPR'2019)        * ImVoxelNet_                         * DGCNN_
+    * Part-A2_             (WACV'2022)                           (TOG'2019)
       (TPAMI'2020)
     * CenterPoint_
       (CVPR'2021)
     * PV-RCNN_
       (CVPR'2020)
 Indoor
     * VoteNet_
       (ICCV'2019)
     * H3DNet_
       (ECCV'2020)
     * Group-Free-3D_
       (ICCV'2021)
     * FCAF3D_
       (ECCV'2022)
-| | ResNet | ResNeXt | SENet | PointNet++ | DGCNN | HRNet | RegNetX | Res2Net |
-DLA | MinkResNet | | ------------- | :----: | :-----: | :---: | :--------: | :-
---: | :---: | :-----: | :-----: | :-: | :--------: | | SECOND |  |  | 
-|  |  |  |  |  |  |  | | PointPillars |  |  |  |
- |  |  |  |  |  |  | | FreeAnchor |  |  |  | 
-|  |  |  |  |  |  | | VoteNet |  |  |  |  |  |
- |  |  |  |  | | H3DNet |  |  |  |  |  |  |
- |  |  |  | | 3DSSD |  |  |  |  |  |  |  | 
-|  |  | | Part-A2 |  |  |  |  |  |  |  |  |  |
- | | MVXNet |  |  |  |  |  |  |  |  |  |  | |
-CenterPoint |  |  |  |  |  |  |  |  |  |  | | SSN
-|  |  |  |  |  |  |  |  |  |  | | ImVoteNet | 
-|  |  |  |  |  |  |  |  |  | | FCOS3D |  |  |
- |  |  |  |  |  |  |  | | PointNet++ |  |  | 
-|  |  |  |  |  |  |  | | Group-Free-3D |  |  |  |
- |  |  |  |  |  |  | | ImVoxelNet |  |  |  | 
-|  |  |  |  |  |  | | PAConv |  |  |  |  |  |
- |  |  |  |  | | DGCNN |  |  |  |  |  |  | 
-|  |  |  | | SMOKE |  |  |  |  |  |  |  |  |
- |  | | PGD |  |  |  |  |  |  |  |  |  |  |
-| MonoFlex |  |  |  |  |  |  |  |  |  |  | | SA-
-SSD |  |  |  |  |  |  |  |  |  |  | | FCAF3D |
- |  |  |  |  |  |  |  |  |  | | PV-RCNN |  |
- |  |  |  |  |  |  |  |  | **Note:** All the about
+| | ResNet | PointNet++ | SECOND | DGCNN | RegNetX | DLA | MinkResNet |
+Cylinder3D | MinkUNet | | :-----------: | :----: | :--------: | :----: | :---:
+| :-----: | :-: | :--------: | :--------: | :------: | | SECOND |  |  |
+ |  |  |  |  |  |  | | PointPillars |  |  |  |
+ |  |  |  |  |  | | FreeAnchor |  |  |  |  | 
+|  |  |  |  | | VoteNet |  |  |  |  |  |  |  |
+ |  | | H3DNet |  |  |  |  |  |  |  |  |  | |
+3DSSD |  |  |  |  |  |  |  |  |  | | Part-A2 |  |
+ |  |  |  |  |  |  |  | | MVXNet |  |  |  |
+ |  |  |  |  |  | | CenterPoint |  |  |  |  | 
+|  |  |  |  | | SSN |  |  |  |  |  |  |  | 
+|  | | ImVoteNet |  |  |  |  |  |  |  |  |  | |
+FCOS3D |  |  |  |  |  |  |  |  |  | | PointNet++ |
+ |  |  |  |  |  |  |  |  | | Group-Free-3D |  |
+ |  |  |  |  |  |  |  | | ImVoxelNet |  |  | 
+|  |  |  |  |  |  | | PAConv |  |  |  |  |  |
+ |  |  |  | | DGCNN |  |  |  |  |  |  |  | 
+|  | | SMOKE |  |  |  |  |  |  |  |  |  | | PGD |
+ |  |  |  |  |  |  |  |  | | MonoFlex |  |  |
+ |  |  |  |  |  |  | | SA-SSD |  |  |  |  |
+ |  |  |  |  | | FCAF3D |  |  |  |  |  |  |
+ |  |  | | PV-RCNN |  |  |  |  |  |  |  |  |
+ | | Cylinder3D |  |  |  |  |  |  |  |  |  | |
+MinkUNet |  |  |  |  |  |  |  |  |  | | SPVCNN | 
+|  |  |  |  |  |  |  |  | **Note:** All the about
 **300+ models, methods of 40+ papers** in 2D detection supported by
 [MMDetection](https://github.com/open-mmlab/mmdetection/blob/3.x/docs/en/
 model_zoo.md) can be trained or used in this codebase. ## Installation Please
-refer to [getting_started.md](docs/en/getting_started.md) for installation. ##
-Get Started Please see [getting_started.md](docs/en/getting_started.md) for the
-basic usage of MMDetection3D. We provide guidance for quick run [with existing
-dataset](docs/en/user_guides/train_test.md) and [with new dataset](docs/en/
-user_guides/2_new_data_model.md) for beginners. There are also tutorials for
-[learning configuration systems](docs/en/user_guides/config.md), [customizing
-dataset](docs/en/advanced_guides/customize_dataset.md), [designing data
-pipeline](docs/en/user_guides/data_pipeline.md), [customizing models](docs/en/
-advanced_guides/customize_models.md), [customizing runtime settings](docs/en/
-advanced_guides/customize_runtime.md) and [Waymo dataset](docs/en/
-advanced_guides/datasets/waymo_det.md). Please refer to [FAQ](docs/en/notes/
-faq.md) for frequently asked questions. When updating the version of
-MMDetection3D, please also check the [compatibility doc](docs/en/notes/
-compatibility.md) to be aware of the BC-breaking updates introduced in each
-version. ## Citation If you find this project useful in your research, please
-consider cite: ```latex @misc{mmdet3d2020, title={{MMDetection3D: OpenMMLab}
-next-generation platform for general {3D} object detection}, author=
-{MMDetection3D Contributors}, howpublished = {\url{https://github.com/open-
-mmlab/mmdetection3d}}, year={2020} } ``` ## Contributing We appreciate all
-contributions to improve MMDetection3D. Please refer to [CONTRIBUTING.md]
-(.github/CONTRIBUTING.md) for the contributing guideline. ## Acknowledgement
-MMDetection3D is an open source project that is contributed by researchers and
-engineers from various colleges and companies. We appreciate all the
-contributors as well as users who give valuable feedbacks. We wish that the
-toolbox and benchmark could serve the growing research community by providing a
-flexible toolkit to reimplement existing methods and develop their own new 3D
-detectors. ## Projects in OpenMMLab - [MMEngine](https://github.com/open-mmlab/
-mmengine): OpenMMLab foundational library for training deep learning models. -
-[MMCV](https://github.com/open-mmlab/mmcv): OpenMMLab foundational library for
-computer vision. - [MMEval](https://github.com/open-mmlab/mmeval): A unified
-evaluation library for multiple machine learning libraries. - [MIM](https://
-github.com/open-mmlab/mim): MIM installs OpenMMLab packages. -
-[MMClassification](https://github.com/open-mmlab/mmclassification): OpenMMLab
-image classification toolbox and benchmark. - [MMDetection](https://github.com/
-open-mmlab/mmdetection): OpenMMLab detection toolbox and benchmark. -
-[MMDetection3D](https://github.com/open-mmlab/mmdetection3d): OpenMMLab's next-
-generation platform for general 3D object detection. - [MMRotate](https://
-github.com/open-mmlab/mmrotate): OpenMMLab rotated object detection toolbox and
-benchmark. - [MMYOLO](https://github.com/open-mmlab/mmyolo): OpenMMLab YOLO
-series toolbox and benchmark. - [MMSegmentation](https://github.com/open-mmlab/
-mmsegmentation): OpenMMLab semantic segmentation toolbox and benchmark. -
-[MMOCR](https://github.com/open-mmlab/mmocr): OpenMMLab text detection,
-recognition, and understanding toolbox. - [MMPose](https://github.com/open-
-mmlab/mmpose): OpenMMLab pose estimation toolbox and benchmark. - [MMHuman3D]
-(https://github.com/open-mmlab/mmhuman3d): OpenMMLab 3D human parametric model
-toolbox and benchmark. - [MMSelfSup](https://github.com/open-mmlab/mmselfsup):
+refer to [get_started.md](docs/en/get_started.md) for installation. ## Get
+Started Please see [get_started.md](docs/en/get_started.md) for the basic usage
+of MMDetection3D. We provide guidance for quick run [with existing dataset]
+(docs/en/user_guides/train_test.md) and [with new dataset](docs/en/user_guides/
+2_new_data_model.md) for beginners. There are also tutorials for [learning
+configuration systems](docs/en/user_guides/config.md), [customizing dataset]
+(docs/en/advanced_guides/customize_dataset.md), [designing data pipeline](docs/
+en/user_guides/data_pipeline.md), [customizing models](docs/en/advanced_guides/
+customize_models.md), [customizing runtime settings](docs/en/advanced_guides/
+customize_runtime.md) and [Waymo dataset](docs/en/advanced_guides/datasets/
+waymo_det.md). Please refer to [FAQ](docs/en/notes/faq.md) for frequently asked
+questions. When updating the version of MMDetection3D, please also check the
+[compatibility doc](docs/en/notes/compatibility.md) to be aware of the BC-
+breaking updates introduced in each version. ## Citation If you find this
+project useful in your research, please consider cite: ```latex @misc
+{mmdet3d2020, title={{MMDetection3D: OpenMMLab} next-generation platform for
+general {3D} object detection}, author={MMDetection3D Contributors},
+howpublished = {\url{https://github.com/open-mmlab/mmdetection3d}}, year={2020}
+} ``` ## Contributing We appreciate all contributions to improve MMDetection3D.
+Please refer to [CONTRIBUTING.md](./docs/en/notes/contribution_guides.md) for
+the contributing guideline. ## Acknowledgement MMDetection3D is an open source
+project that is contributed by researchers and engineers from various colleges
+and companies. We appreciate all the contributors as well as users who give
+valuable feedbacks. We wish that the toolbox and benchmark could serve the
+growing research community by providing a flexible toolkit to reimplement
+existing methods and develop their own new 3D detectors. ## Projects in
+OpenMMLab - [MMEngine](https://github.com/open-mmlab/mmengine): OpenMMLab
+foundational library for training deep learning models. - [MMCV](https://
+github.com/open-mmlab/mmcv): OpenMMLab foundational library for computer
+vision. - [MMEval](https://github.com/open-mmlab/mmeval): A unified evaluation
+library for multiple machine learning libraries. - [MIM](https://github.com/
+open-mmlab/mim): MIM installs OpenMMLab packages. - [MMPreTrain](https://
+github.com/open-mmlab/mmpretrain): OpenMMLab pre-training toolbox and
+benchmark. - [MMDetection](https://github.com/open-mmlab/mmdetection):
+OpenMMLab detection toolbox and benchmark. - [MMDetection3D](https://
+github.com/open-mmlab/mmdetection3d): OpenMMLab's next-generation platform for
+general 3D object detection. - [MMRotate](https://github.com/open-mmlab/
+mmrotate): OpenMMLab rotated object detection toolbox and benchmark. - [MMYOLO]
+(https://github.com/open-mmlab/mmyolo): OpenMMLab YOLO series toolbox and
+benchmark. - [MMSegmentation](https://github.com/open-mmlab/mmsegmentation):
+OpenMMLab semantic segmentation toolbox and benchmark. - [MMOCR](https://
+github.com/open-mmlab/mmocr): OpenMMLab text detection, recognition, and
+understanding toolbox. - [MMPose](https://github.com/open-mmlab/mmpose):
+OpenMMLab pose estimation toolbox and benchmark. - [MMHuman3D](https://
+github.com/open-mmlab/mmhuman3d): OpenMMLab 3D human parametric model toolbox
+and benchmark. - [MMSelfSup](https://github.com/open-mmlab/mmselfsup):
 OpenMMLab self-supervised learning toolbox and benchmark. - [MMRazor](https://
 github.com/open-mmlab/mmrazor): OpenMMLab model compression toolbox and
 benchmark. - [MMFewShot](https://github.com/open-mmlab/mmfewshot): OpenMMLab
 fewshot learning toolbox and benchmark. - [MMAction2](https://github.com/open-
 mmlab/mmaction2): OpenMMLab's next-generation action understanding toolbox and
 benchmark. - [MMTracking](https://github.com/open-mmlab/mmtracking): OpenMMLab
 video perception toolbox and benchmark. - [MMFlow](https://github.com/open-
-mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark. - [MMEditing]
-(https://github.com/open-mmlab/mmediting): OpenMMLab image and video editing
-toolbox. - [MMGeneration](https://github.com/open-mmlab/mmgeneration):
-OpenMMLab image and video generative models toolbox. - [MMDeploy](https://
-github.com/open-mmlab/mmdeploy): OpenMMLab model deployment framework.
+mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark. - [MMagic](https:/
+/github.com/open-mmlab/mmagic): Open**MM**Lab **A**dvanced, **G**enerative and
+**I**ntelligent **C**reation toolbox. - [MMGeneration](https://github.com/open-
+mmlab/mmgeneration): OpenMMLab image and video generative models toolbox. -
+[MMDeploy](https://github.com/open-mmlab/mmdeploy): OpenMMLab model deployment
+framework.
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/3dssd/3dssd_4xb4_kitti-3d-car.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_8xb6-160e_kitti-3d-car.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,109 +1,101 @@
-_base_ = [
-    '../_base_/models/3dssd.py', '../_base_/datasets/kitti-3d-car.py',
-    '../_base_/default_runtime.py'
-]
-
+# model settings
+_base_ = './pointpillars_hv_secfpn_8xb6-160e_kitti-3d-3class.py'
 # dataset settings
 dataset_type = 'KittiDataset'
 data_root = 'data/kitti/'
 class_names = ['Car']
-point_cloud_range = [0, -40, -5, 70, 40, 3]
-input_modality = dict(use_lidar=True, use_camera=False)
+metainfo = dict(classes=class_names)
+backend_args = None
+
+point_cloud_range = [0, -39.68, -3, 69.12, 39.68, 1]
+
+model = dict(
+    bbox_head=dict(
+        type='Anchor3DHead',
+        num_classes=1,
+        anchor_generator=dict(
+            _delete_=True,
+            type='AlignedAnchor3DRangeGenerator',
+            ranges=[[0, -39.68, -1.78, 69.12, 39.68, -1.78]],
+            sizes=[[3.9, 1.6, 1.56]],
+            rotations=[0, 1.57],
+            reshape_out=True)),
+    # model training and testing settings
+    train_cfg=dict(
+        _delete_=True,
+        assigner=dict(
+            type='Max3DIoUAssigner',
+            iou_calculator=dict(type='BboxOverlapsNearest3D'),
+            pos_iou_thr=0.6,
+            neg_iou_thr=0.45,
+            min_pos_iou=0.45,
+            ignore_iof_thr=-1),
+        allowed_border=0,
+        pos_weight=-1,
+        debug=False))
+
 db_sampler = dict(
     data_root=data_root,
     info_path=data_root + 'kitti_dbinfos_train.pkl',
     rate=1.0,
     prepare=dict(filter_by_difficulty=[-1], filter_by_min_points=dict(Car=5)),
     classes=class_names,
     sample_groups=dict(Car=15),
     points_loader=dict(
-        type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4))
-
-file_client_args = dict(backend='disk')
-# Uncomment the following if use ceph or other file clients.
-# See https://mmcv.readthedocs.io/en/latest/api.html#mmcv.fileio.FileClient
-# for more details.
-# file_client_args = dict(
-#     backend='petrel', path_mapping=dict(data='s3://kitti_data/'))
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
+    backend_args=backend_args)
 
 train_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
     dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
-    dict(type='PointsRangeFilter', point_cloud_range=point_cloud_range),
-    dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
-    dict(type='ObjectSample', db_sampler=db_sampler),
+    dict(type='ObjectSample', db_sampler=db_sampler, use_ground_plane=True),
     dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
     dict(
-        type='ObjectNoise',
-        num_try=100,
-        translation_std=[1.0, 1.0, 0],
-        global_rot_range=[0.0, 0.0],
-        rot_range=[-1.0471975511965976, 1.0471975511965976]),
-    dict(
         type='GlobalRotScaleTrans',
         rot_range=[-0.78539816, 0.78539816],
-        scale_ratio_range=[0.9, 1.1]),
-    # 3DSSD can get a higher performance without this transform
-    # dict(type='BackgroundPointsFilter', bbox_enlarge_range=(0.5, 2.0, 0.5)),
-    dict(type='PointSample', num_points=16384),
+        scale_ratio_range=[0.95, 1.05]),
+    dict(type='PointsRangeFilter', point_cloud_range=point_cloud_range),
+    dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
+    dict(type='PointShuffle'),
     dict(
         type='Pack3DDetInputs',
-        keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
+        keys=['points', 'gt_labels_3d', 'gt_bboxes_3d'])
 ]
-
 test_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
             dict(
                 type='GlobalRotScaleTrans',
                 rot_range=[0, 0],
                 scale_ratio_range=[1., 1.],
                 translation_std=[0, 0, 0]),
             dict(type='RandomFlip3D'),
             dict(
-                type='PointsRangeFilter', point_cloud_range=point_cloud_range),
-            dict(type='PointSample', num_points=16384),
+                type='PointsRangeFilter', point_cloud_range=point_cloud_range)
         ]),
     dict(type='Pack3DDetInputs', keys=['points'])
 ]
 
 train_dataloader = dict(
-    batch_size=4, dataset=dict(dataset=dict(pipeline=train_pipeline, )))
-test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
-val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
-
-# model settings
-model = dict(
-    bbox_head=dict(
-        num_classes=1,
-        bbox_coder=dict(
-            type='AnchorFreeBBoxCoder', num_dir_bins=12, with_rot=True)))
-
-# optimizer
-lr = 0.002  # max learning rate
-optim_wrapper = dict(
-    type='OptimWrapper',
-    optimizer=dict(type='AdamW', lr=lr, weight_decay=0.),
-    clip_grad=dict(max_norm=35, norm_type=2),
-)
-
-# training schedule for 1x
-train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=80, val_interval=2)
-val_cfg = dict(type='ValLoop')
-test_cfg = dict(type='TestLoop')
-
-# learning rate
-param_scheduler = [
-    dict(
-        type='MultiStepLR',
-        begin=0,
-        end=80,
-        by_epoch=True,
-        milestones=[45, 60],
-        gamma=0.1)
-]
+    dataset=dict(dataset=dict(pipeline=train_pipeline, metainfo=metainfo)))
+test_dataloader = dict(dataset=dict(pipeline=test_pipeline, metainfo=metainfo))
+val_dataloader = dict(dataset=dict(pipeline=test_pipeline, metainfo=metainfo))
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/3dssd/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/3dssd/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/kitti-3d-3class.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/second/second_hv_secfpn_sbn-all_16xb2-2x_waymoD5-3d-3class.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,132 +1,145 @@
-# dataset settings
-dataset_type = 'KittiDataset'
-data_root = 'data/kitti/'
-class_names = ['Pedestrian', 'Cyclist', 'Car']
-point_cloud_range = [0, -40, -3, 70.4, 40, 1]
-input_modality = dict(use_lidar=True, use_camera=False)
+_base_ = [
+    '../_base_/models/second_hv_secfpn_waymo.py',
+    '../_base_/datasets/waymoD5-3d-3class.py',
+    '../_base_/schedules/schedule-2x.py',
+    '../_base_/default_runtime.py',
+]
+
+dataset_type = 'WaymoDataset'
+data_root = 'data/waymo/kitti_format/'
+class_names = ['Car', 'Pedestrian', 'Cyclist']
 metainfo = dict(classes=class_names)
 
+point_cloud_range = [-76.8, -51.2, -2, 76.8, 51.2, 4]
+input_modality = dict(use_lidar=True, use_camera=False)
+backend_args = None
+
 db_sampler = dict(
     data_root=data_root,
-    info_path=data_root + 'kitti_dbinfos_train.pkl',
+    info_path=data_root + 'waymo_dbinfos_train.pkl',
     rate=1.0,
     prepare=dict(
         filter_by_difficulty=[-1],
-        filter_by_min_points=dict(Car=5, Pedestrian=10, Cyclist=10)),
+        filter_by_min_points=dict(Car=5, Pedestrian=5, Cyclist=5)),
     classes=class_names,
-    sample_groups=dict(Car=12, Pedestrian=6, Cyclist=6),
+    sample_groups=dict(Car=15, Pedestrian=10, Cyclist=10),
     points_loader=dict(
-        type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4))
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=6,
+        use_dim=[0, 1, 2, 3, 4],
+        backend_args=backend_args),
+    backend_args=backend_args)
 
 train_pipeline = [
     dict(
         type='LoadPointsFromFile',
         coord_type='LIDAR',
-        load_dim=4,  # x, y, z, intensity
-        use_dim=4),
+        load_dim=6,
+        use_dim=5,
+        backend_args=backend_args),
     dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
-    dict(type='ObjectSample', db_sampler=db_sampler),
+    # dict(type='ObjectSample', db_sampler=db_sampler),
     dict(
-        type='ObjectNoise',
-        num_try=100,
-        translation_std=[1.0, 1.0, 0.5],
-        global_rot_range=[0.0, 0.0],
-        rot_range=[-0.78539816, 0.78539816]),
-    dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
+        type='RandomFlip3D',
+        sync_2d=False,
+        flip_ratio_bev_horizontal=0.5,
+        flip_ratio_bev_vertical=0.5),
     dict(
         type='GlobalRotScaleTrans',
         rot_range=[-0.78539816, 0.78539816],
         scale_ratio_range=[0.95, 1.05]),
     dict(type='PointsRangeFilter', point_cloud_range=point_cloud_range),
     dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
     dict(type='PointShuffle'),
     dict(
         type='Pack3DDetInputs',
         keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
 ]
+
 test_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=6,
+        use_dim=5,
+        backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
             dict(
                 type='GlobalRotScaleTrans',
                 rot_range=[0, 0],
                 scale_ratio_range=[1., 1.],
                 translation_std=[0, 0, 0]),
             dict(type='RandomFlip3D'),
             dict(
-                type='PointsRangeFilter', point_cloud_range=point_cloud_range)
-        ]),
-    dict(type='Pack3DDetInputs', keys=['points'])
-]
-# construct a pipeline for data and gt loading in show function
-# please keep its loading function consistent with test_pipeline (e.g. client)
-eval_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4),
-    dict(type='Pack3DDetInputs', keys=['points'])
+                type='PointsRangeFilter', point_cloud_range=point_cloud_range),
+            dict(type='Pack3DDetInputs', keys=['points']),
+        ])
 ]
+
 train_dataloader = dict(
-    batch_size=6,
+    batch_size=4,
     num_workers=4,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type='RepeatDataset',
         times=2,
         dataset=dict(
             type=dataset_type,
             data_root=data_root,
-            ann_file='kitti_infos_train.pkl',
-            data_prefix=dict(pts='training/velodyne_reduced'),
+            ann_file='waymo_infos_train.pkl',
+            data_prefix=dict(pts='training/velodyne'),
             pipeline=train_pipeline,
             modality=input_modality,
             test_mode=False,
-            metainfo=metainfo,
             # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
             # and box_type_3d='Depth' in sunrgbd and scannet dataset.
-            box_type_3d='LiDAR')))
+            box_type_3d='LiDAR',
+            # load one frame every five frames
+            load_interval=5,
+            backend_args=backend_args)))
 val_dataloader = dict(
     batch_size=1,
     num_workers=1,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        data_prefix=dict(pts='training/velodyne_reduced'),
-        ann_file='kitti_infos_val.pkl',
+        data_prefix=dict(pts='training/velodyne'),
+        ann_file='waymo_infos_val.pkl',
         pipeline=test_pipeline,
         modality=input_modality,
         test_mode=True,
         metainfo=metainfo,
-        box_type_3d='LiDAR'))
+        box_type_3d='LiDAR',
+        backend_args=backend_args))
 test_dataloader = dict(
     batch_size=1,
     num_workers=1,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        data_prefix=dict(pts='training/velodyne_reduced'),
-        ann_file='kitti_infos_val.pkl',
+        data_prefix=dict(pts='training/velodyne'),
+        ann_file='waymo_infos_val.pkl',
         pipeline=test_pipeline,
         modality=input_modality,
         test_mode=True,
         metainfo=metainfo,
-        box_type_3d='LiDAR'))
-val_evaluator = dict(
-    type='KittiMetric',
-    ann_file=data_root + 'kitti_infos_val.pkl',
-    metric='bbox')
-test_evaluator = val_evaluator
-
-vis_backends = [dict(type='LocalVisBackend')]
-visualizer = dict(
-    type='Det3DLocalVisualizer', vis_backends=vis_backends, name='visualizer')
+        box_type_3d='LiDAR',
+        backend_args=backend_args))
+# Default setting for scaling LR automatically
+#   - `enable` means enable scaling LR automatically
+#       or not by default.
+#   - `base_batch_size` = (16 GPUs) x (2 samples per GPU).
+auto_scale_lr = dict(enable=False, base_batch_size=32)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/kitti-3d-car.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/s3dis-3d.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,130 +1,134 @@
 # dataset settings
-dataset_type = 'KittiDataset'
-data_root = 'data/kitti/'
-class_names = ['Car']
-point_cloud_range = [0, -40, -3, 70.4, 40, 1]
-input_modality = dict(use_lidar=True, use_camera=False)
-metainfo = dict(classes=class_names)
-
-db_sampler = dict(
-    data_root=data_root,
-    info_path=data_root + 'kitti_dbinfos_train.pkl',
-    rate=1.0,
-    prepare=dict(filter_by_difficulty=[-1], filter_by_min_points=dict(Car=5)),
-    classes=class_names,
-    sample_groups=dict(Car=15),
-    points_loader=dict(
-        type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4))
+dataset_type = 'S3DISDataset'
+data_root = 'data/s3dis/'
+
+# Example to use different file client
+# Method 1: simply set the data root and let the file I/O module
+# automatically infer from prefix (not support LMDB and Memcache yet)
+
+# data_root = 's3://openmmlab/datasets/detection3d/s3dis/'
+
+# Method 2: Use backend_args, file_client_args in versions before 1.1.0
+# backend_args = dict(
+#     backend='petrel',
+#     path_mapping=dict({
+#         './data/': 's3://openmmlab/datasets/detection3d/',
+#          'data/': 's3://openmmlab/datasets/detection3d/'
+#      }))
+backend_args = None
+
+metainfo = dict(classes=('table', 'chair', 'sofa', 'bookcase', 'board'))
+train_area = [1, 2, 3, 4, 6]
+test_area = 5
 
 train_pipeline = [
     dict(
         type='LoadPointsFromFile',
-        coord_type='LIDAR',
-        load_dim=4,  # x, y, z, intensity
-        use_dim=4),
+        coord_type='DEPTH',
+        shift_height=False,
+        use_color=True,
+        load_dim=6,
+        use_dim=[0, 1, 2, 3, 4, 5],
+        backend_args=backend_args),
     dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
-    dict(type='ObjectSample', db_sampler=db_sampler),
+    dict(type='PointSample', num_points=100000),
     dict(
-        type='ObjectNoise',
-        num_try=100,
-        translation_std=[1.0, 1.0, 0.5],
-        global_rot_range=[0.0, 0.0],
-        rot_range=[-0.78539816, 0.78539816]),
-    dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
+        type='RandomFlip3D',
+        sync_2d=False,
+        flip_ratio_bev_horizontal=0.5,
+        flip_ratio_bev_vertical=0.5),
     dict(
         type='GlobalRotScaleTrans',
-        rot_range=[-0.78539816, 0.78539816],
-        scale_ratio_range=[0.95, 1.05]),
-    dict(type='PointsRangeFilter', point_cloud_range=point_cloud_range),
-    dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
-    dict(type='PointShuffle'),
+        rot_range=[-0.087266, 0.087266],
+        scale_ratio_range=[0.9, 1.1],
+        translation_std=[.1, .1, .1],
+        shift_height=False),
+    dict(type='NormalizePointsColor', color_mean=None),
     dict(
         type='Pack3DDetInputs',
         keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
 ]
 test_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='DEPTH',
+        shift_height=False,
+        use_color=True,
+        load_dim=6,
+        use_dim=[0, 1, 2, 3, 4, 5],
+        backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
             dict(
                 type='GlobalRotScaleTrans',
                 rot_range=[0, 0],
                 scale_ratio_range=[1., 1.],
                 translation_std=[0, 0, 0]),
-            dict(type='RandomFlip3D'),
             dict(
-                type='PointsRangeFilter', point_cloud_range=point_cloud_range)
+                type='RandomFlip3D',
+                sync_2d=False,
+                flip_ratio_bev_horizontal=0.5,
+                flip_ratio_bev_vertical=0.5),
+            dict(type='PointSample', num_points=100000),
+            dict(type='NormalizePointsColor', color_mean=None),
         ]),
     dict(type='Pack3DDetInputs', keys=['points'])
 ]
-# construct a pipeline for data and gt loading in show function
-# please keep its loading function consistent with test_pipeline (e.g. client)
-eval_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4),
-    dict(type='Pack3DDetInputs', keys=['points'])
-]
+
 train_dataloader = dict(
-    batch_size=6,
+    batch_size=8,
     num_workers=4,
-    persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type='RepeatDataset',
-        times=2,
+        times=13,
         dataset=dict(
-            type=dataset_type,
-            data_root=data_root,
-            ann_file='kitti_infos_train.pkl',
-            data_prefix=dict(pts='training/velodyne_reduced'),
-            pipeline=train_pipeline,
-            modality=input_modality,
-            test_mode=False,
-            metainfo=metainfo,
-            # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
-            # and box_type_3d='Depth' in sunrgbd and scannet dataset.
-            box_type_3d='LiDAR')))
+            type='ConcatDataset',
+            datasets=[
+                dict(
+                    type=dataset_type,
+                    data_root=data_root,
+                    ann_file=f's3dis_infos_Area_{i}.pkl',
+                    pipeline=train_pipeline,
+                    filter_empty_gt=True,
+                    metainfo=metainfo,
+                    box_type_3d='Depth',
+                    backend_args=backend_args) for i in train_area
+            ])))
+
 val_dataloader = dict(
     batch_size=1,
     num_workers=1,
-    persistent_workers=True,
-    drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        data_prefix=dict(pts='training/velodyne_reduced'),
-        ann_file='kitti_infos_val.pkl',
+        ann_file=f's3dis_infos_Area_{test_area}.pkl',
         pipeline=test_pipeline,
-        modality=input_modality,
-        test_mode=True,
         metainfo=metainfo,
-        box_type_3d='LiDAR'))
+        test_mode=True,
+        box_type_3d='Depth',
+        backend_args=backend_args))
 test_dataloader = dict(
     batch_size=1,
     num_workers=1,
-    persistent_workers=True,
-    drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        data_prefix=dict(pts='training/velodyne_reduced'),
-        ann_file='kitti_infos_val.pkl',
+        ann_file=f's3dis_infos_Area_{test_area}.pkl',
         pipeline=test_pipeline,
-        modality=input_modality,
-        test_mode=True,
         metainfo=metainfo,
-        box_type_3d='LiDAR'))
-val_evaluator = dict(
-    type='KittiMetric',
-    ann_file=data_root + 'kitti_infos_val.pkl',
-    metric='bbox')
+        test_mode=True,
+        box_type_3d='Depth',
+        backend_args=backend_args))
+val_evaluator = dict(type='IndoorMetric')
 test_evaluator = val_evaluator
 
 vis_backends = [dict(type='LocalVisBackend')]
 visualizer = dict(
     type='Det3DLocalVisualizer', vis_backends=vis_backends, name='visualizer')
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/kitti-mono3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/fcaf3d/fcaf3d_2xb8_scannet-3d-18class.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,90 +1,94 @@
-dataset_type = 'KittiDataset'
-data_root = 'data/kitti/'
-class_names = ['Pedestrian', 'Cyclist', 'Car']
-input_modality = dict(use_lidar=False, use_camera=True)
-metainfo = dict(classes=class_names)
-
-file_client_args = dict(backend='disk')
-# Uncomment the following if use ceph or other file clients.
-# See https://mmcv.readthedocs.io/en/latest/api.html#mmcv.fileio.FileClient
-# for more details.
-# file_client_args = dict(
-#     backend='petrel', path_mapping=dict(data='s3://kitti_data/'))
+_base_ = [
+    '../_base_/models/fcaf3d.py', '../_base_/default_runtime.py',
+    '../_base_/datasets/scannet-3d.py'
+]
+n_points = 100000
+backend_args = None
 
 train_pipeline = [
-    dict(type='LoadImageFromFileMono3D'),
     dict(
-        type='LoadAnnotations3D',
-        with_bbox=True,
-        with_label=True,
-        with_attr_label=False,
-        with_bbox_3d=True,
-        with_label_3d=True,
-        with_bbox_depth=True),
-    dict(type='Resize', scale=(1242, 375), keep_ratio=True),
-    dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
+        type='LoadPointsFromFile',
+        coord_type='DEPTH',
+        shift_height=False,
+        use_color=True,
+        load_dim=6,
+        use_dim=[0, 1, 2, 3, 4, 5],
+        backend_args=backend_args),
+    dict(type='LoadAnnotations3D'),
+    dict(type='GlobalAlignment', rotation_axis=2),
+    dict(type='PointSample', num_points=n_points),
+    dict(
+        type='RandomFlip3D',
+        sync_2d=False,
+        flip_ratio_bev_horizontal=0.5,
+        flip_ratio_bev_vertical=0.5),
+    dict(
+        type='GlobalRotScaleTrans',
+        rot_range=[-0.087266, 0.087266],
+        scale_ratio_range=[.9, 1.1],
+        translation_std=[.1, .1, .1],
+        shift_height=False),
+    dict(type='NormalizePointsColor', color_mean=None),
     dict(
         type='Pack3DDetInputs',
-        keys=[
-            'img', 'gt_bboxes', 'gt_bboxes_labels', 'gt_bboxes_3d',
-            'gt_labels_3d', 'centers_2d', 'depths'
-        ]),
+        keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
 ]
 test_pipeline = [
-    dict(type='LoadImageFromFileMono3D'),
-    dict(type='Resize', scale=(1242, 375), keep_ratio=True),
-    dict(type='Pack3DDetInputs', keys=['img'])
-]
-eval_pipeline = [
-    dict(type='LoadImageFromFileMono3D'),
-    dict(type='Pack3DDetInputs', keys=['img'])
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='DEPTH',
+        shift_height=False,
+        use_color=True,
+        load_dim=6,
+        use_dim=[0, 1, 2, 3, 4, 5],
+        backend_args=backend_args),
+    dict(type='GlobalAlignment', rotation_axis=2),
+    dict(
+        type='MultiScaleFlipAug3D',
+        img_scale=(1333, 800),
+        pts_scale_ratio=1,
+        flip=False,
+        transforms=[
+            dict(
+                type='GlobalRotScaleTrans',
+                rot_range=[0, 0],
+                scale_ratio_range=[1., 1.],
+                translation_std=[0, 0, 0]),
+            dict(
+                type='RandomFlip3D',
+                sync_2d=False,
+                flip_ratio_bev_horizontal=0.5,
+                flip_ratio_bev_vertical=0.5),
+            dict(type='PointSample', num_points=n_points),
+            dict(type='NormalizePointsColor', color_mean=None),
+        ]),
+    dict(type='Pack3DDetInputs', keys=['points'])
 ]
-
 train_dataloader = dict(
-    batch_size=2,
-    num_workers=2,
-    persistent_workers=True,
-    sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
-        type=dataset_type,
-        data_root=data_root,
-        ann_file='kitti_infos_train.pkl',
-        data_prefix=dict(img='training/image_2'),
-        pipeline=train_pipeline,
-        modality=input_modality,
-        load_type='fov_image_based',
-        test_mode=False,
-        metainfo=metainfo,
-        # we use box_type_3d='Camera' in monocular 3d
-        # detection task
-        box_type_3d='Camera'))
-val_dataloader = dict(
-    batch_size=1,
-    num_workers=2,
-    persistent_workers=True,
-    drop_last=False,
-    sampler=dict(type='DefaultSampler', shuffle=False),
-    dataset=dict(
-        type=dataset_type,
-        data_root=data_root,
-        data_prefix=dict(img='training/image_2'),
-        ann_file='kitti_infos_val.pkl',
-        pipeline=test_pipeline,
-        modality=input_modality,
-        load_type='fov_image_based',
-        metainfo=metainfo,
-        test_mode=True,
-        box_type_3d='Camera'))
+        type='RepeatDataset',
+        times=10,
+        dataset=dict(pipeline=train_pipeline, filter_empty_gt=True)))
+val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
 test_dataloader = val_dataloader
 
-val_evaluator = dict(
-    type='KittiMetric',
-    ann_file=data_root + 'kitti_infos_val.pkl',
-    metric='bbox',
-    pred_box_type_3d='Camera')
-
-test_evaluator = val_evaluator
-
-vis_backends = [dict(type='LocalVisBackend')]
-visualizer = dict(
-    type='Det3DLocalVisualizer', vis_backends=vis_backends, name='visualizer')
+optim_wrapper = dict(
+    type='OptimWrapper',
+    optimizer=dict(type='AdamW', lr=0.001, weight_decay=0.0001),
+    clip_grad=dict(max_norm=10, norm_type=2))
+
+# learning rate
+param_scheduler = dict(
+    type='MultiStepLR',
+    begin=0,
+    end=12,
+    by_epoch=True,
+    milestones=[8, 11],
+    gamma=0.1)
+
+custom_hooks = [dict(type='EmptyCacheHook', after_iter=True)]
+
+# training schedule for 1x
+train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=12, val_interval=12)
+val_cfg = dict(type='ValLoop')
+test_cfg = dict(type='TestLoop')
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/lyft-3d-range100.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/pointnet2/pointnet2_msg_2xb16-cosine-250e_scannet-seg-xyz-only.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,105 +1,111 @@
-# If point cloud range is changed, the models should also change their point
-# cloud range accordingly
-point_cloud_range = [-100, -100, -5, 100, 100, 3]
-# For Lyft we usually do 9-class detection
-class_names = [
-    'car', 'truck', 'bus', 'emergency_vehicle', 'other_vehicle', 'motorcycle',
-    'bicycle', 'pedestrian', 'animal'
+_base_ = [
+    '../_base_/datasets/scannet-seg.py', '../_base_/models/pointnet2_msg.py',
+    '../_base_/schedules/seg-cosine-200e.py', '../_base_/default_runtime.py'
 ]
-dataset_type = 'LyftDataset'
-data_root = 'data/lyft/'
-# Input modality for Lyft dataset, this is consistent with the submission
-# format which requires the information in input_modality.
-input_modality = dict(
-    use_lidar=True,
-    use_camera=False,
-    use_radar=False,
-    use_map=False,
-    use_external=False)
-file_client_args = dict(backend='disk')
-# Uncomment the following if use ceph or other file clients.
-# See https://mmcv.readthedocs.io/en/latest/api.html#mmcv.fileio.FileClient
-# for more details.
-# file_client_args = dict(
-#     backend='petrel',
-#     path_mapping=dict({
-#         './data/lyft/': 's3://lyft/lyft/',
-#         'data/lyft/': 's3://lyft/lyft/'
-#    }))
+
+# model settings
+model = dict(
+    backbone=dict(in_channels=3),  # only [xyz]
+    decode_head=dict(
+        num_classes=20,
+        ignore_index=20,
+        # `class_weight` is generated in data pre-processing, saved in
+        # `data/scannet/seg_info/train_label_weight.npy`
+        # you can copy paste the values here, or input the file path as
+        # `class_weight=data/scannet/seg_info/train_label_weight.npy`
+        loss_decode=dict(class_weight=[
+            2.389689, 2.7215734, 4.5944676, 4.8543367, 4.096086, 4.907941,
+            4.690836, 4.512031, 4.623311, 4.9242644, 5.358117, 5.360071,
+            5.019636, 4.967126, 5.3502126, 5.4023647, 5.4027233, 5.4169416,
+            5.3954206, 4.6971426
+        ])),
+    test_cfg=dict(
+        num_points=8192,
+        block_size=1.5,
+        sample_rate=0.5,
+        use_normalized_coord=False,
+        batch_size=24))
+
+# dataset settings
+# in this setting, we only use xyz as network input
+# so we need to re-write all the data pipeline
+class_names = ('wall', 'floor', 'cabinet', 'bed', 'chair', 'sofa', 'table',
+               'door', 'window', 'bookshelf', 'picture', 'counter', 'desk',
+               'curtain', 'refrigerator', 'showercurtrain', 'toilet', 'sink',
+               'bathtub', 'otherfurniture')
+num_points = 8192
+backend_args = None
 train_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=5, use_dim=5),
-    dict(type='LoadPointsFromMultiSweeps', sweeps_num=10),
-    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
-    dict(
-        type='GlobalRotScaleTrans',
-        rot_range=[-0.3925, 0.3925],
-        scale_ratio_range=[0.95, 1.05],
-        translation_std=[0, 0, 0]),
-    dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
-    dict(type='PointsRangeFilter', point_cloud_range=point_cloud_range),
-    dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
-    dict(type='PointShuffle'),
     dict(
-        type='Pack3DDetInputs',
-        keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
+        type='LoadPointsFromFile',
+        coord_type='DEPTH',
+        shift_height=False,
+        use_color=False,
+        load_dim=6,
+        use_dim=[0, 1, 2],  # only load xyz coordinates
+        backend_args=backend_args),
+    dict(
+        type='LoadAnnotations3D',
+        with_bbox_3d=False,
+        with_label_3d=False,
+        with_mask_3d=False,
+        with_seg_3d=True,
+        backend_args=backend_args),
+    dict(type='PointSegClassMapping'),
+    dict(
+        type='IndoorPatchPointSample',
+        num_points=num_points,
+        block_size=1.5,
+        ignore_index=len(class_names),
+        use_normalized_coord=False,
+        enlarge_size=0.2,
+        min_unique_num=None),
+    dict(type='Pack3DDetInputs', keys=['points', 'pts_semantic_mask'])
 ]
 test_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=5, use_dim=5),
-    dict(type='LoadPointsFromMultiSweeps', sweeps_num=10),
     dict(
+        type='LoadPointsFromFile',
+        coord_type='DEPTH',
+        shift_height=False,
+        use_color=False,
+        load_dim=6,
+        use_dim=[0, 1, 2],
+        backend_args=backend_args),
+    dict(
+        type='LoadAnnotations3D',
+        with_bbox_3d=False,
+        with_label_3d=False,
+        with_mask_3d=False,
+        with_seg_3d=True,
+        backend_args=backend_args),
+    dict(
+        # a wrapper in order to successfully call test function
+        # actually we don't perform test-time-aug
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
             dict(
                 type='GlobalRotScaleTrans',
                 rot_range=[0, 0],
                 scale_ratio_range=[1., 1.],
                 translation_std=[0, 0, 0]),
-            dict(type='RandomFlip3D'),
             dict(
-                type='PointsRangeFilter', point_cloud_range=point_cloud_range),
+                type='RandomFlip3D',
+                sync_2d=False,
+                flip_ratio_bev_horizontal=0.0,
+                flip_ratio_bev_vertical=0.0),
         ]),
     dict(type='Pack3DDetInputs', keys=['points'])
 ]
-# construct a pipeline for data and gt loading in show function
-# please keep its loading function consistent with test_pipeline (e.g. client)
-eval_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=5, use_dim=5),
-    dict(type='LoadPointsFromMultiSweeps', sweeps_num=10),
-    dict(type='Pack3DDetInputs', keys=['points'])
-]
 
-data = dict(
-    samples_per_gpu=2,
-    workers_per_gpu=2,
-    train=dict(
-        type=dataset_type,
-        data_root=data_root,
-        ann_file=data_root + 'lyft_infos_train.pkl',
-        pipeline=train_pipeline,
-        classes=class_names,
-        modality=input_modality,
-        test_mode=False),
-    val=dict(
-        type=dataset_type,
-        data_root=data_root,
-        ann_file=data_root + 'lyft_infos_val.pkl',
-        pipeline=test_pipeline,
-        classes=class_names,
-        modality=input_modality,
-        test_mode=True),
-    test=dict(
-        type=dataset_type,
-        data_root=data_root,
-        ann_file=data_root + 'lyft_infos_test.pkl',
-        pipeline=test_pipeline,
-        classes=class_names,
-        modality=input_modality,
-        test_mode=True))
-# For Lyft dataset, we usually evaluate the model at the end of training.
-# Since the models are trained by 24 epochs by default, we set evaluation
-# interval to be 24. Please change the interval accordingly if you do not
-# use a default schedule.
-evaluation = dict(interval=24, pipeline=eval_pipeline)
+train_dataloader = dict(batch_size=16, dataset=dict(pipeline=train_pipeline))
+test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
+val_dataloader = test_dataloader
+
+# runtime settings
+default_hooks = dict(checkpoint=dict(type='CheckpointHook', interval=5))
+
+# PointNet2-MSG needs longer training time than PointNet2-SSG
+train_cfg = dict(by_epoch=True, max_epochs=250, val_interval=5)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/lyft-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/lyft-3d-range100.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,36 +1,53 @@
 # If point cloud range is changed, the models should also change their point
 # cloud range accordingly
-point_cloud_range = [-80, -80, -5, 80, 80, 3]
+point_cloud_range = [-100, -100, -5, 100, 100, 3]
 # For Lyft we usually do 9-class detection
 class_names = [
     'car', 'truck', 'bus', 'emergency_vehicle', 'other_vehicle', 'motorcycle',
     'bicycle', 'pedestrian', 'animal'
 ]
 dataset_type = 'LyftDataset'
 data_root = 'data/lyft/'
+data_prefix = dict(pts='v1.01-train/lidar', img='', sweeps='v1.01-train/lidar')
 # Input modality for Lyft dataset, this is consistent with the submission
 # format which requires the information in input_modality.
-input_modality = dict(use_lidar=True, use_camera=False)
-data_prefix = dict(pts='samples/LIDAR_TOP', img='', sweeps='sweeps/LIDAR_TOP')
+input_modality = dict(
+    use_lidar=True,
+    use_camera=False,
+    use_radar=False,
+    use_map=False,
+    use_external=False)
 
-file_client_args = dict(backend='disk')
-# Uncomment the following if use ceph or other file clients.
-# See https://mmcv.readthedocs.io/en/latest/api.html#mmcv.fileio.FileClient
-# for more details.
-# file_client_args = dict(
+# Example to use different file client
+# Method 1: simply set the data root and let the file I/O module
+# automatically infer from prefix (not support LMDB and Memcache yet)
+
+# data_root = 's3://openmmlab/datasets/detection3d/lyft/'
+
+# Method 2: Use backend_args, file_client_args in versions before 1.1.0
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
-#         './data/lyft/': 's3://lyft/lyft/',
-#         'data/lyft/': 's3://lyft/lyft/'
-#    }))
+#         './data/': 's3://openmmlab/datasets/detection3d/',
+#          'data/': 's3://openmmlab/datasets/detection3d/'
+#      }))
+backend_args = None
 
 train_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=5, use_dim=5),
-    dict(type='LoadPointsFromMultiSweeps', sweeps_num=10),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=5,
+        use_dim=5,
+        backend_args=backend_args),
+    dict(
+        type='LoadPointsFromMultiSweeps',
+        sweeps_num=10,
+        backend_args=backend_args),
     dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
     dict(
         type='GlobalRotScaleTrans',
         rot_range=[-0.3925, 0.3925],
         scale_ratio_range=[0.95, 1.05],
         translation_std=[0, 0, 0]),
     dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
@@ -38,71 +55,73 @@
     dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
     dict(type='PointShuffle'),
     dict(
         type='Pack3DDetInputs',
         keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
 ]
 test_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=5, use_dim=5),
-    dict(type='LoadPointsFromMultiSweeps', sweeps_num=10),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=5,
+        use_dim=5,
+        backend_args=backend_args),
+    dict(
+        type='LoadPointsFromMultiSweeps',
+        sweeps_num=10,
+        backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
             dict(
                 type='GlobalRotScaleTrans',
                 rot_range=[0, 0],
                 scale_ratio_range=[1., 1.],
                 translation_std=[0, 0, 0]),
             dict(type='RandomFlip3D'),
             dict(
-                type='PointsRangeFilter', point_cloud_range=point_cloud_range)
+                type='PointsRangeFilter', point_cloud_range=point_cloud_range),
         ]),
     dict(type='Pack3DDetInputs', keys=['points'])
 ]
 # construct a pipeline for data and gt loading in show function
 # please keep its loading function consistent with test_pipeline (e.g. client)
 eval_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=5, use_dim=5),
-    dict(type='LoadPointsFromMultiSweeps', sweeps_num=10),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=5,
+        use_dim=5,
+        backend_args=backend_args),
+    dict(
+        type='LoadPointsFromMultiSweeps',
+        sweeps_num=10,
+        backend_args=backend_args),
     dict(type='Pack3DDetInputs', keys=['points'])
 ]
+
 train_dataloader = dict(
     batch_size=2,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         ann_file='lyft_infos_train.pkl',
         pipeline=train_pipeline,
         metainfo=dict(classes=class_names),
         modality=input_modality,
         data_prefix=data_prefix,
         test_mode=False,
-        box_type_3d='LiDAR'))
-test_dataloader = dict(
-    batch_size=1,
-    num_workers=1,
-    persistent_workers=True,
-    drop_last=False,
-    sampler=dict(type='DefaultSampler', shuffle=False),
-    dataset=dict(
-        type=dataset_type,
-        data_root=data_root,
-        ann_file='lyft_infos_val.pkl',
-        pipeline=test_pipeline,
-        metainfo=dict(classes=class_names),
-        modality=input_modality,
-        data_prefix=data_prefix,
-        test_mode=True,
-        box_type_3d='LiDAR'))
+        box_type_3d='LiDAR',
+        backend_args=backend_args))
 val_dataloader = dict(
     batch_size=1,
     num_workers=1,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
@@ -110,21 +129,22 @@
         data_root=data_root,
         ann_file='lyft_infos_val.pkl',
         pipeline=test_pipeline,
         metainfo=dict(classes=class_names),
         modality=input_modality,
         test_mode=True,
         data_prefix=data_prefix,
-        box_type_3d='LiDAR'))
+        box_type_3d='LiDAR',
+        backend_args=backend_args))
+test_dataloader = val_dataloader
 
 val_evaluator = dict(
     type='LyftMetric',
-    ann_file=data_root + 'lyft_infos_val.pkl',
-    metric='bbox')
-test_evaluator = dict(
-    type='LyftMetric',
-    ann_file=data_root + 'lyft_infos_val.pkl',
-    metric='bbox')
+    data_root=data_root,
+    ann_file='lyft_infos_val.pkl',
+    metric='bbox',
+    backend_args=backend_args)
+test_evaluator = val_evaluator
 
 vis_backends = [dict(type='LocalVisBackend')]
 visualizer = dict(
     type='Det3DLocalVisualizer', vis_backends=vis_backends, name='visualizer')
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/nuim-instance.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/nuim-instance.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,34 +1,42 @@
 dataset_type = 'CocoDataset'
 data_root = 'data/nuimages/'
 class_names = [
     'car', 'truck', 'trailer', 'bus', 'construction_vehicle', 'bicycle',
     'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
 ]
 
-file_client_args = dict(backend='disk')
-# Uncomment the following if use ceph or other file clients.
-# See https://mmcv.readthedocs.io/en/latest/api.html#mmcv.fileio.FileClient
-# for more details.
-# file_client_args = dict(
-#     backend='petrel', path_mapping=dict(data='s3://nuimages'))
+# Example to use different file client
+# Method 1: simply set the data root and let the file I/O module
+# automatically infer from prefix (not support LMDB and Memcache yet)
+
+# data_root = 's3://openmmlab/datasets/detection3d/nuimages/'
+
+# Method 2: Use backend_args, file_client_args in versions before 1.1.0
+# backend_args = dict(
+#     backend='petrel',
+#     path_mapping=dict({
+#         './data/': 's3://openmmlab/datasets/detection3d/',
+#          'data/': 's3://openmmlab/datasets/detection3d/'
+#      }))
+backend_args = None
 
 train_pipeline = [
-    dict(type='LoadImageFromFile'),
+    dict(type='LoadImageFromFile', backend_args=backend_args),
     dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
     dict(
         type='Resize',
         img_scale=[(1280, 720), (1920, 1080)],
         multiscale_mode='range',
         keep_ratio=True),
     dict(type='RandomFlip', flip_ratio=0.5),
     dict(type='PackDetInputs'),
 ]
 test_pipeline = [
-    dict(type='LoadImageFromFile'),
+    dict(type='LoadImageFromFile', backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug',
         img_scale=(1600, 900),
         flip=False,
         transforms=[
             dict(type='Resize', keep_ratio=True),
             dict(type='RandomFlip'),
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/nus-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/lyft-3d.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,58 +1,73 @@
 # If point cloud range is changed, the models should also change their point
 # cloud range accordingly
-point_cloud_range = [-50, -50, -5, 50, 50, 3]
-# Using calibration info convert the Lidar-coordinate point cloud range to the
-# ego-coordinate point cloud range could bring a little promotion in nuScenes.
-# point_cloud_range = [-50, -50.8, -5, 50, 49.2, 3]
-# For nuScenes we usually do 10-class detection
+point_cloud_range = [-80, -80, -5, 80, 80, 3]
+# For Lyft we usually do 9-class detection
 class_names = [
-    'car', 'truck', 'trailer', 'bus', 'construction_vehicle', 'bicycle',
-    'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
+    'car', 'truck', 'bus', 'emergency_vehicle', 'other_vehicle', 'motorcycle',
+    'bicycle', 'pedestrian', 'animal'
 ]
-metainfo = dict(classes=class_names)
-dataset_type = 'NuScenesDataset'
-data_root = 'data/nuscenes/'
-# Input modality for nuScenes dataset, this is consistent with the submission
+dataset_type = 'LyftDataset'
+data_root = 'data/lyft/'
+# Input modality for Lyft dataset, this is consistent with the submission
 # format which requires the information in input_modality.
 input_modality = dict(use_lidar=True, use_camera=False)
-data_prefix = dict(pts='samples/LIDAR_TOP', img='', sweeps='sweeps/LIDAR_TOP')
+data_prefix = dict(pts='v1.01-train/lidar', img='', sweeps='v1.01-train/lidar')
 
-file_client_args = dict(backend='disk')
-# Uncomment the following if use ceph or other file clients.
-# See https://mmcv.readthedocs.io/en/latest/api.html#mmcv.fileio.FileClient
-# for more details.
-# file_client_args = dict(
+# Example to use different file client
+# Method 1: simply set the data root and let the file I/O module
+# automatically infer from prefix (not support LMDB and Memcache yet)
+
+# data_root = 's3://openmmlab/datasets/detection3d/lyft/'
+
+# Method 2: Use backend_args, file_client_args in versions before 1.1.0
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
-#         './data/nuscenes/': 's3://nuscenes/nuscenes/',
-#         'data/nuscenes/': 's3://nuscenes/nuscenes/'
-#     }))
+#         './data/': 's3://openmmlab/datasets/detection3d/',
+#          'data/': 's3://openmmlab/datasets/detection3d/'
+#      }))
+backend_args = None
 
 train_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=5, use_dim=5),
-    dict(type='LoadPointsFromMultiSweeps', sweeps_num=10),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=5,
+        use_dim=5,
+        backend_args=backend_args),
+    dict(
+        type='LoadPointsFromMultiSweeps',
+        sweeps_num=10,
+        backend_args=backend_args),
     dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
     dict(
         type='GlobalRotScaleTrans',
         rot_range=[-0.3925, 0.3925],
         scale_ratio_range=[0.95, 1.05],
         translation_std=[0, 0, 0]),
     dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
     dict(type='PointsRangeFilter', point_cloud_range=point_cloud_range),
     dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
-    dict(type='ObjectNameFilter', classes=class_names),
     dict(type='PointShuffle'),
     dict(
         type='Pack3DDetInputs',
         keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
 ]
 test_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=5, use_dim=5),
-    dict(type='LoadPointsFromMultiSweeps', sweeps_num=10, test_mode=True),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=5,
+        use_dim=5,
+        backend_args=backend_args),
+    dict(
+        type='LoadPointsFromMultiSweeps',
+        sweeps_num=10,
+        backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
             dict(
@@ -65,71 +80,81 @@
                 type='PointsRangeFilter', point_cloud_range=point_cloud_range)
         ]),
     dict(type='Pack3DDetInputs', keys=['points'])
 ]
 # construct a pipeline for data and gt loading in show function
 # please keep its loading function consistent with test_pipeline (e.g. client)
 eval_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=5, use_dim=5),
-    dict(type='LoadPointsFromMultiSweeps', sweeps_num=10, test_mode=True),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=5,
+        use_dim=5,
+        backend_args=backend_args),
+    dict(
+        type='LoadPointsFromMultiSweeps',
+        sweeps_num=10,
+        backend_args=backend_args),
     dict(type='Pack3DDetInputs', keys=['points'])
 ]
 train_dataloader = dict(
-    batch_size=4,
-    num_workers=4,
+    batch_size=2,
+    num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        ann_file='nuscenes_infos_train.pkl',
+        ann_file='lyft_infos_train.pkl',
         pipeline=train_pipeline,
-        metainfo=metainfo,
+        metainfo=dict(classes=class_names),
         modality=input_modality,
-        test_mode=False,
         data_prefix=data_prefix,
-        # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
-        # and box_type_3d='Depth' in sunrgbd and scannet dataset.
-        box_type_3d='LiDAR'))
+        test_mode=False,
+        box_type_3d='LiDAR',
+        backend_args=backend_args))
 test_dataloader = dict(
     batch_size=1,
     num_workers=1,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        ann_file='nuscenes_infos_val.pkl',
+        ann_file='lyft_infos_val.pkl',
         pipeline=test_pipeline,
-        metainfo=metainfo,
+        metainfo=dict(classes=class_names),
         modality=input_modality,
         data_prefix=data_prefix,
         test_mode=True,
-        box_type_3d='LiDAR'))
+        box_type_3d='LiDAR',
+        backend_args=backend_args))
 val_dataloader = dict(
     batch_size=1,
     num_workers=1,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        ann_file='nuscenes_infos_val.pkl',
+        ann_file='lyft_infos_val.pkl',
         pipeline=test_pipeline,
-        metainfo=metainfo,
+        metainfo=dict(classes=class_names),
         modality=input_modality,
         test_mode=True,
         data_prefix=data_prefix,
-        box_type_3d='LiDAR'))
+        box_type_3d='LiDAR',
+        backend_args=backend_args))
 
 val_evaluator = dict(
-    type='NuScenesMetric',
+    type='LyftMetric',
     data_root=data_root,
-    ann_file=data_root + 'nuscenes_infos_val.pkl',
-    metric='bbox')
+    ann_file='lyft_infos_val.pkl',
+    metric='bbox',
+    backend_args=backend_args)
 test_evaluator = val_evaluator
 
 vis_backends = [dict(type='LocalVisBackend')]
 visualizer = dict(
     type='Det3DLocalVisualizer', vis_backends=vis_backends, name='visualizer')
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/nus-mono3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/kitti-mono3d.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,114 +1,100 @@
-dataset_type = 'NuScenesDataset'
-data_root = 'data/nuscenes/'
-class_names = [
-    'car', 'truck', 'trailer', 'bus', 'construction_vehicle', 'bicycle',
-    'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
-]
-metainfo = dict(classes=class_names)
-# Input modality for nuScenes dataset, this is consistent with the submission
-# format which requires the information in input_modality.
+dataset_type = 'KittiDataset'
+data_root = 'data/kitti/'
+class_names = ['Pedestrian', 'Cyclist', 'Car']
 input_modality = dict(use_lidar=False, use_camera=True)
+metainfo = dict(classes=class_names)
+
+# Example to use different file client
+# Method 1: simply set the data root and let the file I/O module
+# automatically infer from prefix (not support LMDB and Memcache yet)
+
+# data_root = 's3://openmmlab/datasets/detection3d/kitti/'
 
-file_client_args = dict(backend='disk')
-# Uncomment the following if use ceph or other file clients.
-# See https://mmcv.readthedocs.io/en/latest/api.html#mmcv.fileio.FileClient
-# for more details.
-# file_client_args = dict(
+# Method 2: Use backend_args, file_client_args in versions before 1.1.0
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
-#         './data/nuscenes/':
-#         's3://openmmlab/datasets/detection3d/nuscenes/',
-#         'data/nuscenes/':
-#         's3://openmmlab/datasets/detection3d/nuscenes/'
-#     }))
+#         './data/': 's3://openmmlab/datasets/detection3d/',
+#          'data/': 's3://openmmlab/datasets/detection3d/'
+#      }))
+backend_args = None
 
 train_pipeline = [
-    dict(type='LoadImageFromFileMono3D'),
+    dict(type='LoadImageFromFileMono3D', backend_args=backend_args),
     dict(
         type='LoadAnnotations3D',
         with_bbox=True,
         with_label=True,
-        with_attr_label=True,
+        with_attr_label=False,
         with_bbox_3d=True,
         with_label_3d=True,
         with_bbox_depth=True),
-    dict(type='Resize', scale=(1600, 900), keep_ratio=True),
+    dict(type='Resize', scale=(1242, 375), keep_ratio=True),
     dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
     dict(
         type='Pack3DDetInputs',
         keys=[
-            'img', 'gt_bboxes', 'gt_bboxes_labels', 'attr_labels',
-            'gt_bboxes_3d', 'gt_labels_3d', 'centers_2d', 'depths'
+            'img', 'gt_bboxes', 'gt_bboxes_labels', 'gt_bboxes_3d',
+            'gt_labels_3d', 'centers_2d', 'depths'
         ]),
 ]
-
 test_pipeline = [
-    dict(type='LoadImageFromFileMono3D'),
-    dict(type='mmdet.Resize', scale=(1600, 900), keep_ratio=True),
+    dict(type='LoadImageFromFileMono3D', backend_args=backend_args),
+    dict(type='Resize', scale=(1242, 375), keep_ratio=True),
+    dict(type='Pack3DDetInputs', keys=['img'])
+]
+eval_pipeline = [
+    dict(type='LoadImageFromFileMono3D', backend_args=backend_args),
     dict(type='Pack3DDetInputs', keys=['img'])
 ]
 
 train_dataloader = dict(
     batch_size=2,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        data_prefix=dict(
-            pts='',
-            CAM_FRONT='samples/CAM_FRONT',
-            CAM_FRONT_LEFT='samples/CAM_FRONT_LEFT',
-            CAM_FRONT_RIGHT='samples/CAM_FRONT_RIGHT',
-            CAM_BACK='samples/CAM_BACK',
-            CAM_BACK_RIGHT='samples/CAM_BACK_RIGHT',
-            CAM_BACK_LEFT='samples/CAM_BACK_LEFT'),
-        ann_file='nuscenes_infos_train.pkl',
-        load_type='mv_image_based',
+        ann_file='kitti_infos_train.pkl',
+        data_prefix=dict(img='training/image_2'),
         pipeline=train_pipeline,
-        metainfo=metainfo,
         modality=input_modality,
+        load_type='fov_image_based',
         test_mode=False,
+        metainfo=metainfo,
         # we use box_type_3d='Camera' in monocular 3d
         # detection task
         box_type_3d='Camera',
-        use_valid_flag=True))
+        backend_args=backend_args))
 val_dataloader = dict(
     batch_size=1,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        data_prefix=dict(
-            pts='',
-            CAM_FRONT='samples/CAM_FRONT',
-            CAM_FRONT_LEFT='samples/CAM_FRONT_LEFT',
-            CAM_FRONT_RIGHT='samples/CAM_FRONT_RIGHT',
-            CAM_BACK='samples/CAM_BACK',
-            CAM_BACK_RIGHT='samples/CAM_BACK_RIGHT',
-            CAM_BACK_LEFT='samples/CAM_BACK_LEFT'),
-        ann_file='nuscenes_infos_val.pkl',
-        load_type='mv_image_based',
+        data_prefix=dict(img='training/image_2'),
+        ann_file='kitti_infos_val.pkl',
         pipeline=test_pipeline,
         modality=input_modality,
+        load_type='fov_image_based',
         metainfo=metainfo,
         test_mode=True,
         box_type_3d='Camera',
-        use_valid_flag=True))
+        backend_args=backend_args))
 test_dataloader = val_dataloader
 
 val_evaluator = dict(
-    type='NuScenesMetric',
-    data_root=data_root,
-    ann_file=data_root + 'nuscenes_infos_val.pkl',
-    metric='bbox')
+    type='KittiMetric',
+    ann_file=data_root + 'kitti_infos_val.pkl',
+    metric='bbox',
+    backend_args=backend_args)
 
 test_evaluator = val_evaluator
 
 vis_backends = [dict(type='LocalVisBackend')]
 visualizer = dict(
     type='Det3DLocalVisualizer', vis_backends=vis_backends, name='visualizer')
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/s3dis-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/fcaf3d/fcaf3d_2xb8_sunrgbd-3d-10class.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,49 +1,49 @@
-# dataset settings
-dataset_type = 'S3DISDataset'
-data_root = 'data/s3dis/'
-
-metainfo = dict(classes=('table', 'chair', 'sofa', 'bookcase', 'board'))
-train_area = [1, 2, 3, 4, 6]
-test_area = 5
+_base_ = [
+    '../_base_/models/fcaf3d.py', '../_base_/default_runtime.py',
+    '../_base_/datasets/sunrgbd-3d.py'
+]
+n_points = 100000
+backend_args = None
+
+model = dict(
+    bbox_head=dict(
+        num_classes=10,
+        num_reg_outs=8,
+        bbox_loss=dict(type='RotatedIoU3DLoss')))
 
 train_pipeline = [
     dict(
         type='LoadPointsFromFile',
         coord_type='DEPTH',
         shift_height=False,
-        use_color=True,
         load_dim=6,
-        use_dim=[0, 1, 2, 3, 4, 5]),
-    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
-    dict(type='PointSample', num_points=100000),
-    dict(
-        type='RandomFlip3D',
-        sync_2d=False,
-        flip_ratio_bev_horizontal=0.5,
-        flip_ratio_bev_vertical=0.5),
+        use_dim=[0, 1, 2, 3, 4, 5],
+        backend_args=backend_args),
+    dict(type='LoadAnnotations3D'),
+    dict(type='PointSample', num_points=n_points),
+    dict(type='RandomFlip3D', sync_2d=False, flip_ratio_bev_horizontal=0.5),
     dict(
         type='GlobalRotScaleTrans',
-        rot_range=[-0.087266, 0.087266],
-        scale_ratio_range=[0.9, 1.1],
+        rot_range=[-0.523599, 0.523599],
+        scale_ratio_range=[0.85, 1.15],
         translation_std=[.1, .1, .1],
         shift_height=False),
-    dict(type='NormalizePointsColor', color_mean=None),
     dict(
         type='Pack3DDetInputs',
         keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
 ]
 test_pipeline = [
     dict(
         type='LoadPointsFromFile',
         coord_type='DEPTH',
         shift_height=False,
-        use_color=True,
         load_dim=6,
-        use_dim=[0, 1, 2, 3, 4, 5]),
+        use_dim=[0, 1, 2, 3, 4, 5],
+        backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
             dict(
@@ -52,63 +52,41 @@
                 scale_ratio_range=[1., 1.],
                 translation_std=[0, 0, 0]),
             dict(
                 type='RandomFlip3D',
                 sync_2d=False,
                 flip_ratio_bev_horizontal=0.5,
                 flip_ratio_bev_vertical=0.5),
-            dict(type='PointSample', num_points=100000),
-            dict(type='NormalizePointsColor', color_mean=None),
+            dict(type='PointSample', num_points=n_points)
         ]),
     dict(type='Pack3DDetInputs', keys=['points'])
 ]
 
 train_dataloader = dict(
     batch_size=8,
-    num_workers=4,
-    sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type='RepeatDataset',
-        times=13,
-        dataset=dict(
-            type='ConcatDataset',
-            datasets=[
-                dict(
-                    type=dataset_type,
-                    data_root=data_root,
-                    ann_file=f's3dis_infos_Area_{i}.pkl',
-                    pipeline=train_pipeline,
-                    filter_empty_gt=True,
-                    metainfo=metainfo,
-                    box_type_3d='Depth') for i in train_area
-            ])))
-
-val_dataloader = dict(
-    batch_size=1,
-    num_workers=1,
-    sampler=dict(type='DefaultSampler', shuffle=False),
-    dataset=dict(
-        type=dataset_type,
-        data_root=data_root,
-        ann_file=f's3dis_infos_Area_{test_area}.pkl',
-        pipeline=test_pipeline,
-        metainfo=metainfo,
-        test_mode=True,
-        box_type_3d='Depth'))
-test_dataloader = dict(
-    batch_size=1,
-    num_workers=1,
-    sampler=dict(type='DefaultSampler', shuffle=False),
-    dataset=dict(
-        type=dataset_type,
-        data_root=data_root,
-        ann_file=f's3dis_infos_Area_{test_area}.pkl',
-        pipeline=test_pipeline,
-        metainfo=metainfo,
-        test_mode=True,
-        box_type_3d='Depth'))
-val_evaluator = dict(type='IndoorMetric')
-test_evaluator = val_evaluator
-
-vis_backends = [dict(type='LocalVisBackend')]
-visualizer = dict(
-    type='Det3DLocalVisualizer', vis_backends=vis_backends, name='visualizer')
+        times=3,
+        dataset=dict(pipeline=train_pipeline, filter_empty_gt=True)))
+val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
+test_dataloader = val_dataloader
+
+optim_wrapper = dict(
+    type='OptimWrapper',
+    optimizer=dict(type='AdamW', lr=0.001, weight_decay=0.0001),
+    clip_grad=dict(max_norm=10, norm_type=2))
+
+# learning rate
+param_scheduler = dict(
+    type='MultiStepLR',
+    begin=0,
+    end=12,
+    by_epoch=True,
+    milestones=[8, 11],
+    gamma=0.1)
+
+custom_hooks = [dict(type='EmptyCacheHook', after_iter=True)]
+
+# training schedule for 1x
+train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=12, val_interval=12)
+val_cfg = dict(type='ValLoop')
+test_cfg = dict(type='TestLoop')
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/s3dis-seg.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/kitti-3d-3class.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,148 +1,167 @@
-# For S3DIS seg we usually do 13-class segmentation
-class_names = ('ceiling', 'floor', 'wall', 'beam', 'column', 'window', 'door',
-               'table', 'chair', 'sofa', 'bookcase', 'board', 'clutter')
-metainfo = dict(classes=class_names)
-dataset_type = 'S3DISSegDataset'
-data_root = 'data/s3dis/'
+# dataset settings
+dataset_type = 'KittiDataset'
+data_root = 'data/kitti/'
+class_names = ['Pedestrian', 'Cyclist', 'Car']
+point_cloud_range = [0, -40, -3, 70.4, 40, 1]
 input_modality = dict(use_lidar=True, use_camera=False)
-data_prefix = dict(
-    pts='points',
-    pts_instance_mask='instance_mask',
-    pts_semantic_mask='semantic_mask')
-
-file_client_args = dict(backend='disk')
-# Uncomment the following if use ceph or other file clients.
-# See https://mmcv.readthedocs.io/en/latest/api.html#mmcv.fileio.FileClient
-# for more details.
-# file_client_args = dict(
+metainfo = dict(classes=class_names)
+
+# Example to use different file client
+# Method 1: simply set the data root and let the file I/O module
+# automatically infer from prefix (not support LMDB and Memcache yet)
+
+# data_root = 's3://openmmlab/datasets/detection3d/kitti/'
+
+# Method 2: Use backend_args, file_client_args in versions before 1.1.0
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
-#         './data/s3dis/':
-#         's3://s3dis/',
-#     }))
-
-num_points = 4096
-train_area = [1, 2, 3, 4, 6]
-test_area = 5
+#         './data/': 's3://openmmlab/datasets/detection3d/',
+#          'data/': 's3://openmmlab/datasets/detection3d/'
+#      }))
+backend_args = None
+
+db_sampler = dict(
+    data_root=data_root,
+    info_path=data_root + 'kitti_dbinfos_train.pkl',
+    rate=1.0,
+    prepare=dict(
+        filter_by_difficulty=[-1],
+        filter_by_min_points=dict(Car=5, Pedestrian=10, Cyclist=10)),
+    classes=class_names,
+    sample_groups=dict(Car=12, Pedestrian=6, Cyclist=6),
+    points_loader=dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
+    backend_args=backend_args)
+
 train_pipeline = [
     dict(
         type='LoadPointsFromFile',
-        coord_type='DEPTH',
-        shift_height=False,
-        use_color=True,
-        load_dim=6,
-        use_dim=[0, 1, 2, 3, 4, 5]),
-    dict(
-        type='LoadAnnotations3D',
-        with_bbox_3d=False,
-        with_label_3d=False,
-        with_mask_3d=False,
-        with_seg_3d=True),
-    dict(type='PointSegClassMapping'),
-    dict(
-        type='IndoorPatchPointSample',
-        num_points=num_points,
-        block_size=1.0,
-        ignore_index=len(class_names),
-        use_normalized_coord=True,
-        enlarge_size=0.2,
-        min_unique_num=None),
-    dict(type='NormalizePointsColor', color_mean=None),
-    dict(type='Pack3DDetInputs', keys=['points', 'pts_semantic_mask'])
+        coord_type='LIDAR',
+        load_dim=4,  # x, y, z, intensity
+        use_dim=4,
+        backend_args=backend_args),
+    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
+    dict(type='ObjectSample', db_sampler=db_sampler),
+    dict(
+        type='ObjectNoise',
+        num_try=100,
+        translation_std=[1.0, 1.0, 0.5],
+        global_rot_range=[0.0, 0.0],
+        rot_range=[-0.78539816, 0.78539816]),
+    dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
+    dict(
+        type='GlobalRotScaleTrans',
+        rot_range=[-0.78539816, 0.78539816],
+        scale_ratio_range=[0.95, 1.05]),
+    dict(type='PointsRangeFilter', point_cloud_range=point_cloud_range),
+    dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
+    dict(type='PointShuffle'),
+    dict(
+        type='Pack3DDetInputs',
+        keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
 ]
 test_pipeline = [
     dict(
         type='LoadPointsFromFile',
-        coord_type='DEPTH',
-        shift_height=False,
-        use_color=True,
-        load_dim=6,
-        use_dim=[0, 1, 2, 3, 4, 5]),
-    dict(
-        type='LoadAnnotations3D',
-        with_bbox_3d=False,
-        with_label_3d=False,
-        with_mask_3d=False,
-        with_seg_3d=True),
-    dict(type='NormalizePointsColor', color_mean=None),
+        coord_type='LIDAR',
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
     dict(
-        # a wrapper in order to successfully call test function
-        # actually we don't perform test-time-aug
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
             dict(
                 type='GlobalRotScaleTrans',
                 rot_range=[0, 0],
                 scale_ratio_range=[1., 1.],
                 translation_std=[0, 0, 0]),
+            dict(type='RandomFlip3D'),
             dict(
-                type='RandomFlip3D',
-                sync_2d=False,
-                flip_ratio_bev_horizontal=0.0,
-                flip_ratio_bev_vertical=0.0),
+                type='PointsRangeFilter', point_cloud_range=point_cloud_range)
         ]),
     dict(type='Pack3DDetInputs', keys=['points'])
 ]
 # construct a pipeline for data and gt loading in show function
 # please keep its loading function consistent with test_pipeline (e.g. client)
-# we need to load gt seg_mask!
 eval_pipeline = [
     dict(
         type='LoadPointsFromFile',
-        coord_type='DEPTH',
-        shift_height=False,
-        use_color=True,
-        load_dim=6,
-        use_dim=[0, 1, 2, 3, 4, 5]),
-    dict(type='NormalizePointsColor', color_mean=None),
+        coord_type='LIDAR',
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
     dict(type='Pack3DDetInputs', keys=['points'])
 ]
-
-# train on area 1, 2, 3, 4, 6
-# test on area 5
 train_dataloader = dict(
-    batch_size=8,
+    batch_size=6,
     num_workers=4,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
+        type='RepeatDataset',
+        times=2,
+        dataset=dict(
+            type=dataset_type,
+            data_root=data_root,
+            ann_file='kitti_infos_train.pkl',
+            data_prefix=dict(pts='training/velodyne_reduced'),
+            pipeline=train_pipeline,
+            modality=input_modality,
+            test_mode=False,
+            metainfo=metainfo,
+            # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
+            # and box_type_3d='Depth' in sunrgbd and scannet dataset.
+            box_type_3d='LiDAR',
+            backend_args=backend_args)))
+val_dataloader = dict(
+    batch_size=1,
+    num_workers=1,
+    persistent_workers=True,
+    drop_last=False,
+    sampler=dict(type='DefaultSampler', shuffle=False),
+    dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        ann_files=[f's3dis_infos_Area_{i}.pkl' for i in train_area],
-        metainfo=metainfo,
-        data_prefix=data_prefix,
-        pipeline=train_pipeline,
+        data_prefix=dict(pts='training/velodyne_reduced'),
+        ann_file='kitti_infos_val.pkl',
+        pipeline=test_pipeline,
         modality=input_modality,
-        ignore_index=len(class_names),
-        scene_idxs=[
-            f'seg_info/Area_{i}_resampled_scene_idxs.npy' for i in train_area
-        ],
-        test_mode=False))
+        test_mode=True,
+        metainfo=metainfo,
+        box_type_3d='LiDAR',
+        backend_args=backend_args))
 test_dataloader = dict(
     batch_size=1,
     num_workers=1,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        ann_files=f's3dis_infos_Area_{test_area}.pkl',
-        metainfo=metainfo,
-        data_prefix=data_prefix,
+        data_prefix=dict(pts='training/velodyne_reduced'),
+        ann_file='kitti_infos_val.pkl',
         pipeline=test_pipeline,
         modality=input_modality,
-        ignore_index=len(class_names),
-        scene_idxs=f'seg_info/Area_{test_area}_resampled_scene_idxs.npy',
-        test_mode=True))
-val_dataloader = test_dataloader
-
-val_evaluator = dict(type='SegMetric')
+        test_mode=True,
+        metainfo=metainfo,
+        box_type_3d='LiDAR',
+        backend_args=backend_args))
+val_evaluator = dict(
+    type='KittiMetric',
+    ann_file=data_root + 'kitti_infos_val.pkl',
+    metric='bbox',
+    backend_args=backend_args)
 test_evaluator = val_evaluator
 
 vis_backends = [dict(type='LocalVisBackend')]
 visualizer = dict(
     type='Det3DLocalVisualizer', vis_backends=vis_backends, name='visualizer')
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/scannet-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/sunrgbd-3d.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,69 +1,61 @@
-# dataset settings
-dataset_type = 'ScanNetDataset'
-data_root = 'data/scannet/'
-
-metainfo = dict(
-    classes=('cabinet', 'bed', 'chair', 'sofa', 'table', 'door', 'window',
-             'bookshelf', 'picture', 'counter', 'desk', 'curtain',
-             'refrigerator', 'showercurtrain', 'toilet', 'sink', 'bathtub',
-             'garbagebin'))
-
-# file_client_args = dict(backend='disk')
-# Uncomment the following if use ceph or other file clients.
-# See https://mmcv.readthedocs.io/en/latest/api.html#mmcv.fileio.FileClient
-# for more details.
-# file_client_args = dict(
+dataset_type = 'SUNRGBDDataset'
+data_root = 'data/sunrgbd/'
+class_names = ('bed', 'table', 'sofa', 'chair', 'toilet', 'desk', 'dresser',
+               'night_stand', 'bookshelf', 'bathtub')
+
+metainfo = dict(classes=class_names)
+
+# Example to use different file client
+# Method 1: simply set the data root and let the file I/O module
+# automatically infer from prefix (not support LMDB and Memcache yet)
+
+# data_root = 's3://openmmlab/datasets/detection3d/sunrgbd/'
+
+# Method 2: Use backend_args, file_client_args in versions before 1.1.0
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
-#         './data/scannet/':
-#         's3://scannet/',
-#     }))
+#         './data/': 's3://openmmlab/datasets/detection3d/',
+#          'data/': 's3://openmmlab/datasets/detection3d/'
+#      }))
+backend_args = None
 
 train_pipeline = [
     dict(
         type='LoadPointsFromFile',
         coord_type='DEPTH',
         shift_height=True,
         load_dim=6,
-        use_dim=[0, 1, 2]),
-    dict(
-        type='LoadAnnotations3D',
-        with_bbox_3d=True,
-        with_label_3d=True,
-        with_mask_3d=True,
-        with_seg_3d=True),
-    dict(type='GlobalAlignment', rotation_axis=2),
-    dict(type='PointSegClassMapping'),
-    dict(type='PointSample', num_points=40000),
+        use_dim=[0, 1, 2],
+        backend_args=backend_args),
+    dict(type='LoadAnnotations3D'),
     dict(
         type='RandomFlip3D',
         sync_2d=False,
         flip_ratio_bev_horizontal=0.5,
-        flip_ratio_bev_vertical=0.5),
+    ),
     dict(
         type='GlobalRotScaleTrans',
-        rot_range=[-0.087266, 0.087266],
-        scale_ratio_range=[1.0, 1.0],
+        rot_range=[-0.523599, 0.523599],
+        scale_ratio_range=[0.85, 1.15],
         shift_height=True),
+    dict(type='PointSample', num_points=20000),
     dict(
         type='Pack3DDetInputs',
-        keys=[
-            'points', 'gt_bboxes_3d', 'gt_labels_3d', 'pts_semantic_mask',
-            'pts_instance_mask'
-        ])
+        keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
 ]
 test_pipeline = [
     dict(
         type='LoadPointsFromFile',
         coord_type='DEPTH',
         shift_height=True,
         load_dim=6,
-        use_dim=[0, 1, 2]),
-    dict(type='GlobalAlignment', rotation_axis=2),
+        use_dim=[0, 1, 2],
+        backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
             dict(
@@ -71,61 +63,64 @@
                 rot_range=[0, 0],
                 scale_ratio_range=[1., 1.],
                 translation_std=[0, 0, 0]),
             dict(
                 type='RandomFlip3D',
                 sync_2d=False,
                 flip_ratio_bev_horizontal=0.5,
-                flip_ratio_bev_vertical=0.5),
-            dict(type='PointSample', num_points=40000),
+            ),
+            dict(type='PointSample', num_points=20000)
         ]),
     dict(type='Pack3DDetInputs', keys=['points'])
 ]
 
 train_dataloader = dict(
-    batch_size=8,
+    batch_size=16,
     num_workers=4,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type='RepeatDataset',
         times=5,
         dataset=dict(
             type=dataset_type,
             data_root=data_root,
-            ann_file='scannet_infos_train.pkl',
+            ann_file='sunrgbd_infos_train.pkl',
             pipeline=train_pipeline,
             filter_empty_gt=False,
             metainfo=metainfo,
             # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
             # and box_type_3d='Depth' in sunrgbd and scannet dataset.
-            box_type_3d='Depth')))
+            box_type_3d='Depth',
+            backend_args=backend_args)))
 
 val_dataloader = dict(
     batch_size=1,
     num_workers=1,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        ann_file='scannet_infos_val.pkl',
+        ann_file='sunrgbd_infos_val.pkl',
         pipeline=test_pipeline,
         metainfo=metainfo,
         test_mode=True,
-        box_type_3d='Depth'))
+        box_type_3d='Depth',
+        backend_args=backend_args))
 test_dataloader = dict(
     batch_size=1,
     num_workers=1,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        ann_file='scannet_infos_val.pkl',
+        ann_file='sunrgbd_infos_val.pkl',
         pipeline=test_pipeline,
         metainfo=metainfo,
         test_mode=True,
-        box_type_3d='Depth'))
+        box_type_3d='Depth',
+        backend_args=backend_args))
 val_evaluator = dict(type='IndoorMetric')
 test_evaluator = val_evaluator
 
 vis_backends = [dict(type='LocalVisBackend')]
 visualizer = dict(
     type='Det3DLocalVisualizer', vis_backends=vis_backends, name='visualizer')
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/scannet-seg.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/imvoxelnet/imvoxelnet_2xb4_sunrgbd-3d-10class.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,143 +1,137 @@
-# For ScanNet seg we usually do 20-class segmentation
-class_names = ('wall', 'floor', 'cabinet', 'bed', 'chair', 'sofa', 'table',
-               'door', 'window', 'bookshelf', 'picture', 'counter', 'desk',
-               'curtain', 'refrigerator', 'showercurtrain', 'toilet', 'sink',
-               'bathtub', 'otherfurniture')
-metainfo = dict(classes=class_names)
-dataset_type = 'ScanNetSegDataset'
-data_root = 'data/scannet/'
-input_modality = dict(use_lidar=True, use_camera=False)
-data_prefix = dict(
-    pts='points',
-    pts_instance_mask='instance_mask',
-    pts_semantic_mask='semantic_mask')
-
-file_client_args = dict(backend='disk')
-# Uncomment the following if use ceph or other file clients.
-# See https://mmcv.readthedocs.io/en/latest/api.html#mmcv.fileio.FileClient
-# for more details.
-# file_client_args = dict(
-#     backend='petrel',
-#     path_mapping=dict({
-#         './data/scannet/':
-#         's3://scannet/',
-#     }))
+_base_ = [
+    '../_base_/schedules/mmdet-schedule-1x.py', '../_base_/default_runtime.py'
+]
+prior_generator = dict(
+    type='AlignedAnchor3DRangeGenerator',
+    ranges=[[-3.2, -0.2, -2.28, 3.2, 6.2, 0.28]],
+    rotations=[.0])
+model = dict(
+    type='ImVoxelNet',
+    data_preprocessor=dict(
+        type='Det3DDataPreprocessor',
+        mean=[123.675, 116.28, 103.53],
+        std=[58.395, 57.12, 57.375],
+        bgr_to_rgb=True,
+        pad_size_divisor=32),
+    backbone=dict(
+        type='mmdet.ResNet',
+        depth=50,
+        num_stages=4,
+        out_indices=(0, 1, 2, 3),
+        frozen_stages=1,
+        norm_cfg=dict(type='BN', requires_grad=False),
+        norm_eval=True,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
+        style='pytorch'),
+    neck=dict(
+        type='mmdet.FPN',
+        in_channels=[256, 512, 1024, 2048],
+        out_channels=256,
+        num_outs=4),
+    neck_3d=dict(
+        type='IndoorImVoxelNeck',
+        in_channels=256,
+        out_channels=128,
+        n_blocks=[1, 1, 1]),
+    bbox_head=dict(
+        type='ImVoxelHead',
+        n_classes=10,
+        n_levels=3,
+        n_channels=128,
+        n_reg_outs=7,
+        pts_assign_threshold=27,
+        pts_center_threshold=18,
+        prior_generator=prior_generator),
+    prior_generator=prior_generator,
+    n_voxels=[40, 40, 16],
+    coord_type='DEPTH',
+    train_cfg=dict(),
+    test_cfg=dict(nms_pre=1000, iou_thr=.25, score_thr=.01))
+
+dataset_type = 'SUNRGBDDataset'
+data_root = 'data/sunrgbd/'
+class_names = [
+    'bed', 'table', 'sofa', 'chair', 'toilet', 'desk', 'dresser',
+    'night_stand', 'bookshelf', 'bathtub'
+]
+metainfo = dict(CLASSES=class_names)
+
+backend_args = None
 
-num_points = 8192
 train_pipeline = [
-    dict(
-        type='LoadPointsFromFile',
-        coord_type='DEPTH',
-        shift_height=False,
-        use_color=True,
-        load_dim=6,
-        use_dim=[0, 1, 2, 3, 4, 5]),
-    dict(
-        type='LoadAnnotations3D',
-        with_bbox_3d=False,
-        with_label_3d=False,
-        with_mask_3d=False,
-        with_seg_3d=True),
-    dict(type='PointSegClassMapping'),
-    dict(
-        type='IndoorPatchPointSample',
-        num_points=num_points,
-        block_size=1.5,
-        ignore_index=len(class_names),
-        use_normalized_coord=False,
-        enlarge_size=0.2,
-        min_unique_num=None),
-    dict(type='NormalizePointsColor', color_mean=None),
-    dict(type='Pack3DDetInputs', keys=['points', 'pts_semantic_mask'])
+    dict(type='LoadAnnotations3D', backend_args=backend_args),
+    dict(type='LoadImageFromFile', backend_args=backend_args),
+    dict(type='RandomResize', scale=[(512, 384), (768, 576)], keep_ratio=True),
+    dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
+    dict(type='Pack3DDetInputs', keys=['img', 'gt_bboxes_3d', 'gt_labels_3d'])
 ]
 test_pipeline = [
-    dict(
-        type='LoadPointsFromFile',
-        coord_type='DEPTH',
-        shift_height=False,
-        use_color=True,
-        load_dim=6,
-        use_dim=[0, 1, 2, 3, 4, 5]),
-    dict(
-        type='LoadAnnotations3D',
-        with_bbox_3d=False,
-        with_label_3d=False,
-        with_mask_3d=False,
-        with_seg_3d=True),
-    dict(type='NormalizePointsColor', color_mean=None),
-    dict(
-        # a wrapper in order to successfully call test function
-        # actually we don't perform test-time-aug
-        type='MultiScaleFlipAug3D',
-        img_scale=(1333, 800),
-        pts_scale_ratio=1,
-        flip=False,
-        transforms=[
-            dict(
-                type='GlobalRotScaleTrans',
-                rot_range=[0, 0],
-                scale_ratio_range=[1., 1.],
-                translation_std=[0, 0, 0]),
-            dict(
-                type='RandomFlip3D',
-                sync_2d=False,
-                flip_ratio_bev_horizontal=0.0,
-                flip_ratio_bev_vertical=0.0),
-        ]),
-    dict(type='Pack3DDetInputs', keys=['points'])
-]
-# construct a pipeline for data and gt loading in show function
-# please keep its loading function consistent with test_pipeline (e.g. client)
-# we need to load gt seg_mask!
-eval_pipeline = [
-    dict(
-        type='LoadPointsFromFile',
-        coord_type='DEPTH',
-        shift_height=False,
-        use_color=True,
-        load_dim=6,
-        use_dim=[0, 1, 2, 3, 4, 5]),
-    dict(type='NormalizePointsColor', color_mean=None),
-    dict(type='Pack3DDetInputs', keys=['points'])
+    dict(type='LoadImageFromFile', backend_args=backend_args),
+    dict(type='Resize', scale=(640, 480), keep_ratio=True),
+    dict(type='Pack3DDetInputs', keys=['img'])
 ]
 
 train_dataloader = dict(
-    batch_size=8,
+    batch_size=4,
     num_workers=4,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
-        type=dataset_type,
-        data_root=data_root,
-        ann_file='scannet_infos_train.pkl',
-        metainfo=metainfo,
-        data_prefix=data_prefix,
-        pipeline=train_pipeline,
-        modality=input_modality,
-        ignore_index=len(class_names),
-        scene_idxs=data_root + 'seg_info/train_resampled_scene_idxs.npy',
-        test_mode=False))
-test_dataloader = dict(
+        type='RepeatDataset',
+        times=2,
+        dataset=dict(
+            type=dataset_type,
+            data_root=data_root,
+            ann_file='sunrgbd_infos_train.pkl',
+            pipeline=train_pipeline,
+            test_mode=False,
+            filter_empty_gt=True,
+            box_type_3d='Depth',
+            metainfo=metainfo,
+            backend_args=backend_args)))
+val_dataloader = dict(
     batch_size=1,
     num_workers=1,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        ann_file='scannet_infos_val.pkl',
-        metainfo=metainfo,
-        data_prefix=data_prefix,
+        ann_file='sunrgbd_infos_val.pkl',
         pipeline=test_pipeline,
-        modality=input_modality,
-        ignore_index=len(class_names),
-        test_mode=True))
-val_dataloader = test_dataloader
+        test_mode=True,
+        box_type_3d='Depth',
+        metainfo=metainfo,
+        backend_args=backend_args))
+test_dataloader = val_dataloader
 
-val_evaluator = dict(type='SegMetric')
+val_evaluator = dict(
+    type='IndoorMetric',
+    ann_file=data_root + 'sunrgbd_infos_val.pkl',
+    metric='bbox')
 test_evaluator = val_evaluator
 
-vis_backends = [dict(type='LocalVisBackend')]
-visualizer = dict(
-    type='Det3DLocalVisualizer', vis_backends=vis_backends, name='visualizer')
+# optimizer
+optim_wrapper = dict(
+    type='OptimWrapper',
+    optimizer=dict(
+        _delete_=True, type='AdamW', lr=0.0001, weight_decay=0.0001),
+    paramwise_cfg=dict(
+        custom_keys={'backbone': dict(lr_mult=0.1, decay_mult=1.0)}),
+    clip_grad=dict(max_norm=35., norm_type=2))
+param_scheduler = [
+    dict(
+        type='MultiStepLR',
+        begin=0,
+        end=12,
+        by_epoch=True,
+        milestones=[8, 11],
+        gamma=0.1)
+]
+
+# hooks
+default_hooks = dict(checkpoint=dict(type='CheckpointHook', max_keep_ckpts=1))
+
+# runtime
+find_unused_parameters = True  # only 1 of 4 FPN outputs is used
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/sunrgbd-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/sassd/sassd_8xb6-80e_kitti-3d-3class.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,117 +1,99 @@
-dataset_type = 'SUNRGBDDataset'
-data_root = 'data/sunrgbd/'
-class_names = ('bed', 'table', 'sofa', 'chair', 'toilet', 'desk', 'dresser',
-               'night_stand', 'bookshelf', 'bathtub')
-
-metainfo = dict(classes=class_names)
-
-file_client_args = dict(backend='disk')
-# Uncomment the following if use ceph or other file clients.
-# See https://mmcv.readthedocs.io/en/latest/api.html#mmcv.fileio.FileClient
-# for more details.
-# file_client_args = dict(
-#     backend='petrel',
-#     path_mapping=dict({
-#         './data/sunrgbd/':
-#         's3://sunrgbd/',
-#     }))
-
-train_pipeline = [
-    dict(
-        type='LoadPointsFromFile',
-        coord_type='DEPTH',
-        shift_height=True,
-        load_dim=6,
-        use_dim=[0, 1, 2]),
-    dict(type='LoadAnnotations3D'),
-    dict(
-        type='RandomFlip3D',
-        sync_2d=False,
-        flip_ratio_bev_horizontal=0.5,
-    ),
-    dict(
-        type='GlobalRotScaleTrans',
-        rot_range=[-0.523599, 0.523599],
-        scale_ratio_range=[0.85, 1.15],
-        shift_height=True),
-    dict(type='PointSample', num_points=20000),
-    dict(
-        type='Pack3DDetInputs',
-        keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
+_base_ = [
+    '../_base_/datasets/kitti-3d-3class.py',
+    '../_base_/schedules/cyclic-40e.py', '../_base_/default_runtime.py'
 ]
-test_pipeline = [
-    dict(
-        type='LoadPointsFromFile',
-        coord_type='DEPTH',
-        shift_height=True,
-        load_dim=6,
-        use_dim=[0, 1, 2]),
-    dict(
-        type='MultiScaleFlipAug3D',
-        img_scale=(1333, 800),
-        pts_scale_ratio=1,
-        flip=False,
-        transforms=[
-            dict(
-                type='GlobalRotScaleTrans',
-                rot_range=[0, 0],
-                scale_ratio_range=[1., 1.],
-                translation_std=[0, 0, 0]),
-            dict(
-                type='RandomFlip3D',
-                sync_2d=False,
-                flip_ratio_bev_horizontal=0.5,
-            ),
-            dict(type='PointSample', num_points=20000)
-        ]),
-    dict(type='Pack3DDetInputs', keys=['points'])
-]
-
-train_dataloader = dict(
-    batch_size=16,
-    num_workers=4,
-    sampler=dict(type='DefaultSampler', shuffle=True),
-    dataset=dict(
-        type='RepeatDataset',
-        times=5,
-        dataset=dict(
-            type=dataset_type,
-            data_root=data_root,
-            ann_file='sunrgbd_infos_train.pkl',
-            pipeline=train_pipeline,
-            filter_empty_gt=False,
-            metainfo=metainfo,
-            # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
-            # and box_type_3d='Depth' in sunrgbd and scannet dataset.
-            box_type_3d='Depth')))
 
-val_dataloader = dict(
-    batch_size=1,
-    num_workers=1,
-    sampler=dict(type='DefaultSampler', shuffle=False),
-    dataset=dict(
-        type=dataset_type,
-        data_root=data_root,
-        ann_file='sunrgbd_infos_val.pkl',
-        pipeline=test_pipeline,
-        metainfo=metainfo,
-        test_mode=True,
-        box_type_3d='Depth'))
-test_dataloader = dict(
-    batch_size=1,
-    num_workers=1,
-    sampler=dict(type='DefaultSampler', shuffle=False),
-    dataset=dict(
-        type=dataset_type,
-        data_root=data_root,
-        ann_file='sunrgbd_infos_val.pkl',
-        pipeline=test_pipeline,
-        metainfo=metainfo,
-        test_mode=True,
-        box_type_3d='Depth'))
-val_evaluator = dict(type='IndoorMetric')
-test_evaluator = val_evaluator
+voxel_size = [0.05, 0.05, 0.1]
 
-vis_backends = [dict(type='LocalVisBackend')]
-visualizer = dict(
-    type='Det3DLocalVisualizer', vis_backends=vis_backends, name='visualizer')
+model = dict(
+    type='SASSD',
+    data_preprocessor=dict(
+        type='Det3DDataPreprocessor',
+        voxel=True,
+        voxel_layer=dict(
+            max_num_points=5,
+            point_cloud_range=[0, -40, -3, 70.4, 40, 1],
+            voxel_size=voxel_size,
+            max_voxels=(16000, 40000))),
+    voxel_encoder=dict(type='HardSimpleVFE'),
+    middle_encoder=dict(
+        type='SparseEncoderSASSD',
+        in_channels=4,
+        sparse_shape=[41, 1600, 1408],
+        order=('conv', 'norm', 'act')),
+    backbone=dict(
+        type='SECOND',
+        in_channels=256,
+        layer_nums=[5, 5],
+        layer_strides=[1, 2],
+        out_channels=[128, 256]),
+    neck=dict(
+        type='SECONDFPN',
+        in_channels=[128, 256],
+        upsample_strides=[1, 2],
+        out_channels=[256, 256]),
+    bbox_head=dict(
+        type='Anchor3DHead',
+        num_classes=3,
+        in_channels=512,
+        feat_channels=512,
+        use_direction_classifier=True,
+        anchor_generator=dict(
+            type='Anchor3DRangeGenerator',
+            ranges=[
+                [0, -40.0, -0.6, 70.4, 40.0, -0.6],
+                [0, -40.0, -0.6, 70.4, 40.0, -0.6],
+                [0, -40.0, -1.78, 70.4, 40.0, -1.78],
+            ],
+            sizes=[[0.8, 0.6, 1.73], [1.76, 0.6, 1.73], [3.9, 1.6, 1.56]],
+            rotations=[0, 1.57],
+            reshape_out=False),
+        diff_rad_by_sin=True,
+        bbox_coder=dict(type='DeltaXYZWLHRBBoxCoder'),
+        loss_cls=dict(
+            type='mmdet.FocalLoss',
+            use_sigmoid=True,
+            gamma=2.0,
+            alpha=0.25,
+            loss_weight=1.0),
+        loss_bbox=dict(
+            type='mmdet.SmoothL1Loss', beta=1.0 / 9.0, loss_weight=2.0),
+        loss_dir=dict(
+            type='mmdet.CrossEntropyLoss', use_sigmoid=False,
+            loss_weight=0.2)),
+    # model training and testing settings
+    train_cfg=dict(
+        assigner=[
+            dict(  # for Pedestrian
+                type='Max3DIoUAssigner',
+                iou_calculator=dict(type='BboxOverlapsNearest3D'),
+                pos_iou_thr=0.35,
+                neg_iou_thr=0.2,
+                min_pos_iou=0.2,
+                ignore_iof_thr=-1),
+            dict(  # for Cyclist
+                type='Max3DIoUAssigner',
+                iou_calculator=dict(type='BboxOverlapsNearest3D'),
+                pos_iou_thr=0.35,
+                neg_iou_thr=0.2,
+                min_pos_iou=0.2,
+                ignore_iof_thr=-1),
+            dict(  # for Car
+                type='Max3DIoUAssigner',
+                iou_calculator=dict(type='BboxOverlapsNearest3D'),
+                pos_iou_thr=0.6,
+                neg_iou_thr=0.45,
+                min_pos_iou=0.45,
+                ignore_iof_thr=-1),
+        ],
+        allowed_border=0,
+        pos_weight=-1,
+        debug=False),
+    test_cfg=dict(
+        use_rotate_nms=True,
+        nms_across_levels=False,
+        nms_thr=0.01,
+        score_thr=0.1,
+        min_bbox_size=0,
+        nms_pre=100,
+        max_num=50))
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/waymoD5-3d-3class.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/parta2/parta2_hv_secfpn_8xb2-cyclic-80e_kitti-3d-3class.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,71 +1,71 @@
-# dataset settings
-# D5 in the config name means the whole dataset is divided into 5 folds
-# We only use one fold for efficient experiments
-dataset_type = 'WaymoDataset'
-# data_root = 's3://openmmlab/datasets/detection3d/waymo/kitti_format/'
-data_root = 'data/waymo/kitti_format/'
-file_client_args = dict(backend='disk')
-# Uncomment the following if use ceph or other file clients.
-# See https://mmcv.readthedocs.io/en/latest/api.html#mmcv.fileio.FileClient
-# for more details.
-# file_client_args = dict(
-#     backend='petrel',
-#     path_mapping={
-#         './data/waymo': 's3://openmmlab/datasets/detection3d/waymo',
-#         'data/waymo': 's3://openmmlab/datasets/detection3d/waymo'
-#     })
+_base_ = [
+    '../_base_/schedules/cyclic-40e.py', '../_base_/default_runtime.py',
+    '../_base_/models/parta2.py'
+]
 
-class_names = ['Car', 'Pedestrian', 'Cyclist']
-metainfo = dict(classes=class_names)
+point_cloud_range = [0, -40, -3, 70.4, 40, 1]
 
-point_cloud_range = [-74.88, -74.88, -2, 74.88, 74.88, 4]
+# dataset settings
+dataset_type = 'KittiDataset'
+data_root = 'data/kitti/'
+class_names = ['Pedestrian', 'Cyclist', 'Car']
 input_modality = dict(use_lidar=True, use_camera=False)
+backend_args = None
 db_sampler = dict(
     data_root=data_root,
-    info_path=data_root + 'waymo_dbinfos_train.pkl',
+    info_path=data_root + 'kitti_dbinfos_train.pkl',
     rate=1.0,
     prepare=dict(
         filter_by_difficulty=[-1],
         filter_by_min_points=dict(Car=5, Pedestrian=10, Cyclist=10)),
     classes=class_names,
-    sample_groups=dict(Car=15, Pedestrian=10, Cyclist=10),
+    sample_groups=dict(Car=12, Pedestrian=6, Cyclist=6),
     points_loader=dict(
         type='LoadPointsFromFile',
         coord_type='LIDAR',
-        load_dim=6,
-        use_dim=[0, 1, 2, 3, 4]))
-
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
+    backend_args=backend_args)
 train_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=6, use_dim=5),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
     dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
-    # dict(type='ObjectSample', db_sampler=db_sampler),
+    dict(type='ObjectSample', db_sampler=db_sampler),
     dict(
-        type='RandomFlip3D',
-        sync_2d=False,
-        flip_ratio_bev_horizontal=0.5,
-        flip_ratio_bev_vertical=0.5),
+        type='ObjectNoise',
+        num_try=100,
+        translation_std=[1.0, 1.0, 0.5],
+        global_rot_range=[0.0, 0.0],
+        rot_range=[-0.78539816, 0.78539816]),
+    dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
     dict(
         type='GlobalRotScaleTrans',
         rot_range=[-0.78539816, 0.78539816],
         scale_ratio_range=[0.95, 1.05]),
     dict(type='PointsRangeFilter', point_cloud_range=point_cloud_range),
     dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
+    dict(type='ObjectNameFilter', classes=class_names),
     dict(type='PointShuffle'),
     dict(
         type='Pack3DDetInputs',
         keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
 ]
 test_pipeline = [
     dict(
         type='LoadPointsFromFile',
         coord_type='LIDAR',
-        load_dim=6,
-        use_dim=5,
-        file_client_args=file_client_args),
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
             dict(
@@ -78,83 +78,83 @@
                 type='PointsRangeFilter', point_cloud_range=point_cloud_range)
         ]),
     dict(type='Pack3DDetInputs', keys=['points'])
 ]
 # construct a pipeline for data and gt loading in show function
 # please keep its loading function consistent with test_pipeline (e.g. client)
 eval_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=6, use_dim=5),
-    dict(type='Pack3DDetInputs', keys=['points']),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
+    dict(type='Pack3DDetInputs', keys=['points'])
 ]
-
 train_dataloader = dict(
     batch_size=2,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type='RepeatDataset',
         times=2,
         dataset=dict(
             type=dataset_type,
             data_root=data_root,
-            ann_file='waymo_infos_train.pkl',
-            data_prefix=dict(
-                pts='training/velodyne', sweeps='training/velodyne'),
+            ann_file='kitti_infos_train.pkl',
+            data_prefix=dict(pts='training/velodyne_reduced'),
             pipeline=train_pipeline,
             modality=input_modality,
-            test_mode=False,
-            metainfo=metainfo,
-            # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
-            # and box_type_3d='Depth' in sunrgbd and scannet dataset.
+            metainfo=dict(classes=class_names),
             box_type_3d='LiDAR',
-            # load one frame every five frames
-            load_interval=5,
-            file_client_args=file_client_args)))
-val_dataloader = dict(
+            test_mode=False,
+            backend_args=backend_args)))
+test_dataloader = dict(
     batch_size=1,
     num_workers=1,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        data_prefix=dict(pts='training/velodyne', sweeps='training/velodyne'),
-        ann_file='waymo_infos_val.pkl',
-        pipeline=eval_pipeline,
+        ann_file='kitti_infos_val.pkl',
+        data_prefix=dict(pts='training/velodyne_reduced'),
+        pipeline=test_pipeline,
         modality=input_modality,
-        test_mode=True,
-        metainfo=metainfo,
+        metainfo=dict(classes=class_names),
         box_type_3d='LiDAR',
-        file_client_args=file_client_args))
-
-test_dataloader = dict(
+        test_mode=True,
+        backend_args=backend_args))
+val_dataloader = dict(
     batch_size=1,
     num_workers=1,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        data_prefix=dict(pts='training/velodyne', sweeps='training/velodyne'),
-        ann_file='waymo_infos_val.pkl',
+        ann_file='kitti_infos_val.pkl',
+        data_prefix=dict(pts='training/velodyne_reduced'),
         pipeline=eval_pipeline,
         modality=input_modality,
-        test_mode=True,
-        metainfo=metainfo,
+        metainfo=dict(classes=class_names),
         box_type_3d='LiDAR',
-        file_client_args=file_client_args))
-
+        test_mode=True,
+        backend_args=backend_args))
 val_evaluator = dict(
-    type='WaymoMetric',
-    ann_file='./data/waymo/kitti_format/waymo_infos_val.pkl',
-    waymo_bin_file='./data/waymo/waymo_format/gt.bin',
-    data_root='./data/waymo/waymo_format',
-    file_client_args=file_client_args,
-    convert_kitti_format=False)
+    type='KittiMetric',
+    ann_file=data_root + 'kitti_infos_val.pkl',
+    metric='bbox',
+    backend_args=backend_args)
 test_evaluator = val_evaluator
-
-vis_backends = [dict(type='LocalVisBackend')]
-visualizer = dict(
-    type='Det3DLocalVisualizer', vis_backends=vis_backends, name='visualizer')
+# Part-A2 uses a different learning rate from what SECOND uses.
+optim_wrapper = dict(optimizer=dict(lr=0.001))
+find_unused_parameters = True
+
+# Default setting for scaling LR automatically
+#   - `enable` means enable scaling LR automatically
+#       or not by default.
+#   - `base_batch_size` = (8 GPUs) x (2 samples per GPU).
+auto_scale_lr = dict(enable=False, base_batch_size=16)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/waymoD5-3d-car.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/s3dis-seg.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,144 +1,169 @@
-# dataset settings
-# D5 in the config name means the whole dataset is divided into 5 folds
-# We only use one fold for efficient experiments
-dataset_type = 'WaymoDataset'
-data_root = 'data/waymo/kitti_format/'
-file_client_args = dict(backend='disk')
-# Uncomment the following if use ceph or other file clients.
-# See https://mmcv.readthedocs.io/en/latest/api.html#mmcv.fileio.FileClient
-# for more details.
-# file_client_args = dict(
-#     backend='petrel', path_mapping=dict(data='s3://waymo_data/'))
-
-class_names = ['Car']
+# For S3DIS seg we usually do 13-class segmentation
+class_names = ('ceiling', 'floor', 'wall', 'beam', 'column', 'window', 'door',
+               'table', 'chair', 'sofa', 'bookcase', 'board', 'clutter')
 metainfo = dict(classes=class_names)
-
-point_cloud_range = [-74.88, -74.88, -2, 74.88, 74.88, 4]
+dataset_type = 'S3DISSegDataset'
+data_root = 'data/s3dis/'
 input_modality = dict(use_lidar=True, use_camera=False)
-db_sampler = dict(
-    data_root=data_root,
-    info_path=data_root + 'waymo_dbinfos_train.pkl',
-    rate=1.0,
-    prepare=dict(filter_by_difficulty=[-1], filter_by_min_points=dict(Car=5)),
-    classes=class_names,
-    sample_groups=dict(Car=15),
-    points_loader=dict(
+data_prefix = dict(
+    pts='points',
+    pts_instance_mask='instance_mask',
+    pts_semantic_mask='semantic_mask')
+
+# Example to use different file client
+# Method 1: simply set the data root and let the file I/O module
+# automatically infer from prefix (not support LMDB and Memcache yet)
+
+# data_root = 's3://openmmlab/datasets/detection3d/s3dis/'
+
+# Method 2: Use backend_args, file_client_args in versions before 1.1.0
+# backend_args = dict(
+#     backend='petrel',
+#     path_mapping=dict({
+#         './data/': 's3://openmmlab/datasets/detection3d/',
+#          'data/': 's3://openmmlab/datasets/detection3d/'
+#      }))
+backend_args = None
+
+num_points = 4096
+train_area = [1, 2, 3, 4, 6]
+test_area = 5
+train_pipeline = [
+    dict(
         type='LoadPointsFromFile',
-        coord_type='LIDAR',
+        coord_type='DEPTH',
+        shift_height=False,
+        use_color=True,
         load_dim=6,
-        use_dim=[0, 1, 2, 3, 4]))
-
-train_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=6, use_dim=5),
-    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
-    dict(type='ObjectSample', db_sampler=db_sampler),
-    dict(
-        type='RandomFlip3D',
-        sync_2d=False,
-        flip_ratio_bev_horizontal=0.5,
-        flip_ratio_bev_vertical=0.5),
-    dict(
-        type='GlobalRotScaleTrans',
-        rot_range=[-0.78539816, 0.78539816],
-        scale_ratio_range=[0.95, 1.05]),
-    dict(type='PointsRangeFilter', point_cloud_range=point_cloud_range),
-    dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
-    dict(type='PointShuffle'),
+        use_dim=[0, 1, 2, 3, 4, 5],
+        backend_args=backend_args),
+    dict(
+        type='LoadAnnotations3D',
+        with_bbox_3d=False,
+        with_label_3d=False,
+        with_mask_3d=False,
+        with_seg_3d=True,
+        backend_args=backend_args),
+    dict(type='PointSegClassMapping'),
     dict(
-        type='Pack3DDetInputs',
-        keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
+        type='IndoorPatchPointSample',
+        num_points=num_points,
+        block_size=1.0,
+        ignore_index=len(class_names),
+        use_normalized_coord=True,
+        enlarge_size=0.2,
+        min_unique_num=None),
+    dict(type='NormalizePointsColor', color_mean=None),
+    dict(type='Pack3DDetInputs', keys=['points', 'pts_semantic_mask'])
 ]
 test_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=6, use_dim=5),
     dict(
-        type='MultiScaleFlipAug3D',
-        img_scale=(1333, 800),
-        pts_scale_ratio=1,
-        flip=False,
-        transforms=[
-            dict(
-                type='GlobalRotScaleTrans',
-                rot_range=[0, 0],
-                scale_ratio_range=[1., 1.],
-                translation_std=[0, 0, 0]),
-            dict(type='RandomFlip3D'),
-            dict(
-                type='PointsRangeFilter', point_cloud_range=point_cloud_range)
-        ]),
+        type='LoadPointsFromFile',
+        coord_type='DEPTH',
+        shift_height=False,
+        use_color=True,
+        load_dim=6,
+        use_dim=[0, 1, 2, 3, 4, 5],
+        backend_args=backend_args),
+    dict(
+        type='LoadAnnotations3D',
+        with_bbox_3d=False,
+        with_label_3d=False,
+        with_mask_3d=False,
+        with_seg_3d=True,
+        backend_args=backend_args),
+    dict(type='NormalizePointsColor', color_mean=None),
     dict(type='Pack3DDetInputs', keys=['points'])
 ]
 # construct a pipeline for data and gt loading in show function
 # please keep its loading function consistent with test_pipeline (e.g. client)
+# we need to load gt seg_mask!
 eval_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=6, use_dim=5),
-    dict(type='Pack3DDetInputs', keys=['points']),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='DEPTH',
+        shift_height=False,
+        use_color=True,
+        load_dim=6,
+        use_dim=[0, 1, 2, 3, 4, 5],
+        backend_args=backend_args),
+    dict(type='NormalizePointsColor', color_mean=None),
+    dict(type='Pack3DDetInputs', keys=['points'])
+]
+tta_pipeline = [
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='DEPTH',
+        shift_height=False,
+        use_color=True,
+        load_dim=6,
+        use_dim=[0, 1, 2, 3, 4, 5],
+        backend_args=backend_args),
+    dict(
+        type='LoadAnnotations3D',
+        with_bbox_3d=False,
+        with_label_3d=False,
+        with_mask_3d=False,
+        with_seg_3d=True,
+        backend_args=backend_args),
+    dict(type='NormalizePointsColor', color_mean=None),
+    dict(
+        type='TestTimeAug',
+        transforms=[[
+            dict(
+                type='RandomFlip3D',
+                sync_2d=False,
+                flip_ratio_bev_horizontal=0.,
+                flip_ratio_bev_vertical=0.)
+        ], [dict(type='Pack3DDetInputs', keys=['points'])]])
 ]
 
+# train on area 1, 2, 3, 4, 6
+# test on area 5
 train_dataloader = dict(
-    batch_size=2,
-    num_workers=2,
+    batch_size=8,
+    num_workers=4,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
-        type='RepeatDataset',
-        times=2,
-        dataset=dict(
-            type=dataset_type,
-            data_root=data_root,
-            ann_file='waymo_infos_train.pkl',
-            data_prefix=dict(
-                pts='training/velodyne', sweeps='training/velodyne'),
-            pipeline=train_pipeline,
-            modality=input_modality,
-            test_mode=False,
-            metainfo=metainfo,
-            # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
-            # and box_type_3d='Depth' in sunrgbd and scannet dataset.
-            box_type_3d='LiDAR',
-            # load one frame every five frames
-            load_interval=5)))
-val_dataloader = dict(
-    batch_size=1,
-    num_workers=1,
-    persistent_workers=True,
-    drop_last=False,
-    sampler=dict(type='DefaultSampler', shuffle=False),
-    dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        data_prefix=dict(pts='training/velodyne', sweeps='training/velodyne'),
-        ann_file='waymo_infos_val.pkl',
-        pipeline=eval_pipeline,
-        modality=input_modality,
-        test_mode=True,
+        ann_files=[f's3dis_infos_Area_{i}.pkl' for i in train_area],
         metainfo=metainfo,
-        box_type_3d='LiDAR'))
-
+        data_prefix=data_prefix,
+        pipeline=train_pipeline,
+        modality=input_modality,
+        ignore_index=len(class_names),
+        scene_idxs=[
+            f'seg_info/Area_{i}_resampled_scene_idxs.npy' for i in train_area
+        ],
+        test_mode=False,
+        backend_args=backend_args))
 test_dataloader = dict(
     batch_size=1,
     num_workers=1,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        data_prefix=dict(pts='training/velodyne', sweeps='training/velodyne'),
-        ann_file='waymo_infos_val.pkl',
-        pipeline=eval_pipeline,
+        ann_files=f's3dis_infos_Area_{test_area}.pkl',
+        metainfo=metainfo,
+        data_prefix=data_prefix,
+        pipeline=test_pipeline,
         modality=input_modality,
+        ignore_index=len(class_names),
+        scene_idxs=f'seg_info/Area_{test_area}_resampled_scene_idxs.npy',
         test_mode=True,
-        metainfo=metainfo,
-        box_type_3d='LiDAR'))
+        backend_args=backend_args))
+val_dataloader = test_dataloader
 
-val_evaluator = dict(
-    type='WaymoMetric',
-    ann_file='./data/waymo/kitti_format/waymo_infos_val.pkl',
-    waymo_bin_file='./data/waymo/waymo_format/gt.bin',
-    data_root='./data/waymo/waymo_format',
-    convert_kitti_format=False)
+val_evaluator = dict(type='SegMetric')
 test_evaluator = val_evaluator
 
 vis_backends = [dict(type='LocalVisBackend')]
 visualizer = dict(
     type='Det3DLocalVisualizer', vis_backends=vis_backends, name='visualizer')
+
+tta_model = dict(type='Seg3DTTAModel')
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/waymoD5-fov-mono3d-3class.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/kitti-3d-car.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,150 +1,165 @@
 # dataset settings
-# D3 in the config name means the whole dataset is divided into 3 folds
-# We only use one fold for efficient experiments
-dataset_type = 'WaymoDataset'
-data_root = 'data/waymo/kitti_format/'
-class_names = ['Car', 'Pedestrian', 'Cyclist']
-input_modality = dict(use_lidar=False, use_camera=True)
-file_client_args = dict(backend='disk')
-# Uncomment the following if use ceph or other file clients.
-# See https://mmcv.readthedocs.io/en/latest/api.html#mmcv.fileio.FileClient
-# for more details.
+dataset_type = 'KittiDataset'
+data_root = 'data/kitti/'
+class_names = ['Car']
+point_cloud_range = [0, -40, -3, 70.4, 40, 1]
+input_modality = dict(use_lidar=True, use_camera=False)
+metainfo = dict(classes=class_names)
+
+# Example to use different file client
+# Method 1: simply set the data root and let the file I/O module
+# automatically infer from prefix (not support LMDB and Memcache yet)
+
+# data_root = 's3://openmmlab/datasets/detection3d/kitti/'
+
+# Method 2: Use backend_args, file_client_args in versions before 1.1.0
+# backend_args = dict(
+#     backend='petrel',
+#     path_mapping=dict({
+#         './data/': 's3://openmmlab/datasets/detection3d/',
+#          'data/': 's3://openmmlab/datasets/detection3d/'
+#      }))
+backend_args = None
+
+db_sampler = dict(
+    data_root=data_root,
+    info_path=data_root + 'kitti_dbinfos_train.pkl',
+    rate=1.0,
+    prepare=dict(filter_by_difficulty=[-1], filter_by_min_points=dict(Car=5)),
+    classes=class_names,
+    sample_groups=dict(Car=15),
+    points_loader=dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
+    backend_args=backend_args)
+
 train_pipeline = [
-    dict(type='LoadImageFromFileMono3D'),
     dict(
-        type='LoadAnnotations3D',
-        with_bbox=True,
-        with_label=True,
-        with_attr_label=False,
-        with_bbox_3d=True,
-        with_label_3d=True,
-        with_bbox_depth=True),
-    # base shape (1248, 832), scale (0.95, 1.05)
-    dict(
-        type='RandomResize3D',
-        scale=(1284, 832),
-        ratio_range=(0.95, 1.05),
-        keep_ratio=True,
-    ),
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=4,  # x, y, z, intensity
+        use_dim=4,
+        backend_args=backend_args),
+    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
+    dict(type='ObjectSample', db_sampler=db_sampler),
+    dict(
+        type='ObjectNoise',
+        num_try=100,
+        translation_std=[1.0, 1.0, 0.5],
+        global_rot_range=[0.0, 0.0],
+        rot_range=[-0.78539816, 0.78539816]),
     dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
     dict(
+        type='GlobalRotScaleTrans',
+        rot_range=[-0.78539816, 0.78539816],
+        scale_ratio_range=[0.95, 1.05]),
+    dict(type='PointsRangeFilter', point_cloud_range=point_cloud_range),
+    dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
+    dict(type='PointShuffle'),
+    dict(
         type='Pack3DDetInputs',
-        keys=[
-            'img', 'gt_bboxes', 'gt_bboxes_labels', 'gt_bboxes_3d',
-            'gt_labels_3d', 'centers_2d', 'depths'
-        ]),
+        keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
 ]
-
 test_pipeline = [
-    dict(type='LoadImageFromFileMono3D'),
     dict(
-        type='RandomResize3D',
-        scale=(1248, 832),
-        ratio_range=(1., 1.),
-        keep_ratio=True),
-    dict(type='Pack3DDetInputs', keys=['img']),
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
+    dict(
+        type='MultiScaleFlipAug3D',
+        img_scale=(1333, 800),
+        pts_scale_ratio=1,
+        flip=False,
+        transforms=[
+            dict(
+                type='GlobalRotScaleTrans',
+                rot_range=[0, 0],
+                scale_ratio_range=[1., 1.],
+                translation_std=[0, 0, 0]),
+            dict(type='RandomFlip3D'),
+            dict(
+                type='PointsRangeFilter', point_cloud_range=point_cloud_range)
+        ]),
+    dict(type='Pack3DDetInputs', keys=['points'])
 ]
 # construct a pipeline for data and gt loading in show function
 # please keep its loading function consistent with test_pipeline (e.g. client)
 eval_pipeline = [
-    dict(type='LoadImageFromFileMono3D'),
     dict(
-        type='RandomResize3D',
-        scale=(1248, 832),
-        ratio_range=(1., 1.),
-        keep_ratio=True),
-    dict(type='Pack3DDetInputs', keys=['img']),
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
+    dict(type='Pack3DDetInputs', keys=['points'])
 ]
-
-metainfo = dict(CLASSES=class_names)
-
 train_dataloader = dict(
-    batch_size=3,
-    num_workers=3,
+    batch_size=6,
+    num_workers=4,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
-        type=dataset_type,
-        data_root=data_root,
-        ann_file='waymo_infos_train.pkl',
-        data_prefix=dict(
-            pts='training/velodyne',
-            CAM_FRONT='training/image_0',
-            CAM_FRONT_RIGHT='training/image_1',
-            CAM_FRONT_LEFT='training/image_2',
-            CAM_SIDE_RIGHT='training/image_3',
-            CAM_SIDE_LEFT='training/image_4'),
-        pipeline=train_pipeline,
-        modality=input_modality,
-        test_mode=False,
-        metainfo=metainfo,
-        # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
-        # and box_type_3d='Depth' in sunrgbd and scannet dataset.
-        box_type_3d='Camera',
-        load_type='fov_image_based',
-        # load one frame every three frames
-        load_interval=5))
-
+        type='RepeatDataset',
+        times=2,
+        dataset=dict(
+            type=dataset_type,
+            data_root=data_root,
+            ann_file='kitti_infos_train.pkl',
+            data_prefix=dict(pts='training/velodyne_reduced'),
+            pipeline=train_pipeline,
+            modality=input_modality,
+            test_mode=False,
+            metainfo=metainfo,
+            # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
+            # and box_type_3d='Depth' in sunrgbd and scannet dataset.
+            box_type_3d='LiDAR',
+            backend_args=backend_args)))
 val_dataloader = dict(
     batch_size=1,
     num_workers=1,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        data_prefix=dict(
-            pts='training/velodyne',
-            CAM_FRONT='training/image_0',
-            CAM_FRONT_RIGHT='training/image_1',
-            CAM_FRONT_LEFT='training/image_2',
-            CAM_SIDE_RIGHT='training/image_3',
-            CAM_SIDE_LEFT='training/image_4'),
-        ann_file='waymo_infos_val.pkl',
-        pipeline=eval_pipeline,
+        data_prefix=dict(pts='training/velodyne_reduced'),
+        ann_file='kitti_infos_val.pkl',
+        pipeline=test_pipeline,
         modality=input_modality,
         test_mode=True,
         metainfo=metainfo,
-        # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
-        # and box_type_3d='Depth' in sunrgbd and scannet dataset.
-        box_type_3d='Camera',
-        load_type='fov_image_based',
-    ))
-
+        box_type_3d='LiDAR',
+        backend_args=backend_args))
 test_dataloader = dict(
     batch_size=1,
     num_workers=1,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        data_prefix=dict(
-            pts='training/velodyne',
-            CAM_FRONT='training/image_0',
-            CAM_FRONT_RIGHT='training/image_1',
-            CAM_FRONT_LEFT='training/image_2',
-            CAM_SIDE_RIGHT='training/image_3',
-            CAM_SIDE_LEFT='training/image_4'),
-        ann_file='waymo_infos_val.pkl',
-        pipeline=eval_pipeline,
+        data_prefix=dict(pts='training/velodyne_reduced'),
+        ann_file='kitti_infos_val.pkl',
+        pipeline=test_pipeline,
         modality=input_modality,
         test_mode=True,
         metainfo=metainfo,
-        # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
-        # and box_type_3d='Depth' in sunrgbd and scannet dataset.
-        box_type_3d='Camera',
-        load_type='fov_image_based',
-    ))
-
+        box_type_3d='LiDAR',
+        backend_args=backend_args))
 val_evaluator = dict(
-    type='WaymoMetric',
-    ann_file='./data/waymo/kitti_format/waymo_infos_val.pkl',
-    waymo_bin_file='./data/waymo/waymo_format/fov_gt.bin',
-    data_root='./data/waymo/waymo_format',
-    metric='LET_mAP',
-    load_type='fov_image_based',
-)
+    type='KittiMetric',
+    ann_file=data_root + 'kitti_infos_val.pkl',
+    metric='bbox',
+    backend_args=backend_args)
 test_evaluator = val_evaluator
+
+vis_backends = [dict(type='LocalVisBackend')]
+visualizer = dict(
+    type='Det3DLocalVisualizer', vis_backends=vis_backends, name='visualizer')
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/datasets/waymoD5-mv3d-3class.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/waymoD5-mv3d-3class.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,16 +1,27 @@
 # dataset settings
 # D3 in the config name means the whole dataset is divided into 3 folds
 # We only use one fold for efficient experiments
 dataset_type = 'WaymoDataset'
 data_root = 'data/waymo/kitti_format/'
-file_client_args = dict(backend='disk')
-# Uncomment the following if use ceph or other file clients.
-# See https://mmcv.readthedocs.io/en/latest/api.html#mmcv.fileio.FileClient
-# for more details.
+
+# Example to use different file client
+# Method 1: simply set the data root and let the file I/O module
+# automatically infer from prefix (not support LMDB and Memcache yet)
+
+# data_root = 's3://openmmlab/datasets/detection3d/waymo/kitti_format/'
+
+# Method 2: Use backend_args, file_client_args in versions before 1.1.0
+# backend_args = dict(
+#     backend='petrel',
+#     path_mapping=dict({
+#         './data/': 's3://openmmlab/datasets/detection3d/',
+#          'data/': 's3://openmmlab/datasets/detection3d/'
+#      }))
+backend_args = None
 
 class_names = ['Car', 'Pedestrian', 'Cyclist']
 input_modality = dict(use_lidar=False, use_camera=True)
 point_cloud_range = [-35.0, -75.0, -2, 75.0, 75.0, 4]
 
 train_transforms = [
     dict(type='PhotoMetricDistortion3D'),
@@ -20,15 +31,18 @@
         ratio_range=(0.95, 1.05),
         keep_ratio=True),
     dict(type='RandomCrop3D', crop_size=(720, 1080)),
     dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5, flip_box3d=False),
 ]
 
 train_pipeline = [
-    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
+    dict(
+        type='LoadMultiViewImageFromFiles',
+        to_float32=True,
+        backend_args=backend_args),
     dict(
         type='LoadAnnotations3D',
         with_bbox=True,
         with_label=True,
         with_attr_label=False,
         with_bbox_3d=True,
         with_label_3d=True,
@@ -47,22 +61,28 @@
     dict(
         type='RandomResize3D',
         scale=(1248, 832),
         ratio_range=(1., 1.),
         keep_ratio=True)
 ]
 test_pipeline = [
-    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
+    dict(
+        type='LoadMultiViewImageFromFiles',
+        to_float32=True,
+        backend_args=backend_args),
     dict(type='MultiViewWrapper', transforms=test_transforms),
     dict(type='Pack3DDetInputs', keys=['img'])
 ]
 # construct a pipeline for data and gt loading in show function
 # please keep its loading function consistent with test_pipeline (e.g. client)
 eval_pipeline = [
-    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
+    dict(
+        type='LoadMultiViewImageFromFiles',
+        to_float32=True,
+        backend_args=backend_args),
     dict(type='MultiViewWrapper', transforms=test_transforms),
     dict(type='Pack3DDetInputs', keys=['img'])
 ]
 metainfo = dict(classes=class_names)
 
 train_dataloader = dict(
     batch_size=2,
@@ -72,77 +92,75 @@
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         ann_file='waymo_infos_train.pkl',
         data_prefix=dict(
             pts='training/velodyne',
             CAM_FRONT='training/image_0',
-            CAM_FRONT_RIGHT='training/image_1',
-            CAM_FRONT_LEFT='training/image_2',
-            CAM_SIDE_RIGHT='training/image_3',
-            CAM_SIDE_LEFT='training/image_4',
-        ),
+            CAM_FRONT_LEFT='training/image_1',
+            CAM_FRONT_RIGHT='training/image_2',
+            CAM_SIDE_LEFT='training/image_3',
+            CAM_SIDE_RIGHT='training/image_4'),
         pipeline=train_pipeline,
         modality=input_modality,
         test_mode=False,
         metainfo=metainfo,
         box_type_3d='Lidar',
         load_interval=5,
-    ))
+        backend_args=backend_args))
 
 val_dataloader = dict(
     batch_size=1,
     num_workers=1,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         ann_file='waymo_infos_val.pkl',
         data_prefix=dict(
             pts='training/velodyne',
             CAM_FRONT='training/image_0',
-            CAM_FRONT_RIGHT='training/image_1',
-            CAM_FRONT_LEFT='training/image_2',
-            CAM_SIDE_RIGHT='training/image_3',
-            CAM_SIDE_LEFT='training/image_4',
-        ),
+            CAM_FRONT_LEFT='training/image_1',
+            CAM_FRONT_RIGHT='training/image_2',
+            CAM_SIDE_LEFT='training/image_3',
+            CAM_SIDE_RIGHT='training/image_4'),
         pipeline=eval_pipeline,
         modality=input_modality,
         test_mode=True,
         metainfo=metainfo,
         box_type_3d='Lidar',
-    ))
+        backend_args=backend_args))
 
 test_dataloader = dict(
     batch_size=1,
     num_workers=1,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         ann_file='waymo_infos_val.pkl',
         data_prefix=dict(
             pts='training/velodyne',
             CAM_FRONT='training/image_0',
-            CAM_FRONT_RIGHT='training/image_1',
-            CAM_FRONT_LEFT='training/image_2',
-            CAM_SIDE_RIGHT='training/image_3',
-            CAM_SIDE_LEFT='training/image_4',
-        ),
+            CAM_FRONT_LEFT='training/image_1',
+            CAM_FRONT_RIGHT='training/image_2',
+            CAM_SIDE_LEFT='training/image_3',
+            CAM_SIDE_RIGHT='training/image_4'),
         pipeline=eval_pipeline,
         modality=input_modality,
         test_mode=True,
         metainfo=metainfo,
         box_type_3d='Lidar',
-    ))
+        backend_args=backend_args))
 val_evaluator = dict(
     type='WaymoMetric',
     ann_file='./data/waymo/kitti_format/waymo_infos_val.pkl',
     waymo_bin_file='./data/waymo/waymo_format/cam_gt.bin',
     data_root='./data/waymo/waymo_format',
-    metric='LET_mAP')
+    metric='LET_mAP',
+    backend_args=backend_args)
 
 test_evaluator = val_evaluator
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/default_runtime.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/default_runtime.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/3dssd.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/3dssd.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/cascade-mask-rcnn_r50_fpn.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/cascade-mask-rcnn_r50_fpn.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/centerpoint_pillar02_second_secfpn_nus.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/centerpoint_pillar02_second_secfpn_nus.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/centerpoint_voxel01_second_secfpn_nus.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/centerpoint_voxel01_second_secfpn_nus.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/dgcnn.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/dgcnn.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/fcaf3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/fcaf3d.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/fcos3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/fcos3d.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/groupfree3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/groupfree3d.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/h3dnet.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/h3dnet.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/imvotenet.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/imvotenet.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/mask-rcnn_r50_fpn.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/mask-rcnn_r50_fpn.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/multiview_dfm.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/multiview_dfm.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/paconv_ssg.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/paconv_ssg.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/parta2.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/parta2.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/pgd.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/pgd.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/point_rcnn.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/point_rcnn.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/pointnet2_msg.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/pointnet2_msg.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/pointnet2_ssg.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/pointnet2_ssg.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/pointpillars_hv_fpn_lyft.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/pointpillars_hv_fpn_lyft.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/pointpillars_hv_fpn_nus.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/pointpillars_hv_fpn_nus.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/pointpillars_hv_fpn_range100_lyft.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/pointpillars_hv_fpn_range100_lyft.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/pointpillars_hv_secfpn_kitti.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/pointpillars_hv_secfpn_kitti.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/pointpillars_hv_secfpn_waymo.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/pointpillars_hv_secfpn_waymo.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/second_hv_secfpn_kitti.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/second_hv_secfpn_kitti.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/second_hv_secfpn_waymo.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/second_hv_secfpn_waymo.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/smoke.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/smoke.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/models/votenet.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/models/votenet.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/schedules/cosine.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/schedules/cosine.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/schedules/cyclic-20e.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/schedules/cyclic-20e.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/schedules/cyclic-40e.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/schedules/cyclic-40e.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/schedules/mmdet-schedule-1x.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/schedules/mmdet-schedule-1x.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/schedules/schedule-2x.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/schedules/schedule-2x.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/schedules/schedule-3x.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/schedules/schedule-3x.py`

 * *Files 1% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 lr = 0.008  # max learning rate
 optim_wrapper = dict(
     type='OptimWrapper',
     optimizer=dict(type='AdamW', lr=lr, weight_decay=0.01),
     clip_grad=dict(max_norm=10, norm_type=2),
 )
 
-# training schedule for 1x
+# training schedule for 3x
 train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=36, val_interval=1)
 val_cfg = dict(type='ValLoop')
 test_cfg = dict(type='TestLoop')
 
 # learning rate
 param_scheduler = [
     dict(
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/schedules/seg-cosine-100e.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/schedules/seg-cosine-100e.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/schedules/seg-cosine-150e.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/schedules/seg-cosine-150e.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/schedules/seg-cosine-200e.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/schedules/seg-cosine-200e.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/_base_/schedules/seg-cosine-50e.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/schedules/seg-cosine-50e.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/benchmark/hv_PartA2_secfpn_4x8_cyclic_80e_pcdet_kitti-3d-3class.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/benchmark/hv_PartA2_secfpn_4x8_cyclic_80e_pcdet_kitti-3d-3class.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/benchmark/hv_pointpillars_secfpn_3x8_100e_det3d_kitti-3d-car.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/benchmark/hv_pointpillars_secfpn_3x8_100e_det3d_kitti-3d-car.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/benchmark/hv_pointpillars_secfpn_4x8_80e_pcdet_kitti-3d-3class.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/benchmark/hv_pointpillars_secfpn_4x8_80e_pcdet_kitti-3d-3class.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/benchmark/hv_second_secfpn_4x8_80e_pcdet_kitti-3d-3class.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/benchmark/hv_second_secfpn_4x8_80e_pcdet_kitti-3d-3class.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/centerpoint/centerpoint_pillar02_second_secfpn_8xb4-cyclic-20e_nus-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/centerpoint/centerpoint_pillar02_second_secfpn_8xb4-cyclic-20e_nus-3d.py`

 * *Files 10% similar despite different names*

```diff
@@ -23,15 +23,15 @@
     pts_bbox_head=dict(bbox_coder=dict(pc_range=point_cloud_range[:2])),
     # model training and testing settings
     train_cfg=dict(pts=dict(point_cloud_range=point_cloud_range)),
     test_cfg=dict(pts=dict(pc_range=point_cloud_range[:2])))
 
 dataset_type = 'NuScenesDataset'
 data_root = 'data/nuscenes/'
-file_client_args = dict(backend='disk')
+backend_args = None
 
 db_sampler = dict(
     data_root=data_root,
     info_path=data_root + 'nuscenes_dbinfos_train.pkl',
     rate=1.0,
     prepare=dict(
         filter_by_difficulty=[-1],
@@ -58,24 +58,32 @@
         bicycle=6,
         pedestrian=2,
         traffic_cone=2),
     points_loader=dict(
         type='LoadPointsFromFile',
         coord_type='LIDAR',
         load_dim=5,
-        use_dim=[0, 1, 2, 3, 4]))
+        use_dim=[0, 1, 2, 3, 4],
+        backend_args=backend_args),
+    backend_args=backend_args)
 
 train_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=5, use_dim=5),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=5,
+        use_dim=5,
+        backend_args=backend_args),
     dict(
         type='LoadPointsFromMultiSweeps',
         sweeps_num=9,
         use_dim=[0, 1, 2, 3, 4],
         pad_empty_sweeps=True,
-        remove_close=True),
+        remove_close=True,
+        backend_args=backend_args),
     dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
     dict(type='ObjectSample', db_sampler=db_sampler),
     dict(
         type='GlobalRotScaleTrans',
         rot_range=[-0.3925, 0.3925],
         scale_ratio_range=[0.95, 1.05],
         translation_std=[0, 0, 0]),
@@ -89,21 +97,27 @@
     dict(type='ObjectNameFilter', classes=class_names),
     dict(type='PointShuffle'),
     dict(
         type='Pack3DDetInputs',
         keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
 ]
 test_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=5, use_dim=5),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=5,
+        use_dim=5,
+        backend_args=backend_args),
     dict(
         type='LoadPointsFromMultiSweeps',
         sweeps_num=9,
         use_dim=[0, 1, 2, 3, 4],
         pad_empty_sweeps=True,
-        remove_close=True),
+        remove_close=True,
+        backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
             dict(
@@ -131,14 +145,15 @@
             pipeline=train_pipeline,
             metainfo=dict(classes=class_names),
             test_mode=False,
             data_prefix=data_prefix,
             use_valid_flag=True,
             # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
             # and box_type_3d='Depth' in sunrgbd and scannet dataset.
-            box_type_3d='LiDAR')))
+            box_type_3d='LiDAR',
+            backend_args=backend_args)))
 test_dataloader = dict(
     dataset=dict(pipeline=test_pipeline, metainfo=dict(classes=class_names)))
 val_dataloader = dict(
     dataset=dict(pipeline=test_pipeline, metainfo=dict(classes=class_names)))
 
 train_cfg = dict(val_interval=20)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel0075_second_secfpn_8xb4-cyclic-20e_nus-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/parta2/parta2_hv_secfpn_8xb2-cyclic-80e_kitti-3d-car.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,131 +1,154 @@
-_base_ = ['./centerpoint_voxel01_second_secfpn_8xb4-cyclic-20e_nus-3d.py']
+_base_ = './parta2_hv_secfpn_8xb2-cyclic-80e_kitti-3d-3class.py'
+
+point_cloud_range = [0, -40, -3, 70.4, 40, 1]  # velodyne coordinates, x, y, z
 
-# If point cloud range is changed, the models should also change their point
-# cloud range accordingly
-voxel_size = [0.075, 0.075, 0.2]
-point_cloud_range = [-54, -54, -5.0, 54, 54, 3.0]
-# Using calibration info convert the Lidar-coordinate point cloud range to the
-# ego-coordinate point cloud range could bring a little promotion in nuScenes.
-# point_cloud_range = [-54, -54.8, -5.0, 54, 53.2, 3.0]
-# For nuScenes we usually do 10-class detection
-class_names = [
-    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
-    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
-]
-data_prefix = dict(pts='samples/LIDAR_TOP', img='', sweeps='sweeps/LIDAR_TOP')
 model = dict(
-    data_preprocessor=dict(
-        voxel_layer=dict(
-            voxel_size=voxel_size, point_cloud_range=point_cloud_range)),
-    pts_middle_encoder=dict(sparse_shape=[41, 1440, 1440]),
-    pts_bbox_head=dict(
-        bbox_coder=dict(
-            voxel_size=voxel_size[:2], pc_range=point_cloud_range[:2])),
+    rpn_head=dict(
+        type='PartA2RPNHead',
+        num_classes=1,
+        anchor_generator=dict(
+            _delete_=True,
+            type='Anchor3DRangeGenerator',
+            ranges=[[0, -40.0, -1.78, 70.4, 40.0, -1.78]],
+            sizes=[[3.9, 1.6, 1.56]],
+            rotations=[0, 1.57],
+            reshape_out=False)),
+    roi_head=dict(
+        num_classes=1,
+        semantic_head=dict(num_classes=1),
+        bbox_head=dict(num_classes=1)),
+    # model training and testing settings
     train_cfg=dict(
-        pts=dict(
-            grid_size=[1440, 1440, 40],
-            voxel_size=voxel_size,
-            point_cloud_range=point_cloud_range)),
+        _delete_=True,
+        rpn=dict(
+            assigner=dict(
+                type='Max3DIoUAssigner',
+                iou_calculator=dict(type='BboxOverlapsNearest3D'),
+                pos_iou_thr=0.6,
+                neg_iou_thr=0.45,
+                min_pos_iou=0.45,
+                ignore_iof_thr=-1),
+            allowed_border=0,
+            pos_weight=-1,
+            debug=False),
+        rpn_proposal=dict(
+            nms_pre=9000,
+            nms_post=512,
+            max_num=512,
+            nms_thr=0.8,
+            score_thr=0,
+            use_rotate_nms=False),
+        rcnn=dict(
+            assigner=dict(  # for Car
+                type='Max3DIoUAssigner',
+                iou_calculator=dict(type='BboxOverlaps3D', coordinate='lidar'),
+                pos_iou_thr=0.55,
+                neg_iou_thr=0.55,
+                min_pos_iou=0.55,
+                ignore_iof_thr=-1),
+            sampler=dict(
+                type='IoUNegPiecewiseSampler',
+                num=128,
+                pos_fraction=0.55,
+                neg_piece_fractions=[0.8, 0.2],
+                neg_iou_piece_thrs=[0.55, 0.1],
+                neg_pos_ub=-1,
+                add_gt_as_proposals=False,
+                return_iou=True),
+            cls_pos_thr=0.75,
+            cls_neg_thr=0.25)),
     test_cfg=dict(
-        pts=dict(voxel_size=voxel_size[:2], pc_range=point_cloud_range[:2])))
-
-dataset_type = 'NuScenesDataset'
-data_root = 'data/nuscenes/'
-file_client_args = dict(backend='disk')
+        rpn=dict(
+            nms_pre=1024,
+            nms_post=100,
+            max_num=100,
+            nms_thr=0.7,
+            score_thr=0,
+            use_rotate_nms=True),
+        rcnn=dict(
+            use_rotate_nms=True,
+            use_raw_score=True,
+            nms_thr=0.01,
+            score_thr=0.1)))
 
+# dataset settings
+dataset_type = 'KittiDataset'
+data_root = 'data/kitti/'
+class_names = ['Car']
+input_modality = dict(use_lidar=True, use_camera=False)
+backend_args = None
 db_sampler = dict(
     data_root=data_root,
-    info_path=data_root + 'nuscenes_dbinfos_train.pkl',
+    info_path=data_root + 'kitti_dbinfos_train.pkl',
     rate=1.0,
-    prepare=dict(
-        filter_by_difficulty=[-1],
-        filter_by_min_points=dict(
-            car=5,
-            truck=5,
-            bus=5,
-            trailer=5,
-            construction_vehicle=5,
-            traffic_cone=5,
-            barrier=5,
-            motorcycle=5,
-            bicycle=5,
-            pedestrian=5)),
+    prepare=dict(filter_by_difficulty=[-1], filter_by_min_points=dict(Car=5)),
     classes=class_names,
-    sample_groups=dict(
-        car=2,
-        truck=3,
-        construction_vehicle=7,
-        bus=4,
-        trailer=6,
-        barrier=2,
-        motorcycle=6,
-        bicycle=6,
-        pedestrian=2,
-        traffic_cone=2),
+    sample_groups=dict(Car=15),
     points_loader=dict(
         type='LoadPointsFromFile',
         coord_type='LIDAR',
-        load_dim=5,
-        use_dim=[0, 1, 2, 3, 4]))
-
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
+    backend_args=backend_args)
 train_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=5, use_dim=5),
     dict(
-        type='LoadPointsFromMultiSweeps',
-        sweeps_num=9,
-        use_dim=[0, 1, 2, 3, 4],
-        pad_empty_sweeps=True,
-        remove_close=True),
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
     dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
     dict(type='ObjectSample', db_sampler=db_sampler),
     dict(
+        type='ObjectNoise',
+        num_try=100,
+        translation_std=[1.0, 1.0, 0.5],
+        global_rot_range=[0.0, 0.0],
+        rot_range=[-0.78539816, 0.78539816]),
+    dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
+    dict(
         type='GlobalRotScaleTrans',
-        rot_range=[-0.3925, 0.3925],
-        scale_ratio_range=[0.95, 1.05],
-        translation_std=[0, 0, 0]),
-    dict(
-        type='RandomFlip3D',
-        sync_2d=False,
-        flip_ratio_bev_horizontal=0.5,
-        flip_ratio_bev_vertical=0.5),
+        rot_range=[-0.78539816, 0.78539816],
+        scale_ratio_range=[0.95, 1.05]),
     dict(type='PointsRangeFilter', point_cloud_range=point_cloud_range),
     dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
     dict(type='ObjectNameFilter', classes=class_names),
     dict(type='PointShuffle'),
     dict(
         type='Pack3DDetInputs',
         keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
 ]
 test_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=5, use_dim=5),
     dict(
-        type='LoadPointsFromMultiSweeps',
-        sweeps_num=9,
-        use_dim=[0, 1, 2, 3, 4],
-        pad_empty_sweeps=True,
-        remove_close=True),
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
             dict(
                 type='GlobalRotScaleTrans',
                 rot_range=[0, 0],
                 scale_ratio_range=[1., 1.],
                 translation_std=[0, 0, 0]),
             dict(type='RandomFlip3D'),
             dict(
-                type='PointsRangeFilter', point_cloud_range=point_cloud_range)
+                type='PointsRangeFilter', point_cloud_range=point_cloud_range),
         ]),
     dict(type='Pack3DDetInputs', keys=['points'])
 ]
+
 train_dataloader = dict(
     dataset=dict(
         dataset=dict(
             pipeline=train_pipeline, metainfo=dict(classes=class_names))))
 test_dataloader = dict(
     dataset=dict(pipeline=test_pipeline, metainfo=dict(classes=class_names)))
-val_dataloader = dict(
-    dataset=dict(pipeline=test_pipeline, metainfo=dict(classes=class_names)))
+val_dataloader = dict(dataset=dict(metainfo=dict(classes=class_names)))
+find_unused_parameters = True
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel0075_second_secfpn_head-dcn-circlenms_8xb4-flip-tta-cyclic-20e_nus-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel0075_second_secfpn_head-dcn_8xb4-tta-cyclic-20e_nus-3d.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,44 +1,52 @@
-_base_ = './centerpoint_voxel0075_second_secfpn_' \
-         'head-dcn-circlenms_8xb4_cyclic-20e_nus-3d.py'
+_base_ = \
+    './centerpoint_voxel0075_second_secfpn_head-dcn_8xb4-cyclic-20e_nus-3d.py'
+
+model = dict(test_cfg=dict(pts=dict(use_rotate_nms=True, max_num=500)))
 
 point_cloud_range = [-54, -54, -5.0, 54, 54, 3.0]
 # Using calibration info convert the Lidar-coordinate point cloud range to the
 # ego-coordinate point cloud range could bring a little promotion in nuScenes.
 # point_cloud_range = [-54, -54.8, -5.0, 54, 53.2, 3.0]
-file_client_args = dict(backend='disk')
+backend_args = None
 class_names = [
     'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
     'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
 ]
 
 test_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=5, use_dim=5),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=5,
+        use_dim=5,
+        backend_args=backend_args),
     dict(
         type='LoadPointsFromMultiSweeps',
         sweeps_num=9,
         use_dim=[0, 1, 2, 3, 4],
         pad_empty_sweeps=True,
-        remove_close=True),
+        remove_close=True,
+        backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
-        pts_scale_ratio=1,
+        pts_scale_ratio=[0.95, 1.0, 1.05],
         # Add double-flip augmentation
         flip=True,
         pcd_horizontal_flip=True,
         pcd_vertical_flip=True,
         transforms=[
             dict(
                 type='GlobalRotScaleTrans',
                 rot_range=[0, 0],
                 scale_ratio_range=[1., 1.],
                 translation_std=[0, 0, 0]),
             dict(type='RandomFlip3D', sync_2d=False),
             dict(
-                type='PointsRangeFilter', point_cloud_range=point_cloud_range),
+                type='PointsRangeFilter', point_cloud_range=point_cloud_range)
         ]),
     dict(type='Pack3DDetInputs', keys=['points'])
 ]
 
 data = dict(
     val=dict(pipeline=test_pipeline), test=dict(pipeline=test_pipeline))
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel0075_second_secfpn_head-dcn_8xb4-flip-tta-cyclic-20e_nus-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel0075_second_secfpn_head-dcn_8xb4-flip-tta-cyclic-20e_nus-3d.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,28 +1,34 @@
-_base_ = './centerpoint_voxel0075_second_secfpn' \
-         '_head-dcn_8xb4-cyclic-20e_nus-3d.py'
+_base_ = \
+    './centerpoint_voxel0075_second_secfpn_head-dcn_8xb4-cyclic-20e_nus-3d.py'
 
 point_cloud_range = [-54, -54, -5.0, 54, 54, 3.0]
 # Using calibration info convert the Lidar-coordinate point cloud range to the
 # ego-coordinate point cloud range could bring a little promotion in nuScenes.
 # point_cloud_range = [-54, -54.8, -5.0, 54, 53.2, 3.0]
-file_client_args = dict(backend='disk')
+backend_args = None
 class_names = [
     'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
     'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
 ]
 
 test_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=5, use_dim=5),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=5,
+        use_dim=5,
+        backend_args=backend_args),
     dict(
         type='LoadPointsFromMultiSweeps',
         sweeps_num=9,
         use_dim=[0, 1, 2, 3, 4],
         pad_empty_sweeps=True,
-        remove_close=True),
+        remove_close=True,
+        backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         # Add double-flip augmentation
         flip=True,
         pcd_horizontal_flip=True,
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel0075_second_secfpn_head-dcn_8xb4-tta-cyclic-20e_nus-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel0075_second_secfpn_head-dcn-circlenms_8xb4-flip-tta-cyclic-20e_nus-3d.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,46 +1,49 @@
-_base_ = './centerpoint_voxel0075_second_secfpn' \
-         '_head-dcn_8xb4-cyclic-20e_nus-3d.py'
-
-model = dict(test_cfg=dict(pts=dict(use_rotate_nms=True, max_num=500)))
+_base_ = './centerpoint_voxel0075_second_secfpn_head-dcn-circlenms_8xb4-cyclic-20e_nus-3d.py'  # noqa: E501
 
 point_cloud_range = [-54, -54, -5.0, 54, 54, 3.0]
 # Using calibration info convert the Lidar-coordinate point cloud range to the
 # ego-coordinate point cloud range could bring a little promotion in nuScenes.
 # point_cloud_range = [-54, -54.8, -5.0, 54, 53.2, 3.0]
-file_client_args = dict(backend='disk')
+backend_args = None
 class_names = [
     'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
     'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
 ]
 
 test_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=5, use_dim=5),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=5,
+        use_dim=5,
+        backend_args=backend_args),
     dict(
         type='LoadPointsFromMultiSweeps',
         sweeps_num=9,
         use_dim=[0, 1, 2, 3, 4],
         pad_empty_sweeps=True,
-        remove_close=True),
+        remove_close=True,
+        backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
-        pts_scale_ratio=[0.95, 1.0, 1.05],
+        pts_scale_ratio=1,
         # Add double-flip augmentation
         flip=True,
         pcd_horizontal_flip=True,
         pcd_vertical_flip=True,
         transforms=[
             dict(
                 type='GlobalRotScaleTrans',
                 rot_range=[0, 0],
                 scale_ratio_range=[1., 1.],
                 translation_std=[0, 0, 0]),
             dict(type='RandomFlip3D', sync_2d=False),
             dict(
-                type='PointsRangeFilter', point_cloud_range=point_cloud_range)
+                type='PointsRangeFilter', point_cloud_range=point_cloud_range),
         ]),
     dict(type='Pack3DDetInputs', keys=['points'])
 ]
 
 data = dict(
     val=dict(pipeline=test_pipeline), test=dict(pipeline=test_pipeline))
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel01_second_secfpn_8xb4-cyclic-20e_nus-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/centerpoint/centerpoint_voxel01_second_secfpn_8xb4-cyclic-20e_nus-3d.py`

 * *Files 13% similar despite different names*

```diff
@@ -22,15 +22,15 @@
     pts_bbox_head=dict(bbox_coder=dict(pc_range=point_cloud_range[:2])),
     # model training and testing settings
     train_cfg=dict(pts=dict(point_cloud_range=point_cloud_range)),
     test_cfg=dict(pts=dict(pc_range=point_cloud_range[:2])))
 
 dataset_type = 'NuScenesDataset'
 data_root = 'data/nuscenes/'
-file_client_args = dict(backend='disk')
+backend_args = None
 
 db_sampler = dict(
     data_root=data_root,
     info_path=data_root + 'nuscenes_dbinfos_train.pkl',
     rate=1.0,
     prepare=dict(
         filter_by_difficulty=[-1],
@@ -57,24 +57,32 @@
         bicycle=6,
         pedestrian=2,
         traffic_cone=2),
     points_loader=dict(
         type='LoadPointsFromFile',
         coord_type='LIDAR',
         load_dim=5,
-        use_dim=[0, 1, 2, 3, 4]))
+        use_dim=[0, 1, 2, 3, 4],
+        backend_args=backend_args),
+    backend_args=backend_args)
 
 train_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=5, use_dim=5),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=5,
+        use_dim=5,
+        backend_args=backend_args),
     dict(
         type='LoadPointsFromMultiSweeps',
         sweeps_num=9,
         use_dim=[0, 1, 2, 3, 4],
         pad_empty_sweeps=True,
-        remove_close=True),
+        remove_close=True,
+        backend_args=backend_args),
     dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
     dict(type='ObjectSample', db_sampler=db_sampler),
     dict(
         type='GlobalRotScaleTrans',
         rot_range=[-0.3925, 0.3925],
         scale_ratio_range=[0.95, 1.05],
         translation_std=[0, 0, 0]),
@@ -88,21 +96,27 @@
     dict(type='ObjectNameFilter', classes=class_names),
     dict(type='PointShuffle'),
     dict(
         type='Pack3DDetInputs',
         keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
 ]
 test_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=5, use_dim=5),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=5,
+        use_dim=5,
+        backend_args=backend_args),
     dict(
         type='LoadPointsFromMultiSweeps',
         sweeps_num=9,
         use_dim=[0, 1, 2, 3, 4],
         pad_empty_sweeps=True,
-        remove_close=True),
+        remove_close=True,
+        backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
             dict(
@@ -132,14 +146,15 @@
             pipeline=train_pipeline,
             metainfo=dict(classes=class_names),
             test_mode=False,
             data_prefix=data_prefix,
             use_valid_flag=True,
             # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
             # and box_type_3d='Depth' in sunrgbd and scannet dataset.
-            box_type_3d='LiDAR')))
+            box_type_3d='LiDAR',
+            backend_args=backend_args)))
 test_dataloader = dict(
     dataset=dict(pipeline=test_pipeline, metainfo=dict(classes=class_names)))
 val_dataloader = dict(
     dataset=dict(pipeline=test_pipeline, metainfo=dict(classes=class_names)))
 
 train_cfg = dict(val_interval=20)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/centerpoint/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/centerpoint/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dfm/multiview-dfm_r101-dcn_16xb2_waymoD5-3d-3class.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/dfm/multiview-dfm_r101-dcn_16xb2_waymoD5-3d-3class.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dfm/multiview-dfm_r101-dcn_centerhead_16xb2_waymoD5-3d-3class.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/dfm/multiview-dfm_r101-dcn_centerhead_16xb2_waymoD5-3d-3class.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-_base_ = ['./multiview-dfm_r101_dcn_2x16_waymoD5-3d-3class.py']
+_base_ = ['./multiview-dfm_r101-dcn_16xb2_waymoD5-3d-3class.py']
 
 model = dict(
     bbox_head=dict(
         _delete_=True,
         type='CenterHead',
         in_channels=256,
         tasks=[
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area1.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area1.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area2.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area2.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area3.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area3.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area4.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area4.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area5.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area5.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area6.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area6.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dgcnn/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/dgcnn/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dynamic_voxelization/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/dynamic_voxelization/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dynamic_voxelization/pointpillars_dv_secfpn_8xb6-160e_kitti-3d-car.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/dynamic_voxelization/pointpillars_dv_secfpn_8xb6-160e_kitti-3d-car.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dynamic_voxelization/second_dv_secfpn_8xb2-cosine-80e_kitti-3d-3class.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/dynamic_voxelization/second_dv_secfpn_8xb2-cosine-80e_kitti-3d-3class.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/dynamic_voxelization/second_dv_secfpn_8xb6-80e_kitti-3d-car.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/dynamic_voxelization/second_dv_secfpn_8xb6-80e_kitti-3d-car.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-_base_ = '../second/hv_second_secfpn_6x8_80e_kitti-3d-car.py'
+_base_ = '../second/second_hv_secfpn_8xb6-80e_kitti-3d-car.py'
 
 point_cloud_range = [0, -40, -3, 70.4, 40, 1]
 voxel_size = [0.05, 0.05, 0.1]
 
 model = dict(
     type='DynamicVoxelNet',
     data_preprocessor=dict(
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/fcaf3d/fcaf3d_2xb8_s3dis-3d-5class.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/fcaf3d/fcaf3d_2xb8_s3dis-3d-5class.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/fcaf3d/fcaf3d_2xb8_scannet-3d-18class.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/fcos3d/fcos3d_r101-caffe-dcn_fpn_head-gn_8xb2-1x_nus-mono3d.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,91 +1,70 @@
 _base_ = [
-    '../_base_/models/fcaf3d.py', '../_base_/default_runtime.py',
-    '../_base_/datasets/scannet-3d.py'
+    '../_base_/datasets/nus-mono3d.py', '../_base_/models/fcos3d.py',
+    '../_base_/schedules/mmdet-schedule-1x.py', '../_base_/default_runtime.py'
 ]
-n_points = 100000
+# model settings
+model = dict(
+    data_preprocessor=dict(
+        type='Det3DDataPreprocessor',
+        mean=[103.530, 116.280, 123.675],
+        std=[1.0, 1.0, 1.0],
+        bgr_to_rgb=False,
+        pad_size_divisor=32),
+    backbone=dict(
+        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
+        stage_with_dcn=(False, False, True, True)))
+
+backend_args = None
 
 train_pipeline = [
+    dict(type='LoadImageFromFileMono3D', backend_args=backend_args),
     dict(
-        type='LoadPointsFromFile',
-        coord_type='DEPTH',
-        shift_height=False,
-        use_color=True,
-        load_dim=6,
-        use_dim=[0, 1, 2, 3, 4, 5]),
-    dict(type='LoadAnnotations3D'),
-    dict(type='GlobalAlignment', rotation_axis=2),
-    dict(type='PointSample', num_points=n_points),
-    dict(
-        type='RandomFlip3D',
-        sync_2d=False,
-        flip_ratio_bev_horizontal=0.5,
-        flip_ratio_bev_vertical=0.5),
-    dict(
-        type='GlobalRotScaleTrans',
-        rot_range=[-0.087266, 0.087266],
-        scale_ratio_range=[.9, 1.1],
-        translation_std=[.1, .1, .1],
-        shift_height=False),
-    dict(type='NormalizePointsColor', color_mean=None),
+        type='LoadAnnotations3D',
+        with_bbox=True,
+        with_label=True,
+        with_attr_label=True,
+        with_bbox_3d=True,
+        with_label_3d=True,
+        with_bbox_depth=True),
+    dict(type='mmdet.Resize', scale=(1600, 900), keep_ratio=True),
+    dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
     dict(
         type='Pack3DDetInputs',
-        keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
+        keys=[
+            'img', 'gt_bboxes', 'gt_bboxes_labels', 'attr_labels',
+            'gt_bboxes_3d', 'gt_labels_3d', 'centers_2d', 'depths'
+        ]),
 ]
 test_pipeline = [
-    dict(
-        type='LoadPointsFromFile',
-        coord_type='DEPTH',
-        shift_height=False,
-        use_color=True,
-        load_dim=6,
-        use_dim=[0, 1, 2, 3, 4, 5]),
-    dict(type='GlobalAlignment', rotation_axis=2),
-    dict(
-        type='MultiScaleFlipAug3D',
-        img_scale=(1333, 800),
-        pts_scale_ratio=1,
-        flip=False,
-        transforms=[
-            dict(
-                type='GlobalRotScaleTrans',
-                rot_range=[0, 0],
-                scale_ratio_range=[1., 1.],
-                translation_std=[0, 0, 0]),
-            dict(
-                type='RandomFlip3D',
-                sync_2d=False,
-                flip_ratio_bev_horizontal=0.5,
-                flip_ratio_bev_vertical=0.5),
-            dict(type='PointSample', num_points=n_points),
-            dict(type='NormalizePointsColor', color_mean=None),
-        ]),
-    dict(type='Pack3DDetInputs', keys=['points'])
+    dict(type='LoadImageFromFileMono3D', backend_args=backend_args),
+    dict(type='mmdet.Resize', scale_factor=1.0),
+    dict(type='Pack3DDetInputs', keys=['img'])
 ]
+
 train_dataloader = dict(
-    dataset=dict(
-        type='RepeatDataset',
-        times=10,
-        dataset=dict(pipeline=train_pipeline, filter_empty_gt=True)))
+    batch_size=2, num_workers=2, dataset=dict(pipeline=train_pipeline))
+test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
 val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
-test_dataloader = val_dataloader
 
+# optimizer
 optim_wrapper = dict(
-    type='OptimWrapper',
-    optimizer=dict(type='AdamW', lr=0.001, weight_decay=0.0001),
-    clip_grad=dict(max_norm=10, norm_type=2))
+    optimizer=dict(lr=0.002),
+    paramwise_cfg=dict(bias_lr_mult=2., bias_decay_mult=0.),
+    clip_grad=dict(max_norm=35, norm_type=2))
 
 # learning rate
-param_scheduler = dict(
-    type='MultiStepLR',
-    begin=0,
-    end=12,
-    by_epoch=True,
-    milestones=[8, 11],
-    gamma=0.1)
-
-custom_hooks = [dict(type='EmptyCacheHook', after_iter=True)]
-
-# training schedule for 1x
-train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=12, val_interval=12)
-val_cfg = dict(type='ValLoop')
-test_cfg = dict(type='TestLoop')
+param_scheduler = [
+    dict(
+        type='LinearLR',
+        start_factor=1.0 / 3,
+        by_epoch=False,
+        begin=0,
+        end=500),
+    dict(
+        type='MultiStepLR',
+        begin=0,
+        end=12,
+        by_epoch=True,
+        milestones=[8, 11],
+        gamma=0.1)
+]
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/fcaf3d/fcaf3d_2xb8_sunrgbd-3d-10class.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/3dssd/3dssd_4xb4_kitti-3d-car.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,89 +1,119 @@
 _base_ = [
-    '../_base_/models/fcaf3d.py', '../_base_/default_runtime.py',
-    '../_base_/datasets/sunrgbd-3d.py'
+    '../_base_/models/3dssd.py', '../_base_/datasets/kitti-3d-car.py',
+    '../_base_/default_runtime.py'
 ]
-n_points = 100000
 
-model = dict(
-    bbox_head=dict(
-        num_classes=10,
-        num_reg_outs=8,
-        bbox_loss=dict(type='RotatedIoU3DLoss')))
+# dataset settings
+dataset_type = 'KittiDataset'
+data_root = 'data/kitti/'
+class_names = ['Car']
+point_cloud_range = [0, -40, -5, 70, 40, 3]
+input_modality = dict(use_lidar=True, use_camera=False)
+backend_args = None
+
+db_sampler = dict(
+    data_root=data_root,
+    info_path=data_root + 'kitti_dbinfos_train.pkl',
+    rate=1.0,
+    prepare=dict(filter_by_difficulty=[-1], filter_by_min_points=dict(Car=5)),
+    classes=class_names,
+    sample_groups=dict(Car=15),
+    points_loader=dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
+    backend_args=backend_args)
 
 train_pipeline = [
     dict(
         type='LoadPointsFromFile',
-        coord_type='DEPTH',
-        shift_height=False,
-        load_dim=6,
-        use_dim=[0, 1, 2, 3, 4, 5]),
-    dict(type='LoadAnnotations3D'),
-    dict(type='PointSample', num_points=n_points),
-    dict(type='RandomFlip3D', sync_2d=False, flip_ratio_bev_horizontal=0.5),
+        coord_type='LIDAR',
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
+    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
+    dict(type='PointsRangeFilter', point_cloud_range=point_cloud_range),
+    dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
+    dict(type='ObjectSample', db_sampler=db_sampler),
+    dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
+    dict(
+        type='ObjectNoise',
+        num_try=100,
+        translation_std=[1.0, 1.0, 0],
+        global_rot_range=[0.0, 0.0],
+        rot_range=[-1.0471975511965976, 1.0471975511965976]),
     dict(
         type='GlobalRotScaleTrans',
-        rot_range=[-0.523599, 0.523599],
-        scale_ratio_range=[0.85, 1.15],
-        translation_std=[.1, .1, .1],
-        shift_height=False),
+        rot_range=[-0.78539816, 0.78539816],
+        scale_ratio_range=[0.9, 1.1]),
+    # 3DSSD can get a higher performance without this transform
+    # dict(type='BackgroundPointsFilter', bbox_enlarge_range=(0.5, 2.0, 0.5)),
+    dict(type='PointSample', num_points=16384),
     dict(
         type='Pack3DDetInputs',
         keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
 ]
+
 test_pipeline = [
     dict(
         type='LoadPointsFromFile',
-        coord_type='DEPTH',
-        shift_height=False,
-        load_dim=6,
-        use_dim=[0, 1, 2, 3, 4, 5]),
+        coord_type='LIDAR',
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
             dict(
                 type='GlobalRotScaleTrans',
                 rot_range=[0, 0],
                 scale_ratio_range=[1., 1.],
                 translation_std=[0, 0, 0]),
+            dict(type='RandomFlip3D'),
             dict(
-                type='RandomFlip3D',
-                sync_2d=False,
-                flip_ratio_bev_horizontal=0.5,
-                flip_ratio_bev_vertical=0.5),
-            dict(type='PointSample', num_points=n_points)
+                type='PointsRangeFilter', point_cloud_range=point_cloud_range),
+            dict(type='PointSample', num_points=16384),
         ]),
     dict(type='Pack3DDetInputs', keys=['points'])
 ]
 
 train_dataloader = dict(
-    batch_size=8,
-    dataset=dict(
-        type='RepeatDataset',
-        times=3,
-        dataset=dict(pipeline=train_pipeline, filter_empty_gt=True)))
+    batch_size=4, dataset=dict(dataset=dict(pipeline=train_pipeline, )))
+test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
 val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
-test_dataloader = val_dataloader
 
+# model settings
+model = dict(
+    bbox_head=dict(
+        num_classes=1,
+        bbox_coder=dict(
+            type='AnchorFreeBBoxCoder', num_dir_bins=12, with_rot=True)))
+
+# optimizer
+lr = 0.002  # max learning rate
 optim_wrapper = dict(
     type='OptimWrapper',
-    optimizer=dict(type='AdamW', lr=0.001, weight_decay=0.0001),
-    clip_grad=dict(max_norm=10, norm_type=2))
-
-# learning rate
-param_scheduler = dict(
-    type='MultiStepLR',
-    begin=0,
-    end=12,
-    by_epoch=True,
-    milestones=[8, 11],
-    gamma=0.1)
-
-custom_hooks = [dict(type='EmptyCacheHook', after_iter=True)]
+    optimizer=dict(type='AdamW', lr=lr, weight_decay=0.),
+    clip_grad=dict(max_norm=35, norm_type=2),
+)
 
 # training schedule for 1x
-train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=12, val_interval=12)
+train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=80, val_interval=2)
 val_cfg = dict(type='ValLoop')
 test_cfg = dict(type='TestLoop')
+
+# learning rate
+param_scheduler = [
+    dict(
+        type='MultiStepLR',
+        begin=0,
+        end=80,
+        by_epoch=True,
+        milestones=[45, 60],
+        gamma=0.1)
+]
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/fcaf3d/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/fcaf3d/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/fcos3d/fcos3d_r101-caffe-dcn_fpn_head-gn_8xb2-1x_nus-mono3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/spvcnn/spvcnn_w32_8xb2-amp-15e_semantickitti.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,81 +1,54 @@
 _base_ = [
-    '../_base_/datasets/nus-mono3d.py', '../_base_/models/fcos3d.py',
-    '../_base_/schedules/mmdet-schedule-1x.py', '../_base_/default_runtime.py'
+    '../_base_/datasets/semantickitti.py', '../_base_/models/spvcnn.py',
+    '../_base_/default_runtime.py'
 ]
-# model settings
-model = dict(
-    data_preprocessor=dict(
-        type='Det3DDataPreprocessor',
-        mean=[103.530, 116.280, 123.675],
-        std=[1.0, 1.0, 1.0],
-        bgr_to_rgb=False,
-        pad_size_divisor=32),
-    backbone=dict(
-        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
-        stage_with_dcn=(False, False, True, True)))
-
-file_client_args = dict(backend='disk')
-# Uncomment the following if use ceph or other file clients.
-# See https://mmcv.readthedocs.io/en/latest/api.html#mmcv.fileio.FileClient
-# for more details.
-# file_client_args = dict(
-#     backend='petrel',
-#     path_mapping=dict({
-#         './data/nuscenes/':
-#         's3://openmmlab/datasets/detection3d/nuscenes/',
-#         'data/nuscenes/':
-#         's3://openmmlab/datasets/detection3d/nuscenes/'
-#     }))
 
 train_pipeline = [
-    dict(type='LoadImageFromFileMono3D'),
+    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4),
     dict(
         type='LoadAnnotations3D',
-        with_bbox=True,
-        with_label=True,
-        with_attr_label=True,
-        with_bbox_3d=True,
-        with_label_3d=True,
-        with_bbox_depth=True),
-    dict(type='mmdet.Resize', scale=(1600, 900), keep_ratio=True),
-    dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
+        with_bbox_3d=False,
+        with_label_3d=False,
+        with_seg_3d=True,
+        seg_3d_dtype='np.int32',
+        seg_offset=2**16,
+        dataset_type='semantickitti'),
+    dict(type='PointSegClassMapping'),
     dict(
-        type='Pack3DDetInputs',
-        keys=[
-            'img', 'gt_bboxes', 'gt_bboxes_labels', 'attr_labels',
-            'gt_bboxes_3d', 'gt_labels_3d', 'centers_2d', 'depths'
-        ]),
-]
-test_pipeline = [
-    dict(type='LoadImageFromFileMono3D'),
-    dict(type='mmdet.Resize', scale_factor=1.0),
-    dict(type='Pack3DDetInputs', keys=['img'])
+        type='GlobalRotScaleTrans',
+        rot_range=[0., 6.28318531],
+        scale_ratio_range=[0.95, 1.05],
+        translation_std=[0, 0, 0],
+    ),
+    dict(type='Pack3DDetInputs', keys=['points', 'pts_semantic_mask'])
 ]
 
 train_dataloader = dict(
-    batch_size=2, num_workers=2, dataset=dict(pipeline=train_pipeline))
-test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
-val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
+    sampler=dict(seed=0), dataset=dict(pipeline=train_pipeline))
 
-# optimizer
+lr = 0.24
 optim_wrapper = dict(
-    optimizer=dict(lr=0.002),
-    paramwise_cfg=dict(bias_lr_mult=2., bias_decay_mult=0.),
-    clip_grad=dict(max_norm=35, norm_type=2))
+    type='AmpOptimWrapper',
+    loss_scale='dynamic',
+    optimizer=dict(
+        type='SGD', lr=lr, weight_decay=0.0001, momentum=0.9, nesterov=True))
 
-# learning rate
 param_scheduler = [
     dict(
-        type='LinearLR',
-        start_factor=1.0 / 3,
-        by_epoch=False,
-        begin=0,
-        end=500),
+        type='LinearLR', start_factor=0.008, by_epoch=False, begin=0, end=125),
     dict(
-        type='MultiStepLR',
+        type='CosineAnnealingLR',
         begin=0,
-        end=12,
+        T_max=15,
         by_epoch=True,
-        milestones=[8, 11],
-        gamma=0.1)
+        eta_min=1e-5,
+        convert_to_iter_based=True)
 ]
+
+train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=15, val_interval=1)
+val_cfg = dict(type='ValLoop')
+test_cfg = dict(type='TestLoop')
+
+default_hooks = dict(checkpoint=dict(type='CheckpointHook', interval=1))
+randomness = dict(seed=0, deterministic=False, diff_rank_seed=True)
+env_cfg = dict(cudnn_benchmark=True)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/fcos3d/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/fcos3d/metafile.yml`

 * *Files 8% similar despite different names*

```diff
@@ -12,28 +12,28 @@
       Title: 'FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection'
     README: configs/fcos3d/README.md
     Code:
       URL: https://github.com/open-mmlab/mmdetection3d/blob/master/mmdet3d/models/detectors/fcos_mono3d.py#L7
       Version: v0.13.0
 
 Models:
-  - Name: fcos3d_r101_caffe_fpn_gn-head_dcn_2x8_1x_nus-mono3d
+  - Name: fcos3d_r101-caffe-dcn_fpn_head-gn_8xb2-1x_nus-mono3d_finetune
     In Collection: FCOS3D
     Config: configs/fcos3d/fcos3d_r101-caffe-dcn_fpn_head-gn_8xb2-1x_nus-mono3d.py
     Metadata:
       Training Memory (GB): 8.7
     Results:
       - Task: 3D Object Detection
         Dataset: NuScenes
         Metrics:
           mAP: 29.9
           NDS: 37.3
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/fcos3d/fcos3d_r101_caffe_fpn_gn-head_dcn_2x8_1x_nus-mono3d/fcos3d_r101_caffe_fpn_gn-head_dcn_2x8_1x_nus-mono3d_20210715_235813-4bed5239.pth
 
-  - Name: fcos3d_r101_caffe_fpn_gn-head_dcn_2x8_1x_nus-mono3d_finetune
+  - Name: fcos3d_r101-caffe-dcn_fpn_head-gn_8xb2-1x_nus-mono3d_finetune
     In Collection: FCOS3D
     Config: configs/fcos3d/fcos3d_r101-caffe-dcn_fpn_head-gn_8xb2-1x_nus-mono3d_finetune.py
     Metadata:
       Training Memory (GB): 8.7
     Results:
       - Task: 3D Object Detection
         Dataset: NuScenes
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/free_anchor/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/free_anchor/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/free_anchor/pointpillars_hv_fpn_head-free-anchor_sbn-all_8xb4-2x_nus-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/free_anchor/pointpillars_hv_fpn_head-free-anchor_sbn-all_8xb4-2x_nus-3d.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/free_anchor/pointpillars_hv_regnet-1.6gf_fpn_head-free-anchor_sbn-all_8xb4-2x_nus-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/free_anchor/pointpillars_hv_regnet-1.6gf_fpn_head-free-anchor_sbn-all_8xb4-2x_nus-3d.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/free_anchor/pointpillars_hv_regnet-1.6gf_fpn_head-free-anchor_sbn-all_8xb4-strong-aug-3x_nus-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/free_anchor/pointpillars_hv_regnet-1.6gf_fpn_head-free-anchor_sbn-all_8xb4-strong-aug-3x_nus-3d.py`

 * *Files 18% similar despite different names*

```diff
@@ -21,27 +21,27 @@
 # cloud range accordingly
 point_cloud_range = [-50, -50, -5, 50, 50, 3]
 # For nuScenes we usually do 10-class detection
 class_names = [
     'car', 'truck', 'trailer', 'bus', 'construction_vehicle', 'bicycle',
     'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
 ]
-file_client_args = dict(backend='disk')
-# Uncomment the following if use ceph or other file clients.
-# See https://mmcv.readthedocs.io/en/latest/api.html#mmcv.fileio.FileClient
-# for more details.
-# file_client_args = dict(
-#     backend='petrel',
-#     path_mapping=dict({
-#         './data/nuscenes/': 's3://nuscenes/nuscenes/',
-#         'data/nuscenes/': 's3://nuscenes/nuscenes/'
-#     }))
+backend_args = None
+
 train_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=5, use_dim=5),
-    dict(type='LoadPointsFromMultiSweeps', sweeps_num=10),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=5,
+        use_dim=5,
+        backend_args=backend_args),
+    dict(
+        type='LoadPointsFromMultiSweeps',
+        sweeps_num=10,
+        backend_args=backend_args),
     dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
     dict(
         type='GlobalRotScaleTrans',
         rot_range=[-0.7854, 0.7854],
         scale_ratio_range=[0.95, 1.05],
         translation_std=[0.2, 0.2, 0.2]),
     dict(
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/free_anchor/pointpillars_hv_regnet-3.2gf_fpn_head-free-anchor_sbn-all_8xb4-2x_nus-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/free_anchor/pointpillars_hv_regnet-3.2gf_fpn_head-free-anchor_sbn-all_8xb4-2x_nus-3d.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/free_anchor/pointpillars_hv_regnet-3.2gf_fpn_head-free-anchor_sbn-all_8xb4-strong-aug-3x_nus-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/free_anchor/pointpillars_hv_regnet-3.2gf_fpn_head-free-anchor_sbn-all_8xb4-strong-aug-3x_nus-3d.py`

 * *Files 10% similar despite different names*

```diff
@@ -21,35 +21,27 @@
 # cloud range accordingly
 point_cloud_range = [-50, -50, -5, 50, 50, 3]
 # For nuScenes we usually do 10-class detection
 class_names = [
     'car', 'truck', 'trailer', 'bus', 'construction_vehicle', 'bicycle',
     'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
 ]
-file_client_args = dict(backend='disk')
-# Uncomment the following if use ceph or other file clients.
-# See https://mmcv.readthedocs.io/en/latest/api.html#mmcv.fileio.FileClient
-# for more details.
-# file_client_args = dict(
-#     backend='petrel',
-#     path_mapping=dict({
-#         './data/nuscenes/': 's3://nuscenes/nuscenes/',
-#         'data/nuscenes/': 's3://nuscenes/nuscenes/'
-#     }))
+backend_args = None
+
 train_pipeline = [
     dict(
         type='LoadPointsFromFile',
         coord_type='LIDAR',
         load_dim=5,
         use_dim=5,
-        file_client_args=file_client_args),
+        backend_args=backend_args),
     dict(
         type='LoadPointsFromMultiSweeps',
         sweeps_num=10,
-        file_client_args=file_client_args),
+        backend_args=backend_args),
     dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
     dict(
         type='GlobalRotScaleTrans',
         rot_range=[-0.7854, 0.7854],
         scale_ratio_range=[0.9, 1.1],
         translation_std=[0.2, 0.2, 0.2]),
     dict(
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/free_anchor/pointpillars_hv_regnet-400mf_fpn_head-free-anchor_sbn-all_8xb4-2x_nus-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/free_anchor/pointpillars_hv_regnet-400mf_fpn_head-free-anchor_sbn-all_8xb4-2x_nus-3d.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/groupfree3d/groupfree3d_head-L12-O256_4xb8_scannet-seg.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/groupfree3d/groupfree3d_head-L6-O256_4xb8_scannet-seg.py`

 * *Files 5% similar despite different names*

```diff
@@ -3,15 +3,14 @@
     '../_base_/schedules/schedule-3x.py', '../_base_/default_runtime.py'
 ]
 
 # model settings
 model = dict(
     bbox_head=dict(
         num_classes=18,
-        num_decoder_layers=12,
         size_cls_agnostic=False,
         bbox_coder=dict(
             type='GroupFree3DBBoxCoder',
             num_sizes=18,
             num_dir_bins=1,
             with_rot=False,
             size_cls_agnostic=False,
@@ -75,27 +74,30 @@
 data_root = './data/scannet/'
 class_names = ('cabinet', 'bed', 'chair', 'sofa', 'table', 'door', 'window',
                'bookshelf', 'picture', 'counter', 'desk', 'curtain',
                'refrigerator', 'showercurtrain', 'toilet', 'sink', 'bathtub',
                'garbagebin')
 
 metainfo = dict(classes=class_names)
+backend_args = None
 
 train_pipeline = [
     dict(
         type='LoadPointsFromFile',
         coord_type='DEPTH',
         load_dim=6,
-        use_dim=[0, 1, 2]),
+        use_dim=[0, 1, 2],
+        backend_args=backend_args),
     dict(
         type='LoadAnnotations3D',
         with_bbox_3d=True,
         with_label_3d=True,
         with_mask_3d=True,
-        with_seg_3d=True),
+        with_seg_3d=True,
+        backend_args=backend_args),
     dict(type='GlobalAlignment', rotation_axis=2),
     dict(type='PointSegClassMapping'),
     dict(type='PointSample', num_points=50000),
     dict(
         type='RandomFlip3D',
         sync_2d=False,
         flip_ratio_bev_horizontal=0.5,
@@ -112,15 +114,16 @@
         ])
 ]
 test_pipeline = [
     dict(
         type='LoadPointsFromFile',
         coord_type='DEPTH',
         load_dim=6,
-        use_dim=[0, 1, 2]),
+        use_dim=[0, 1, 2],
+        backend_args=backend_args),
     dict(type='GlobalAlignment', rotation_axis=2),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
@@ -151,39 +154,42 @@
             data_root=data_root,
             ann_file='scannet_infos_train.pkl',
             pipeline=train_pipeline,
             filter_empty_gt=False,
             metainfo=metainfo,
             # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
             # and box_type_3d='Depth' in sunrgbd and scannet dataset.
-            box_type_3d='Depth')))
+            box_type_3d='Depth',
+            backend_args=backend_args)))
 val_dataloader = dict(
     batch_size=1,
     num_workers=1,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         ann_file='scannet_infos_val.pkl',
         pipeline=test_pipeline,
         metainfo=metainfo,
         test_mode=True,
-        box_type_3d='Depth'))
+        box_type_3d='Depth',
+        backend_args=backend_args))
 test_dataloader = dict(
     batch_size=1,
     num_workers=1,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         ann_file='scannet_infos_val.pkl',
         pipeline=test_pipeline,
         metainfo=metainfo,
         test_mode=True,
-        box_type_3d='Depth'))
+        box_type_3d='Depth',
+        backend_args=backend_args))
 val_evaluator = dict(type='IndoorMetric')
 test_evaluator = val_evaluator
 
 # optimizer
 lr = 0.006
 optim_wrapper = dict(
     type='OptimWrapper',
@@ -214,7 +220,8 @@
 # training schedule for 1x
 train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=80, val_interval=1)
 val_cfg = dict(type='ValLoop')
 test_cfg = dict(type='TestLoop')
 
 default_hooks = dict(
     checkpoint=dict(type='CheckpointHook', interval=1, max_keep_ckpts=10))
+randomness = dict(seed=4)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/groupfree3d/groupfree3d_head-L6-O256_4xb8_scannet-seg.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/groupfree3d/groupfree3d_head-L12-O256_4xb8_scannet-seg.py`

 * *Files 10% similar despite different names*

```diff
@@ -3,14 +3,15 @@
     '../_base_/schedules/schedule-3x.py', '../_base_/default_runtime.py'
 ]
 
 # model settings
 model = dict(
     bbox_head=dict(
         num_classes=18,
+        num_decoder_layers=12,
         size_cls_agnostic=False,
         bbox_coder=dict(
             type='GroupFree3DBBoxCoder',
             num_sizes=18,
             num_dir_bins=1,
             with_rot=False,
             size_cls_agnostic=False,
@@ -74,27 +75,30 @@
 data_root = './data/scannet/'
 class_names = ('cabinet', 'bed', 'chair', 'sofa', 'table', 'door', 'window',
                'bookshelf', 'picture', 'counter', 'desk', 'curtain',
                'refrigerator', 'showercurtrain', 'toilet', 'sink', 'bathtub',
                'garbagebin')
 
 metainfo = dict(classes=class_names)
+backend_args = None
 
 train_pipeline = [
     dict(
         type='LoadPointsFromFile',
         coord_type='DEPTH',
         load_dim=6,
-        use_dim=[0, 1, 2]),
+        use_dim=[0, 1, 2],
+        backend_args=backend_args),
     dict(
         type='LoadAnnotations3D',
         with_bbox_3d=True,
         with_label_3d=True,
         with_mask_3d=True,
-        with_seg_3d=True),
+        with_seg_3d=True,
+        backend_args=backend_args),
     dict(type='GlobalAlignment', rotation_axis=2),
     dict(type='PointSegClassMapping'),
     dict(type='PointSample', num_points=50000),
     dict(
         type='RandomFlip3D',
         sync_2d=False,
         flip_ratio_bev_horizontal=0.5,
@@ -111,15 +115,16 @@
         ])
 ]
 test_pipeline = [
     dict(
         type='LoadPointsFromFile',
         coord_type='DEPTH',
         load_dim=6,
-        use_dim=[0, 1, 2]),
+        use_dim=[0, 1, 2],
+        backend_args=backend_args),
     dict(type='GlobalAlignment', rotation_axis=2),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
@@ -150,39 +155,42 @@
             data_root=data_root,
             ann_file='scannet_infos_train.pkl',
             pipeline=train_pipeline,
             filter_empty_gt=False,
             metainfo=metainfo,
             # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
             # and box_type_3d='Depth' in sunrgbd and scannet dataset.
-            box_type_3d='Depth')))
+            box_type_3d='Depth',
+            backend_args=backend_args)))
 val_dataloader = dict(
     batch_size=1,
     num_workers=1,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         ann_file='scannet_infos_val.pkl',
         pipeline=test_pipeline,
         metainfo=metainfo,
         test_mode=True,
-        box_type_3d='Depth'))
+        box_type_3d='Depth',
+        backend_args=backend_args))
 test_dataloader = dict(
     batch_size=1,
     num_workers=1,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         ann_file='scannet_infos_val.pkl',
         pipeline=test_pipeline,
         metainfo=metainfo,
         test_mode=True,
-        box_type_3d='Depth'))
+        box_type_3d='Depth',
+        backend_args=backend_args))
 val_evaluator = dict(type='IndoorMetric')
 test_evaluator = val_evaluator
 
 # optimizer
 lr = 0.006
 optim_wrapper = dict(
     type='OptimWrapper',
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/groupfree3d/groupfree3d_w2x-head-L12-O256_4xb8_scannet-seg.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/groupfree3d/groupfree3d_w2x-head-L12-O256_4xb8_scannet-seg.py`

 * *Files 8% similar despite different names*

```diff
@@ -90,27 +90,30 @@
 data_root = './data/scannet/'
 class_names = ('cabinet', 'bed', 'chair', 'sofa', 'table', 'door', 'window',
                'bookshelf', 'picture', 'counter', 'desk', 'curtain',
                'refrigerator', 'showercurtrain', 'toilet', 'sink', 'bathtub',
                'garbagebin')
 
 metainfo = dict(classes=class_names)
+backend_args = None
 
 train_pipeline = [
     dict(
         type='LoadPointsFromFile',
         coord_type='DEPTH',
         load_dim=6,
-        use_dim=[0, 1, 2]),
+        use_dim=[0, 1, 2],
+        backend_args=backend_args),
     dict(
         type='LoadAnnotations3D',
         with_bbox_3d=True,
         with_label_3d=True,
         with_mask_3d=True,
-        with_seg_3d=True),
+        with_seg_3d=True,
+        backend_args=backend_args),
     dict(type='GlobalAlignment', rotation_axis=2),
     dict(type='PointSegClassMapping'),
     dict(type='PointSample', num_points=50000),
     dict(
         type='RandomFlip3D',
         sync_2d=False,
         flip_ratio_bev_horizontal=0.5,
@@ -127,15 +130,16 @@
         ])
 ]
 test_pipeline = [
     dict(
         type='LoadPointsFromFile',
         coord_type='DEPTH',
         load_dim=6,
-        use_dim=[0, 1, 2]),
+        use_dim=[0, 1, 2],
+        backend_args=backend_args),
     dict(type='GlobalAlignment', rotation_axis=2),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
@@ -166,39 +170,42 @@
             data_root=data_root,
             ann_file='scannet_infos_train.pkl',
             pipeline=train_pipeline,
             filter_empty_gt=False,
             metainfo=metainfo,
             # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
             # and box_type_3d='Depth' in sunrgbd and scannet dataset.
-            box_type_3d='Depth')))
+            box_type_3d='Depth',
+            backend_args=backend_args)))
 val_dataloader = dict(
     batch_size=1,
     num_workers=1,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         ann_file='scannet_infos_val.pkl',
         pipeline=test_pipeline,
         metainfo=metainfo,
         test_mode=True,
-        box_type_3d='Depth'))
+        box_type_3d='Depth',
+        backend_args=backend_args))
 test_dataloader = dict(
     batch_size=1,
     num_workers=1,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         ann_file='scannet_infos_val.pkl',
         pipeline=test_pipeline,
         metainfo=metainfo,
         test_mode=True,
-        box_type_3d='Depth'))
+        box_type_3d='Depth',
+        backend_args=backend_args))
 val_evaluator = dict(type='IndoorMetric')
 test_evaluator = val_evaluator
 
 # optimizer
 lr = 0.006
 optim_wrapper = dict(
     type='OptimWrapper',
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/groupfree3d/groupfree3d_w2x-head-L12-O512_4xb8_scannet-seg.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/groupfree3d/groupfree3d_w2x-head-L12-O512_4xb8_scannet-seg.py`

 * *Files 8% similar despite different names*

```diff
@@ -91,27 +91,30 @@
 data_root = './data/scannet/'
 class_names = ('cabinet', 'bed', 'chair', 'sofa', 'table', 'door', 'window',
                'bookshelf', 'picture', 'counter', 'desk', 'curtain',
                'refrigerator', 'showercurtrain', 'toilet', 'sink', 'bathtub',
                'garbagebin')
 
 metainfo = dict(classes=class_names)
+backend_args = None
 
 train_pipeline = [
     dict(
         type='LoadPointsFromFile',
         coord_type='DEPTH',
         load_dim=6,
-        use_dim=[0, 1, 2]),
+        use_dim=[0, 1, 2],
+        backend_args=backend_args),
     dict(
         type='LoadAnnotations3D',
         with_bbox_3d=True,
         with_label_3d=True,
         with_mask_3d=True,
-        with_seg_3d=True),
+        with_seg_3d=True,
+        backend_args=backend_args),
     dict(type='GlobalAlignment', rotation_axis=2),
     dict(type='PointSegClassMapping'),
     dict(type='PointSample', num_points=50000),
     dict(
         type='RandomFlip3D',
         sync_2d=False,
         flip_ratio_bev_horizontal=0.5,
@@ -128,15 +131,16 @@
         ])
 ]
 test_pipeline = [
     dict(
         type='LoadPointsFromFile',
         coord_type='DEPTH',
         load_dim=6,
-        use_dim=[0, 1, 2]),
+        use_dim=[0, 1, 2],
+        backend_args=backend_args),
     dict(type='GlobalAlignment', rotation_axis=2),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
@@ -167,39 +171,42 @@
             data_root=data_root,
             ann_file='scannet_infos_train.pkl',
             pipeline=train_pipeline,
             filter_empty_gt=False,
             metainfo=metainfo,
             # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
             # and box_type_3d='Depth' in sunrgbd and scannet dataset.
-            box_type_3d='Depth')))
+            box_type_3d='Depth',
+            backend_args=backend_args)))
 val_dataloader = dict(
     batch_size=1,
     num_workers=1,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         ann_file='scannet_infos_val.pkl',
         pipeline=test_pipeline,
         metainfo=metainfo,
         test_mode=True,
-        box_type_3d='Depth'))
+        box_type_3d='Depth',
+        backend_args=backend_args))
 test_dataloader = dict(
     batch_size=1,
     num_workers=1,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         ann_file='scannet_infos_val.pkl',
         pipeline=test_pipeline,
         metainfo=metainfo,
         test_mode=True,
-        box_type_3d='Depth'))
+        box_type_3d='Depth',
+        backend_args=backend_args))
 val_evaluator = dict(type='IndoorMetric')
 test_evaluator = val_evaluator
 
 # optimizer
 lr = 0.006
 optim_wrapper = dict(
     type='OptimWrapper',
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/groupfree3d/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/groupfree3d/metafile.yml`

 * *Files 2% similar despite different names*

```diff
@@ -21,16 +21,16 @@
     Metadata:
       Training Data: ScanNet
       Training Memory (GB): 6.7
     Results:
       - Task: 3D Object Detection
         Dataset: ScanNet
         Metrics:
-          AP@0.25: 66.32
-          AP@0.5: 47.82
+          AP@0.25: 66.17
+          AP@0.5: 48.47
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/groupfree3d/groupfree3d_8x4_scannet-3d-18class-L6-O256/groupfree3d_8x4_scannet-3d-18class-L6-O256_20210702_145347-3499eb55.pth
 
   - Name: groupfree3d_head-L12-O256_4xb8_scannet-seg.py
     In Collection: Group-Free-3D
     Config: configs/groupfree3d/groupfree3d_head-L12-O256_4xb8_scannet-seg.py
     Metadata:
       Training Data: ScanNet
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/h3dnet/h3dnet_8xb3_scannet-seg.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/h3dnet/h3dnet_8xb3_scannet-seg.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/h3dnet/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/h3dnet/metafile.yml`

 * *Files 1% similar despite different names*

```diff
@@ -22,8 +22,8 @@
       Training Memory (GB): 7.9
     Results:
       - Task: 3D Object Detection
         Dataset: ScanNet
         Metrics:
           AP@0.25: 66.07
           AP@0.5: 47.68
-    Weights: https://download.openmmlab.com/mmdetection3d/v1.0.0_models/h3dnet/h3dnet_scannet-3d-18class/h3dnet_3x8_scannet-3d-18class_20210824_003149-414bd304.pth
+    Weights: https://download.openmmlab.com/mmdetection3d/v1.0.0_models/h3dnet/h3dnet_3x8_scannet-3d-18class/h3dnet_3x8_scannet-3d-18class_20210824_003149-414bd304.pth
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/imvotenet/imvotenet_faster-rcnn-r50_fpn_4xb2_sunrgbd-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/imvotenet/imvotenet_faster-rcnn-r50_fpn_4xb2_sunrgbd-3d.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 _base_ = [
     '../_base_/datasets/sunrgbd-3d.py', '../_base_/default_runtime.py',
     '../_base_/models/imvotenet.py'
 ]
 
+backend_args = None
+
 train_pipeline = [
-    dict(type='LoadImageFromFile'),
+    dict(type='LoadImageFromFile', backend_args=backend_args),
     dict(
         type='LoadAnnotations3D',
         with_bbox=True,
         with_label=True,
         with_bbox_3d=False,
         with_label_3d=False),
     dict(
@@ -18,22 +20,15 @@
         keep_ratio=True),
     dict(type='RandomFlip', prob=0.5),
     dict(
         type='Pack3DDetInputs', keys=['img', 'gt_bboxes', 'gt_bboxes_labels']),
 ]
 
 test_pipeline = [
-    dict(type='LoadImageFromFile'),
-    # online evaluation
-    dict(
-        type='LoadAnnotations3D',
-        with_bbox=True,
-        with_label=True,
-        with_bbox_3d=False,
-        with_label_3d=False),
+    dict(type='LoadImageFromFile', backend_args=backend_args),
     dict(type='Resize', scale=(1333, 600), keep_ratio=True),
     dict(
         type='Pack3DDetInputs',
         keys=(['img']),
         meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',
                    'scale_factor'))
 ]
@@ -60,14 +55,15 @@
         begin=0,
         end=8,
         by_epoch=True,
         milestones=[6],
         gamma=0.1)
 ]
 val_evaluator = dict(type='Indoor2DMetric')
+test_evaluator = val_evaluator
 
 # optimizer
 optim_wrapper = dict(
     type='OptimWrapper',
     optimizer=dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001))
 
 load_from = 'http://download.openmmlab.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'  # noqa
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/imvotenet/imvotenet_stage2_8xb16_sunrgbd-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/imvotenet/imvotenet_stage2_8xb16_sunrgbd-3d.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 _base_ = [
     '../_base_/datasets/sunrgbd-3d.py', '../_base_/schedules/schedule-3x.py',
     '../_base_/default_runtime.py', '../_base_/models/imvotenet.py'
 ]
 
 class_names = ('bed', 'table', 'sofa', 'chair', 'toilet', 'desk', 'dresser',
                'night_stand', 'bookshelf', 'bathtub')
+backend_args = None
 
 model = dict(
     pts_backbone=dict(
         type='PointNet2SASSG',
         in_channels=4,
         num_points=(2048, 1024, 512, 256),
         radius=(0.2, 0.4, 0.8, 1.2),
@@ -166,16 +167,17 @@
 
 train_pipeline = [
     dict(
         type='LoadPointsFromFile',
         coord_type='DEPTH',
         shift_height=True,
         load_dim=6,
-        use_dim=[0, 1, 2]),
-    dict(type='LoadImageFromFile'),
+        use_dim=[0, 1, 2],
+        backend_args=backend_args),
+    dict(type='LoadImageFromFile', backend_args=backend_args),
     dict(
         type='LoadAnnotations3D',
         with_bbox=True,
         with_label=True,
         with_bbox_3d=True,
         with_label_3d=True),
     dict(type='Resize', scale=(1333, 600), keep_ratio=True),
@@ -194,31 +196,33 @@
         type='Pack3DDetInputs',
         keys=([
             'img', 'gt_bboxes', 'gt_bboxes_labels', 'points', 'gt_bboxes_3d',
             'gt_labels_3d'
         ]))
 ]
 test_pipeline = [
-    dict(type='LoadImageFromFile'),
+    dict(type='LoadImageFromFile', backend_args=backend_args),
     dict(
         type='LoadPointsFromFile',
         coord_type='DEPTH',
         shift_height=True,
         load_dim=6,
-        use_dim=[0, 1, 2]),
+        use_dim=[0, 1, 2],
+        backend_args=backend_args),
     dict(type='Resize', scale=(1333, 600), keep_ratio=True),
     dict(type='PointSample', num_points=20000),
     dict(type='Pack3DDetInputs', keys=['img', 'points'])
 ]
 
 train_dataloader = dict(dataset=dict(dataset=dict(pipeline=train_pipeline)))
 
 val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
 test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
 
 # may also use your own pre-trained image branch
-load_from = 'https://download.openmmlab.com/mmdetection3d/v0.1.0_models/imvotenet/imvotenet_faster_rcnn_r50_fpn_2x4_sunrgbd-3d-10class/imvotenet_faster_rcnn_r50_fpn_2x4_sunrgbd-3d-10class_20210323_173222-cad62aeb.pth'  # noqa
+load_from = 'https://download.openmmlab.com/mmdetection3d/v1.0.0_models/imvotenet/imvotenet_faster_rcnn_r50_fpn_2x4_sunrgbd-3d-10class/imvotenet_faster_rcnn_r50_fpn_2x4_sunrgbd-3d-10class_20210819_225618-62eba6ce.pth'  # noqa
 # Default setting for scaling LR automatically
 #   - `enable` means enable scaling LR automatically
 #       or not by default.
 #   - `base_batch_size` = (8 GPUs) x (16 samples per GPU).
 auto_scale_lr = dict(enable=False, base_batch_size=128)
+randomness = dict(seed=8)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/imvotenet/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/imvotenet/metafile.yml`

 * *Files 2% similar despite different names*

```diff
@@ -35,9 +35,9 @@
     Config: configs/imvotenet/imvotenet_stage2_8xb16_sunrgbd-3d.py
     Metadata:
       Training Memory (GB): 9.4
     Results:
       - Task: 3D Object Detection
         Dataset: SUNRGBD-3D
         Metrics:
-          AP@0.25: 64.55
+          AP@0.25: 64.48
     Weights: https://download.openmmlab.com/mmdetection3d/v1.0.0_models/imvotenet/imvotenet_stage2_16x8_sunrgbd-3d-10class/imvotenet_stage2_16x8_sunrgbd-3d-10class_20210819_192851-1bcd1b97.pth
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/imvoxelnet/imvoxelnet_2xb4_sunrgbd-3d-10class.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/imvoxelnet/imvoxelnet_8xb4_kitti-3d-car.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,14 +1,11 @@
 _base_ = [
     '../_base_/schedules/mmdet-schedule-1x.py', '../_base_/default_runtime.py'
 ]
-prior_generator = dict(
-    type='AlignedAnchor3DRangeGenerator',
-    ranges=[[-3.2, -0.2, -2.28, 3.2, 6.2, 0.28]],
-    rotations=[.0])
+
 model = dict(
     type='ImVoxelNet',
     data_preprocessor=dict(
         type='Det3DDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True,
@@ -22,107 +19,136 @@
         norm_cfg=dict(type='BN', requires_grad=False),
         norm_eval=True,
         init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
         style='pytorch'),
     neck=dict(
         type='mmdet.FPN',
         in_channels=[256, 512, 1024, 2048],
-        out_channels=256,
+        out_channels=64,
         num_outs=4),
-    neck_3d=dict(
-        type='IndoorImVoxelNeck',
-        in_channels=256,
-        out_channels=128,
-        n_blocks=[1, 1, 1]),
+    neck_3d=dict(type='OutdoorImVoxelNeck', in_channels=64, out_channels=256),
     bbox_head=dict(
-        type='ImVoxelHead',
-        n_classes=10,
-        n_levels=3,
-        n_channels=128,
-        n_reg_outs=7,
-        pts_assign_threshold=27,
-        pts_center_threshold=18,
-        prior_generator=prior_generator),
-    prior_generator=prior_generator,
-    n_voxels=[40, 40, 16],
-    coord_type='DEPTH',
-    train_cfg=dict(),
-    test_cfg=dict(nms_pre=1000, iou_thr=.25, score_thr=.01))
-
-dataset_type = 'SUNRGBDDataset'
-data_root = 'data/sunrgbd/'
-class_names = [
-    'bed', 'table', 'sofa', 'chair', 'toilet', 'desk', 'dresser',
-    'night_stand', 'bookshelf', 'bathtub'
-]
-metainfo = dict(CLASSES=class_names)
+        type='Anchor3DHead',
+        num_classes=1,
+        in_channels=256,
+        feat_channels=256,
+        use_direction_classifier=True,
+        anchor_generator=dict(
+            type='AlignedAnchor3DRangeGenerator',
+            ranges=[[-0.16, -39.68, -1.78, 68.96, 39.68, -1.78]],
+            sizes=[[3.9, 1.6, 1.56]],
+            rotations=[0, 1.57],
+            reshape_out=True),
+        diff_rad_by_sin=True,
+        bbox_coder=dict(type='DeltaXYZWLHRBBoxCoder'),
+        loss_cls=dict(
+            type='mmdet.FocalLoss',
+            use_sigmoid=True,
+            gamma=2.0,
+            alpha=0.25,
+            loss_weight=1.0),
+        loss_bbox=dict(
+            type='mmdet.SmoothL1Loss', beta=1.0 / 9.0, loss_weight=2.0),
+        loss_dir=dict(
+            type='mmdet.CrossEntropyLoss', use_sigmoid=False,
+            loss_weight=0.2)),
+    n_voxels=[216, 248, 12],
+    coord_type='LIDAR',
+    prior_generator=dict(
+        type='AlignedAnchor3DRangeGenerator',
+        ranges=[[-0.16, -39.68, -3.08, 68.96, 39.68, 0.76]],
+        rotations=[.0]),
+    train_cfg=dict(
+        assigner=dict(
+            type='Max3DIoUAssigner',
+            iou_calculator=dict(type='mmdet3d.BboxOverlapsNearest3D'),
+            pos_iou_thr=0.6,
+            neg_iou_thr=0.45,
+            min_pos_iou=0.45,
+            ignore_iof_thr=-1),
+        allowed_border=0,
+        pos_weight=-1,
+        debug=False),
+    test_cfg=dict(
+        use_rotate_nms=True,
+        nms_across_levels=False,
+        nms_thr=0.01,
+        score_thr=0.1,
+        min_bbox_size=0,
+        nms_pre=100,
+        max_num=50))
+
+dataset_type = 'KittiDataset'
+data_root = 'data/kitti/'
+class_names = ['Car']
+input_modality = dict(use_lidar=False, use_camera=True)
+point_cloud_range = [0, -39.68, -3, 69.12, 39.68, 1]
+metainfo = dict(classes=class_names)
 
-file_client_args = dict(backend='disk')
-# Uncomment the following if use ceph or other file clients.
-# See https://mmcv.readthedocs.io/en/latest/api.html#mmcv.fileio.FileClient
-# for more details.
-# file_client_args = dict(
-#     backend='petrel',
-#     path_mapping=dict({
-#         './data/sunrgbd/':
-#         's3://openmmlab/datasets/detection3d/sunrgbd_processed/',
-#         'data/sunrgbd/':
-#         's3://openmmlab/datasets/detection3d/sunrgbd_processed/'
-#     }))
+backend_args = None
 
 train_pipeline = [
-    dict(type='LoadAnnotations3D'),
-    dict(type='LoadImageFromFile', file_client_args=file_client_args),
-    dict(type='RandomResize', scale=[(512, 384), (768, 576)], keep_ratio=True),
+    dict(type='LoadAnnotations3D', backend_args=backend_args),
+    dict(type='LoadImageFromFileMono3D', backend_args=backend_args),
     dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
+    dict(
+        type='RandomResize', scale=[(1173, 352), (1387, 416)],
+        keep_ratio=True),
+    dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
     dict(type='Pack3DDetInputs', keys=['img', 'gt_bboxes_3d', 'gt_labels_3d'])
 ]
 test_pipeline = [
-    dict(type='LoadImageFromFile', file_client_args=file_client_args),
-    dict(type='Resize', scale=(640, 480), keep_ratio=True),
+    dict(type='LoadImageFromFileMono3D', backend_args=backend_args),
+    dict(type='Resize', scale=(1280, 384), keep_ratio=True),
     dict(type='Pack3DDetInputs', keys=['img'])
 ]
 
 train_dataloader = dict(
     batch_size=4,
     num_workers=4,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type='RepeatDataset',
-        times=2,
+        times=3,
         dataset=dict(
             type=dataset_type,
             data_root=data_root,
-            ann_file='sunrgbd_infos_train.pkl',
+            ann_file='kitti_infos_train.pkl',
+            data_prefix=dict(img='training/image_2'),
             pipeline=train_pipeline,
+            modality=input_modality,
             test_mode=False,
-            filter_empty_gt=True,
-            box_type_3d='Depth',
-            metainfo=metainfo)))
+            metainfo=metainfo,
+            box_type_3d='LiDAR',
+            backend_args=backend_args)))
 val_dataloader = dict(
     batch_size=1,
     num_workers=1,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        ann_file='sunrgbd_infos_val.pkl',
+        ann_file='kitti_infos_val.pkl',
+        data_prefix=dict(img='training/image_2'),
         pipeline=test_pipeline,
+        modality=input_modality,
         test_mode=True,
-        box_type_3d='Depth',
-        metainfo=metainfo))
+        metainfo=metainfo,
+        box_type_3d='LiDAR',
+        backend_args=backend_args))
 test_dataloader = val_dataloader
 
 val_evaluator = dict(
-    type='IndoorMetric',
-    ann_file=data_root + 'sunrgbd_infos_val.pkl',
-    metric='bbox')
+    type='KittiMetric',
+    ann_file=data_root + 'kitti_infos_val.pkl',
+    metric='bbox',
+    backend_args=backend_args)
 test_evaluator = val_evaluator
 
 # optimizer
 optim_wrapper = dict(
     type='OptimWrapper',
     optimizer=dict(
         _delete_=True, type='AdamW', lr=0.0001, weight_decay=0.0001),
@@ -140,7 +166,11 @@
 ]
 
 # hooks
 default_hooks = dict(checkpoint=dict(type='CheckpointHook', max_keep_ckpts=1))
 
 # runtime
 find_unused_parameters = True  # only 1 of 4 FPN outputs is used
+
+vis_backends = [dict(type='LocalVisBackend')]
+visualizer = dict(
+    type='Det3DLocalVisualizer', vis_backends=vis_backends, name='visualizer')
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/imvoxelnet/imvoxelnet_8xb4_kitti-3d-car.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/waymoD5-3d-car.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,178 +1,174 @@
-_base_ = [
-    '../_base_/schedules/mmdet-schedule-1x.py', '../_base_/default_runtime.py'
-]
-
-model = dict(
-    type='ImVoxelNet',
-    data_preprocessor=dict(
-        type='Det3DDataPreprocessor',
-        mean=[123.675, 116.28, 103.53],
-        std=[58.395, 57.12, 57.375],
-        bgr_to_rgb=True,
-        pad_size_divisor=32),
-    backbone=dict(
-        type='mmdet.ResNet',
-        depth=50,
-        num_stages=4,
-        out_indices=(0, 1, 2, 3),
-        frozen_stages=1,
-        norm_cfg=dict(type='BN', requires_grad=False),
-        norm_eval=True,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
-        style='pytorch'),
-    neck=dict(
-        type='mmdet.FPN',
-        in_channels=[256, 512, 1024, 2048],
-        out_channels=64,
-        num_outs=4),
-    neck_3d=dict(type='OutdoorImVoxelNeck', in_channels=64, out_channels=256),
-    bbox_head=dict(
-        type='Anchor3DHead',
-        num_classes=1,
-        in_channels=256,
-        feat_channels=256,
-        use_direction_classifier=True,
-        anchor_generator=dict(
-            type='AlignedAnchor3DRangeGenerator',
-            ranges=[[-0.16, -39.68, -1.78, 68.96, 39.68, -1.78]],
-            sizes=[[3.9, 1.6, 1.56]],
-            rotations=[0, 1.57],
-            reshape_out=True),
-        diff_rad_by_sin=True,
-        bbox_coder=dict(type='DeltaXYZWLHRBBoxCoder'),
-        loss_cls=dict(
-            type='mmdet.FocalLoss',
-            use_sigmoid=True,
-            gamma=2.0,
-            alpha=0.25,
-            loss_weight=1.0),
-        loss_bbox=dict(
-            type='mmdet.SmoothL1Loss', beta=1.0 / 9.0, loss_weight=2.0),
-        loss_dir=dict(
-            type='mmdet.CrossEntropyLoss', use_sigmoid=False,
-            loss_weight=0.2)),
-    n_voxels=[216, 248, 12],
-    coord_type='LIDAR',
-    prior_generator=dict(
-        type='AlignedAnchor3DRangeGenerator',
-        ranges=[[-0.16, -39.68, -3.08, 68.96, 39.68, 0.76]],
-        rotations=[.0]),
-    train_cfg=dict(
-        assigner=dict(
-            type='Max3DIoUAssigner',
-            iou_calculator=dict(type='mmdet3d.BboxOverlapsNearest3D'),
-            pos_iou_thr=0.6,
-            neg_iou_thr=0.45,
-            min_pos_iou=0.45,
-            ignore_iof_thr=-1),
-        allowed_border=0,
-        pos_weight=-1,
-        debug=False),
-    test_cfg=dict(
-        use_rotate_nms=True,
-        nms_across_levels=False,
-        nms_thr=0.01,
-        score_thr=0.1,
-        min_bbox_size=0,
-        nms_pre=100,
-        max_num=50))
+# dataset settings
+# D5 in the config name means the whole dataset is divided into 5 folds
+# We only use one fold for efficient experiments
+dataset_type = 'WaymoDataset'
+data_root = 'data/waymo/kitti_format/'
+
+# Example to use different file client
+# Method 1: simply set the data root and let the file I/O module
+# automatically infer from prefix (not support LMDB and Memcache yet)
+
+# data_root = 's3://openmmlab/datasets/detection3d/waymo/kitti_format/'
+
+# Method 2: Use backend_args, file_client_args in versions before 1.1.0
+# backend_args = dict(
+#     backend='petrel',
+#     path_mapping=dict({
+#         './data/': 's3://openmmlab/datasets/detection3d/',
+#          'data/': 's3://openmmlab/datasets/detection3d/'
+#      }))
+backend_args = None
 
-dataset_type = 'KittiDataset'
-data_root = 'data/kitti/'
 class_names = ['Car']
-input_modality = dict(use_lidar=False, use_camera=True)
-point_cloud_range = [0, -39.68, -3, 69.12, 39.68, 1]
 metainfo = dict(classes=class_names)
 
-# file_client_args = dict(backend='disk')
-# Uncomment the following if use ceph or other file clients.
-# See https://mmcv.readthedocs.io/en/latest/api.html#mmcv.fileio.FileClient
-# for more details.
-file_client_args = dict(
-    backend='petrel',
-    path_mapping=dict({
-        './data/kitti/':
-        's3://openmmlab/datasets/detection3d/kitti/',
-        'data/kitti/':
-        's3://openmmlab/datasets/detection3d/kitti/'
-    }))
+point_cloud_range = [-74.88, -74.88, -2, 74.88, 74.88, 4]
+input_modality = dict(use_lidar=True, use_camera=False)
+db_sampler = dict(
+    data_root=data_root,
+    info_path=data_root + 'waymo_dbinfos_train.pkl',
+    rate=1.0,
+    prepare=dict(filter_by_difficulty=[-1], filter_by_min_points=dict(Car=5)),
+    classes=class_names,
+    sample_groups=dict(Car=15),
+    points_loader=dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=6,
+        use_dim=[0, 1, 2, 3, 4],
+        backend_args=backend_args),
+    backend_args=backend_args)
 
 train_pipeline = [
-    dict(type='LoadAnnotations3D'),
-    dict(type='LoadImageFromFileMono3D'),
-    dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
     dict(
-        type='RandomResize', scale=[(1173, 352), (1387, 416)],
-        keep_ratio=True),
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=6,
+        use_dim=5,
+        backend_args=backend_args),
+    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
+    dict(type='ObjectSample', db_sampler=db_sampler),
+    dict(
+        type='RandomFlip3D',
+        sync_2d=False,
+        flip_ratio_bev_horizontal=0.5,
+        flip_ratio_bev_vertical=0.5),
+    dict(
+        type='GlobalRotScaleTrans',
+        rot_range=[-0.78539816, 0.78539816],
+        scale_ratio_range=[0.95, 1.05]),
+    dict(type='PointsRangeFilter', point_cloud_range=point_cloud_range),
     dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
-    dict(type='Pack3DDetInputs', keys=['img', 'gt_bboxes_3d', 'gt_labels_3d'])
+    dict(type='PointShuffle'),
+    dict(
+        type='Pack3DDetInputs',
+        keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
 ]
 test_pipeline = [
-    dict(type='LoadImageFromFileMono3D'),
-    dict(type='Resize', scale=(1280, 384), keep_ratio=True),
-    dict(type='Pack3DDetInputs', keys=['img'])
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=6,
+        use_dim=5,
+        backend_args=backend_args),
+    dict(
+        type='MultiScaleFlipAug3D',
+        img_scale=(1333, 800),
+        pts_scale_ratio=1,
+        flip=False,
+        transforms=[
+            dict(
+                type='GlobalRotScaleTrans',
+                rot_range=[0, 0],
+                scale_ratio_range=[1., 1.],
+                translation_std=[0, 0, 0]),
+            dict(type='RandomFlip3D'),
+            dict(
+                type='PointsRangeFilter', point_cloud_range=point_cloud_range)
+        ]),
+    dict(type='Pack3DDetInputs', keys=['points'])
+]
+# construct a pipeline for data and gt loading in show function
+# please keep its loading function consistent with test_pipeline (e.g. client)
+eval_pipeline = [
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=6,
+        use_dim=5,
+        backend_args=backend_args),
+    dict(type='Pack3DDetInputs', keys=['points']),
 ]
 
 train_dataloader = dict(
-    batch_size=4,
-    num_workers=4,
+    batch_size=2,
+    num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type='RepeatDataset',
-        times=3,
+        times=2,
         dataset=dict(
             type=dataset_type,
             data_root=data_root,
-            ann_file='kitti_infos_train.pkl',
-            data_prefix=dict(img='training/image_2'),
+            ann_file='waymo_infos_train.pkl',
+            data_prefix=dict(
+                pts='training/velodyne', sweeps='training/velodyne'),
             pipeline=train_pipeline,
             modality=input_modality,
             test_mode=False,
-            metainfo=metainfo)))
+            metainfo=metainfo,
+            # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
+            # and box_type_3d='Depth' in sunrgbd and scannet dataset.
+            box_type_3d='LiDAR',
+            # load one frame every five frames
+            load_interval=5,
+            backend_args=backend_args)))
 val_dataloader = dict(
     batch_size=1,
     num_workers=1,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        ann_file='kitti_infos_val.pkl',
-        data_prefix=dict(img='training/image_2'),
-        pipeline=test_pipeline,
+        data_prefix=dict(pts='training/velodyne', sweeps='training/velodyne'),
+        ann_file='waymo_infos_val.pkl',
+        pipeline=eval_pipeline,
         modality=input_modality,
         test_mode=True,
-        metainfo=metainfo))
-test_dataloader = val_dataloader
+        metainfo=metainfo,
+        box_type_3d='LiDAR',
+        backend_args=backend_args))
+
+test_dataloader = dict(
+    batch_size=1,
+    num_workers=1,
+    persistent_workers=True,
+    drop_last=False,
+    sampler=dict(type='DefaultSampler', shuffle=False),
+    dataset=dict(
+        type=dataset_type,
+        data_root=data_root,
+        data_prefix=dict(pts='training/velodyne', sweeps='training/velodyne'),
+        ann_file='waymo_infos_val.pkl',
+        pipeline=eval_pipeline,
+        modality=input_modality,
+        test_mode=True,
+        metainfo=metainfo,
+        box_type_3d='LiDAR',
+        backend_args=backend_args))
 
 val_evaluator = dict(
-    type='KittiMetric',
-    ann_file=data_root + 'kitti_infos_val.pkl',
-    metric='bbox')
+    type='WaymoMetric',
+    ann_file='./data/waymo/kitti_format/waymo_infos_val.pkl',
+    waymo_bin_file='./data/waymo/waymo_format/gt.bin',
+    data_root='./data/waymo/waymo_format',
+    convert_kitti_format=False,
+    backend_args=backend_args)
 test_evaluator = val_evaluator
 
-# optimizer
-optim_wrapper = dict(
-    type='OptimWrapper',
-    optimizer=dict(
-        _delete_=True, type='AdamW', lr=0.0001, weight_decay=0.0001),
-    paramwise_cfg=dict(
-        custom_keys={'backbone': dict(lr_mult=0.1, decay_mult=1.0)}),
-    clip_grad=dict(max_norm=35., norm_type=2))
-param_scheduler = [
-    dict(
-        type='MultiStepLR',
-        begin=0,
-        end=12,
-        by_epoch=True,
-        milestones=[8, 11],
-        gamma=0.1)
-]
-
-# hooks
-default_hooks = dict(checkpoint=dict(type='CheckpointHook', max_keep_ckpts=1))
-
-# runtime
-find_unused_parameters = True  # only 1 of 4 FPN outputs is used
+vis_backends = [dict(type='LocalVisBackend')]
+visualizer = dict(
+    type='Det3DLocalVisualizer', vis_backends=vis_backends, name='visualizer')
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/imvoxelnet/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/imvoxelnet/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/monoflex/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/monoflex/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/mvxnet/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/mvxnet/metafile.yml`

 * *Files 14% similar despite different names*

```diff
@@ -14,17 +14,18 @@
     README: configs/mvxnet/README.md
     Code:
       URL: https://github.com/open-mmlab/mmdetection3d/blob/master/mmdet3d/models/detectors/mvx_two_stage.py#L20
       Version: v0.5.0
 
 Models:
   - Name: dv_mvx-fpn_second_secfpn_adamw_2x8_80e_kitti-3d-3class
+    Alias: mvxnet_kitti-3class
     In Collection: MVX-Net
     Config: configs/mvxnet/mvxnet_fpn_dv_second_secfpn_8xb2-80e_kitti-3d-3class.py
     Metadata:
       Training Memory (GB): 6.7
     Results:
       - Task: 3D Object Detection
         Dataset: KITTI
         Metrics:
-          mAP: 63.22
-    Weights: https://download.openmmlab.com/mmdetection3d/v1.0.0_models/mvxnet/dv_mvx-fpn_second_secfpn_adamw_2x8_80e_kitti-3d-3class/dv_mvx-fpn_second_secfpn_adamw_2x8_80e_kitti-3d-3class_20210831_060805-83442923.pth
+          mAP: 63.5
+    Weights: https://download.openmmlab.com/mmdetection3d/v1.1.0_models/mvxnet/mvxnet_fpn_dv_second_secfpn_8xb2-80e_kitti-3d-3class/mvxnet_fpn_dv_second_secfpn_8xb2-80e_kitti-3d-3class-8963258a.pth
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/mvxnet/mvxnet_fpn_dv_second_secfpn_8xb2-80e_kitti-3d-3class.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/mvxnet/mvxnet_fpn_dv_second_secfpn_8xb2-80e_kitti-3d-3class.py`

 * *Files 8% similar despite different names*

```diff
@@ -28,14 +28,16 @@
         norm_cfg=dict(type='BN', requires_grad=False),
         norm_eval=True,
         style='caffe'),
     img_neck=dict(
         type='mmdet.FPN',
         in_channels=[256, 512, 1024, 2048],
         out_channels=256,
+        # make the image features more stable numerically to avoid loss nan
+        norm_cfg=dict(type='BN', requires_grad=False),
         num_outs=5),
     pts_voxel_encoder=dict(
         type='DynamicVFE',
         in_channels=4,
         feat_channels=[64, 64],
         with_distance=False,
         voxel_size=voxel_size,
@@ -140,17 +142,23 @@
 
 # dataset settings
 dataset_type = 'KittiDataset'
 data_root = 'data/kitti/'
 class_names = ['Pedestrian', 'Cyclist', 'Car']
 metainfo = dict(classes=class_names)
 input_modality = dict(use_lidar=True, use_camera=True)
+backend_args = None
 train_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4),
-    dict(type='LoadImageFromFile'),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
+    dict(type='LoadImageFromFile', backend_args=backend_args),
     dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
     dict(
         type='RandomResize', scale=[(640, 192), (2560, 768)], keep_ratio=True),
     dict(
         type='GlobalRotScaleTrans',
         rot_range=[-0.78539816, 0.78539816],
         scale_ratio_range=[0.95, 1.05],
@@ -163,16 +171,21 @@
         type='Pack3DDetInputs',
         keys=[
             'points', 'img', 'gt_bboxes_3d', 'gt_labels_3d', 'gt_bboxes',
             'gt_labels'
         ])
 ]
 test_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4),
-    dict(type='LoadImageFromFile'),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
+    dict(type='LoadImageFromFile', backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1280, 384),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
             # Temporary solution, fix this after refactor the augtest
@@ -204,15 +217,16 @@
             data_prefix=dict(
                 pts='training/velodyne_reduced', img='training/image_2'),
             pipeline=train_pipeline,
             filter_empty_gt=False,
             metainfo=metainfo,
             # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
             # and box_type_3d='Depth' in sunrgbd and scannet dataset.
-            box_type_3d='LiDAR')))
+            box_type_3d='LiDAR',
+            backend_args=backend_args)))
 
 val_dataloader = dict(
     batch_size=1,
     num_workers=1,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
@@ -220,34 +234,40 @@
         modality=modality,
         ann_file='kitti_infos_val.pkl',
         data_prefix=dict(
             pts='training/velodyne_reduced', img='training/image_2'),
         pipeline=test_pipeline,
         metainfo=metainfo,
         test_mode=True,
-        box_type_3d='LiDAR'))
+        box_type_3d='LiDAR',
+        backend_args=backend_args))
 test_dataloader = dict(
     batch_size=1,
     num_workers=1,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         ann_file='kitti_infos_val.pkl',
         modality=modality,
         data_prefix=dict(
             pts='training/velodyne_reduced', img='training/image_2'),
         pipeline=test_pipeline,
         metainfo=metainfo,
         test_mode=True,
-        box_type_3d='LiDAR'))
+        box_type_3d='LiDAR',
+        backend_args=backend_args))
 
 optim_wrapper = dict(
     optimizer=dict(weight_decay=0.01),
     clip_grad=dict(max_norm=35, norm_type=2),
 )
 val_evaluator = dict(
     type='KittiMetric', ann_file='data/kitti/kitti_infos_val.pkl')
 test_evaluator = val_evaluator
 
+vis_backends = [dict(type='LocalVisBackend')]
+visualizer = dict(
+    type='Det3DLocalVisualizer', vis_backends=vis_backends, name='visualizer')
+
 # You may need to download the model first is the network is unstable
 load_from = 'https://download.openmmlab.com/mmdetection3d/pretrain_models/mvx_faster_rcnn_detectron2-caffe_20e_coco-pretrain_gt-sample_kitti-3-class_moderate-79.3_20200207-a4a6a3c7.pth'  # noqa
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/cascade-mask-rcnn_r50_fpn_1x_nuim.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/cascade-mask-rcnn_r50_fpn_1x_nuim.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = [
-    '../_base_/models/cascade_mask_rcnn_r50_fpn.py',
-    '../_base_/datasets/nuim_instance.py',
-    '../_base_/schedules/mmdet_schedule_1x.py', '../_base_/default_runtime.py'
+    '../_base_/models/cascade-mask-rcnn_r50_fpn.py',
+    '../_base_/datasets/nuim-instance.py',
+    '../_base_/schedules/mmdet-schedule-1x.py', '../_base_/default_runtime.py'
 ]
 model = dict(
     roi_head=dict(
         bbox_head=[
             dict(
                 type='Shared2FCBBoxHead',
                 in_channels=256,
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/htc_r50_fpn_1x_nuim.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/mask-rcnn_r50_caffe_fpn_1x_nuim.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,37 +1,41 @@
-_base_ = './htc_without_semantic_r50_fpn_1x_nuim.py'
+_base_ = [
+    '../_base_/models/mask-rcnn_r50_fpn.py',
+    '../_base_/datasets/nuim-instance.py',
+    '../_base_/schedules/mmdet-schedule-1x.py', '../_base_/default_runtime.py'
+]
 model = dict(
+    pretrained='open-mmlab://detectron2/resnet50_caffe',
+    backbone=dict(norm_cfg=dict(requires_grad=False), style='caffe'),
     roi_head=dict(
-        semantic_roi_extractor=dict(
-            type='SingleRoIExtractor',
-            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),
-            out_channels=256,
-            featmap_strides=[8]),
-        semantic_head=dict(
-            type='FusedSemanticHead',
-            num_ins=5,
-            fusion_level=1,
-            num_convs=4,
-            in_channels=256,
-            conv_out_channels=256,
-            num_classes=32,
-            ignore_label=0,
-            loss_weight=0.2)))
-
-data_root = 'data/nuimages/'
+        bbox_head=dict(num_classes=10), mask_head=dict(num_classes=10)))
+backend_args = None
 train_pipeline = [
-    dict(type='LoadImageFromFile'),
-    dict(
-        type='LoadAnnotations', with_bbox=True, with_mask=True, with_seg=True),
+    dict(type='LoadImageFromFile', backend_args=backend_args),
+    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
     dict(
         type='Resize',
         img_scale=[(1280, 720), (1920, 1080)],
         multiscale_mode='range',
         keep_ratio=True),
     dict(type='RandomFlip', flip_ratio=0.5),
-    dict(type='SegRescale', scale_factor=1 / 8),
-    dict(type='PackDetInputs')
+    dict(type='PackDetInputs'),
+]
+test_pipeline = [
+    dict(type='LoadImageFromFile', backend_args=backend_args),
+    dict(
+        type='MultiScaleFlipAug',
+        img_scale=(1600, 900),
+        flip=False,
+        transforms=[
+            dict(type='Resize', keep_ratio=True),
+            dict(type='RandomFlip'),
+        ]),
+    dict(
+        type='PackDetInputs',
+        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',
+                   'scale_factor')),
 ]
 data = dict(
-    train=dict(
-        seg_prefix=data_root + 'annotations/semantic_masks/',
-        pipeline=train_pipeline))
+    train=dict(pipeline=train_pipeline),
+    val=dict(pipeline=test_pipeline),
+    test=dict(pipeline=test_pipeline))
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/htc_r50_fpn_head-without-semantic_1x_nuim.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/htc_r50_fpn_head-without-semantic_1x_nuim.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/htc_x101_64x4d_fpn_dconv_c3-c5_coco-20e-1xb16_nuim.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/htc_x101_64x4d_fpn_dconv_c3-c5_coco-20e-1xb16_nuim.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/mask-rcnn_r50_caffe_fpn_1x_nuim.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/minkunet/minkunet18_w32_torchsparse_8xb2-amp-15e_semantickitti.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,40 +1,54 @@
 _base_ = [
-    '../_base_/models/mask-rcnn_r50_fpn.py',
-    '../_base_/datasets/nuim-instance.py',
-    '../_base_/schedules/mmdet-schedule-1x.py', '../_base_/default_runtime.py'
+    '../_base_/datasets/semantickitti.py', '../_base_/models/minkunet.py',
+    '../_base_/default_runtime.py'
 ]
-model = dict(
-    pretrained='open-mmlab://detectron2/resnet50_caffe',
-    backbone=dict(norm_cfg=dict(requires_grad=False), style='caffe'),
-    roi_head=dict(
-        bbox_head=dict(num_classes=10), mask_head=dict(num_classes=10)))
+
 train_pipeline = [
-    dict(type='LoadImageFromFile'),
-    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
+    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4),
     dict(
-        type='Resize',
-        img_scale=[(1280, 720), (1920, 1080)],
-        multiscale_mode='range',
-        keep_ratio=True),
-    dict(type='RandomFlip', flip_ratio=0.5),
-    dict(type='PackDetInputs'),
+        type='LoadAnnotations3D',
+        with_bbox_3d=False,
+        with_label_3d=False,
+        with_seg_3d=True,
+        seg_3d_dtype='np.int32',
+        seg_offset=2**16,
+        dataset_type='semantickitti'),
+    dict(type='PointSegClassMapping'),
+    dict(
+        type='GlobalRotScaleTrans',
+        rot_range=[0., 6.28318531],
+        scale_ratio_range=[0.95, 1.05],
+        translation_std=[0, 0, 0],
+    ),
+    dict(type='Pack3DDetInputs', keys=['points', 'pts_semantic_mask'])
 ]
-test_pipeline = [
-    dict(type='LoadImageFromFile'),
+
+train_dataloader = dict(
+    sampler=dict(seed=0), dataset=dict(pipeline=train_pipeline))
+
+lr = 0.24
+optim_wrapper = dict(
+    type='AmpOptimWrapper',
+    loss_scale='dynamic',
+    optimizer=dict(
+        type='SGD', lr=lr, weight_decay=0.0001, momentum=0.9, nesterov=True))
+
+param_scheduler = [
     dict(
-        type='MultiScaleFlipAug',
-        img_scale=(1600, 900),
-        flip=False,
-        transforms=[
-            dict(type='Resize', keep_ratio=True),
-            dict(type='RandomFlip'),
-        ]),
+        type='LinearLR', start_factor=0.008, by_epoch=False, begin=0, end=125),
     dict(
-        type='PackDetInputs',
-        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',
-                   'scale_factor')),
+        type='CosineAnnealingLR',
+        begin=0,
+        T_max=15,
+        by_epoch=True,
+        eta_min=1e-5,
+        convert_to_iter_based=True)
 ]
-data = dict(
-    train=dict(pipeline=train_pipeline),
-    val=dict(pipeline=test_pipeline),
-    test=dict(pipeline=test_pipeline))
+
+train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=15, val_interval=1)
+val_cfg = dict(type='ValLoop')
+test_cfg = dict(type='TestLoop')
+
+default_hooks = dict(checkpoint=dict(type='CheckpointHook', interval=1))
+randomness = dict(seed=0, deterministic=False, diff_rank_seed=True)
+env_cfg = dict(cudnn_benchmark=True)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/mask-rcnn_r50_caffe_fpn_coco-3x_1x_nuim.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/mask-rcnn_r50_caffe_fpn_coco-3x_1x_nuim.py`

 * *Files 5% similar despite different names*

```diff
@@ -4,27 +4,28 @@
     '../_base_/schedules/mmdet-schedule-1x.py', '../_base_/default_runtime.py'
 ]
 model = dict(
     pretrained='open-mmlab://detectron2/resnet50_caffe',
     backbone=dict(norm_cfg=dict(requires_grad=False), style='caffe'),
     roi_head=dict(
         bbox_head=dict(num_classes=10), mask_head=dict(num_classes=10)))
+backend_args = None
 train_pipeline = [
-    dict(type='LoadImageFromFile'),
+    dict(type='LoadImageFromFile', backend_args=backend_args),
     dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
     dict(
         type='Resize',
         img_scale=[(1280, 720), (1920, 1080)],
         multiscale_mode='range',
         keep_ratio=True),
     dict(type='RandomFlip', flip_ratio=0.5),
     dict(type='PackDetInputs'),
 ]
 test_pipeline = [
-    dict(type='LoadImageFromFile'),
+    dict(type='LoadImageFromFile', backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug',
         img_scale=(1600, 900),
         flip=False,
         transforms=[
             dict(type='Resize', keep_ratio=True),
             dict(type='RandomFlip'),
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/mask-rcnn_r50_caffe_fpn_coco-3x_20e_nuim.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/mask-rcnn_r50_caffe_fpn_coco-3x_20e_nuim.py`

 * *Files 14% similar despite different names*

```diff
@@ -4,27 +4,28 @@
     '../_base_/schedules/mmdet-schedule-1x.py', '../_base_/default_runtime.py'
 ]
 model = dict(
     pretrained='open-mmlab://detectron2/resnet50_caffe',
     backbone=dict(norm_cfg=dict(requires_grad=False), style='caffe'),
     roi_head=dict(
         bbox_head=dict(num_classes=10), mask_head=dict(num_classes=10)))
+backend_args = None
 train_pipeline = [
-    dict(type='LoadImageFromFile'),
+    dict(type='LoadImageFromFile', backend_args=backend_args),
     dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
     dict(
         type='Resize',
         img_scale=[(1280, 720), (1920, 1080)],
         multiscale_mode='range',
         keep_ratio=True),
     dict(type='RandomFlip', flip_ratio=0.5),
     dict(type='PackDetInputs'),
 ]
 test_pipeline = [
-    dict(type='LoadImageFromFile'),
+    dict(type='LoadImageFromFile', backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug',
         img_scale=(1600, 900),
         flip=False,
         transforms=[
             dict(type='Resize', keep_ratio=True),
             dict(type='RandomFlip'),
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/mask-rcnn_r50_fpn_coco-2x_1x_nus-2d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/mask-rcnn_r50_fpn_coco-2x_1x_nus-2d.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,25 +1,20 @@
 _base_ = [
-    '../_base_/models/mask_rcnn_r50_fpn.py',
-    '../_base_/datasets/nuim_instance.py',
-    '../_base_/schedules/mmdet_schedule_1x.py', '../_base_/default_runtime.py'
+    '../_base_/models/mask-rcnn_r50_fpn.py',
+    '../_base_/datasets/nuim-instance.py',
+    '../_base_/schedules/mmdet-schedule-1x.py', '../_base_/default_runtime.py'
 ]
 model = dict(
     roi_head=dict(
         bbox_head=dict(num_classes=10), mask_head=dict(num_classes=10)))
 
-file_client_args = dict(
-    backend='petrel',
-    path_mapping=dict({
-        './data/nuscenes/': 's3://nuscenes/nuscenes/',
-        'data/nuscenes/': 's3://nuscenes/nuscenes/'
-    }))
+backend_args = None
 
 test_pipeline = [
-    dict(type='LoadImageFromFile'),
+    dict(type='LoadImageFromFile', backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug',
         img_scale=(1600, 900),
         flip=False,
         transforms=[
             dict(type='Resize', keep_ratio=True),
             dict(type='RandomFlip'),
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/nuimages/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/nuimages/metafile.yml`

 * *Files 12% similar despite different names*

```diff
@@ -19,100 +19,100 @@
       Title: "Mask R-CNN"
     README: configs/nuimages/README.md
     Code:
       URL: https://github.com/open-mmlab/mmdetection/blob/v2.0.0/mmdet/models/detectors/mask_rcnn.py#L6
       Version: v2.0.0
 
 Models:
-  - Name: mask_rcnn_r50_fpn_1x_nuim
+  - Name: mask-rcnn_r50_fpn_1x_nuim
     In Collection: Mask R-CNN
-    Config: configs/nuimages/mask_rcnn_r50_fpn_1x_nuim.py
+    Config: configs/nuimages/mask-rcnn_r50_fpn_1x_nuim.py
     Metadata:
       Training Memory (GB): 7.4
       Training Resources: 8x TITAN Xp
     Results:
       - Task: Object Detection
         Dataset: nuImages
         Metrics:
           Box AP: 47.8
       - Task: Instance Segmentation
         Dataset: nuImages
         Metrics:
           Mask AP: 38.4
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/nuimages_semseg/mask_rcnn_r50_fpn_1x_nuim/mask_rcnn_r50_fpn_1x_nuim_20201008_195238-e99f5182.pth
 
-  - Name: mask_rcnn_r50_fpn_coco-2x_1x_nuim
+  - Name: mask-rcnn_r50_fpn_coco-2x_1x_nuim
     In Collection: Mask R-CNN
-    Config: configs/nuimages/mask_rcnn_r50_fpn_coco-2x_1x_nuim.py
+    Config: configs/nuimages/mask-rcnn_r50_fpn_coco-2x_1x_nuim.py
     Metadata:
       Training Memory (GB): 7.4
       Training Resources: 8x TITAN Xp
     Results:
       - Task: Object Detection
         Dataset: nuImages
         Metrics:
           Box AP: 49.7
       - Task: Instance Segmentation
         Dataset: nuImages
         Metrics:
           Mask AP: 40.5
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/nuimages_semseg/mask_rcnn_r50_fpn_coco-2x_1x_nuim/mask_rcnn_r50_fpn_coco-2x_1x_nuim_20201008_195238-b1742a60.pth
 
-  - Name: mask_rcnn_r50_caffe_fpn_1x_nuim
+  - Name: mask-rcnn_r50_caffe_fpn_1x_nuim
     In Collection: Mask R-CNN
-    Config: configs/nuimages/mask_rcnn_r50_caffe_fpn_1x_nuim.py
+    Config: configs/nuimages/mask-rcnn_r50_caffe_fpn_1x_nuim.py
     Metadata:
       Training Memory (GB): 7.0
       Training Resources: 8x TITAN Xp
     Results:
       - Task: Object Detection
         Dataset: nuImages
         Metrics:
           Box AP: 47.7
       - Task: Instance Segmentation
         Dataset: nuImages
         Metrics:
           Mask AP: 38.2
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/nuimages_semseg/mask_rcnn_r50_caffe_fpn_1x_nuim/
 
-  - Name: mask_rcnn_r50_caffe_fpn_coco-3x_1x_nuim
+  - Name: mask-rcnn_r50_caffe_fpn_coco-3x_1x_nuim
     In Collection: Mask R-CNN
-    Config: configs/nuimages/mask_rcnn_r50_caffe_fpn_coco-3x_1x_nuim.py
+    Config: configs/nuimages/mask-rcnn_r50_caffe_fpn_coco-3x_1x_nuim.py
     Metadata:
       Training Memory (GB): 7.0
       Training Resources: 8x TITAN Xp
     Results:
       - Task: Object Detection
         Dataset: nuImages
         Metrics:
           Box AP: 49.9
       - Task: Instance Segmentation
         Dataset: nuImages
         Metrics:
           Mask AP: 40.8
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/nuimages_semseg/mask_rcnn_r50_caffe_fpn_coco-3x_1x_nuim/mask_rcnn_r50_caffe_fpn_coco-3x_1x_nuim_20201008_195305-661a992e.pth
 
-  - Name: mask_rcnn_r50_caffe_fpn_coco-3x_20e_nuim
+  - Name: mask-rcnn_r50_caffe_fpn_coco-3x_20e_nuim
     In Collection: Mask R-CNN
-    Config: configs/nuimages/mask_rcnn_r50_caffe_fpn_coco-3x_20e_nuim.py
+    Config: configs/nuimages/mask-rcnn_r50_caffe_fpn_coco-3x_20e_nuim.py
     Metadata:
       Training Memory (GB): 7.0
       Training Resources: 8x TITAN Xp
     Results:
       - Task: Object Detection
         Dataset: nuImages
         Metrics:
           Box AP: 50.6
       - Task: Instance Segmentation
         Dataset: nuImages
         Metrics:
           Mask AP: 41.3
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/nuimages_semseg/mask_rcnn_r50_caffe_fpn_coco-3x_20e_nuim/mask_rcnn_r50_caffe_fpn_coco-3x_20e_nuim_20201009_125002-5529442c.pth
 
-  - Name: mask_rcnn_r101_fpn_1x_nuim
+  - Name: mask-rcnn_r101_fpn_1x_nuim
     In Collection: Mask R-CNN
     Config: configs/nuimages/mask-rcnn_r101_fpn_1x_nuim.py
     Metadata:
       Training Memory (GB): 10.9
       Training Resources: 8x TITAN Xp
     Results:
       - Task: Object Detection
@@ -121,15 +121,15 @@
           Box AP: 48.9
       - Task: Instance Segmentation
         Dataset: nuImages
         Metrics:
           Mask AP: 39.1
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/nuimages_semseg/mask_rcnn_r101_fpn_1x_nuim/mask_rcnn_r101_fpn_1x_nuim_20201024_134803-65c7623a.pth
 
-  - Name: mask_rcnn_x101_32x4d_fpn_1x_nuim
+  - Name: mask-rcnn_x101_32x4d_fpn_1x_nuim
     In Collection: Mask R-CNN
     Config: configs/nuimages/mask-rcnn_x101_32x4d_fpn_1x_nuim.py
     Metadata:
       Training Memory (GB): 13.3
       Training Resources: 8x TITAN Xp
     Results:
       - Task: Object Detection
@@ -138,85 +138,85 @@
           Box AP: 50.4
       - Task: Instance Segmentation
         Dataset: nuImages
         Metrics:
           Mask AP: 40.5
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/nuimages_semseg/mask_rcnn_x101_32x4d_fpn_1x_nuim/mask_rcnn_x101_32x4d_fpn_1x_nuim_20201024_135741-b699ab37.pth
 
-  - Name: cascade_mask_rcnn_r50_fpn_1x_nuim
+  - Name: cascade-mask-rcnn_r50_fpn_1x_nuim
     In Collection: Mask R-CNN
-    Config: configs/nuimages/cascade_mask_rcnn_r50_fpn_1x_nuim.py
+    Config: configs/nuimages/cascade-mask-rcnn_r50_fpn_1x_nuim.py
     Metadata:
       Training Memory (GB): 8.9
       Training Resources: 8x TITAN Xp
     Results:
       - Task: Object Detection
         Dataset: nuImages
         Metrics:
           Box AP: 50.8
       - Task: Instance Segmentation
         Dataset: nuImages
         Metrics:
           Mask AP: 40.4
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/nuimages_semseg/cascade_mask_rcnn_r50_fpn_1x_nuim/cascade_mask_rcnn_r50_fpn_1x_nuim_20201008_195342-1147c036.pth
 
-  - Name: cascade_mask_rcnn_r50_fpn_coco-20e_1x_nuim
+  - Name: cascade-mask-rcnn_r50_fpn_coco-20e_1x_nuim
     In Collection: Mask R-CNN
-    Config: configs/nuimages/cascade_mask_rcnn_r50_fpn_coco-20e_1x_nuim.py
+    Config: configs/nuimages/cascade-mask-rcnn_r50_fpn_coco-20e_1x_nuim.py
     Metadata:
       Training Memory (GB): 8.9
       Training Resources: 8x TITAN Xp
     Results:
       - Task: Object Detection
         Dataset: nuImages
         Metrics:
           Box AP: 52.8
       - Task: Instance Segmentation
         Dataset: nuImages
         Metrics:
           Mask AP: 42.2
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/nuimages_semseg/cascade_mask_rcnn_r50_fpn_coco-20e_1x_nuim/cascade_mask_rcnn_r50_fpn_coco-20e_1x_nuim_20201009_124158-ad0540e3.pth
 
-  - Name: cascade_mask_rcnn_r50_fpn_coco-20e_20e_nuim
+  - Name: cascade-mask-rcnn_r50_fpn_coco-20e_20e_nuim
     In Collection: Mask R-CNN
-    Config: configs/nuimages/cascade_mask_rcnn_r50_fpn_coco-20e_20e_nuim.py
+    Config: configs/nuimages/cascade-mask-rcnn_r50_fpn_coco-20e_20e_nuim.py
     Metadata:
       Training Memory (GB): 8.9
       Training Resources: 8x TITAN Xp
     Results:
       - Task: Object Detection
         Dataset: nuImages
         Metrics:
           Box AP: 52.8
       - Task: Instance Segmentation
         Dataset: nuImages
         Metrics:
           Mask AP: 42.2
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/nuimages_semseg/cascade_mask_rcnn_r50_fpn_coco-20e_20e_nuim/cascade_mask_rcnn_r50_fpn_coco-20e_20e_nuim_20201009_124951-40963960.pth
 
-  - Name: cascade_mask_rcnn_r101_fpn_1x_nuim
+  - Name: cascade-mask-rcnn_r101_fpn_1x_nuim
     In Collection: Mask R-CNN
-    Config: configs/nuimages/cascade_mask_rcnn_r101_fpn_1x_nuim.py
+    Config: configs/nuimages/cascade-mask-rcnn_r101_fpn_1x_nuim.py
     Metadata:
       Training Memory (GB): 12.5
       Training Resources: 8x TITAN Xp
     Results:
       - Task: Object Detection
         Dataset: nuImages
         Metrics:
           Box AP: 51.5
       - Task: Instance Segmentation
         Dataset: nuImages
         Metrics:
           Mask AP: 40.7
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/nuimages_semseg/cascade_mask_rcnn_r101_fpn_1x_nuim/cascade_mask_rcnn_r101_fpn_1x_nuim_20201024_134804-45215b1e.pth
 
-  - Name: cascade_mask_rcnn_x101_32x4d_fpn_1x_nuim
+  - Name: cascade-mask-rcnn_x101_32x4d_fpn_1x_nuim
     In Collection: Mask R-CNN
-    Config: configs/nuimages/cascade_mask_rcnn_x101_32x4d_fpn_1x_nuim.py
+    Config: configs/nuimages/cascade-mask-rcnn_x101_32x4d_fpn_1x_nuim.py
     Metadata:
       Training Memory (GB): 14.9
       Training Resources: 8x TITAN Xp
     Results:
       - Task: Object Detection
         Dataset: nuImages
         Metrics:
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/paconv/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/paconv/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/paconv/paconv_ssg-cuda_8xb8-cosine-200e_s3dis-seg.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/paconv/paconv_ssg-cuda_8xb8-cosine-200e_s3dis-seg.py`

 * *Files 5% similar despite different names*

```diff
@@ -13,28 +13,31 @@
         block_size=1.0,
         sample_rate=0.5,
         use_normalized_coord=True,
         batch_size=12))
 
 # data settings
 num_points = 4096
+backend_args = None
 train_pipeline = [
     dict(
         type='LoadPointsFromFile',
         coord_type='DEPTH',
         shift_height=False,
         use_color=True,
         load_dim=6,
-        use_dim=[0, 1, 2, 3, 4, 5]),
+        use_dim=[0, 1, 2, 3, 4, 5],
+        backend_args=backend_args),
     dict(
         type='LoadAnnotations3D',
         with_bbox_3d=False,
         with_label_3d=False,
         with_mask_3d=False,
-        with_seg_3d=True),
+        with_seg_3d=True,
+        backend_args=backend_args),
     dict(type='PointSegClassMapping'),
     dict(
         type='IndoorPatchPointSample',
         num_points=num_points,
         block_size=1.0,
         use_normalized_coord=True,
         num_try=10000,
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/paconv/paconv_ssg_8xb8-cosine-150e_s3dis-seg.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/paconv/paconv_ssg_8xb8-cosine-150e_s3dis-seg.py`

 * *Files 5% similar despite different names*

```diff
@@ -13,28 +13,31 @@
         block_size=1.0,
         sample_rate=0.5,
         use_normalized_coord=True,
         batch_size=12))
 
 # data settings
 num_points = 4096
+backend_args = None
 train_pipeline = [
     dict(
         type='LoadPointsFromFile',
         coord_type='DEPTH',
         shift_height=False,
         use_color=True,
         load_dim=6,
-        use_dim=[0, 1, 2, 3, 4, 5]),
+        use_dim=[0, 1, 2, 3, 4, 5],
+        backend_args=backend_args),
     dict(
         type='LoadAnnotations3D',
         with_bbox_3d=False,
         with_label_3d=False,
         with_mask_3d=False,
-        with_seg_3d=True),
+        with_seg_3d=True,
+        backend_args=backend_args),
     dict(type='PointSegClassMapping'),
     dict(
         type='IndoorPatchPointSample',
         num_points=num_points,
         block_size=1.0,
         use_normalized_coord=True,
         num_try=10000,
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/parta2/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/parta2/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/parta2/parta2_hv_secfpn_8xb2-cyclic-80e_kitti-3d-3class.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/waymoD5-3d-3class.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,55 +1,83 @@
-_base_ = [
-    '../_base_/schedules/cyclic-40e.py', '../_base_/default_runtime.py',
-    '../_base_/models/parta2.py'
-]
+# dataset settings
+# D5 in the config name means the whole dataset is divided into 5 folds
+# We only use one fold for efficient experiments
+dataset_type = 'WaymoDataset'
+# data_root = 's3://openmmlab/datasets/detection3d/waymo/kitti_format/'
+data_root = 'data/waymo/kitti_format/'
 
-point_cloud_range = [0, -40, -3, 70.4, 40, 1]
+# Example to use different file client
+# Method 1: simply set the data root and let the file I/O module
+# automatically infer from prefix (not support LMDB and Memcache yet)
 
-# dataset settings
-dataset_type = 'KittiDataset'
-data_root = 'data/kitti/'
-class_names = ['Pedestrian', 'Cyclist', 'Car']
+# data_root = 's3://openmmlab/datasets/detection3d/waymo/kitti_format/'
+
+# Method 2: Use backend_args, file_client_args in versions before 1.1.0
+# backend_args = dict(
+#     backend='petrel',
+#     path_mapping=dict({
+#         './data/': 's3://openmmlab/datasets/detection3d/',
+#          'data/': 's3://openmmlab/datasets/detection3d/'
+#      }))
+backend_args = None
+
+class_names = ['Car', 'Pedestrian', 'Cyclist']
+metainfo = dict(classes=class_names)
+
+point_cloud_range = [-74.88, -74.88, -2, 74.88, 74.88, 4]
 input_modality = dict(use_lidar=True, use_camera=False)
 db_sampler = dict(
     data_root=data_root,
-    info_path=data_root + 'kitti_dbinfos_train.pkl',
+    info_path=data_root + 'waymo_dbinfos_train.pkl',
     rate=1.0,
     prepare=dict(
         filter_by_difficulty=[-1],
         filter_by_min_points=dict(Car=5, Pedestrian=10, Cyclist=10)),
     classes=class_names,
-    sample_groups=dict(Car=12, Pedestrian=6, Cyclist=6),
+    sample_groups=dict(Car=15, Pedestrian=10, Cyclist=10),
     points_loader=dict(
-        type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4))
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=6,
+        use_dim=[0, 1, 2, 3, 4],
+        backend_args=backend_args),
+    backend_args=backend_args)
+
 train_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=6,
+        use_dim=5,
+        backend_args=backend_args),
     dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
-    dict(type='ObjectSample', db_sampler=db_sampler),
+    # dict(type='ObjectSample', db_sampler=db_sampler),
     dict(
-        type='ObjectNoise',
-        num_try=100,
-        translation_std=[1.0, 1.0, 0.5],
-        global_rot_range=[0.0, 0.0],
-        rot_range=[-0.78539816, 0.78539816]),
-    dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
+        type='RandomFlip3D',
+        sync_2d=False,
+        flip_ratio_bev_horizontal=0.5,
+        flip_ratio_bev_vertical=0.5),
     dict(
         type='GlobalRotScaleTrans',
         rot_range=[-0.78539816, 0.78539816],
         scale_ratio_range=[0.95, 1.05]),
     dict(type='PointsRangeFilter', point_cloud_range=point_cloud_range),
     dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
-    dict(type='ObjectNameFilter', classes=class_names),
     dict(type='PointShuffle'),
     dict(
         type='Pack3DDetInputs',
         keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
 ]
 test_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=6,
+        use_dim=5,
+        backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
             dict(
@@ -62,74 +90,88 @@
                 type='PointsRangeFilter', point_cloud_range=point_cloud_range)
         ]),
     dict(type='Pack3DDetInputs', keys=['points'])
 ]
 # construct a pipeline for data and gt loading in show function
 # please keep its loading function consistent with test_pipeline (e.g. client)
 eval_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4),
-    dict(type='Pack3DDetInputs', keys=['points'])
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=6,
+        use_dim=5,
+        backend_args=backend_args),
+    dict(type='Pack3DDetInputs', keys=['points']),
 ]
+
 train_dataloader = dict(
     batch_size=2,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type='RepeatDataset',
         times=2,
         dataset=dict(
             type=dataset_type,
             data_root=data_root,
-            ann_file='kitti_infos_train.pkl',
-            data_prefix=dict(pts='training/velodyne_reduced'),
+            ann_file='waymo_infos_train.pkl',
+            data_prefix=dict(
+                pts='training/velodyne', sweeps='training/velodyne'),
             pipeline=train_pipeline,
             modality=input_modality,
-            metainfo=dict(classes=class_names),
+            test_mode=False,
+            metainfo=metainfo,
+            # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
+            # and box_type_3d='Depth' in sunrgbd and scannet dataset.
             box_type_3d='LiDAR',
-            test_mode=False)))
-test_dataloader = dict(
+            # load one frame every five frames
+            load_interval=5,
+            backend_args=backend_args)))
+val_dataloader = dict(
     batch_size=1,
     num_workers=1,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        ann_file='kitti_infos_val.pkl',
-        data_prefix=dict(pts='training/velodyne_reduced'),
-        pipeline=test_pipeline,
+        data_prefix=dict(pts='training/velodyne', sweeps='training/velodyne'),
+        ann_file='waymo_infos_val.pkl',
+        pipeline=eval_pipeline,
         modality=input_modality,
-        metainfo=dict(classes=class_names),
+        test_mode=True,
+        metainfo=metainfo,
         box_type_3d='LiDAR',
-        test_mode=True))
-val_dataloader = dict(
+        backend_args=backend_args))
+
+test_dataloader = dict(
     batch_size=1,
     num_workers=1,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        ann_file='kitti_infos_val.pkl',
-        data_prefix=dict(pts='training/velodyne_reduced'),
+        data_prefix=dict(pts='training/velodyne', sweeps='training/velodyne'),
+        ann_file='waymo_infos_val.pkl',
         pipeline=eval_pipeline,
         modality=input_modality,
-        metainfo=dict(classes=class_names),
+        test_mode=True,
+        metainfo=metainfo,
         box_type_3d='LiDAR',
-        test_mode=True))
+        backend_args=backend_args))
+
 val_evaluator = dict(
-    type='KittiMetric',
-    ann_file=data_root + 'kitti_infos_val.pkl',
-    metric='bbox')
+    type='WaymoMetric',
+    ann_file='./data/waymo/kitti_format/waymo_infos_val.pkl',
+    waymo_bin_file='./data/waymo/waymo_format/gt.bin',
+    data_root='./data/waymo/waymo_format',
+    backend_args=backend_args,
+    convert_kitti_format=False)
 test_evaluator = val_evaluator
-# Part-A2 uses a different learning rate from what SECOND uses.
-optim_wrapper = dict(optimizer=dict(lr=0.001))
-find_unused_parameters = True
-
-# Default setting for scaling LR automatically
-#   - `enable` means enable scaling LR automatically
-#       or not by default.
-#   - `base_batch_size` = (8 GPUs) x (2 samples per GPU).
-auto_scale_lr = dict(enable=False, base_batch_size=16)
+
+vis_backends = [dict(type='LocalVisBackend')]
+visualizer = dict(
+    type='Det3DLocalVisualizer', vis_backends=vis_backends, name='visualizer')
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/parta2/parta2_hv_secfpn_8xb2-cyclic-80e_kitti-3d-car.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/scannet-3d.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,138 +1,141 @@
-_base_ = './parta2_hv_secfpn_8xb2-cyclic-80e_kitti-3d-3class.py'
+# dataset settings
+dataset_type = 'ScanNetDataset'
+data_root = 'data/scannet/'
 
-point_cloud_range = [0, -40, -3, 70.4, 40, 1]  # velodyne coordinates, x, y, z
+metainfo = dict(
+    classes=('cabinet', 'bed', 'chair', 'sofa', 'table', 'door', 'window',
+             'bookshelf', 'picture', 'counter', 'desk', 'curtain',
+             'refrigerator', 'showercurtrain', 'toilet', 'sink', 'bathtub',
+             'garbagebin'))
 
-model = dict(
-    rpn_head=dict(
-        type='PartA2RPNHead',
-        num_classes=1,
-        anchor_generator=dict(
-            _delete_=True,
-            type='Anchor3DRangeGenerator',
-            ranges=[[0, -40.0, -1.78, 70.4, 40.0, -1.78]],
-            sizes=[[3.9, 1.6, 1.56]],
-            rotations=[0, 1.57],
-            reshape_out=False)),
-    roi_head=dict(
-        num_classes=1,
-        semantic_head=dict(num_classes=1),
-        bbox_head=dict(num_classes=1)),
-    # model training and testing settings
-    train_cfg=dict(
-        _delete_=True,
-        rpn=dict(
-            assigner=dict(
-                type='Max3DIoUAssigner',
-                iou_calculator=dict(type='BboxOverlapsNearest3D'),
-                pos_iou_thr=0.6,
-                neg_iou_thr=0.45,
-                min_pos_iou=0.45,
-                ignore_iof_thr=-1),
-            allowed_border=0,
-            pos_weight=-1,
-            debug=False),
-        rpn_proposal=dict(
-            nms_pre=9000,
-            nms_post=512,
-            max_num=512,
-            nms_thr=0.8,
-            score_thr=0,
-            use_rotate_nms=False),
-        rcnn=dict(
-            assigner=dict(  # for Car
-                type='Max3DIoUAssigner',
-                iou_calculator=dict(type='BboxOverlaps3D', coordinate='lidar'),
-                pos_iou_thr=0.55,
-                neg_iou_thr=0.55,
-                min_pos_iou=0.55,
-                ignore_iof_thr=-1),
-            sampler=dict(
-                type='IoUNegPiecewiseSampler',
-                num=128,
-                pos_fraction=0.55,
-                neg_piece_fractions=[0.8, 0.2],
-                neg_iou_piece_thrs=[0.55, 0.1],
-                neg_pos_ub=-1,
-                add_gt_as_proposals=False,
-                return_iou=True),
-            cls_pos_thr=0.75,
-            cls_neg_thr=0.25)),
-    test_cfg=dict(
-        rpn=dict(
-            nms_pre=1024,
-            nms_post=100,
-            max_num=100,
-            nms_thr=0.7,
-            score_thr=0,
-            use_rotate_nms=True),
-        rcnn=dict(
-            use_rotate_nms=True,
-            use_raw_score=True,
-            nms_thr=0.01,
-            score_thr=0.1)))
+# Example to use different file client
+# Method 1: simply set the data root and let the file I/O module
+# automatically infer from prefix (not support LMDB and Memcache yet)
+
+# data_root = 's3://openmmlab/datasets/detection3d/scannet/'
+
+# Method 2: Use backend_args, file_client_args in versions before 1.1.0
+# backend_args = dict(
+#     backend='petrel',
+#     path_mapping=dict({
+#         './data/': 's3://openmmlab/datasets/detection3d/',
+#          'data/': 's3://openmmlab/datasets/detection3d/'
+#      }))
+backend_args = None
 
-# dataset settings
-dataset_type = 'KittiDataset'
-data_root = 'data/kitti/'
-class_names = ['Car']
-input_modality = dict(use_lidar=True, use_camera=False)
-db_sampler = dict(
-    data_root=data_root,
-    info_path=data_root + 'kitti_dbinfos_train.pkl',
-    rate=1.0,
-    prepare=dict(filter_by_difficulty=[-1], filter_by_min_points=dict(Car=5)),
-    classes=class_names,
-    sample_groups=dict(Car=15),
-    points_loader=dict(
-        type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4))
 train_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4),
-    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
-    dict(type='ObjectSample', db_sampler=db_sampler),
-    dict(
-        type='ObjectNoise',
-        num_try=100,
-        translation_std=[1.0, 1.0, 0.5],
-        global_rot_range=[0.0, 0.0],
-        rot_range=[-0.78539816, 0.78539816]),
-    dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='DEPTH',
+        shift_height=True,
+        load_dim=6,
+        use_dim=[0, 1, 2],
+        backend_args=backend_args),
+    dict(
+        type='LoadAnnotations3D',
+        with_bbox_3d=True,
+        with_label_3d=True,
+        with_mask_3d=True,
+        with_seg_3d=True,
+        backend_args=backend_args),
+    dict(type='GlobalAlignment', rotation_axis=2),
+    dict(type='PointSegClassMapping'),
+    dict(type='PointSample', num_points=40000),
+    dict(
+        type='RandomFlip3D',
+        sync_2d=False,
+        flip_ratio_bev_horizontal=0.5,
+        flip_ratio_bev_vertical=0.5),
     dict(
         type='GlobalRotScaleTrans',
-        rot_range=[-0.78539816, 0.78539816],
-        scale_ratio_range=[0.95, 1.05]),
-    dict(type='PointsRangeFilter', point_cloud_range=point_cloud_range),
-    dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
-    dict(type='ObjectNameFilter', classes=class_names),
-    dict(type='PointShuffle'),
+        rot_range=[-0.087266, 0.087266],
+        scale_ratio_range=[1.0, 1.0],
+        shift_height=True),
     dict(
         type='Pack3DDetInputs',
-        keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
+        keys=[
+            'points', 'gt_bboxes_3d', 'gt_labels_3d', 'pts_semantic_mask',
+            'pts_instance_mask'
+        ])
 ]
 test_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='DEPTH',
+        shift_height=True,
+        load_dim=6,
+        use_dim=[0, 1, 2],
+        backend_args=backend_args),
+    dict(type='GlobalAlignment', rotation_axis=2),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
             dict(
                 type='GlobalRotScaleTrans',
                 rot_range=[0, 0],
                 scale_ratio_range=[1., 1.],
                 translation_std=[0, 0, 0]),
-            dict(type='RandomFlip3D'),
             dict(
-                type='PointsRangeFilter', point_cloud_range=point_cloud_range),
+                type='RandomFlip3D',
+                sync_2d=False,
+                flip_ratio_bev_horizontal=0.5,
+                flip_ratio_bev_vertical=0.5),
+            dict(type='PointSample', num_points=40000),
         ]),
     dict(type='Pack3DDetInputs', keys=['points'])
 ]
 
 train_dataloader = dict(
+    batch_size=8,
+    num_workers=4,
+    sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
+        type='RepeatDataset',
+        times=5,
         dataset=dict(
-            pipeline=train_pipeline, metainfo=dict(classes=class_names))))
+            type=dataset_type,
+            data_root=data_root,
+            ann_file='scannet_infos_train.pkl',
+            pipeline=train_pipeline,
+            filter_empty_gt=False,
+            metainfo=metainfo,
+            # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
+            # and box_type_3d='Depth' in sunrgbd and scannet dataset.
+            box_type_3d='Depth',
+            backend_args=backend_args)))
+
+val_dataloader = dict(
+    batch_size=1,
+    num_workers=1,
+    sampler=dict(type='DefaultSampler', shuffle=False),
+    dataset=dict(
+        type=dataset_type,
+        data_root=data_root,
+        ann_file='scannet_infos_val.pkl',
+        pipeline=test_pipeline,
+        metainfo=metainfo,
+        test_mode=True,
+        box_type_3d='Depth',
+        backend_args=backend_args))
 test_dataloader = dict(
-    dataset=dict(pipeline=test_pipeline, metainfo=dict(classes=class_names)))
-val_dataloader = dict(dataset=dict(metainfo=dict(classes=class_names)))
-find_unused_parameters = True
+    batch_size=1,
+    num_workers=1,
+    sampler=dict(type='DefaultSampler', shuffle=False),
+    dataset=dict(
+        type=dataset_type,
+        data_root=data_root,
+        ann_file='scannet_infos_val.pkl',
+        pipeline=test_pipeline,
+        metainfo=metainfo,
+        test_mode=True,
+        box_type_3d='Depth',
+        backend_args=backend_args))
+val_evaluator = dict(type='IndoorMetric')
+test_evaluator = val_evaluator
+
+vis_backends = [dict(type='LocalVisBackend')]
+visualizer = dict(
+    type='Det3DLocalVisualizer', vis_backends=vis_backends, name='visualizer')
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pgd/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/pgd/metafile.yml`

 * *Files 1% similar despite different names*

```diff
@@ -13,14 +13,16 @@
     README: configs/pgd/README.md
     Code:
       URL: https://github.com/open-mmlab/mmdetection3d/blob/v1.0.0.dev0/mmdet3d/models/dense_heads/pgd_head.py#17
       Version: v1.0.0
 
 Models:
   - Name: pgd_r101-caffe_fpn_head-gn_4xb3-4x_kitti-mono3d
+    Alias:
+       - pgd_kitti
     In Collection: PGD
     Config: configs/pgd/pgd_r101-caffe_fpn_head-gn_4xb3-4x_kitti-mono3d.py
     Metadata:
       Training Memory (GB): 9.1
     Results:
       - Task: 3D Object Detection
         Dataset: KITTI
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pgd/pgd_r101-caffe_fpn_head-gn_16xb2-1x_nus-mono3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/pgd/pgd_r101-caffe_fpn_head-gn_16xb2-1x_nus-mono3d.py`

 * *Files 4% similar despite different names*

```diff
@@ -42,16 +42,18 @@
     # set weight 1.0 for base 7 dims (offset, depth, size, rot)
     # 0.05 for 2-dim velocity and 0.2 for 4-dim 2D distance targets
     train_cfg=dict(code_weight=[
         1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.05, 0.05, 0.2, 0.2, 0.2, 0.2
     ]),
     test_cfg=dict(nms_pre=1000, nms_thr=0.8, score_thr=0.01, max_per_img=200))
 
+backend_args = None
+
 train_pipeline = [
-    dict(type='LoadImageFromFileMono3D'),
+    dict(type='LoadImageFromFileMono3D', backend_args=backend_args),
     dict(
         type='LoadAnnotations3D',
         with_bbox=True,
         with_label=True,
         with_attr_label=True,
         with_bbox_3d=True,
         with_label_3d=True,
@@ -62,15 +64,15 @@
         type='Pack3DDetInputs',
         keys=[
             'img', 'gt_bboxes', 'gt_bboxes_labels', 'attr_labels',
             'gt_bboxes_3d', 'gt_labels_3d', 'centers_2d', 'depths'
         ]),
 ]
 test_pipeline = [
-    dict(type='LoadImageFromFileMono3D'),
+    dict(type='LoadImageFromFileMono3D', backend_args=backend_args),
     dict(type='mmdet.Resize', scale_factor=1.0),
     dict(type='Pack3DDetInputs', keys=['img']),
 ]
 train_dataloader = dict(
     batch_size=2, num_workers=2, dataset=dict(pipeline=train_pipeline))
 test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
 val_dataloader = dict(dataset=dict(pipeline=test_pipeline))
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pgd/pgd_r101-caffe_fpn_head-gn_4xb3-4x_kitti-mono3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/pgd/pgd_r101-caffe_fpn_head-gn_4xb3-4x_kitti-mono3d.py`

 * *Files 12% similar despite different names*

```diff
@@ -64,29 +64,18 @@
     # 0.2 for 16-dim keypoint offsets and 1.0 for 4-dim 2D distance targets
     train_cfg=dict(code_weight=[
         1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,
         0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 1.0, 1.0, 1.0, 1.0
     ]),
     test_cfg=dict(nms_pre=100, nms_thr=0.05, score_thr=0.001, max_per_img=20))
 
-file_client_args = dict(backend='disk')
-# Uncomment the following if use ceph or other file clients.
-# See https://mmcv.readthedocs.io/en/latest/api.html#mmcv.fileio.FileClient
-# for more details.
-# file_client_args = dict(
-#     backend='petrel',
-#     path_mapping=dict({
-#         './data/kitti/':
-#         's3://openmmlab/datasets/detection3d/kitti/',
-#         'data/kitti/':
-#         's3://openmmlab/datasets/detection3d/kitti/'
-#     }))
+backend_args = None
 
 train_pipeline = [
-    dict(type='LoadImageFromFileMono3D'),
+    dict(type='LoadImageFromFileMono3D', backend_args=backend_args),
     dict(
         type='LoadAnnotations3D',
         with_bbox=True,
         with_label=True,
         with_attr_label=False,
         with_bbox_3d=True,
         with_label_3d=True,
@@ -97,15 +86,15 @@
         type='Pack3DDetInputs',
         keys=[
             'img', 'gt_bboxes', 'gt_bboxes_labels', 'gt_bboxes_3d',
             'gt_labels_3d', 'centers_2d', 'depths'
         ]),
 ]
 test_pipeline = [
-    dict(type='LoadImageFromFileMono3D'),
+    dict(type='LoadImageFromFileMono3D', backend_args=backend_args),
     dict(type='mmdet.Resize', scale_factor=1.0),
     dict(type='Pack3DDetInputs', keys=['img'])
 ]
 
 train_dataloader = dict(
     batch_size=3, num_workers=3, dataset=dict(pipeline=train_pipeline))
 test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pgd/pgd_r101_fpn-head_dcn_16xb3_waymoD5-fov-mono3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/pgd/pgd_r101_fpn-head_dcn_16xb3_waymoD5-fov-mono3d.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pgd/pgd_r101_fpn-head_dcn_16xb3_waymoD5-mono3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/pgd/pgd_r101_fpn-head_dcn_16xb3_waymoD5-mv-mono3d.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 _base_ = [
-    '../_base_/datasets/waymoD5-mono3d-3class.py', '../_base_/models/pgd.py',
-    '../_base_/schedules/mmdet-schedule-1x.py', '../_base_/default_runtime.py'
+    '../_base_/datasets/waymoD5-mv-mono3d-3class.py',
+    '../_base_/models/pgd.py', '../_base_/schedules/mmdet-schedule-1x.py',
+    '../_base_/default_runtime.py'
 ]
 # model settings
 model = dict(
     backbone=dict(
         type='mmdet.ResNet',
         depth=101,
         num_stages=4,
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/point_rcnn/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/point_rcnn/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/point_rcnn/point-rcnn_8xb2_kitti-3d-3class.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/point_rcnn/point-rcnn_8xb2_kitti-3d-3class.py`

 * *Files 10% similar despite different names*

```diff
@@ -6,29 +6,40 @@
 # dataset settings
 dataset_type = 'KittiDataset'
 data_root = 'data/kitti/'
 class_names = ['Pedestrian', 'Cyclist', 'Car']
 metainfo = dict(classes=class_names)
 point_cloud_range = [0, -40, -3, 70.4, 40, 1]
 input_modality = dict(use_lidar=True, use_camera=False)
+backend_args = None
 
 db_sampler = dict(
     data_root=data_root,
     info_path=data_root + 'kitti_dbinfos_train.pkl',
     rate=1.0,
     prepare=dict(
         filter_by_difficulty=[-1],
         filter_by_min_points=dict(Car=5, Pedestrian=5, Cyclist=5)),
     sample_groups=dict(Car=20, Pedestrian=15, Cyclist=15),
     classes=class_names,
     points_loader=dict(
-        type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4))
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
+    backend_args=backend_args)
 
 train_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
     dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
     dict(type='PointsRangeFilter', point_cloud_range=point_cloud_range),
     dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
     dict(type='ObjectSample', db_sampler=db_sampler),
     dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
     dict(
         type='ObjectNoise',
@@ -44,15 +55,20 @@
     dict(type='PointSample', num_points=16384, sample_range=40.0),
     dict(type='PointShuffle'),
     dict(
         type='Pack3DDetInputs',
         keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
 ]
 test_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
             dict(
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointnet2/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/pointnet2/metafile.yml`

 * *Files 2% similar despite different names*

```diff
@@ -13,80 +13,81 @@
     Code:
       URL: https://github.com/open-mmlab/mmdetection3d/blob/master/mmdet3d/models/backbones/pointnet2_sa_ssg.py#L12
       Version: v0.14.0
 
 Models:
   - Name: pointnet2_ssg_2xb16-cosine-200e_scannet-seg-xyz-only
     In Collection: PointNet++
-    Config: configs/pointnet/pointnet2_ssg_2xb16-cosine-200e_scannet-seg-xyz-only.py
+    Config: configs/pointnet2/pointnet2_ssg_2xb16-cosine-200e_scannet-seg-xyz-only.py
     Metadata:
       Training Data: ScanNet
       Training Memory (GB): 1.9
     Results:
       - Task: 3D Semantic Segmentation
         Dataset: ScanNet
         Metrics:
           mIoU: 53.91
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/pointnet2/pointnet2_ssg_xyz-only_16x2_cosine_200e_scannet_seg-3d-20class/pointnet2_ssg_xyz-only_16x2_cosine_200e_scannet_seg-3d-20class_20210514_143628-4e341a48.pth
 
   - Name: pointnet2_ssg_2xb16-cosine-200e_scannet-seg
     In Collection: PointNet++
-    Config: configs/pointnet/pointnet2_ssg_2xb16-cosine-200e_scannet-seg.py
+    Config: configs/pointnet2/pointnet2_ssg_2xb16-cosine-200e_scannet-seg.py
     Metadata:
       Training Data: ScanNet
       Training Memory (GB): 1.9
     Results:
       - Task: 3D Semantic Segmentation
         Dataset: ScanNet
         Metrics:
           mIoU: 54.44
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/pointnet2/pointnet2_ssg_16x2_cosine_200e_scannet_seg-3d-20class/pointnet2_ssg_16x2_cosine_200e_scannet_seg-3d-20class_20210514_143644-ee73704a.pth
 
   - Name: pointnet2_msg_2xb16-cosine-250e_scannet-seg-xyz-only
     In Collection: PointNet++
-    Config: configs/pointnet/pointnet2_msg_2xb16-cosine-250e_scannet-seg-xyz-only.py
+    Config: configs/pointnet2/pointnet2_msg_2xb16-cosine-250e_scannet-seg-xyz-only.py
     Metadata:
       Training Data: ScanNet
       Training Memory (GB): 2.4
     Results:
       - Task: 3D Semantic Segmentation
         Dataset: ScanNet
         Metrics:
           mIoU: 54.26
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/pointnet2/pointnet2_msg_xyz-only_16x2_cosine_250e_scannet_seg-3d-20class/pointnet2_msg_xyz-only_16x2_cosine_250e_scannet_seg-3d-20class_20210514_143838-b4a3cf89.pth
 
   - Name: pointnet2_msg_2xb16-cosine-250e_scannet-seg
     In Collection: PointNet++
-    Config: configs/pointnet/pointnet2_msg_2xb16-cosine-250e_scannet-seg.py
+    Config: configs/pointnet2/pointnet2_msg_2xb16-cosine-250e_scannet-seg.py
     Metadata:
       Training Data: ScanNet
       Training Memory (GB): 2.4
     Results:
       - Task: 3D Semantic Segmentation
         Dataset: ScanNet
         Metrics:
           mIoU: 55.05
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/pointnet2/pointnet2_msg_16x2_cosine_250e_scannet_seg-3d-20class/pointnet2_msg_16x2_cosine_250e_scannet_seg-3d-20class_20210514_144009-24477ab1.pth
 
   - Name: pointnet2_ssg_2xb16-cosine-50e_s3dis-seg
+    Alias: pointnet2-ssg_s3dis-seg
     In Collection: PointNet++
-    Config: configs/pointnet/pointnet2_ssg_2xb16-cosine-50e_s3dis-seg.py
+    Config: configs/pointnet2/pointnet2_ssg_2xb16-cosine-50e_s3dis-seg.py
     Metadata:
       Training Data: S3DIS
       Training Memory (GB): 3.6
     Results:
       - Task: 3D Semantic Segmentation
         Dataset: S3DIS
         Metrics:
           mIoU: 56.93
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/pointnet2/pointnet2_ssg_16x2_cosine_50e_s3dis_seg-3d-13class/pointnet2_ssg_16x2_cosine_50e_s3dis_seg-3d-13class_20210514_144205-995d0119.pth
 
   - Name: pointnet2_msg_2xb16-cosine-80e_s3dis-seg
     In Collection: PointNet++
-    Config: configs/pointnet/pointnet2_msg_2xb16-cosine-80e_s3dis-seg.py
+    Config: configs/pointnet2/pointnet2_msg_2xb16-cosine-80e_s3dis-seg.py
     Metadata:
       Training Data: S3DIS
       Training Memory (GB): 3.6
     Results:
       - Task: 3D Semantic Segmentation
         Dataset: S3DIS
         Metrics:
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointnet2/pointnet2_msg_2xb16-cosine-250e_scannet-seg-xyz-only.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/pointnet2/pointnet2_ssg_2xb16-cosine-200e_scannet-seg-xyz-only.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 _base_ = [
-    '../_base_/datasets/scannet-seg.py', '../_base_/models/pointnet2_msg.py',
+    '../_base_/datasets/scannet-seg.py', '../_base_/models/pointnet2_ssg.py',
     '../_base_/schedules/seg-cosine-200e.py', '../_base_/default_runtime.py'
 ]
 
 # model settings
 model = dict(
     backbone=dict(in_channels=3),  # only [xyz]
     decode_head=dict(
@@ -30,28 +30,31 @@
 # in this setting, we only use xyz as network input
 # so we need to re-write all the data pipeline
 class_names = ('wall', 'floor', 'cabinet', 'bed', 'chair', 'sofa', 'table',
                'door', 'window', 'bookshelf', 'picture', 'counter', 'desk',
                'curtain', 'refrigerator', 'showercurtrain', 'toilet', 'sink',
                'bathtub', 'otherfurniture')
 num_points = 8192
+backend_args = None
 train_pipeline = [
     dict(
         type='LoadPointsFromFile',
         coord_type='DEPTH',
         shift_height=False,
         use_color=False,
         load_dim=6,
-        use_dim=[0, 1, 2]),  # only load xyz coordinates
+        use_dim=[0, 1, 2],  # only load xyz coordinates
+        backend_args=backend_args),
     dict(
         type='LoadAnnotations3D',
         with_bbox_3d=False,
         with_label_3d=False,
         with_mask_3d=False,
-        with_seg_3d=True),
+        with_seg_3d=True,
+        backend_args=backend_args),
     dict(type='PointSegClassMapping'),
     dict(
         type='IndoorPatchPointSample',
         num_points=num_points,
         block_size=1.5,
         ignore_index=len(class_names),
         use_normalized_coord=False,
@@ -62,21 +65,23 @@
 test_pipeline = [
     dict(
         type='LoadPointsFromFile',
         coord_type='DEPTH',
         shift_height=False,
         use_color=False,
         load_dim=6,
-        use_dim=[0, 1, 2]),
+        use_dim=[0, 1, 2],
+        backend_args=backend_args),
     dict(
         type='LoadAnnotations3D',
         with_bbox_3d=False,
         with_label_3d=False,
         with_mask_3d=False,
-        with_seg_3d=True),
+        with_seg_3d=True,
+        backend_args=backend_args),
     dict(
         # a wrapper in order to successfully call test function
         # actually we don't perform test-time-aug
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
@@ -97,10 +102,8 @@
 
 train_dataloader = dict(batch_size=16, dataset=dict(pipeline=train_pipeline))
 test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
 val_dataloader = test_dataloader
 
 # runtime settings
 default_hooks = dict(checkpoint=dict(type='CheckpointHook', interval=5))
-
-# PointNet2-MSG needs longer training time than PointNet2-SSG
-train_cfg = dict(by_epoch=True, max_epochs=250, val_interval=5)
+train_cfg = dict(val_interval=5)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointnet2/pointnet2_msg_2xb16-cosine-250e_scannet-seg.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/pointnet2/pointnet2_msg_2xb16-cosine-250e_scannet-seg.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointnet2/pointnet2_msg_2xb16-cosine-80e_s3dis-seg.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/pointnet2/pointnet2_msg_2xb16-cosine-80e_s3dis-seg.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointnet2/pointnet2_ssg_2xb16-cosine-200e_scannet-seg.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/pointnet2/pointnet2_ssg_2xb16-cosine-200e_scannet-seg.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointnet2/pointnet2_ssg_2xb16-cosine-50e_s3dis-seg.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/pointnet2/pointnet2_ssg_2xb16-cosine-50e_s3dis-seg.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointpillars/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/pointpillars/metafile.yml`

 * *Files 2% similar despite different names*

```diff
@@ -25,14 +25,15 @@
       - Task: 3D Object Detection
         Dataset: KITTI
         Metrics:
           AP: 77.6
     Weights: https://download.openmmlab.com/mmdetection3d/v1.0.0_models/pointpillars/hv_pointpillars_secfpn_6x8_160e_kitti-3d-car/hv_pointpillars_secfpn_6x8_160e_kitti-3d-car_20220331_134606-d42d15ed.pth
 
   - Name: pointpillars_hv_secfpn_8xb6-160e_kitti-3d-3class
+    Alias: pointpillars_kitti-3class
     In Collection: PointPillars
     Config: configs/pointpillars/pointpillars_hv_secfpn_8xb6-160e_kitti-3d-3class.py
     Metadata:
       Training Data: KITTI
       Training Memory (GB): 5.5
       Training Resources: 8x V100 GPUs
     Results:
@@ -142,49 +143,50 @@
       - Task: 3D Object Detection
         Dataset: Lyft
         Metrics:
           Private Score: 14.0
           Public Score: 15.0
     Weights: https://download.openmmlab.com/mmdetection3d/v1.0.0_models/pointpillars/hv_pointpillars_fpn_sbn-all_2x8_2x_lyft-3d/hv_pointpillars_fpn_sbn-all_2x8_2x_lyft-3d_20210822_095429-0b3d6196.pth
 
-  - Name: hv_pointpillars_secfpn_sbn_2x16_2x_waymoD5-3d-car
+  - Name: pointpillars_hv_secfpn_sbn_2x16_2x_waymoD5-3d-car
     In Collection: PointPillars
-    Config: configs/pointpillars/hv_pointpillars_secfpn_sbn_2x16_2x_waymoD5-3d-car.py
+    Config: configs/pointpillars/pointpillars_hv_secfpn_sbn_2x16_2x_waymoD5-3d-car.py
     Metadata:
       Training Data: Waymo
       Training Memory (GB): 7.76
       Training Resources: 8x GeForce GTX 1080 Ti
     Results:
       - Task: 3D Object Detection
         Dataset: Waymo
         Metrics:
           mAP@L1: 70.2
           mAPH@L1: 69.6
           mAP@L2: 62.6
           mAPH@L2: 62.1
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/pointpillars/hv_pointpillars_secfpn_sbn_2x16_2x_waymoD5-3d-car/hv_pointpillars_secfpn_sbn_2x16_2x_waymoD5-3d-car_20200901_204315-302fc3e7.pth
 
-  - Name: hv_pointpillars_secfpn_sbn_2x16_2x_waymoD5-3d-3class
+  - Name: pointpillars_hv_secfpn_sbn_2x16_2x_waymoD5-3d-3class
+    Alias: pointpillars_waymod5-3class
     In Collection: PointPillars
-    Config: configs/pointpillars/hv_pointpillars_secfpn_sbn_2x16_2x_waymoD5-3d-3class.py
+    Config: configs/pointpillars/pointpillars_hv_secfpn_sbn-all_16xb2-2x_waymoD5-3d-3class.py
     Metadata:
       Training Data: Waymo
       Training Memory (GB): 8.12
       Training Resources: 8x GeForce GTX 1080 Ti
     Results:
       - Task: 3D Object Detection
         Dataset: Waymo
         Metrics:
           mAP@L1: 64.7
           mAPH@L1: 57.6
           mAP@L2: 58.4
           mAPH@L2: 52.1
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/pointpillars/hv_pointpillars_secfpn_sbn_2x16_2x_waymoD5-3d-3class/hv_pointpillars_secfpn_sbn_2x16_2x_waymoD5-3d-3class_20200831_204144-d1a706b1.pth
 
-  - Name: hv_pointpillars_secfpn_sbn_2x16_2x_waymo-3d-car
+  - Name: pointpillars_hv_secfpn_sbn_2x16_2x_waymo-3d-car
     In Collection: PointPillars
     Config: configs/pointpillars/pointpillars_hv_secfpn_sbn-all_16xb2-2x_waymo-3d-car.py
     Metadata:
       Training Data: Waymo
       Training Memory (GB): 7.76
       Training Resources: 8x GeForce GTX 1080 Ti
     Results:
@@ -192,15 +194,15 @@
         Dataset: Waymo
         Metrics:
           mAP@L1: 72.1
           mAPH@L1: 71.5
           mAP@L2: 63.6
           mAPH@L2: 63.1
 
-  - Name: hv_pointpillars_secfpn_sbn_2x16_2x_waymo-3d-3class
+  - Name: pointpillars_hv_secfpn_sbn_2x16_2x_waymo-3d-3class
     In Collection: PointPillars
     Config: configs/pointpillars/pointpillars_hv_secfpn_sbn-all_16xb2-2x_waymo-3d-3class.py
     Metadata:
       Training Data: Waymo
       Training Memory (GB): 8.12
       Training Resources: 8x GeForce GTX 1080 Ti
     Results:
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_16xb2-2x_waymo-3d-3class.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_16xb2-2x_waymo-3d-3class.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_16xb2-2x_waymo-3d-car.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_16xb2-2x_waymo-3d-car.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_16xb2-2x_waymoD5-3d-car.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_16xb2-2x_waymoD5-3d-car.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_8xb2-2x_lyft-3d-range100.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_8xb2-2x_lyft-3d-range100.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_8xb2-2x_lyft-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_8xb2-2x_lyft-3d.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_8xb4-2x_nus-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_8xb4-2x_nus-3d.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pv_rcnn/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/pv_rcnn/metafile.yml`

 * *Files 13% similar despite different names*

```diff
@@ -22,8 +22,8 @@
     Metadata:
       Training Memory (GB): 5.4
     Results:
       - Task: 3D Object Detection
         Dataset: KITTI
         Metrics:
           mAP: 72.28
-    Weights: <https://download.openmmlab.com/mmdetection3d/v1.1.0_models/pv_rcnn/pv_rcnn_8xb2-80e_kitti-3d-3class/pv_rcnn_8xb2-80e_kitti-3d-3class_20221117_234428-b384d22f.pth
+    Weights: https://download.openmmlab.com/mmdetection3d/v1.1.0_models/pv_rcnn/pv_rcnn_8xb2-80e_kitti-3d-3class/pv_rcnn_8xb2-80e_kitti-3d-3class_20221117_234428-b384d22f.pth
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/pv_rcnn/pv_rcnn_8xb2-80e_kitti-3d-3class.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/pv_rcnn/pv_rcnn_8xb2-80e_kitti-3d-3class.py`

 * *Files 12% similar despite different names*

```diff
@@ -5,28 +5,39 @@
 
 voxel_size = [0.05, 0.05, 0.1]
 point_cloud_range = [0, -40, -3, 70.4, 40, 1]
 
 data_root = 'data/kitti/'
 class_names = ['Pedestrian', 'Cyclist', 'Car']
 metainfo = dict(CLASSES=class_names)
+backend_args = None
 db_sampler = dict(
     data_root=data_root,
     info_path=data_root + 'kitti_dbinfos_train.pkl',
     rate=1.0,
     prepare=dict(
         filter_by_difficulty=[-1],
         filter_by_min_points=dict(Car=5, Pedestrian=5, Cyclist=5)),
     classes=class_names,
     sample_groups=dict(Car=15, Pedestrian=10, Cyclist=10),
     points_loader=dict(
-        type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4))
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
+    backend_args=backend_args)
 
 train_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
     dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
     dict(type='ObjectSample', db_sampler=db_sampler, use_ground_plane=True),
     dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
     dict(
         type='GlobalRotScaleTrans',
         rot_range=[-0.78539816, 0.78539816],
         scale_ratio_range=[0.95, 1.05]),
@@ -34,15 +45,20 @@
     dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
     dict(type='PointShuffle'),
     dict(
         type='Pack3DDetInputs',
         keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
 ]
 test_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=4, use_dim=4),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=4,
+        use_dim=4,
+        backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
             dict(
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/regnet/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/regnet/metafile.yml`

 * *Files 5% similar despite different names*

```diff
@@ -1,79 +1,79 @@
 Models:
-  - Name: hv_pointpillars_regnet-400mf_secfpn_sbn-all_4x8_2x_nus-3d
+  - Name: pointpillars_hv_regnet-400mf_secfpn_sbn-all_4x8_2x_nus-3d
     In Collection: PointPillars
-    Config: configs/regnet/hv_pointpillars_regnet-400mf_secfpn_sbn-all_8xb4-2x_nus-3d.py
+    Config: configs/regnet/pointpillars_hv_regnet-400mf_secfpn_sbn-all_8xb4-2x_nus-3d.py
     Metadata:
       Training Data: nuScenes
       Training Memory (GB): 16.4
       Architecture:
         - RegNetX
         - Hard Voxelization
     Results:
       - Task: 3D Object Detection
         Dataset: nuScenes
         Metrics:
           mAP: 41.2
           NDS: 55.2
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/regnet/hv_pointpillars_regnet-400mf_secfpn_sbn-all_4x8_2x_nus-3d/hv_pointpillars_regnet-400mf_secfpn_sbn-all_4x8_2x_nus-3d_20200620_230334-53044f32.pth
 
-  - Name: hv_pointpillars_regnet-400mf_fpn_sbn-all_8xb4-2x_nus-3d
+  - Name: pointpillars_hv_regnet-400mf_fpn_sbn-all_8xb4-2x_nus-3d
     In Collection: PointPillars
-    Config: configs/regnet/hv_pointpillars_regnet-400mf_fpn_sbn-all_8xb4-2x_nus-3d.py
+    Config: configs/regnet/pointpillars_hv_regnet-400mf_fpn_sbn-all_8xb4-2x_nus-3d.py
     Metadata:
       Training Data: nuScenes
       Training Memory (GB): 17.3
       Architecture:
         - RegNetX
         - Hard Voxelization
     Results:
       - Task: 3D Object Detection
         Dataset: nuScenes
         Metrics:
           mAP: 44.8
           NDS: 56.4
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/regnet/hv_pointpillars_regnet-400mf_fpn_sbn-all_4x8_2x_nus-3d/hv_pointpillars_regnet-400mf_fpn_sbn-all_4x8_2x_nus-3d_20200620_230239-c694dce7.pth
 
-  - Name: hv_pointpillars_regnet-1.6gf_fpn_sbn-all_8xb4-2x_nus-3d
+  - Name: pointpillars_hv_regnet-1.6gf_fpn_sbn-all_8xb4-2x_nus-3d
     In Collection: PointPillars
-    Config: configs/regnet/hv_pointpillars_regnet-1.6gf_fpn_sbn-all_8xb4-2x_nus-3d.py
+    Config: configs/regnet/pointpillars_hv_regnet-1.6gf_fpn_sbn-all_8xb4-2x_nus-3d.py
     Metadata:
       Training Data: nuScenes
       Training Memory (GB): 24.0
       Architecture:
         - RegNetX
         - Hard Voxelization
     Results:
       - Task: 3D Object Detection
         Dataset: nuScenes
         Metrics:
           mAP: 48.2
           NDS: 59.3
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/regnet/hv_pointpillars_regnet-1.6gf_fpn_sbn-all_4x8_2x_nus-3d/hv_pointpillars_regnet-1.6gf_fpn_sbn-all_4x8_2x_nus-3d_20200629_050311-dcd4e090.pth
 
-  - Name: hv_pointpillars_regnet-400mf_secfpn_sbn-all_2x8_2x_lyft-3d
+  - Name: pointpillars_hv_regnet-400mf_secfpn_sbn-all_2x8_2x_lyft-3d
     In Collection: PointPillars
-    Config: configs/regnet/hv_pointpillars_regnet-400mf_secfpn_sbn-all_2x8_2x_lyft-3d.py
+    Config: configs/regnet/pointpillars_hv_regnet-400mf_secfpn_sbn-all_2x8_2x_lyft-3d.py
     Metadata:
       Training Data: Lyft
       Training Memory (GB): 15.9
       Architecture:
         - RegNetX
         - Hard Voxelization
     Results:
       - Task: 3D Object Detection
         Dataset: Lyft
         Metrics:
           Private Score: 14.9
           Public Score: 15.1
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/regnet/hv_pointpillars_regnet-400mf_secfpn_sbn-all_2x8_2x_lyft-3d/hv_pointpillars_regnet-400mf_secfpn_sbn-all_2x8_2x_lyft-3d_20210524_092151-42513826.pth
 
-  - Name: hv_pointpillars_regnet-400mf_fpn_sbn-all_2x8_2x_lyft-3d
+  - Name: pointpillars_hv_regnet-400mf_fpn_sbn-all_2x8_2x_lyft-3d
     In Collection: PointPillars
-    Config: configs/regnet/hv_pointpillars_regnet-400mf_fpn_sbn-all_2x8_2x_lyft-3d.py
+    Config: configs/regnet/pointpillars_hv_regnet-400mf_fpn_sbn-all_2x8_2x_lyft-3d.py
     Metadata:
       Training Data: Lyft
       Training Memory (GB): 13.0
       Architecture:
         - RegNetX
         - Hard Voxelization
     Results:
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-1.6gf_fpn_sbn-all_8xb4-2x_nus-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-1.6gf_fpn_sbn-all_8xb4-2x_nus-3d.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_fpn_sbn-all_8xb2-2x_lyft-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_fpn_sbn-all_8xb2-2x_lyft-3d.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_fpn_sbn-all_8xb4-2x_nus-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_fpn_sbn-all_8xb4-2x_nus-3d.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_fpn_sbn-all_range100_8xb2-2x_lyft-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_fpn_sbn-all_range100_8xb2-2x_lyft-3d.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_secfpn_sbn-all_8xb2-2x_lyft-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_secfpn_sbn-all_8xb2-2x_lyft-3d.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-_base_ = './hv_pointpillars_regnet-400mf_fpn_sbn-all_2x8_2x_lyft-3d.py'
+_base_ = './pointpillars_hv_regnet-400mf_fpn_sbn-all_8xb2-2x_lyft-3d.py'
 # model settings
 model = dict(
     pts_neck=dict(
         type='SECONDFPN',
         _delete_=True,
         norm_cfg=dict(type='naiveSyncBN2d', eps=1e-3, momentum=0.01),
         in_channels=[64, 160, 384],
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_secfpn_sbn-all_8xb4-2x_nus-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_secfpn_sbn-all_8xb4-2x_nus-3d.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-_base_ = './hv_pointpillars_regnet-400mf_fpn_sbn-all_4x8_2x_nus-3d.py'
+_base_ = './pointpillars_hv_regnet-400mf_fpn_sbn-all_8xb4-2x_nus-3d.py'
 # model settings
 model = dict(
     pts_neck=dict(
         type='SECONDFPN',
         _delete_=True,
         norm_cfg=dict(type='naiveSyncBN2d', eps=1e-3, momentum=0.01),
         in_channels=[64, 160, 384],
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_secfpn_sbn-all_range100_8xb2-2x_lyft-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_secfpn_sbn-all_range100_8xb2-2x_lyft-3d.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 _base_ = \
-    './hv_pointpillars_regnet-400mf_fpn_sbn-all_range100_2x8_2x_lyft-3d.py'
+    './pointpillars_hv_regnet-400mf_fpn_sbn-all_range100_8xb2-2x_lyft-3d.py'
 # model settings
 model = dict(
     pts_neck=dict(
         type='SECONDFPN',
         _delete_=True,
         norm_cfg=dict(type='naiveSyncBN2d', eps=1e-3, momentum=0.01),
         in_channels=[64, 160, 384],
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/second/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/second/metafile.yml`

 * *Files 10% similar despite different names*

```diff
@@ -21,30 +21,30 @@
       Training Data: KITTI
       Training Memory (GB): 5.4
       Training Resources: 8x V100 GPUs
     Results:
       - Task: 3D Object Detection
         Dataset: KITTI
         Metrics:
-          mAP: 79.07
-    Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/second/hv_second_secfpn_6x8_80e_kitti-3d-car/hv_second_secfpn_6x8_80e_kitti-3d-car_20200620_230238-393f000c.pth
+          mAP: 78.2
+    Weights: https://download.openmmlab.com/mmdetection3d/v1.1.0_models/second/second_hv_secfpn_8xb6-80e_kitti-3d-car/second_hv_secfpn_8xb6-80e_kitti-3d-car-75d9305e.pth
 
   - Name: second_hv_secfpn_8xb6-80e_kitti-3d-3class
     In Collection: SECOND
     Config: configs/second/second_hv_secfpn_8xb6-80e_kitti-3d-3class.py
     Metadata:
       Training Data: KITTI
       Training Memory (GB): 5.4
       Training Resources: 8x V100 GPUs
     Results:
       - Task: 3D Object Detection
         Dataset: KITTI
         Metrics:
-          mAP: 65.74
-    Weights: https://download.openmmlab.com/mmdetection3d/v1.0.0_models/second/hv_second_secfpn_6x8_80e_kitti-3d-3class/hv_second_secfpn_6x8_80e_kitti-3d-3class_20210831_022017-ae782e87.pth
+          mAP: 65.3
+    Weights: https://download.openmmlab.com/mmdetection3d/v1.1.0_models/second/second_hv_secfpn_8xb6-80e_kitti-3d-3class/second_hv_secfpn_8xb6-80e_kitti-3d-3class-b086d0a3.pth
 
   - Name: second_hv_secfpn_sbn-all_16xb2-2x_waymoD5-3d-3class
     In Collection: SECOND
     Config: configs/second/second_hv_secfpn_sbn-all_16xb2-2x_waymoD5-3d-3class.py
     Metadata:
       Training Data: Waymo
       Training Memory (GB): 8.12
@@ -56,15 +56,15 @@
           mAP@L1: 65.3
           mAPH@L1: 61.7
           mAP@L2: 58.9
           mAPH@L2: 55.7
 
   - Name: second_hv_secfpn_8xb6-amp-80e_kitti-3d-car
     In Collection: SECOND
-    Config: configs/second/hv_second_secfpn_8xb6-amp-80e_kitti-3d-car.py
+    Config: configs/second/second_hv_secfpn_8xb6-amp-80e_kitti-3d-car.py
     Metadata:
       Training Techniques:
         - AdamW
         - Mixed Precision Training
       Training Resources: 8x TITAN Xp
       Training Data: KITTI
       Training Memory (GB): 2.9
@@ -75,15 +75,15 @@
           mAP: 78.72
     Weights: https://download.openmmlab.com/mmdetection3d/v0.1.0_models/fp16/hv_second_secfpn_fp16_6x8_80e_kitti-3d-car/hv_second_secfpn_fp16_6x8_80e_kitti-3d-car_20200924_211301-1f5ad833.pth
     Code:
       Version: v0.7.0
 
   - Name: second_hv_secfpn_8xb6-amp-80e_kitti-3d-3class
     In Collection: SECOND
-    Config: configs/second/hv_second_secfpn_8xb6-amp-80e_kitti-3d-3class.py
+    Config: configs/second/second_hv_secfpn_8xb6-amp-80e_kitti-3d-3class.py
     Metadata:
       Training Techniques:
         - AdamW
         - Mixed Precision Training
       Training Resources: 8x TITAN Xp
       Training Data: KITTI
       Training Memory (GB): 2.9
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/second/second_hv_secfpn_8xb6-80e_kitti-3d-car.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/second/second_hv_secfpn_8xb6-80e_kitti-3d-car.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/second/second_hv_secfpn_sbn-all_16xb2-2x_waymoD5-3d-3class.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/_base_/datasets/waymoD5-mv-mono3d-3class.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,147 +1,163 @@
-_base_ = [
-    '../_base_/models/second_hv_secfpn_waymo.py',
-    '../_base_/datasets/waymoD5-3d-3class.py',
-    '../_base_/schedules/schedule-2x.py',
-    '../_base_/default_runtime.py',
-]
-
+# dataset settings
+# D3 in the config name means the whole dataset is divided into 3 folds
+# We only use one fold for efficient experiments
 dataset_type = 'WaymoDataset'
 data_root = 'data/waymo/kitti_format/'
 class_names = ['Car', 'Pedestrian', 'Cyclist']
-metainfo = dict(classes=class_names)
+input_modality = dict(use_lidar=False, use_camera=True)
 
-point_cloud_range = [-76.8, -51.2, -2, 76.8, 51.2, 4]
-input_modality = dict(use_lidar=True, use_camera=False)
-file_client_args = dict(
-    backend='petrel',
-    path_mapping=dict({
-        './data/waymo/':
-        's3://openmmlab/datasets/detection3d/waymo/',
-        'data/waymo/':
-        's3://openmmlab/datasets/detection3d/waymo/'
-    }))
-
-db_sampler = dict(
-    data_root=data_root,
-    info_path=data_root + 'waymo_dbinfos_train.pkl',
-    rate=1.0,
-    prepare=dict(
-        filter_by_difficulty=[-1],
-        filter_by_min_points=dict(Car=5, Pedestrian=5, Cyclist=5)),
-    classes=class_names,
-    sample_groups=dict(Car=15, Pedestrian=10, Cyclist=10),
-    points_loader=dict(
-        type='LoadPointsFromFile',
-        coord_type='LIDAR',
-        load_dim=6,
-        use_dim=[0, 1, 2, 3, 4]))
+# Example to use different file client
+# Method 1: simply set the data root and let the file I/O module
+# automatically infer from prefix (not support LMDB and Memcache yet)
+
+# data_root = 's3://openmmlab/datasets/detection3d/waymo/kitti_format/'
+
+# Method 2: Use backend_args, file_client_args in versions before 1.1.0
+# backend_args = dict(
+#     backend='petrel',
+#     path_mapping=dict({
+#         './data/': 's3://openmmlab/datasets/detection3d/',
+#          'data/': 's3://openmmlab/datasets/detection3d/'
+#      }))
+backend_args = None
 
 train_pipeline = [
+    dict(type='LoadImageFromFileMono3D', backend_args=backend_args),
     dict(
-        type='LoadPointsFromFile',
-        coord_type='LIDAR',
-        load_dim=6,
-        use_dim=5,
-        file_client_args=file_client_args),
-    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
-    # dict(type='ObjectSample', db_sampler=db_sampler),
-    dict(
-        type='RandomFlip3D',
-        sync_2d=False,
-        flip_ratio_bev_horizontal=0.5,
-        flip_ratio_bev_vertical=0.5),
-    dict(
-        type='GlobalRotScaleTrans',
-        rot_range=[-0.78539816, 0.78539816],
-        scale_ratio_range=[0.95, 1.05]),
-    dict(type='PointsRangeFilter', point_cloud_range=point_cloud_range),
-    dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
-    dict(type='PointShuffle'),
+        type='LoadAnnotations3D',
+        with_bbox=True,
+        with_label=True,
+        with_attr_label=False,
+        with_bbox_3d=True,
+        with_label_3d=True,
+        with_bbox_depth=True),
+    # base shape (1248, 832), scale (0.95, 1.05)
+    dict(
+        type='RandomResize3D',
+        scale=(1284, 832),
+        ratio_range=(0.95, 1.05),
+        keep_ratio=True,
+    ),
+    dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
     dict(
         type='Pack3DDetInputs',
-        keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
+        keys=[
+            'img', 'gt_bboxes', 'gt_bboxes_labels', 'gt_bboxes_3d',
+            'gt_labels_3d', 'centers_2d', 'depths'
+        ]),
 ]
 
 test_pipeline = [
+    dict(type='LoadImageFromFileMono3D', backend_args=backend_args),
     dict(
-        type='LoadPointsFromFile',
-        coord_type='LIDAR',
-        load_dim=6,
-        use_dim=5,
-        file_client_args=file_client_args),
-    dict(
-        type='MultiScaleFlipAug3D',
-        img_scale=(1333, 800),
-        pts_scale_ratio=1,
-        flip=False,
-        transforms=[
-            dict(
-                type='GlobalRotScaleTrans',
-                rot_range=[0, 0],
-                scale_ratio_range=[1., 1.],
-                translation_std=[0, 0, 0]),
-            dict(type='RandomFlip3D'),
-            dict(
-                type='PointsRangeFilter', point_cloud_range=point_cloud_range),
-            dict(type='Pack3DDetInputs', keys=['points']),
-        ])
+        type='RandomResize3D',
+        scale=(1248, 832),
+        ratio_range=(1., 1.),
+        keep_ratio=True),
+    dict(type='Pack3DDetInputs', keys=['img']),
+]
+# construct a pipeline for data and gt loading in show function
+# please keep its loading function consistent with test_pipeline (e.g. client)
+eval_pipeline = [
+    dict(type='LoadImageFromFileMono3D', backend_args=backend_args),
+    dict(
+        type='RandomResize3D',
+        scale=(1248, 832),
+        ratio_range=(1., 1.),
+        keep_ratio=True),
+    dict(type='Pack3DDetInputs', keys=['img']),
 ]
 
+metainfo = dict(classes=class_names)
+
 train_dataloader = dict(
-    batch_size=4,
-    num_workers=4,
+    batch_size=3,
+    num_workers=3,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
-        type='RepeatDataset',
-        times=2,
-        dataset=dict(
-            type=dataset_type,
-            data_root=data_root,
-            ann_file='waymo_infos_train.pkl',
-            data_prefix=dict(pts='training/velodyne'),
-            pipeline=train_pipeline,
-            modality=input_modality,
-            test_mode=False,
-            # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
-            # and box_type_3d='Depth' in sunrgbd and scannet dataset.
-            box_type_3d='LiDAR',
-            # load one frame every five frames
-            load_interval=5)))
+        type=dataset_type,
+        data_root=data_root,
+        ann_file='waymo_infos_train.pkl',
+        data_prefix=dict(
+            pts='training/velodyne',
+            CAM_FRONT='training/image_0',
+            CAM_FRONT_LEFT='training/image_1',
+            CAM_FRONT_RIGHT='training/image_2',
+            CAM_SIDE_LEFT='training/image_3',
+            CAM_SIDE_RIGHT='training/image_4'),
+        pipeline=train_pipeline,
+        modality=input_modality,
+        test_mode=False,
+        metainfo=metainfo,
+        # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
+        # and box_type_3d='Depth' in sunrgbd and scannet dataset.
+        box_type_3d='Camera',
+        load_type='mv_image_based',
+        # load one frame every three frames
+        load_interval=5,
+        backend_args=backend_args))
+
 val_dataloader = dict(
     batch_size=1,
     num_workers=1,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        data_prefix=dict(pts='training/velodyne'),
+        data_prefix=dict(
+            pts='training/velodyne',
+            CAM_FRONT='training/image_0',
+            CAM_FRONT_LEFT='training/image_1',
+            CAM_FRONT_RIGHT='training/image_2',
+            CAM_SIDE_LEFT='training/image_3',
+            CAM_SIDE_RIGHT='training/image_4'),
         ann_file='waymo_infos_val.pkl',
-        pipeline=test_pipeline,
+        pipeline=eval_pipeline,
         modality=input_modality,
         test_mode=True,
         metainfo=metainfo,
-        box_type_3d='LiDAR'))
+        # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
+        # and box_type_3d='Depth' in sunrgbd and scannet dataset.
+        box_type_3d='Camera',
+        load_type='mv_image_based',
+        backend_args=backend_args))
+
 test_dataloader = dict(
     batch_size=1,
     num_workers=1,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
-        data_prefix=dict(pts='training/velodyne'),
+        data_prefix=dict(
+            pts='training/velodyne',
+            CAM_FRONT='training/image_0',
+            CAM_FRONT_LEFT='training/image_1',
+            CAM_FRONT_RIGHT='training/image_2',
+            CAM_SIDE_LEFT='training/image_3',
+            CAM_SIDE_RIGHT='training/image_4'),
         ann_file='waymo_infos_val.pkl',
-        pipeline=test_pipeline,
+        pipeline=eval_pipeline,
         modality=input_modality,
         test_mode=True,
         metainfo=metainfo,
-        box_type_3d='LiDAR'))
-# Default setting for scaling LR automatically
-#   - `enable` means enable scaling LR automatically
-#       or not by default.
-#   - `base_batch_size` = (16 GPUs) x (2 samples per GPU).
-auto_scale_lr = dict(enable=False, base_batch_size=32)
+        # we use box_type_3d='LiDAR' in kitti and nuscenes dataset
+        # and box_type_3d='Depth' in sunrgbd and scannet dataset.
+        box_type_3d='Camera',
+        load_type='mv_image_based',
+        backend_args=backend_args))
+
+val_evaluator = dict(
+    type='WaymoMetric',
+    ann_file='./data/waymo/kitti_format/waymo_infos_val.pkl',
+    waymo_bin_file='./data/waymo/waymo_format/cam_gt.bin',
+    data_root='./data/waymo/waymo_format',
+    metric='LET_mAP',
+    load_type='mv_image_based',
+    backend_args=backend_args)
+test_evaluator = val_evaluator
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/smoke/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/smoke/metafile.yml`

 * *Files 2% similar despite different names*

```diff
@@ -13,17 +13,17 @@
       Title: 'SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint Estimation'
     README: configs/smoke/README.md
     Code:
       URL: https://github.com/open-mmlab/mmdetection3d/blob/v1.0.0.dev0/mmdet3d/models/detectors/smoke_mono3d.py#L7
       Version: v1.0.0
 
 Models:
-  - Name: smoke_dla34_pytorch_dlaneck_gn-all_4xb8-6x_kitti-mono3d
+  - Name: smoke_dla34_dlaneck_gn-all_4xb8-6x_kitti-mono3d
     In Collection: SMOKE
-    Config: configs/smoke/smoke_dla34_pytorch_dlaneck_gn-all_4xb8-6x_kitti-mono3d.py
+    Config: configs/smoke/smoke_dla34_dlaneck_gn-all_4xb8-6x_kitti-mono3d.py
     Metadata:
       Training Memory (GB): 9.6
     Results:
       - Task: 3D Object Detection
         Dataset: KITTI
         Metrics:
           mAP: 13.8
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/smoke/smoke_dla34_dlaneck_gn-all_4xb8-6x_kitti-mono3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/smoke/smoke_dla34_dlaneck_gn-all_4xb8-6x_kitti-mono3d.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,27 +1,16 @@
 _base_ = [
     '../_base_/datasets/kitti-mono3d.py', '../_base_/models/smoke.py',
     '../_base_/default_runtime.py'
 ]
 
-# file_client_args = dict(backend='disk')
-# Uncomment the following if use ceph or other file clients.
-# See https://mmcv.readthedocs.io/en/latest/api.html#mmcv.fileio.FileClient
-# for more details.
-file_client_args = dict(
-    backend='petrel',
-    path_mapping=dict({
-        './data/kitti/':
-        's3://openmmlab/datasets/detection3d/kitti/',
-        'data/kitti/':
-        's3://openmmlab/datasets/detection3d/kitti/'
-    }))
+backend_args = None
 
 train_pipeline = [
-    dict(type='LoadImageFromFileMono3D'),
+    dict(type='LoadImageFromFileMono3D', backend_args=backend_args),
     dict(
         type='LoadAnnotations3D',
         with_bbox=True,
         with_label=True,
         with_attr_label=False,
         with_bbox_3d=True,
         with_label_3d=True,
@@ -33,15 +22,15 @@
         type='Pack3DDetInputs',
         keys=[
             'img', 'gt_bboxes', 'gt_bboxes_labels', 'gt_bboxes_3d',
             'gt_labels_3d', 'centers_2d', 'depths'
         ]),
 ]
 test_pipeline = [
-    dict(type='LoadImageFromFileMono3D'),
+    dict(type='LoadImageFromFileMono3D', backend_args=backend_args),
     dict(type='AffineResize', img_scale=(1280, 384), down_ratio=4),
     dict(type='Pack3DDetInputs', keys=['img'])
 ]
 
 train_dataloader = dict(
     batch_size=8, num_workers=4, dataset=dict(pipeline=train_pipeline))
 test_dataloader = dict(dataset=dict(pipeline=test_pipeline))
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/ssn/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/ssn/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/ssn/ssn_hv_regnet-400mf_secfpn_sbn-all_16xb1-2x_lyft-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/ssn/ssn_hv_regnet-400mf_secfpn_sbn-all_16xb1-2x_lyft-3d.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/ssn/ssn_hv_regnet-400mf_secfpn_sbn-all_16xb2-2x_nus-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/ssn/ssn_hv_regnet-400mf_secfpn_sbn-all_16xb2-2x_nus-3d.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/ssn/ssn_hv_secfpn_sbn-all_16xb2-2x_lyft-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/ssn/ssn_hv_secfpn_sbn-all_16xb2-2x_lyft-3d.py`

 * *Files 13% similar despite different names*

```diff
@@ -7,18 +7,27 @@
 point_cloud_range = [-100, -100, -5, 100, 100, 3]
 # Note that the order of class names should be consistent with
 # the following anchors' order
 class_names = [
     'bicycle', 'motorcycle', 'pedestrian', 'animal', 'car',
     'emergency_vehicle', 'bus', 'other_vehicle', 'truck'
 ]
+backend_args = None
 
 train_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=5, use_dim=5),
-    dict(type='LoadPointsFromMultiSweeps', sweeps_num=10),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=5,
+        use_dim=5,
+        backend_args=backend_args),
+    dict(
+        type='LoadPointsFromMultiSweeps',
+        sweeps_num=10,
+        backend_args=backend_args),
     dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
     dict(
         type='GlobalRotScaleTrans',
         rot_range=[-0.3925, 0.3925],
         scale_ratio_range=[0.95, 1.05],
         translation_std=[0, 0, 0]),
     dict(
@@ -30,16 +39,24 @@
     dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
     dict(type='PointShuffle'),
     dict(
         type='Pack3DDetInputs',
         keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
 ]
 test_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=5, use_dim=5),
-    dict(type='LoadPointsFromMultiSweeps', sweeps_num=10),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=5,
+        use_dim=5,
+        backend_args=backend_args),
+    dict(
+        type='LoadPointsFromMultiSweeps',
+        sweeps_num=10,
+        backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
             dict(
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/ssn/ssn_hv_secfpn_sbn-all_16xb2-2x_nus-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/ssn/ssn_hv_secfpn_sbn-all_16xb2-2x_nus-3d.py`

 * *Files 4% similar despite different names*

```diff
@@ -7,18 +7,27 @@
 # Note that the order of class names should be consistent with
 # the following anchors' order
 point_cloud_range = [-50, -50, -5, 50, 50, 3]
 class_names = [
     'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier', 'car',
     'truck', 'trailer', 'bus', 'construction_vehicle'
 ]
+backend_args = None
 
 train_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=5, use_dim=5),
-    dict(type='LoadPointsFromMultiSweeps', sweeps_num=10),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=5,
+        use_dim=5,
+        backend_args=backend_args),
+    dict(
+        type='LoadPointsFromMultiSweeps',
+        sweeps_num=10,
+        backend_args=backend_args),
     dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
     dict(
         type='GlobalRotScaleTrans',
         rot_range=[-0.3925, 0.3925],
         scale_ratio_range=[0.95, 1.05],
         translation_std=[0, 0, 0]),
     dict(
@@ -30,16 +39,24 @@
     dict(type='ObjectRangeFilter', point_cloud_range=point_cloud_range),
     dict(type='PointShuffle'),
     dict(
         type='Pack3DDetInputs',
         keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
 ]
 test_pipeline = [
-    dict(type='LoadPointsFromFile', coord_type='LIDAR', load_dim=5, use_dim=5),
-    dict(type='LoadPointsFromMultiSweeps', sweeps_num=10),
+    dict(
+        type='LoadPointsFromFile',
+        coord_type='LIDAR',
+        load_dim=5,
+        use_dim=5,
+        backend_args=backend_args),
+    dict(
+        type='LoadPointsFromMultiSweeps',
+        sweeps_num=10,
+        backend_args=backend_args),
     dict(
         type='MultiScaleFlipAug3D',
         img_scale=(1333, 800),
         pts_scale_ratio=1,
         flip=False,
         transforms=[
             dict(
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/votenet/metafile.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/votenet/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/votenet/votenet_8xb16_sunrgbd-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/votenet/votenet_8xb16_sunrgbd-3d.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/configs/votenet/votenet_8xb8_scannet-3d.py` & `mmdet3d-1.1.1/mmdet3d/.mim/configs/votenet/votenet_8xb8_scannet-3d.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/model-index.yml` & `mmdet3d-1.1.1/mmdet3d/.mim/model-index.yml`

 * *Files 23% similar despite different names*

```diff
@@ -19,7 +19,12 @@
   - configs/pointnet2/metafile.yml
   - configs/pointpillars/metafile.yml
   - configs/regnet/metafile.yml
   - configs/second/metafile.yml
   - configs/smoke/metafile.yml
   - configs/ssn/metafile.yml
   - configs/votenet/metafile.yml
+  - configs/minkunet/metafile.yml
+  - configs/cylinder3d/metafile.yml
+  - configs/pv_rcnn/metafile.yml
+  - configs/fcaf3d/metafile.yml
+  - configs/spvcnn/metafile.yml
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/analysis_tools/analyze_logs.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/analysis_tools/analyze_logs.py`

 * *Files 8% similar despite different names*

```diff
@@ -13,26 +13,28 @@
         print(f'{"-" * 5}Analyze train time of {args.json_logs[i]}{"-" * 5}')
         all_times = []
         for epoch in log_dict.keys():
             if args.include_outliers:
                 all_times.append(log_dict[epoch]['time'])
             else:
                 all_times.append(log_dict[epoch]['time'][1:])
-        all_times = np.array(all_times)
-        epoch_ave_time = all_times.mean(-1)
+        if not all_times:
+            raise KeyError(
+                'Please reduce the log interval in the config so that '
+                'interval is less than iterations of one epoch.')
+        epoch_ave_time = np.array(list(map(lambda x: np.mean(x), all_times)))
         slowest_epoch = epoch_ave_time.argmax()
         fastest_epoch = epoch_ave_time.argmin()
         std_over_epoch = epoch_ave_time.std()
         print(f'slowest epoch {slowest_epoch + 1}, '
-              f'average time is {epoch_ave_time[slowest_epoch]:.4f}')
+              f'average time is {epoch_ave_time[slowest_epoch]:.4f} s/iter')
         print(f'fastest epoch {fastest_epoch + 1}, '
-              f'average time is {epoch_ave_time[fastest_epoch]:.4f}')
+              f'average time is {epoch_ave_time[fastest_epoch]:.4f} s/iter')
         print(f'time std over epochs is {std_over_epoch:.4f}')
-        print(f'average iter time: {np.mean(all_times):.4f} s/iter')
-        print()
+        print(f'average iter time: {np.mean(epoch_ave_time):.4f} s/iter\n')
 
 
 def plot_curve(log_dicts, args):
     if args.backend is not None:
         plt.switch_backend(args.backend)
     sns.set_style(args.style)
     # if legend is None, use {filename}_{key} as legend
@@ -46,64 +48,49 @@
     metrics = args.keys
 
     num_metrics = len(metrics)
     for i, log_dict in enumerate(log_dicts):
         epochs = list(log_dict.keys())
         for j, metric in enumerate(metrics):
             print(f'plot curve of {args.json_logs[i]}, metric is {metric}')
-            if metric not in log_dict[epochs[args.interval - 1]]:
+            if metric not in log_dict[epochs[int(args.eval_interval) - 1]]:
+                if args.eval:
+                    raise KeyError(
+                        f'{args.json_logs[i]} does not contain metric '
+                        f'{metric}. Please check if "--no-validate" is '
+                        'specified when you trained the model. Or check '
+                        f'if the eval_interval {args.eval_interval} in args '
+                        'is equal to the `eval_interval` during training.')
                 raise KeyError(
-                    f'{args.json_logs[i]} does not contain metric {metric}')
+                    f'{args.json_logs[i]} does not contain metric {metric}. '
+                    'Please reduce the log interval in the config so that '
+                    'interval is less than iterations of one epoch.')
 
-            if args.mode == 'eval':
-                if min(epochs) == args.interval:
-                    x0 = args.interval
-                else:
-                    # if current training is resumed from previous checkpoint
-                    # we lost information in early epochs
-                    # `xs` should start according to `min(epochs)`
-                    if min(epochs) % args.interval == 0:
-                        x0 = min(epochs)
-                    else:
-                        # find the first epoch that do eval
-                        x0 = min(epochs) + args.interval - \
-                            min(epochs) % args.interval
-                xs = np.arange(x0, max(epochs) + 1, args.interval)
+            if args.eval:
+                xs = []
                 ys = []
-                for epoch in epochs[args.interval - 1::args.interval]:
+                for epoch in epochs:
                     ys += log_dict[epoch][metric]
-
-                # if training is aborted before eval of the last epoch
-                # `xs` and `ys` will have different length and cause an error
-                # check if `ys[-1]` is empty here
-                if not log_dict[epoch][metric]:
-                    xs = xs[:-1]
-
-                ax = plt.gca()
-                ax.set_xticks(xs)
+                    if log_dict[epoch][metric]:
+                        xs += [epoch]
                 plt.xlabel('epoch')
                 plt.plot(xs, ys, label=legend[i * num_metrics + j], marker='o')
             else:
                 xs = []
                 ys = []
-                num_iters_per_epoch = \
-                    log_dict[epochs[args.interval-1]]['iter'][-1]
-                for epoch in epochs[args.interval - 1::args.interval]:
-                    iters = log_dict[epoch]['iter']
-                    if log_dict[epoch]['mode'][-1] == 'val':
-                        iters = iters[:-1]
-                    xs.append(
-                        np.array(iters) + (epoch - 1) * num_iters_per_epoch)
+                for epoch in epochs:
+                    iters = log_dict[epoch]['step']
+                    xs.append(np.array(iters))
                     ys.append(np.array(log_dict[epoch][metric][:len(iters)]))
                 xs = np.concatenate(xs)
                 ys = np.concatenate(ys)
                 plt.xlabel('iter')
                 plt.plot(
                     xs, ys, label=legend[i * num_metrics + j], linewidth=0.5)
-            plt.legend()
+                plt.legend()
         if args.title is not None:
             plt.title(args.title)
     if args.out is None:
         plt.show()
     else:
         print(f'save curve to: {args.out}')
         plt.savefig(args.out)
@@ -120,28 +107,35 @@
         help='path of train log in json format')
     parser_plt.add_argument(
         '--keys',
         type=str,
         nargs='+',
         default=['mAP_0.25'],
         help='the metric that you want to plot')
+    parser_plt.add_argument(
+        '--eval',
+        action='store_true',
+        help='whether to plot evaluation metric')
+    parser_plt.add_argument(
+        '--eval-interval',
+        type=str,
+        default='1',
+        help='the eval interval when training')
     parser_plt.add_argument('--title', type=str, help='title of figure')
     parser_plt.add_argument(
         '--legend',
         type=str,
         nargs='+',
         default=None,
         help='legend of each plot')
     parser_plt.add_argument(
         '--backend', type=str, default=None, help='backend of plt')
     parser_plt.add_argument(
         '--style', type=str, default='dark', help='style of plt')
     parser_plt.add_argument('--out', type=str, default=None)
-    parser_plt.add_argument('--mode', type=str, default='train')
-    parser_plt.add_argument('--interval', type=int, default=1)
 
 
 def add_time_parser(subparsers):
     parser_time = subparsers.add_parser(
         'cal_train_time',
         help='parser for computing the average time per training iteration')
     parser_time.add_argument(
@@ -170,25 +164,36 @@
     # load and convert json_logs to log_dict, key is epoch, value is a sub dict
     # keys of sub dict is different metrics, e.g. memory, bbox_mAP
     # value of sub dict is a list of corresponding values of all iterations
     log_dicts = [dict() for _ in json_logs]
     for json_log, log_dict in zip(json_logs, log_dicts):
         with open(json_log, 'r') as log_file:
             epoch = 1
-            for line in log_file:
+            for i, line in enumerate(log_file):
                 log = json.loads(line.strip())
+                val_flag = False
                 # skip lines only contains one key
                 if not len(log) > 1:
                     continue
+
                 if epoch not in log_dict:
                     log_dict[epoch] = defaultdict(list)
+
                 for k, v in log.items():
-                    log_dict[epoch][k].append(v)
+                    if '/' in k:
+                        log_dict[epoch][k.split('/')[-1]].append(v)
+                        val_flag = True
+                    elif val_flag:
+                        continue
+                    else:
+                        log_dict[epoch][k].append(v)
+
                 if 'epoch' in log.keys():
-                    epoch = log['epoch'] + 1
+                    epoch = log['epoch']
+
     return log_dicts
 
 
 def main():
     args = parse_args()
 
     json_logs = args.json_logs
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/analysis_tools/benchmark.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/analysis_tools/get_flops.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,97 +1,83 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import argparse
-import time
 
 import torch
-from mmcv import Config
-from mmcv.parallel import MMDataParallel
-from mmengine.runner import load_checkpoint
+from mmengine import Config, DictAction
+from mmengine.registry import init_default_scope
 
-from mmdet3d.registry import DATASETS, MODELS
-from tools.misc.fuse_conv_bn import fuse_module
+from mmdet3d.registry import MODELS
+
+try:
+    from mmcv.cnn import get_model_complexity_info
+except ImportError:
+    raise ImportError('Please upgrade mmcv to >0.6.2')
 
 
 def parse_args():
-    parser = argparse.ArgumentParser(description='MMDet benchmark a model')
-    parser.add_argument('config', help='test config file path')
-    parser.add_argument('checkpoint', help='checkpoint file')
-    parser.add_argument('--samples', default=2000, help='samples to benchmark')
+    parser = argparse.ArgumentParser(description='Train a detector')
+    parser.add_argument('config', help='train config file path')
+    parser.add_argument(
+        '--shape',
+        type=int,
+        nargs='+',
+        default=[40000, 4],
+        help='input point cloud size')
     parser.add_argument(
-        '--log-interval', default=50, help='interval of logging')
+        '--modality',
+        type=str,
+        default='point',
+        choices=['point', 'image', 'multi'],
+        help='input data modality')
     parser.add_argument(
-        '--fuse-conv-bn',
-        action='store_true',
-        help='Whether to fuse conv and bn, this will slightly increase'
-        'the inference speed')
+        '--cfg-options',
+        nargs='+',
+        action=DictAction,
+        help='override some settings in the used config, the key-value pair '
+        'in xxx=yyy format will be merged into config file. If the value to '
+        'be overwritten is a list, it should be like key="[a,b]" or key=a,b '
+        'It also allows nested list/tuple values, e.g. key="[(a,b),(c,d)]" '
+        'Note that the quotation marks are necessary and that no white space '
+        'is allowed.')
     args = parser.parse_args()
     return args
 
 
 def main():
     args = parse_args()
 
-    cfg = Config.fromfile(args.config)
-    # set cudnn_benchmark
-    if cfg.get('cudnn_benchmark', False):
-        torch.backends.cudnn.benchmark = True
-    cfg.model.pretrained = None
-    cfg.data.test.test_mode = True
-
-    # build the dataloader
-    # TODO: support multiple images per gpu (only minor changes are needed)
-    dataset = DATASETS.build(cfg.data.test)
-
-    # TODO fix this
-    def build_dataloader():
-        pass
-
-    data_loader = build_dataloader(
-        dataset,
-        samples_per_gpu=1,
-        workers_per_gpu=cfg.data.workers_per_gpu,
-        dist=False,
-        shuffle=False)
-
-    # build the model and load checkpoint
-    cfg.model.train_cfg = None
-    model = MODELS.build(cfg.model, test_cfg=cfg.get('test_cfg'))
-    load_checkpoint(model, args.checkpoint, map_location='cpu')
-    if args.fuse_conv_bn:
-        model = fuse_module(model)
-
-    model = MMDataParallel(model, device_ids=[0])
+    if args.modality == 'point':
+        assert len(args.shape) == 2, 'invalid input shape'
+        input_shape = tuple(args.shape)
+    elif args.modality == 'image':
+        if len(args.shape) == 1:
+            input_shape = (3, args.shape[0], args.shape[0])
+        elif len(args.shape) == 2:
+            input_shape = (3, ) + tuple(args.shape)
+        else:
+            raise ValueError('invalid input shape')
+    elif args.modality == 'multi':
+        raise NotImplementedError(
+            'FLOPs counter is currently not supported for models with '
+            'multi-modality input')
 
+    cfg = Config.fromfile(args.config)
+    if args.cfg_options is not None:
+        cfg.merge_from_dict(args.cfg_options)
+    init_default_scope(cfg.get('default_scope', 'mmdet3d'))
+
+    model = MODELS.build(cfg.model)
+    if torch.cuda.is_available():
+        model.cuda()
     model.eval()
 
-    # the first several iterations may be very slow so skip them
-    num_warmup = 5
-    pure_inf_time = 0
-
-    # benchmark with several samples and take the average
-    for i, data in enumerate(data_loader):
-
-        torch.cuda.synchronize()
-        start_time = time.perf_counter()
-
-        with torch.no_grad():
-            model(return_loss=False, rescale=True, **data)
-
-        torch.cuda.synchronize()
-        elapsed = time.perf_counter() - start_time
-
-        if i >= num_warmup:
-            pure_inf_time += elapsed
-            if (i + 1) % args.log_interval == 0:
-                fps = (i + 1 - num_warmup) / pure_inf_time
-                print(f'Done image [{i + 1:<3}/ {args.samples}], '
-                      f'fps: {fps:.1f} img / s')
-
-        if (i + 1) == args.samples:
-            pure_inf_time += elapsed
-            fps = (i + 1 - num_warmup) / pure_inf_time
-            print(f'Overall fps: {fps:.1f} img / s')
-            break
+    flops, params = get_model_complexity_info(model, input_shape)
+    split_line = '=' * 30
+    print(f'{split_line}\nInput shape: {input_shape}\n'
+          f'Flops: {flops}\nParams: {params}\n{split_line}')
+    print('!!!Please be cautious if you use the results in papers. '
+          'You may need to check if all ops are supported and verify that the '
+          'flops computation is correct.')
 
 
 if __name__ == '__main__':
     main()
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/create_data.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/create_data.py`

 * *Files 18% similar despite different names*

```diff
@@ -2,14 +2,15 @@
 import argparse
 from os import path as osp
 
 from tools.dataset_converters import indoor_converter as indoor
 from tools.dataset_converters import kitti_converter as kitti
 from tools.dataset_converters import lyft_converter as lyft_converter
 from tools.dataset_converters import nuscenes_converter as nuscenes_converter
+from tools.dataset_converters import semantickitti_converter
 from tools.dataset_converters.create_gt_database import (
     GTDatabaseCreater, create_groundtruth_database)
 from tools.dataset_converters.update_infos_to_v2 import update_pkl_infos
 
 
 def kitti_data_prep(root_path,
                     info_prefix,
@@ -31,17 +32,19 @@
     """
     kitti.create_kitti_info_file(root_path, info_prefix, with_plane)
     kitti.create_reduced_point_cloud(root_path, info_prefix)
 
     info_train_path = osp.join(out_dir, f'{info_prefix}_infos_train.pkl')
     info_val_path = osp.join(out_dir, f'{info_prefix}_infos_val.pkl')
     info_trainval_path = osp.join(out_dir, f'{info_prefix}_infos_trainval.pkl')
+    info_test_path = osp.join(out_dir, f'{info_prefix}_infos_test.pkl')
     update_pkl_infos('kitti', out_dir=out_dir, pkl_path=info_train_path)
     update_pkl_infos('kitti', out_dir=out_dir, pkl_path=info_val_path)
     update_pkl_infos('kitti', out_dir=out_dir, pkl_path=info_trainval_path)
+    update_pkl_infos('kitti', out_dir=out_dir, pkl_path=info_test_path)
     create_groundtruth_database(
         'KittiDataset',
         root_path,
         info_prefix,
         f'{info_prefix}_infos_train.pkl',
         relative_path=False,
         mask_anno_path='instances_train.json',
@@ -118,19 +121,19 @@
         info_prefix (str): The prefix of info filenames.
         out_dir (str): Output directory of the generated info file.
         workers (int): Number of threads to be used.
     """
     indoor.create_indoor_info_file(
         root_path, info_prefix, out_dir, workers=workers)
     info_train_path = osp.join(out_dir, f'{info_prefix}_infos_train.pkl')
-    info_test_path = osp.join(out_dir, f'{info_prefix}_infos_test.pkl')
     info_val_path = osp.join(out_dir, f'{info_prefix}_infos_val.pkl')
+    info_test_path = osp.join(out_dir, f'{info_prefix}_infos_test.pkl')
     update_pkl_infos('scannet', out_dir=out_dir, pkl_path=info_train_path)
-    update_pkl_infos('scannet', out_dir=out_dir, pkl_path=info_test_path)
     update_pkl_infos('scannet', out_dir=out_dir, pkl_path=info_val_path)
+    update_pkl_infos('scannet', out_dir=out_dir, pkl_path=info_test_path)
 
 
 def s3dis_data_prep(root_path, info_prefix, out_dir, workers):
     """Prepare the info file for s3dis dataset.
 
     Args:
         root_path (str): Path of dataset root.
@@ -155,16 +158,16 @@
         out_dir (str): Output directory of the generated info file.
         workers (int): Number of threads to be used.
     """
     indoor.create_indoor_info_file(
         root_path, info_prefix, out_dir, workers=workers)
     info_train_path = osp.join(out_dir, f'{info_prefix}_infos_train.pkl')
     info_val_path = osp.join(out_dir, f'{info_prefix}_infos_val.pkl')
-    update_pkl_infos('scannet', out_dir=out_dir, pkl_path=info_train_path)
-    update_pkl_infos('scannet', out_dir=out_dir, pkl_path=info_val_path)
+    update_pkl_infos('sunrgbd', out_dir=out_dir, pkl_path=info_train_path)
+    update_pkl_infos('sunrgbd', out_dir=out_dir, pkl_path=info_val_path)
 
 
 def waymo_data_prep(root_path,
                     info_prefix,
                     version,
                     out_dir,
                     workers,
@@ -195,34 +198,51 @@
             load_dir,
             save_dir,
             prefix=str(i),
             workers=workers,
             test_mode=(split
                        in ['testing', 'testing_3d_camera_only_detection']))
         converter.convert()
+
+    from tools.dataset_converters.waymo_converter import \
+        create_ImageSets_img_ids
+    create_ImageSets_img_ids(osp.join(out_dir, 'kitti_format'), splits)
     # Generate waymo infos
     out_dir = osp.join(out_dir, 'kitti_format')
     kitti.create_waymo_info_file(
         out_dir, info_prefix, max_sweeps=max_sweeps, workers=workers)
     info_train_path = osp.join(out_dir, f'{info_prefix}_infos_train.pkl')
     info_val_path = osp.join(out_dir, f'{info_prefix}_infos_val.pkl')
     info_trainval_path = osp.join(out_dir, f'{info_prefix}_infos_trainval.pkl')
+    info_test_path = osp.join(out_dir, f'{info_prefix}_infos_test.pkl')
     update_pkl_infos('waymo', out_dir=out_dir, pkl_path=info_train_path)
     update_pkl_infos('waymo', out_dir=out_dir, pkl_path=info_val_path)
     update_pkl_infos('waymo', out_dir=out_dir, pkl_path=info_trainval_path)
+    update_pkl_infos('waymo', out_dir=out_dir, pkl_path=info_test_path)
     GTDatabaseCreater(
         'WaymoDataset',
         out_dir,
         info_prefix,
         f'{info_prefix}_infos_train.pkl',
         relative_path=False,
         with_mask=False,
         num_worker=workers).create()
 
 
+def semantickitti_data_prep(info_prefix, out_dir):
+    """Prepare the info file for SemanticKITTI dataset.
+
+    Args:
+        info_prefix (str): The prefix of info filenames.
+        out_dir (str): Output directory of the generated info file.
+    """
+    semantickitti_converter.create_semantickitti_info_file(
+        info_prefix, out_dir)
+
+
 parser = argparse.ArgumentParser(description='Data converter arg parser')
 parser.add_argument('dataset', metavar='kitti', help='name of the dataset')
 parser.add_argument(
     '--root-path',
     type=str,
     default='./data/kitti',
     help='specify the root path of dataset')
@@ -247,57 +267,77 @@
     type=str,
     default='./data/kitti',
     required=False,
     help='name of info pkl')
 parser.add_argument('--extra-tag', type=str, default='kitti')
 parser.add_argument(
     '--workers', type=int, default=4, help='number of threads to be used')
+parser.add_argument(
+    '--only-gt-database',
+    action='store_true',
+    help='Whether to only generate ground truth database.')
 args = parser.parse_args()
 
 if __name__ == '__main__':
     from mmdet3d.utils import register_all_modules
     register_all_modules()
 
-    # Set to spawn mode to avoid stuck when process dataset creating
-    import multiprocessing
-    multiprocessing.set_start_method('spawn')
-
     if args.dataset == 'kitti':
-        kitti_data_prep(
-            root_path=args.root_path,
-            info_prefix=args.extra_tag,
-            version=args.version,
-            out_dir=args.out_dir,
-            with_plane=args.with_plane)
+        if args.only_gt_database:
+            create_groundtruth_database(
+                'KittiDataset',
+                args.root_path,
+                args.extra_tag,
+                f'{args.extra_tag}_infos_train.pkl',
+                relative_path=False,
+                mask_anno_path='instances_train.json',
+                with_mask=(args.version == 'mask'))
+        else:
+            kitti_data_prep(
+                root_path=args.root_path,
+                info_prefix=args.extra_tag,
+                version=args.version,
+                out_dir=args.out_dir,
+                with_plane=args.with_plane)
     elif args.dataset == 'nuscenes' and args.version != 'v1.0-mini':
-        train_version = f'{args.version}-trainval'
-        nuscenes_data_prep(
-            root_path=args.root_path,
-            info_prefix=args.extra_tag,
-            version=train_version,
-            dataset_name='NuScenesDataset',
-            out_dir=args.out_dir,
-            max_sweeps=args.max_sweeps)
-        test_version = f'{args.version}-test'
-        nuscenes_data_prep(
-            root_path=args.root_path,
-            info_prefix=args.extra_tag,
-            version=test_version,
-            dataset_name='NuScenesDataset',
-            out_dir=args.out_dir,
-            max_sweeps=args.max_sweeps)
+        if args.only_gt_database:
+            create_groundtruth_database('NuScenesDataset', args.root_path,
+                                        args.extra_tag,
+                                        f'{args.extra_tag}_infos_train.pkl')
+        else:
+            train_version = f'{args.version}-trainval'
+            nuscenes_data_prep(
+                root_path=args.root_path,
+                info_prefix=args.extra_tag,
+                version=train_version,
+                dataset_name='NuScenesDataset',
+                out_dir=args.out_dir,
+                max_sweeps=args.max_sweeps)
+            test_version = f'{args.version}-test'
+            nuscenes_data_prep(
+                root_path=args.root_path,
+                info_prefix=args.extra_tag,
+                version=test_version,
+                dataset_name='NuScenesDataset',
+                out_dir=args.out_dir,
+                max_sweeps=args.max_sweeps)
     elif args.dataset == 'nuscenes' and args.version == 'v1.0-mini':
-        train_version = f'{args.version}'
-        nuscenes_data_prep(
-            root_path=args.root_path,
-            info_prefix=args.extra_tag,
-            version=train_version,
-            dataset_name='NuScenesDataset',
-            out_dir=args.out_dir,
-            max_sweeps=args.max_sweeps)
+        if args.only_gt_database:
+            create_groundtruth_database('NuScenesDataset', args.root_path,
+                                        args.extra_tag,
+                                        f'{args.extra_tag}_infos_train.pkl')
+        else:
+            train_version = f'{args.version}'
+            nuscenes_data_prep(
+                root_path=args.root_path,
+                info_prefix=args.extra_tag,
+                version=train_version,
+                dataset_name='NuScenesDataset',
+                out_dir=args.out_dir,
+                max_sweeps=args.max_sweeps)
     elif args.dataset == 'lyft':
         train_version = f'{args.version}-train'
         lyft_data_prep(
             root_path=args.root_path,
             info_prefix=args.extra_tag,
             version=train_version,
             max_sweeps=args.max_sweeps)
@@ -329,9 +369,12 @@
             workers=args.workers)
     elif args.dataset == 'sunrgbd':
         sunrgbd_data_prep(
             root_path=args.root_path,
             info_prefix=args.extra_tag,
             out_dir=args.out_dir,
             workers=args.workers)
+    elif args.dataset == 'semantickitti':
+        semantickitti_data_prep(
+            info_prefix=args.extra_tag, out_dir=args.out_dir)
     else:
         raise NotImplementedError(f'Don\'t support {args.dataset} dataset.')
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/create_data.sh` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/create_data.sh`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/create_gt_database.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/create_gt_database.py`

 * *Files 4% similar despite different names*

```diff
@@ -143,34 +143,34 @@
         with_mask (bool, optional): Whether to use mask.
             Default: False.
     """
     print(f'Create GT Database of {dataset_class_name}')
     dataset_cfg = dict(
         type=dataset_class_name, data_root=data_path, ann_file=info_path)
     if dataset_class_name == 'KittiDataset':
-        file_client_args = dict(backend='disk')
+        backend_args = None
         dataset_cfg.update(
             modality=dict(
                 use_lidar=True,
                 use_camera=with_mask,
             ),
             data_prefix=dict(
                 pts='training/velodyne_reduced', img='training/image_2'),
             pipeline=[
                 dict(
                     type='LoadPointsFromFile',
                     coord_type='LIDAR',
                     load_dim=4,
                     use_dim=4,
-                    file_client_args=file_client_args),
+                    backend_args=backend_args),
                 dict(
                     type='LoadAnnotations3D',
                     with_bbox_3d=True,
                     with_label_3d=True,
-                    file_client_args=file_client_args)
+                    backend_args=backend_args)
             ])
 
     elif dataset_class_name == 'NuScenesDataset':
         dataset_cfg.update(
             use_valid_flag=True,
             data_prefix=dict(
                 pts='samples/LIDAR_TOP', img='', sweeps='sweeps/LIDAR_TOP'),
@@ -189,15 +189,15 @@
                 dict(
                     type='LoadAnnotations3D',
                     with_bbox_3d=True,
                     with_label_3d=True)
             ])
 
     elif dataset_class_name == 'WaymoDataset':
-        file_client_args = dict(backend='disk')
+        backend_args = None
         dataset_cfg.update(
             test_mode=False,
             data_prefix=dict(
                 pts='training/velodyne', img='', sweeps='training/velodyne'),
             modality=dict(
                 use_lidar=True,
                 use_depth=False,
@@ -206,20 +206,20 @@
             ),
             pipeline=[
                 dict(
                     type='LoadPointsFromFile',
                     coord_type='LIDAR',
                     load_dim=6,
                     use_dim=6,
-                    file_client_args=file_client_args),
+                    backend_args=backend_args),
                 dict(
                     type='LoadAnnotations3D',
                     with_bbox_3d=True,
                     with_label_3d=True,
-                    file_client_args=file_client_args)
+                    backend_args=backend_args)
             ])
 
     dataset = DATASETS.build(dataset_cfg)
 
     if database_save_path is None:
         database_save_path = osp.join(data_path, f'{info_prefix}_gt_database')
     if db_info_save_path is None:
@@ -237,16 +237,16 @@
 
     group_counter = 0
     for j in track_iter_progress(list(range(len(dataset)))):
         data_info = dataset.get_data_info(j)
         example = dataset.pipeline(data_info)
         annos = example['ann_info']
         image_idx = example['sample_idx']
-        points = example['points'].tensor.numpy()
-        gt_boxes_3d = annos['gt_bboxes_3d'].tensor.numpy()
+        points = example['points'].numpy()
+        gt_boxes_3d = annos['gt_bboxes_3d'].numpy()
         names = [dataset.metainfo['classes'][i] for i in annos['gt_labels_3d']]
         group_dict = dict()
         if 'group_ids' in annos:
             group_ids = annos['group_ids']
         else:
             group_ids = np.arange(gt_boxes_3d.shape[0], dtype=np.int64)
         difficulty = np.zeros(gt_boxes_3d.shape[0], dtype=np.int32)
@@ -402,16 +402,16 @@
 
     def create_single(self, input_dict):
         group_counter = 0
         single_db_infos = dict()
         example = self.pipeline(input_dict)
         annos = example['ann_info']
         image_idx = example['sample_idx']
-        points = example['points'].tensor.numpy()
-        gt_boxes_3d = annos['gt_bboxes_3d'].tensor.numpy()
+        points = example['points'].numpy()
+        gt_boxes_3d = annos['gt_bboxes_3d'].numpy()
         names = [
             self.dataset.metainfo['classes'][i] for i in annos['gt_labels_3d']
         ]
         group_dict = dict()
         if 'group_ids' in annos:
             group_ids = annos['group_ids']
         else:
@@ -506,15 +506,15 @@
     def create(self):
         print(f'Create GT Database of {self.dataset_class_name}')
         dataset_cfg = dict(
             type=self.dataset_class_name,
             data_root=self.data_path,
             ann_file=self.info_path)
         if self.dataset_class_name == 'KittiDataset':
-            file_client_args = dict(backend='disk')
+            backend_args = None
             dataset_cfg.update(
                 test_mode=False,
                 data_prefix=dict(
                     pts='training/velodyne_reduced', img='training/image_2'),
                 modality=dict(
                     use_lidar=True,
                     use_depth=False,
@@ -523,20 +523,20 @@
                 ),
                 pipeline=[
                     dict(
                         type='LoadPointsFromFile',
                         coord_type='LIDAR',
                         load_dim=4,
                         use_dim=4,
-                        file_client_args=file_client_args),
+                        backend_args=backend_args),
                     dict(
                         type='LoadAnnotations3D',
                         with_bbox_3d=True,
                         with_label_3d=True,
-                        file_client_args=file_client_args)
+                        backend_args=backend_args)
                 ])
 
         elif self.dataset_class_name == 'NuScenesDataset':
             dataset_cfg.update(
                 use_valid_flag=True,
                 data_prefix=dict(
                     pts='samples/LIDAR_TOP', img='',
@@ -556,15 +556,15 @@
                     dict(
                         type='LoadAnnotations3D',
                         with_bbox_3d=True,
                         with_label_3d=True)
                 ])
 
         elif self.dataset_class_name == 'WaymoDataset':
-            file_client_args = dict(backend='disk')
+            backend_args = None
             dataset_cfg.update(
                 test_mode=False,
                 data_prefix=dict(
                     pts='training/velodyne',
                     img='',
                     sweeps='training/velodyne'),
                 modality=dict(
@@ -575,20 +575,20 @@
                 ),
                 pipeline=[
                     dict(
                         type='LoadPointsFromFile',
                         coord_type='LIDAR',
                         load_dim=6,
                         use_dim=6,
-                        file_client_args=file_client_args),
+                        backend_args=backend_args),
                     dict(
                         type='LoadAnnotations3D',
                         with_bbox_3d=True,
                         with_label_3d=True,
-                        file_client_args=file_client_args)
+                        backend_args=backend_args)
                 ])
 
         self.dataset = DATASETS.build(dataset_cfg)
         self.pipeline = self.dataset.pipeline
         if self.database_save_path is None:
             self.database_save_path = osp.join(
                 self.data_path, f'{self.info_prefix}_gt_database')
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/indoor_converter.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/indoor_converter.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/kitti_converter.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/kitti_converter.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/kitti_data_utils.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/kitti_data_utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -361,22 +361,22 @@
         if self.velodyne:
             pc_info['velodyne_path'] = get_velodyne_path(
                 idx,
                 self.path,
                 self.training,
                 self.relative_path,
                 use_prefix_id=True)
-            with open(
-                    get_timestamp_path(
-                        idx,
-                        self.path,
-                        self.training,
-                        relative_path=False,
-                        use_prefix_id=True)) as f:
-                info['timestamp'] = np.int64(f.read())
+        with open(
+                get_timestamp_path(
+                    idx,
+                    self.path,
+                    self.training,
+                    relative_path=False,
+                    use_prefix_id=True)) as f:
+            info['timestamp'] = np.int64(f.read())
         image_info['image_path'] = get_image_path(
             idx,
             self.path,
             self.training,
             self.relative_path,
             info_type='image_0',
             file_tail='.jpg',
@@ -590,17 +590,17 @@
     annos = info['annos']
     dims = annos['dimensions']  # lhw format
     bbox = annos['bbox']
     height = bbox[:, 3] - bbox[:, 1]
     occlusion = annos['occluded']
     truncation = annos['truncated']
     diff = []
-    easy_mask = np.ones((len(dims), ), dtype=np.bool)
-    moderate_mask = np.ones((len(dims), ), dtype=np.bool)
-    hard_mask = np.ones((len(dims), ), dtype=np.bool)
+    easy_mask = np.ones((len(dims), ), dtype=bool)
+    moderate_mask = np.ones((len(dims), ), dtype=bool)
+    hard_mask = np.ones((len(dims), ), dtype=bool)
     i = 0
     for h, o, t in zip(height, occlusion, truncation):
         if o > max_occlusion[0] or h <= min_height[0] or t > max_trunc[0]:
             easy_mask[i] = False
         if o > max_occlusion[1] or h <= min_height[1] or t > max_trunc[1]:
             moderate_mask[i] = False
         if o > max_occlusion[2] or h <= min_height[2] or t > max_trunc[2]:
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/lyft_converter.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/lyft_converter.py`

 * *Files 3% similar despite different names*

```diff
@@ -36,18 +36,18 @@
     lyft = Lyft(
         data_path=osp.join(root_path, version),
         json_path=osp.join(root_path, version, version),
         verbose=True)
     available_vers = ['v1.01-train', 'v1.01-test']
     assert version in available_vers
     if version == 'v1.01-train':
-        train_scenes = mmcv.list_from_file('data/lyft/train.txt')
-        val_scenes = mmcv.list_from_file('data/lyft/val.txt')
+        train_scenes = mmengine.list_from_file('data/lyft/train.txt')
+        val_scenes = mmengine.list_from_file('data/lyft/val.txt')
     elif version == 'v1.01-test':
-        train_scenes = mmcv.list_from_file('data/lyft/test.txt')
+        train_scenes = mmengine.list_from_file('data/lyft/test.txt')
         val_scenes = []
     else:
         raise ValueError('unknown')
 
     # filter existing scenes.
     available_scenes = get_available_scenes(lyft)
     available_scene_names = [s['name'] for s in available_scenes]
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/lyft_data_fixer.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/lyft_data_fixer.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/nuimage_converter.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/nuimage_converter.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/nuscenes_converter.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/nuscenes_converter.py`

 * *Files 1% similar despite different names*

```diff
@@ -263,14 +263,19 @@
             info['gt_velocity'] = velocity.reshape(-1, 2)
             info['num_lidar_pts'] = np.array(
                 [a['num_lidar_pts'] for a in annotations])
             info['num_radar_pts'] = np.array(
                 [a['num_radar_pts'] for a in annotations])
             info['valid_flag'] = valid_flag
 
+            if 'lidarseg' in nusc.table_names:
+                info['pts_semantic_mask_path'] = osp.join(
+                    nusc.dataroot,
+                    nusc.get('lidarseg', lidar_token)['filename'])
+
         if sample['scene_token'] in train_scenes:
             train_nusc_infos.append(info)
         else:
             val_nusc_infos.append(info)
 
     return train_nusc_infos, val_nusc_infos
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/s3dis_data_utils.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/s3dis_data_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -74,16 +74,18 @@
                 self.root_dir, 's3dis_data',
                 f'{self.split}_{sample_idx}_ins_label.npy')
             pts_semantic_mask_path = osp.join(
                 self.root_dir, 's3dis_data',
                 f'{self.split}_{sample_idx}_sem_label.npy')
 
             points = np.load(pts_filename).astype(np.float32)
-            pts_instance_mask = np.load(pts_instance_mask_path).astype(np.int)
-            pts_semantic_mask = np.load(pts_semantic_mask_path).astype(np.int)
+            pts_instance_mask = np.load(pts_instance_mask_path).astype(
+                np.int64)
+            pts_semantic_mask = np.load(pts_semantic_mask_path).astype(
+                np.int64)
 
             mmengine.mkdir_or_exist(osp.join(self.root_dir, 'points'))
             mmengine.mkdir_or_exist(osp.join(self.root_dir, 'instance_mask'))
             mmengine.mkdir_or_exist(osp.join(self.root_dir, 'semantic_mask'))
 
             points.tofile(
                 osp.join(self.root_dir, 'points',
@@ -176,16 +178,16 @@
         self.num_points = num_points
 
         self.all_ids = np.arange(13)  # all possible ids
         self.cat_ids = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
                                  12])  # used for seg task
         self.ignore_index = len(self.cat_ids)
 
-        self.cat_id2class = np.ones((self.all_ids.shape[0],), dtype=np.int) * \
-            self.ignore_index
+        self.cat_id2class = np.ones(
+            (self.all_ids.shape[0], ), dtype=np.int64) * self.ignore_index
         for i, cat_id in enumerate(self.cat_ids):
             self.cat_id2class[cat_id] = i
 
         # label weighting function is taken from
         # https://github.com/charlesq34/pointnet2/blob/master/scannet/scannet_dataset.py#L24
         self.label_weight_func = (lambda x: 1.0 / np.log(1.2 + x)) if \
             label_weight_func is None else label_weight_func
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/scannet_data_utils.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/scannet_data_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -228,16 +228,16 @@
         self.all_ids = np.arange(41)  # all possible ids
         self.cat_ids = np.array([
             1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 16, 24, 28, 33, 34, 36,
             39
         ])  # used for seg task
         self.ignore_index = len(self.cat_ids)
 
-        self.cat_id2class = np.ones((self.all_ids.shape[0],), dtype=np.int) * \
-            self.ignore_index
+        self.cat_id2class = np.ones(
+            (self.all_ids.shape[0], ), dtype=np.int64) * self.ignore_index
         for i, cat_id in enumerate(self.cat_ids):
             self.cat_id2class[cat_id] = i
 
         # label weighting function is taken from
         # https://github.com/charlesq34/pointnet2/blob/master/scannet/scannet_dataset.py#L24
         self.label_weight_func = (lambda x: 1.0 / np.log(1.2 + x)) if \
             label_weight_func is None else label_weight_func
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/sunrgbd_data_utils.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/sunrgbd_data_utils.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/update_infos_to_v2.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/update_infos_to_v2.py`

 * *Files 2% similar despite different names*

```diff
@@ -333,37 +333,45 @@
             rot = ori_info_dict['cams'][cam]['sensor2lidar_rotation']
             trans = ori_info_dict['cams'][cam]['sensor2lidar_translation']
             lidar2sensor[:3, :3] = rot.T
             lidar2sensor[:3, 3:4] = -1 * np.matmul(rot.T, trans.reshape(3, 1))
             empty_img_info['lidar2cam'] = lidar2sensor.astype(
                 np.float32).tolist()
             temp_data_info['images'][cam] = empty_img_info
-        num_instances = ori_info_dict['gt_boxes'].shape[0]
         ignore_class_name = set()
-        for i in range(num_instances):
-            empty_instance = get_empty_instance()
-            empty_instance['bbox_3d'] = ori_info_dict['gt_boxes'][
-                i, :].tolist()
-            if ori_info_dict['gt_names'][i] in METAINFO['classes']:
-                empty_instance['bbox_label'] = METAINFO['classes'].index(
-                    ori_info_dict['gt_names'][i])
-            else:
-                ignore_class_name.add(ori_info_dict['gt_names'][i])
-                empty_instance['bbox_label'] = -1
-            empty_instance['bbox_label_3d'] = copy.deepcopy(
-                empty_instance['bbox_label'])
-            empty_instance['velocity'] = ori_info_dict['gt_velocity'][
-                i, :].tolist()
-            empty_instance['num_lidar_pts'] = ori_info_dict['num_lidar_pts'][i]
-            empty_instance['num_radar_pts'] = ori_info_dict['num_radar_pts'][i]
-            empty_instance['bbox_3d_isvalid'] = ori_info_dict['valid_flag'][i]
-            empty_instance = clear_instance_unused_keys(empty_instance)
-            temp_data_info['instances'].append(empty_instance)
-        temp_data_info['cam_instances'] = generate_nuscenes_camera_instances(
-            ori_info_dict, nusc)
+        if 'gt_boxes' in ori_info_dict:
+            num_instances = ori_info_dict['gt_boxes'].shape[0]
+            for i in range(num_instances):
+                empty_instance = get_empty_instance()
+                empty_instance['bbox_3d'] = ori_info_dict['gt_boxes'][
+                    i, :].tolist()
+                if ori_info_dict['gt_names'][i] in METAINFO['classes']:
+                    empty_instance['bbox_label'] = METAINFO['classes'].index(
+                        ori_info_dict['gt_names'][i])
+                else:
+                    ignore_class_name.add(ori_info_dict['gt_names'][i])
+                    empty_instance['bbox_label'] = -1
+                empty_instance['bbox_label_3d'] = copy.deepcopy(
+                    empty_instance['bbox_label'])
+                empty_instance['velocity'] = ori_info_dict['gt_velocity'][
+                    i, :].tolist()
+                empty_instance['num_lidar_pts'] = ori_info_dict[
+                    'num_lidar_pts'][i]
+                empty_instance['num_radar_pts'] = ori_info_dict[
+                    'num_radar_pts'][i]
+                empty_instance['bbox_3d_isvalid'] = ori_info_dict[
+                    'valid_flag'][i]
+                empty_instance = clear_instance_unused_keys(empty_instance)
+                temp_data_info['instances'].append(empty_instance)
+            temp_data_info[
+                'cam_instances'] = generate_nuscenes_camera_instances(
+                    ori_info_dict, nusc)
+        if 'pts_semantic_mask_path' in ori_info_dict:
+            temp_data_info['pts_semantic_mask_path'] = Path(
+                ori_info_dict['pts_semantic_mask_path']).name
         temp_data_info, _ = clear_data_info_unused_keys(temp_data_info)
         converted_list.append(temp_data_info)
     pkl_name = Path(pkl_path).name
     out_path = osp.join(out_dir, pkl_name)
     print(f'Writing to output file: {out_path}.')
     print(f'ignore classes: {ignore_class_name}')
 
@@ -440,69 +448,71 @@
 
         # for potential usage
         temp_data_info['images']['R0_rect'] = ori_info_dict['calib'][
             'R0_rect'].astype(np.float32).tolist()
         temp_data_info['lidar_points']['Tr_imu_to_velo'] = ori_info_dict[
             'calib']['Tr_imu_to_velo'].astype(np.float32).tolist()
 
-        anns = ori_info_dict['annos']
-        num_instances = len(anns['name'])
         cam2img = ori_info_dict['calib']['P2']
 
+        anns = ori_info_dict.get('annos', None)
         ignore_class_name = set()
-        instance_list = []
-        for instance_id in range(num_instances):
-            empty_instance = get_empty_instance()
-            empty_instance['bbox'] = anns['bbox'][instance_id].tolist()
-
-            if anns['name'][instance_id] in METAINFO['classes']:
-                empty_instance['bbox_label'] = METAINFO['classes'].index(
-                    anns['name'][instance_id])
-            else:
-                ignore_class_name.add(anns['name'][instance_id])
-                empty_instance['bbox_label'] = -1
+        if anns is not None:
+            num_instances = len(anns['name'])
+            instance_list = []
+            for instance_id in range(num_instances):
+                empty_instance = get_empty_instance()
+                empty_instance['bbox'] = anns['bbox'][instance_id].tolist()
 
-            empty_instance['bbox'] = anns['bbox'][instance_id].tolist()
+                if anns['name'][instance_id] in METAINFO['classes']:
+                    empty_instance['bbox_label'] = METAINFO['classes'].index(
+                        anns['name'][instance_id])
+                else:
+                    ignore_class_name.add(anns['name'][instance_id])
+                    empty_instance['bbox_label'] = -1
 
-            loc = anns['location'][instance_id]
-            dims = anns['dimensions'][instance_id]
-            rots = anns['rotation_y'][:, None][instance_id]
-
-            dst = np.array([0.5, 0.5, 0.5])
-            src = np.array([0.5, 1.0, 0.5])
-
-            center_3d = loc + dims * (dst - src)
-            center_2d = points_cam2img(
-                center_3d.reshape([1, 3]), cam2img, with_depth=True)
-            center_2d = center_2d.squeeze().tolist()
-            empty_instance['center_2d'] = center_2d[:2]
-            empty_instance['depth'] = center_2d[2]
-
-            gt_bboxes_3d = np.concatenate([loc, dims, rots]).tolist()
-            empty_instance['bbox_3d'] = gt_bboxes_3d
-            empty_instance['bbox_label_3d'] = copy.deepcopy(
-                empty_instance['bbox_label'])
-            empty_instance['bbox'] = anns['bbox'][instance_id].tolist()
-            empty_instance['truncated'] = anns['truncated'][
-                instance_id].tolist()
-            empty_instance['occluded'] = anns['occluded'][instance_id].tolist()
-            empty_instance['alpha'] = anns['alpha'][instance_id].tolist()
-            empty_instance['score'] = anns['score'][instance_id].tolist()
-            empty_instance['index'] = anns['index'][instance_id].tolist()
-            empty_instance['group_id'] = anns['group_ids'][instance_id].tolist(
-            )
-            empty_instance['difficulty'] = anns['difficulty'][
-                instance_id].tolist()
-            empty_instance['num_lidar_pts'] = anns['num_points_in_gt'][
-                instance_id].tolist()
-            empty_instance = clear_instance_unused_keys(empty_instance)
-            instance_list.append(empty_instance)
-        temp_data_info['instances'] = instance_list
-        cam_instances = generate_kitti_camera_instances(ori_info_dict)
-        temp_data_info['cam_instances'] = cam_instances
+                empty_instance['bbox'] = anns['bbox'][instance_id].tolist()
+
+                loc = anns['location'][instance_id]
+                dims = anns['dimensions'][instance_id]
+                rots = anns['rotation_y'][:, None][instance_id]
+
+                dst = np.array([0.5, 0.5, 0.5])
+                src = np.array([0.5, 1.0, 0.5])
+
+                center_3d = loc + dims * (dst - src)
+                center_2d = points_cam2img(
+                    center_3d.reshape([1, 3]), cam2img, with_depth=True)
+                center_2d = center_2d.squeeze().tolist()
+                empty_instance['center_2d'] = center_2d[:2]
+                empty_instance['depth'] = center_2d[2]
+
+                gt_bboxes_3d = np.concatenate([loc, dims, rots]).tolist()
+                empty_instance['bbox_3d'] = gt_bboxes_3d
+                empty_instance['bbox_label_3d'] = copy.deepcopy(
+                    empty_instance['bbox_label'])
+                empty_instance['bbox'] = anns['bbox'][instance_id].tolist()
+                empty_instance['truncated'] = anns['truncated'][
+                    instance_id].tolist()
+                empty_instance['occluded'] = anns['occluded'][
+                    instance_id].tolist()
+                empty_instance['alpha'] = anns['alpha'][instance_id].tolist()
+                empty_instance['score'] = anns['score'][instance_id].tolist()
+                empty_instance['index'] = anns['index'][instance_id].tolist()
+                empty_instance['group_id'] = anns['group_ids'][
+                    instance_id].tolist()
+                empty_instance['difficulty'] = anns['difficulty'][
+                    instance_id].tolist()
+                empty_instance['num_lidar_pts'] = anns['num_points_in_gt'][
+                    instance_id].tolist()
+                empty_instance = clear_instance_unused_keys(empty_instance)
+                instance_list.append(empty_instance)
+            temp_data_info['instances'] = instance_list
+            cam_instances = generate_kitti_camera_instances(ori_info_dict)
+            temp_data_info['cam_instances'] = cam_instances
         temp_data_info, _ = clear_data_info_unused_keys(temp_data_info)
         converted_list.append(temp_data_info)
     pkl_name = Path(pkl_path).name
     out_path = osp.join(out_dir, pkl_name)
     print(f'Writing to output file: {out_path}.')
     print(f'ignore classes: {ignore_class_name}')
 
@@ -533,18 +543,20 @@
     for i, ori_info_dict in enumerate(mmengine.track_iter_progress(data_list)):
         temp_data_info = get_empty_standard_data_info()
         temp_data_info['sample_idx'] = i
         temp_data_info['lidar_points']['num_pts_feats'] = ori_info_dict[
             'point_cloud']['num_features']
         temp_data_info['lidar_points']['lidar_path'] = Path(
             ori_info_dict['pts_path']).name
-        temp_data_info['pts_semantic_mask_path'] = Path(
-            ori_info_dict['pts_semantic_mask_path']).name
-        temp_data_info['pts_instance_mask_path'] = Path(
-            ori_info_dict['pts_instance_mask_path']).name
+        if 'pts_semantic_mask_path' in ori_info_dict:
+            temp_data_info['pts_semantic_mask_path'] = Path(
+                ori_info_dict['pts_semantic_mask_path']).name
+        if 'pts_instance_mask_path' in ori_info_dict:
+            temp_data_info['pts_instance_mask_path'] = Path(
+                ori_info_dict['pts_instance_mask_path']).name
 
         # TODO support camera
         # np.linalg.inv(info['axis_align_matrix'] @ extrinsic): depth2cam
         anns = ori_info_dict.get('annos', None)
         ignore_class_name = set()
         if anns is not None:
             if anns['gt_num'] == 0:
@@ -607,45 +619,48 @@
     converted_list = []
     for ori_info_dict in mmengine.track_iter_progress(data_list):
         temp_data_info = get_empty_standard_data_info()
         temp_data_info['lidar_points']['num_pts_feats'] = ori_info_dict[
             'point_cloud']['num_features']
         temp_data_info['lidar_points']['lidar_path'] = Path(
             ori_info_dict['pts_path']).name
-        temp_data_info['pts_semantic_mask_path'] = Path(
-            ori_info_dict['pts_semantic_mask_path']).name
-        temp_data_info['pts_instance_mask_path'] = Path(
-            ori_info_dict['pts_instance_mask_path']).name
+        if 'pts_semantic_mask_path' in ori_info_dict:
+            temp_data_info['pts_semantic_mask_path'] = Path(
+                ori_info_dict['pts_semantic_mask_path']).name
+        if 'pts_instance_mask_path' in ori_info_dict:
+            temp_data_info['pts_instance_mask_path'] = Path(
+                ori_info_dict['pts_instance_mask_path']).name
 
         # TODO support camera
         # np.linalg.inv(info['axis_align_matrix'] @ extrinsic): depth2cam
-        anns = ori_info_dict['annos']
-        temp_data_info['axis_align_matrix'] = anns['axis_align_matrix'].tolist(
-        )
-        if anns['gt_num'] == 0:
-            instance_list = []
-        else:
-            num_instances = len(anns['name'])
-            ignore_class_name = set()
-            instance_list = []
-            for instance_id in range(num_instances):
-                empty_instance = get_empty_instance()
-                empty_instance['bbox_3d'] = anns['gt_boxes_upright_depth'][
-                    instance_id].tolist()
+        anns = ori_info_dict.get('annos', None)
+        ignore_class_name = set()
+        if anns is not None:
+            temp_data_info['axis_align_matrix'] = anns[
+                'axis_align_matrix'].tolist()
+            if anns['gt_num'] == 0:
+                instance_list = []
+            else:
+                num_instances = len(anns['name'])
+                instance_list = []
+                for instance_id in range(num_instances):
+                    empty_instance = get_empty_instance()
+                    empty_instance['bbox_3d'] = anns['gt_boxes_upright_depth'][
+                        instance_id].tolist()
 
-                if anns['name'][instance_id] in METAINFO['classes']:
-                    empty_instance['bbox_label_3d'] = METAINFO[
-                        'classes'].index(anns['name'][instance_id])
-                else:
-                    ignore_class_name.add(anns['name'][instance_id])
-                    empty_instance['bbox_label_3d'] = -1
+                    if anns['name'][instance_id] in METAINFO['classes']:
+                        empty_instance['bbox_label_3d'] = METAINFO[
+                            'classes'].index(anns['name'][instance_id])
+                    else:
+                        ignore_class_name.add(anns['name'][instance_id])
+                        empty_instance['bbox_label_3d'] = -1
 
-                empty_instance = clear_instance_unused_keys(empty_instance)
-                instance_list.append(empty_instance)
-        temp_data_info['instances'] = instance_list
+                    empty_instance = clear_instance_unused_keys(empty_instance)
+                    instance_list.append(empty_instance)
+            temp_data_info['instances'] = instance_list
         temp_data_info, _ = clear_data_info_unused_keys(temp_data_info)
         converted_list.append(temp_data_info)
     pkl_name = Path(pkl_path).name
     out_path = osp.join(out_dir, pkl_name)
     print(f'Writing to output file: {out_path}.')
     print(f'ignore classes: {ignore_class_name}')
 
@@ -692,38 +707,39 @@
         temp_data_info['images']['CAM0']['depth2img'] = depth2img.tolist()
         temp_data_info['images']['CAM0']['img_path'] = Path(
             ori_info_dict['image']['image_path']).name
         h, w = ori_info_dict['image']['image_shape']
         temp_data_info['images']['CAM0']['height'] = h
         temp_data_info['images']['CAM0']['width'] = w
 
-        anns = ori_info_dict['annos']
-        if anns['gt_num'] == 0:
-            instance_list = []
-        else:
-            num_instances = len(anns['name'])
-            ignore_class_name = set()
-            instance_list = []
-            for instance_id in range(num_instances):
-                empty_instance = get_empty_instance()
-                empty_instance['bbox_3d'] = anns['gt_boxes_upright_depth'][
-                    instance_id].tolist()
-                empty_instance['bbox'] = anns['bbox'][instance_id].tolist()
-                if anns['name'][instance_id] in METAINFO['classes']:
-                    empty_instance['bbox_label_3d'] = METAINFO[
-                        'classes'].index(anns['name'][instance_id])
-                    empty_instance['bbox_label'] = empty_instance[
-                        'bbox_label_3d']
-                else:
-                    ignore_class_name.add(anns['name'][instance_id])
-                    empty_instance['bbox_label_3d'] = -1
-                    empty_instance['bbox_label'] = -1
-                empty_instance = clear_instance_unused_keys(empty_instance)
-                instance_list.append(empty_instance)
-        temp_data_info['instances'] = instance_list
+        anns = ori_info_dict.get('annos', None)
+        if anns is not None:
+            if anns['gt_num'] == 0:
+                instance_list = []
+            else:
+                num_instances = len(anns['name'])
+                ignore_class_name = set()
+                instance_list = []
+                for instance_id in range(num_instances):
+                    empty_instance = get_empty_instance()
+                    empty_instance['bbox_3d'] = anns['gt_boxes_upright_depth'][
+                        instance_id].tolist()
+                    empty_instance['bbox'] = anns['bbox'][instance_id].tolist()
+                    if anns['name'][instance_id] in METAINFO['classes']:
+                        empty_instance['bbox_label_3d'] = METAINFO[
+                            'classes'].index(anns['name'][instance_id])
+                        empty_instance['bbox_label'] = empty_instance[
+                            'bbox_label_3d']
+                    else:
+                        ignore_class_name.add(anns['name'][instance_id])
+                        empty_instance['bbox_label_3d'] = -1
+                        empty_instance['bbox_label'] = -1
+                    empty_instance = clear_instance_unused_keys(empty_instance)
+                    instance_list.append(empty_instance)
+            temp_data_info['instances'] = instance_list
         temp_data_info, _ = clear_data_info_unused_keys(temp_data_info)
         converted_list.append(temp_data_info)
     pkl_name = Path(pkl_path).name
     out_path = osp.join(out_dir, pkl_name)
     print(f'Writing to output file: {out_path}.')
     print(f'ignore classes: {ignore_class_name}')
 
@@ -814,30 +830,31 @@
             rot = ori_info_dict['cams'][cam]['sensor2lidar_rotation']
             trans = ori_info_dict['cams'][cam]['sensor2lidar_translation']
             lidar2sensor[:3, :3] = rot.T
             lidar2sensor[:3, 3:4] = -1 * np.matmul(rot.T, trans.reshape(3, 1))
             empty_img_info['lidar2cam'] = lidar2sensor.astype(
                 np.float32).tolist()
             temp_data_info['images'][cam] = empty_img_info
-        num_instances = ori_info_dict['gt_boxes'].shape[0]
         ignore_class_name = set()
-        for i in range(num_instances):
-            empty_instance = get_empty_instance()
-            empty_instance['bbox_3d'] = ori_info_dict['gt_boxes'][
-                i, :].tolist()
-            if ori_info_dict['gt_names'][i] in METAINFO['classes']:
-                empty_instance['bbox_label'] = METAINFO['classes'].index(
-                    ori_info_dict['gt_names'][i])
-            else:
-                ignore_class_name.add(ori_info_dict['gt_names'][i])
-                empty_instance['bbox_label'] = -1
-            empty_instance['bbox_label_3d'] = copy.deepcopy(
-                empty_instance['bbox_label'])
-            empty_instance = clear_instance_unused_keys(empty_instance)
-            temp_data_info['instances'].append(empty_instance)
+        if 'gt_boxes' in ori_info_dict:
+            num_instances = ori_info_dict['gt_boxes'].shape[0]
+            for i in range(num_instances):
+                empty_instance = get_empty_instance()
+                empty_instance['bbox_3d'] = ori_info_dict['gt_boxes'][
+                    i, :].tolist()
+                if ori_info_dict['gt_names'][i] in METAINFO['classes']:
+                    empty_instance['bbox_label'] = METAINFO['classes'].index(
+                        ori_info_dict['gt_names'][i])
+                else:
+                    ignore_class_name.add(ori_info_dict['gt_names'][i])
+                    empty_instance['bbox_label'] = -1
+                empty_instance['bbox_label_3d'] = copy.deepcopy(
+                    empty_instance['bbox_label'])
+                empty_instance = clear_instance_unused_keys(empty_instance)
+                temp_data_info['instances'].append(empty_instance)
         temp_data_info, _ = clear_data_info_unused_keys(temp_data_info)
         converted_list.append(temp_data_info)
     pkl_name = Path(pkl_path).name
     out_path = osp.join(out_dir, pkl_name)
     print(f'Writing to output file: {out_path}.')
     print(f'ignore classes: {ignore_class_name}')
 
@@ -855,18 +872,18 @@
 
 
 def update_waymo_infos(pkl_path, out_dir):
     # the input pkl is based on the
     # pkl generated in the waymo cam only challenage.
     camera_types = [
         'CAM_FRONT',
-        'CAM_FRONT_RIGHT',
         'CAM_FRONT_LEFT',
-        'CAM_SIDE_RIGHT',
+        'CAM_FRONT_RIGHT',
         'CAM_SIDE_LEFT',
+        'CAM_SIDE_RIGHT',
     ]
     print(f'{pkl_path} will be modified.')
     if out_dir in pkl_path:
         print(f'Warning, you may overwriting '
               f'the original data {pkl_path}.')
         time.sleep(5)
     # TODO update to full label
@@ -915,16 +932,17 @@
         # for potential usage
         temp_data_info['images'][camera_types[0]]['height'] = h
         temp_data_info['images'][camera_types[0]]['width'] = w
         temp_data_info['lidar_points']['num_pts_feats'] = ori_info_dict[
             'point_cloud']['num_features']
         temp_data_info['lidar_points']['timestamp'] = ori_info_dict[
             'timestamp']
-        temp_data_info['lidar_points']['lidar_path'] = Path(
-            ori_info_dict['point_cloud']['velodyne_path']).name
+        velo_path = ori_info_dict['point_cloud'].get('velodyne_path')
+        if velo_path is not None:
+            temp_data_info['lidar_points']['lidar_path'] = Path(velo_path).name
 
         # TODO discuss the usage of Tr_velo_to_cam in lidar
         Trv2c = ori_info_dict['calib']['Tr_velo_to_cam'].astype(np.float32)
 
         temp_data_info['lidar_points']['Tr_velo_to_cam'] = Trv2c.tolist()
 
         # for potential usage
@@ -949,101 +967,105 @@
             img_path = Path(ori_sweep['image_path']).name
             for cam_idx, cam_key in enumerate(camera_types):
                 image_sweep['images'][cam_key]['img_path'] = img_path
 
             temp_data_info['lidar_sweeps'].append(lidar_sweep)
             temp_data_info['image_sweeps'].append(image_sweep)
 
-        anns = ori_info_dict['annos']
-        num_instances = len(anns['name'])
-
+        anns = ori_info_dict.get('annos', None)
         ignore_class_name = set()
-        instance_list = []
-        for instance_id in range(num_instances):
-            empty_instance = get_empty_instance()
-            empty_instance['bbox'] = anns['bbox'][instance_id].tolist()
-
-            if anns['name'][instance_id] in METAINFO['classes']:
-                empty_instance['bbox_label'] = METAINFO['classes'].index(
-                    anns['name'][instance_id])
-            else:
-                ignore_class_name.add(anns['name'][instance_id])
-                empty_instance['bbox_label'] = -1
+        if anns is not None:
+            num_instances = len(anns['name'])
 
-            empty_instance['bbox'] = anns['bbox'][instance_id].tolist()
+            instance_list = []
+            for instance_id in range(num_instances):
+                empty_instance = get_empty_instance()
+                empty_instance['bbox'] = anns['bbox'][instance_id].tolist()
 
-            loc = anns['location'][instance_id]
-            dims = anns['dimensions'][instance_id]
-            rots = anns['rotation_y'][:, None][instance_id]
-            gt_bboxes_3d = np.concatenate([loc, dims,
-                                           rots]).astype(np.float32).tolist()
-            empty_instance['bbox_3d'] = gt_bboxes_3d
-            empty_instance['bbox_label_3d'] = copy.deepcopy(
-                empty_instance['bbox_label'])
-            empty_instance['bbox'] = anns['bbox'][instance_id].tolist()
-            empty_instance['truncated'] = int(
-                anns['truncated'][instance_id].tolist())
-            empty_instance['occluded'] = anns['occluded'][instance_id].tolist()
-            empty_instance['alpha'] = anns['alpha'][instance_id].tolist()
-            empty_instance['index'] = anns['index'][instance_id].tolist()
-            empty_instance['group_id'] = anns['group_ids'][instance_id].tolist(
-            )
-            empty_instance['difficulty'] = anns['difficulty'][
-                instance_id].tolist()
-            empty_instance['num_lidar_pts'] = anns['num_points_in_gt'][
-                instance_id].tolist()
-            empty_instance['camera_id'] = anns['camera_id'][
-                instance_id].tolist()
-            empty_instance = clear_instance_unused_keys(empty_instance)
-            instance_list.append(empty_instance)
-        temp_data_info['instances'] = instance_list
+                if anns['name'][instance_id] in METAINFO['classes']:
+                    empty_instance['bbox_label'] = METAINFO['classes'].index(
+                        anns['name'][instance_id])
+                else:
+                    ignore_class_name.add(anns['name'][instance_id])
+                    empty_instance['bbox_label'] = -1
+
+                empty_instance['bbox'] = anns['bbox'][instance_id].tolist()
+
+                loc = anns['location'][instance_id]
+                dims = anns['dimensions'][instance_id]
+                rots = anns['rotation_y'][:, None][instance_id]
+                gt_bboxes_3d = np.concatenate([loc, dims, rots
+                                               ]).astype(np.float32).tolist()
+                empty_instance['bbox_3d'] = gt_bboxes_3d
+                empty_instance['bbox_label_3d'] = copy.deepcopy(
+                    empty_instance['bbox_label'])
+                empty_instance['bbox'] = anns['bbox'][instance_id].tolist()
+                empty_instance['truncated'] = int(
+                    anns['truncated'][instance_id].tolist())
+                empty_instance['occluded'] = anns['occluded'][
+                    instance_id].tolist()
+                empty_instance['alpha'] = anns['alpha'][instance_id].tolist()
+                empty_instance['index'] = anns['index'][instance_id].tolist()
+                empty_instance['group_id'] = anns['group_ids'][
+                    instance_id].tolist()
+                empty_instance['difficulty'] = anns['difficulty'][
+                    instance_id].tolist()
+                empty_instance['num_lidar_pts'] = anns['num_points_in_gt'][
+                    instance_id].tolist()
+                empty_instance['camera_id'] = anns['camera_id'][
+                    instance_id].tolist()
+                empty_instance = clear_instance_unused_keys(empty_instance)
+                instance_list.append(empty_instance)
+            temp_data_info['instances'] = instance_list
 
         # waymo provide the labels that sync with cam
-        anns = ori_info_dict['cam_sync_annos']
-        num_instances = len(anns['name'])
+        anns = ori_info_dict.get('cam_sync_annos', None)
         ignore_class_name = set()
-        instance_list = []
-        for instance_id in range(num_instances):
-            empty_instance = get_empty_instance()
-            empty_instance['bbox'] = anns['bbox'][instance_id].tolist()
-
-            if anns['name'][instance_id] in METAINFO['classes']:
-                empty_instance['bbox_label'] = METAINFO['classes'].index(
-                    anns['name'][instance_id])
-            else:
-                ignore_class_name.add(anns['name'][instance_id])
-                empty_instance['bbox_label'] = -1
+        if anns is not None:
+            num_instances = len(anns['name'])
+            instance_list = []
+            for instance_id in range(num_instances):
+                empty_instance = get_empty_instance()
+                empty_instance['bbox'] = anns['bbox'][instance_id].tolist()
+
+                if anns['name'][instance_id] in METAINFO['classes']:
+                    empty_instance['bbox_label'] = METAINFO['classes'].index(
+                        anns['name'][instance_id])
+                else:
+                    ignore_class_name.add(anns['name'][instance_id])
+                    empty_instance['bbox_label'] = -1
 
-            empty_instance['bbox'] = anns['bbox'][instance_id].tolist()
+                empty_instance['bbox'] = anns['bbox'][instance_id].tolist()
+
+                loc = anns['location'][instance_id]
+                dims = anns['dimensions'][instance_id]
+                rots = anns['rotation_y'][:, None][instance_id]
+                gt_bboxes_3d = np.concatenate([loc, dims, rots
+                                               ]).astype(np.float32).tolist()
+                empty_instance['bbox_3d'] = gt_bboxes_3d
+                empty_instance['bbox_label_3d'] = copy.deepcopy(
+                    empty_instance['bbox_label'])
+                empty_instance['bbox'] = anns['bbox'][instance_id].tolist()
+                empty_instance['truncated'] = int(
+                    anns['truncated'][instance_id].tolist())
+                empty_instance['occluded'] = anns['occluded'][
+                    instance_id].tolist()
+                empty_instance['alpha'] = anns['alpha'][instance_id].tolist()
+                empty_instance['index'] = anns['index'][instance_id].tolist()
+                empty_instance['group_id'] = anns['group_ids'][
+                    instance_id].tolist()
+                empty_instance['camera_id'] = anns['camera_id'][
+                    instance_id].tolist()
+                empty_instance = clear_instance_unused_keys(empty_instance)
+                instance_list.append(empty_instance)
+            temp_data_info['cam_sync_instances'] = instance_list
 
-            loc = anns['location'][instance_id]
-            dims = anns['dimensions'][instance_id]
-            rots = anns['rotation_y'][:, None][instance_id]
-            gt_bboxes_3d = np.concatenate([loc, dims,
-                                           rots]).astype(np.float32).tolist()
-            empty_instance['bbox_3d'] = gt_bboxes_3d
-            empty_instance['bbox_label_3d'] = copy.deepcopy(
-                empty_instance['bbox_label'])
-            empty_instance['bbox'] = anns['bbox'][instance_id].tolist()
-            empty_instance['truncated'] = int(
-                anns['truncated'][instance_id].tolist())
-            empty_instance['occluded'] = anns['occluded'][instance_id].tolist()
-            empty_instance['alpha'] = anns['alpha'][instance_id].tolist()
-            empty_instance['index'] = anns['index'][instance_id].tolist()
-            empty_instance['group_id'] = anns['group_ids'][instance_id].tolist(
-            )
-            empty_instance['camera_id'] = anns['camera_id'][
-                instance_id].tolist()
-            empty_instance = clear_instance_unused_keys(empty_instance)
-            instance_list.append(empty_instance)
-        temp_data_info['cam_sync_instances'] = instance_list
-
-        cam_instances = generate_waymo_camera_instances(
-            ori_info_dict, camera_types)
-        temp_data_info['cam_instances'] = cam_instances
+            cam_instances = generate_waymo_camera_instances(
+                ori_info_dict, camera_types)
+            temp_data_info['cam_instances'] = cam_instances
 
         temp_data_info, _ = clear_data_info_unused_keys(temp_data_info)
         converted_list.append(temp_data_info)
     pkl_name = Path(pkl_path).name
     out_path = osp.join(out_dir, pkl_name)
     print(f'Writing to output file: {out_path}.')
     print(f'ignore classes: {ignore_class_name}')
@@ -1051,15 +1073,15 @@
     # dataset metainfo
     metainfo = dict()
     metainfo['categories'] = {k: i for i, k in enumerate(METAINFO['classes'])}
     if ignore_class_name:
         for ignore_class in ignore_class_name:
             metainfo['categories'][ignore_class] = -1
     metainfo['dataset'] = 'waymo'
-    metainfo['version'] = '1.2'
+    metainfo['version'] = '1.4'
     metainfo['info_version'] = '1.1'
 
     converted_data_info = dict(metainfo=metainfo, data_list=converted_list)
 
     mmengine.dump(converted_data_info, out_path, 'pkl')
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/dataset_converters/waymo_converter.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/dataset_converters/waymo_converter.py`

 * *Files 4% similar despite different names*

```diff
@@ -2,19 +2,20 @@
 r"""Adapted from `Waymo to KITTI converter
     <https://github.com/caizhongang/waymo_kitti_converter>`_.
 """
 
 try:
     from waymo_open_dataset import dataset_pb2
 except ImportError:
-    raise ImportError('Please run "pip install waymo-open-dataset-tf-2-5-0" '
+    raise ImportError('Please run "pip install waymo-open-dataset-tf-2-6-0" '
                       '>1.4.5 to install the official devkit first.')
 
+import os
 from glob import glob
-from os.path import join
+from os.path import exists, join
 
 import mmengine
 import numpy as np
 import tensorflow as tf
 from waymo_open_dataset.utils import range_image_utils, transform_utils
 from waymo_open_dataset.utils.frame_utils import \
     parse_range_image_and_camera_projection
@@ -128,17 +129,15 @@
             if (self.selected_waymo_locations is not None
                     and frame.context.stats.location
                     not in self.selected_waymo_locations):
                 continue
 
             self.save_image(frame, file_idx, frame_idx)
             self.save_calib(frame, file_idx, frame_idx)
-            if 'testing_3d_camera_only_detection' not in self.load_dir:
-                # the camera only split doesn't contain lidar points.
-                self.save_lidar(frame, file_idx, frame_idx)
+            self.save_lidar(frame, file_idx, frame_idx)
             self.save_pose(frame, file_idx, frame_idx)
             self.save_timestamp(frame, file_idx, frame_idx)
 
             if not self.test_mode:
                 # TODO save the depth image for waymo challenge solution.
                 self.save_label(frame, file_idx, frame_idx)
                 if self.save_cam_sync_labels:
@@ -226,14 +225,17 @@
             frame (:obj:`Frame`): Open dataset frame proto.
             file_idx (int): Current file index.
             frame_idx (int): Current frame index.
         """
         range_images, camera_projections, seg_labels, range_image_top_pose = \
             parse_range_image_and_camera_projection(frame)
 
+        if range_image_top_pose is None:
+            # the camera only split doesn't contain lidar points.
+            return
         # First return
         points_0, cp_points_0, intensity_0, elongation_0, mask_indices_0 = \
             self.convert_range_image_to_point_cloud(
                 frame,
                 range_images,
                 camera_projections,
                 range_image_top_pose,
@@ -597,7 +599,34 @@
         if mat.shape == (3, 3):
             ret[:3, :3] = mat
         elif mat.shape == (3, 4):
             ret[:3, :] = mat
         else:
             raise ValueError(mat.shape)
         return ret
+
+
+def create_ImageSets_img_ids(root_dir, splits):
+    save_dir = join(root_dir, 'ImageSets/')
+    if not exists(save_dir):
+        os.mkdir(save_dir)
+
+    idx_all = [[] for i in splits]
+    for i, split in enumerate(splits):
+        path = join(root_dir, splits[i], 'calib')
+        if not exists(path):
+            RawNames = []
+        else:
+            RawNames = os.listdir(path)
+
+        for name in RawNames:
+            if name.endswith('.txt'):
+                idx = name.replace('.txt', '\n')
+                idx_all[int(idx[0])].append(idx)
+        idx_all[i].sort()
+
+    open(save_dir + 'train.txt', 'w').writelines(idx_all[0])
+    open(save_dir + 'val.txt', 'w').writelines(idx_all[1])
+    open(save_dir + 'trainval.txt', 'w').writelines(idx_all[0] + idx_all[1])
+    open(save_dir + 'test.txt', 'w').writelines(idx_all[2])
+    # open(save_dir+'test_cam_only.txt','w').writelines(idx_all[3])
+    print('created txt files indicating what to collect in ', splits)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/deployment/mmdet3d2torchserve.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/deployment/mmdet3d2torchserve.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/deployment/mmdet3d_handler.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/deployment/mmdet3d_handler.py`

 * *Files 2% similar despite different names*

```diff
@@ -101,18 +101,18 @@
             List: The post process function returns a list of the predicted
                 output.
         """
         output = []
         for pts_index, result in enumerate(data):
             output.append([])
             if 'pts_bbox' in result.keys():
-                pred_bboxes = result['pts_bbox']['boxes_3d'].tensor.numpy()
+                pred_bboxes = result['pts_bbox']['boxes_3d'].numpy()
                 pred_scores = result['pts_bbox']['scores_3d'].numpy()
             else:
-                pred_bboxes = result['boxes_3d'].tensor.numpy()
+                pred_bboxes = result['boxes_3d'].numpy()
                 pred_scores = result['scores_3d'].numpy()
 
             index = pred_scores > self.threshold
             bbox_coords = pred_bboxes[index].tolist()
             score = pred_scores[index].tolist()
 
             output[pts_index].append({'3dbbox': bbox_coords, 'score': score})
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/deployment/test_torchserver.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/deployment/test_torchserver.py`

 * *Files 5% similar despite different names*

```diff
@@ -33,18 +33,18 @@
 def main(args):
     # build the model from a config file and a checkpoint file
     model = init_model(args.config, args.checkpoint, device=args.device)
     # test a single point cloud file
     model_result, _ = inference_detector(model, args.pcd)
     # filter the 3d bboxes whose scores > 0.5
     if 'pts_bbox' in model_result[0].keys():
-        pred_bboxes = model_result[0]['pts_bbox']['boxes_3d'].tensor.numpy()
+        pred_bboxes = model_result[0]['pts_bbox']['boxes_3d'].numpy()
         pred_scores = model_result[0]['pts_bbox']['scores_3d'].numpy()
     else:
-        pred_bboxes = model_result[0]['boxes_3d'].tensor.numpy()
+        pred_bboxes = model_result[0]['boxes_3d'].numpy()
         pred_scores = model_result[0]['scores_3d'].numpy()
     model_result = pred_bboxes[pred_scores > 0.5]
 
     url = 'http://' + args.inference_addr + '/predictions/' + args.model_name
     with open(args.pcd, 'rb') as points:
         response = requests.post(url, points)
     server_result = parse_result(response.json())
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/misc/browse_dataset.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/misc/browse_dataset.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,16 +1,17 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 from os import path as osp
 
 from mmengine.config import Config, DictAction
+from mmengine.registry import init_default_scope
 from mmengine.utils import ProgressBar, mkdir_or_exist
 
 from mmdet3d.registry import DATASETS, VISUALIZERS
-from mmdet3d.utils import register_all_modules, replace_ceph_backend
+from mmdet3d.utils import replace_ceph_backend
 
 
 def parse_args():
     parser = argparse.ArgumentParser(description='Browse a dataset')
     parser.add_argument('config', help='train config file path')
     parser.add_argument(
         '--output-dir',
@@ -62,14 +63,17 @@
     # `cfg.train_dataloader.dataset` so we don't
     # need to worry about it later
     if cfg.train_dataloader.dataset['type'] == 'RepeatDataset':
         cfg.train_dataloader.dataset = cfg.train_dataloader.dataset.dataset
     # use only first dataset for `ConcatDataset`
     if cfg.train_dataloader.dataset['type'] == 'ConcatDataset':
         cfg.train_dataloader.dataset = cfg.train_dataloader.dataset.datasets[0]
+    if cfg.train_dataloader.dataset['type'] == 'CBGSDataset':
+        cfg.train_dataloader.dataset = cfg.train_dataloader.dataset.dataset
+
     train_data_cfg = cfg.train_dataloader.dataset
 
     if aug:
         show_pipeline = cfg.train_pipeline
     else:
         show_pipeline = cfg.test_pipeline
         for i in range(len(cfg.train_pipeline)):
@@ -95,16 +99,15 @@
 
     cfg = build_data_cfg(args.config, args.aug, args.cfg_options)
 
     # TODO: We will unify the ceph support approach with other OpenMMLab repos
     if args.ceph:
         cfg = replace_ceph_backend(cfg)
 
-    # register all modules in mmdet3d into the registries
-    register_all_modules()
+    init_default_scope(cfg.get('default_scope', 'mmdet3d'))
 
     try:
         dataset = DATASETS.build(
             cfg.train_dataloader.dataset,
             default_args=dict(filter_empty_gt=False))
     except TypeError:  # seg dataset doesn't have `filter_empty_gt` key
         dataset = DATASETS.build(cfg.train_dataloader.dataset)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/misc/fuse_conv_bn.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/misc/fuse_conv_bn.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/misc/print_config.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/misc/print_config.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/misc/visualize_results.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/misc/visualize_results.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/model_converters/convert_h3dnet_checkpoints.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/model_converters/convert_h3dnet_checkpoints.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/model_converters/convert_votenet_checkpoints.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/model_converters/convert_votenet_checkpoints.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/model_converters/publish_model.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/model_converters/publish_model.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/model_converters/regnet2mmdet.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/model_converters/regnet2mmdet.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/slurm_test.sh` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/slurm_test.sh`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/slurm_train.sh` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/slurm_train.sh`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/test.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/test.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import os
 import os.path as osp
 
-from mmengine.config import Config, DictAction
+from mmengine.config import Config, ConfigDict, DictAction
 from mmengine.registry import RUNNERS
 from mmengine.runner import Runner
 
-from mmdet3d.utils import register_all_modules, replace_ceph_backend
+from mmdet3d.utils import replace_ceph_backend
 
 
 # TODO: support fuse_conv_bn and format_only
 def parse_args():
     parser = argparse.ArgumentParser(
         description='MMDet3D test (and eval) a model')
     parser.add_argument('config', help='test config file path')
@@ -25,14 +25,16 @@
         '--show', action='store_true', help='show prediction results')
     parser.add_argument(
         '--show-dir',
         help='directory where painted images will be saved. '
         'If specified, it will be automatically saved '
         'to the work_dir/timestamp/show_dir')
     parser.add_argument(
+        '--score-thr', type=float, default=0.1, help='bbox score threshold')
+    parser.add_argument(
         '--task',
         type=str,
         choices=[
             'mono_det', 'multi-view_det', 'lidar_det', 'lidar_seg',
             'multi-modality_det'
         ],
         help='Determine the visualization method depending on the task.')
@@ -49,15 +51,20 @@
         'Note that the quotation marks are necessary and that no white space '
         'is allowed.')
     parser.add_argument(
         '--launcher',
         choices=['none', 'pytorch', 'slurm', 'mpi'],
         default='none',
         help='job launcher')
-    parser.add_argument('--local_rank', type=int, default=0)
+    parser.add_argument(
+        '--tta', action='store_true', help='Test time augmentation')
+    # When using PyTorch version >= 2.0.0, the `torch.distributed.launch`
+    # will pass the `--local-rank` parameter to `tools/test.py` instead
+    # of `--local_rank`.
+    parser.add_argument('--local_rank', '--local-rank', type=int, default=0)
     args = parser.parse_args()
     if 'LOCAL_RANK' not in os.environ:
         os.environ['LOCAL_RANK'] = str(args.local_rank)
     return args
 
 
 def trigger_visualization_hook(cfg, args):
@@ -67,31 +74,35 @@
         # Turn on visualization
         visualization_hook['draw'] = True
         if args.show:
             visualization_hook['show'] = True
             visualization_hook['wait_time'] = args.wait_time
         if args.show_dir:
             visualization_hook['test_out_dir'] = args.show_dir
+        all_task_choices = [
+            'mono_det', 'multi-view_det', 'lidar_det', 'lidar_seg',
+            'multi-modality_det'
+        ]
+        assert args.task in all_task_choices, 'You must set '\
+            f"'--task' in {all_task_choices} in the command " \
+            'if you want to use visualization hook'
         visualization_hook['vis_task'] = args.task
+        visualization_hook['score_thr'] = args.score_thr
     else:
         raise RuntimeError(
             'VisualizationHook must be included in default_hooks.'
             'refer to usage '
             '"visualization=dict(type=\'VisualizationHook\')"')
 
     return cfg
 
 
 def main():
     args = parse_args()
 
-    # register all modules in mmdet3d into the registries
-    # do not init the default scope here because it will be init in the runner
-    register_all_modules(init_default_scope=False)
-
     # load config
     cfg = Config.fromfile(args.config)
 
     # TODO: We will unify the ceph support approach with other OpenMMLab repos
     if args.ceph:
         cfg = replace_ceph_backend(cfg)
 
@@ -109,14 +120,22 @@
                                 osp.splitext(osp.basename(args.config))[0])
 
     cfg.load_from = args.checkpoint
 
     if args.show or args.show_dir:
         cfg = trigger_visualization_hook(cfg, args)
 
+    if args.tta:
+        # Currently, we only support tta for 3D segmentation
+        # TODO: Support tta for 3D detection
+        assert 'tta_model' in cfg, 'Cannot find ``tta_model`` in config.'
+        assert 'tta_pipeline' in cfg, 'Cannot find ``tta_pipeline`` in config.'
+        cfg.test_dataloader.dataset.pipeline = cfg.tta_pipeline
+        cfg.model = ConfigDict(**cfg.tta_model, module=cfg.model)
+
     # build the runner from config
     if 'runner_type' not in cfg:
         # build the default runner
         runner = Runner.from_cfg(cfg)
     else:
         # build customized runner from the registry
         # if 'runner_type' is set in the cfg
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/train.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/train.py`

 * *Files 14% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 import os.path as osp
 
 from mmengine.config import Config, DictAction
 from mmengine.logging import print_log
 from mmengine.registry import RUNNERS
 from mmengine.runner import Runner
 
-from mmdet3d.utils import register_all_modules, replace_ceph_backend
+from mmdet3d.utils import replace_ceph_backend
 
 
 def parse_args():
     parser = argparse.ArgumentParser(description='Train a 3D detector')
     parser.add_argument('config', help='train config file path')
     parser.add_argument('--work-dir', help='the dir to save logs and models')
     parser.add_argument(
@@ -23,16 +23,20 @@
         help='enable automatic-mixed-precision training')
     parser.add_argument(
         '--auto-scale-lr',
         action='store_true',
         help='enable automatically scaling LR.')
     parser.add_argument(
         '--resume',
-        action='store_true',
-        help='resume from the latest checkpoint in the work_dir automatically')
+        nargs='?',
+        type=str,
+        const='auto',
+        help='If specify checkpoint path, resume from it, while if not '
+        'specify, try to auto resume from the latest checkpoint '
+        'in the work directory.')
     parser.add_argument(
         '--ceph', action='store_true', help='Use ceph as data storage backend')
     parser.add_argument(
         '--cfg-options',
         nargs='+',
         action=DictAction,
         help='override some settings in the used config, the key-value pair '
@@ -42,26 +46,26 @@
         'Note that the quotation marks are necessary and that no white space '
         'is allowed.')
     parser.add_argument(
         '--launcher',
         choices=['none', 'pytorch', 'slurm', 'mpi'],
         default='none',
         help='job launcher')
-    parser.add_argument('--local_rank', type=int, default=0)
+    # When using PyTorch version >= 2.0.0, the `torch.distributed.launch`
+    # will pass the `--local-rank` parameter to `tools/train.py` instead
+    # of `--local_rank`.
+    parser.add_argument('--local_rank', '--local-rank', type=int, default=0)
     args = parser.parse_args()
     if 'LOCAL_RANK' not in os.environ:
         os.environ['LOCAL_RANK'] = str(args.local_rank)
     return args
 
 
 def main():
     args = parse_args()
-    # register all modules in mmdet3d into the registries
-    # do not init the default scope here because it will be init in the runner
-    register_all_modules(init_default_scope=False)
 
     # load config
     cfg = Config.fromfile(args.config)
 
     # TODO: We will unify the ceph support approach with other OpenMMLab repos
     if args.ceph:
         cfg = replace_ceph_backend(cfg)
@@ -101,15 +105,22 @@
                 'base_batch_size' in cfg.auto_scale_lr:
             cfg.auto_scale_lr.enable = True
         else:
             raise RuntimeError('Can not find "auto_scale_lr" or '
                                '"auto_scale_lr.enable" or '
                                '"auto_scale_lr.base_batch_size" in your'
                                ' configuration file.')
-    cfg.resume = args.resume
+
+    # resume is determined in this priority: resume from > auto_resume
+    if args.resume == 'auto':
+        cfg.resume = True
+        cfg.load_from = None
+    elif args.resume is not None:
+        cfg.resume = True
+        cfg.load_from = args.resume
 
     # build the runner from config
     if 'runner_type' not in cfg:
         # build the default runner
         runner = Runner.from_cfg(cfg)
     else:
         # build customized runner from the registry
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/update_data_coords.py` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/update_data_coords.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/.mim/tools/update_data_coords.sh` & `mmdet3d-1.1.1/mmdet3d/.mim/tools/update_data_coords.sh`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/__init__.py` & `mmdet3d-1.1.1/mmdet3d/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -2,23 +2,23 @@
 import mmcv
 import mmdet
 import mmengine
 from mmengine.utils import digit_version
 
 from .version import __version__, version_info
 
-mmcv_minimum_version = '2.0.0rc0'
+mmcv_minimum_version = '2.0.0rc4'
 mmcv_maximum_version = '2.1.0'
 mmcv_version = digit_version(mmcv.__version__)
 
-mmengine_minimum_version = '0.1.0'
+mmengine_minimum_version = '0.7.1'
 mmengine_maximum_version = '1.0.0'
 mmengine_version = digit_version(mmengine.__version__)
 
-mmdet_minimum_version = '3.0.0rc0'
+mmdet_minimum_version = '3.0.0'
 mmdet_maximum_version = '3.1.0'
 mmdet_version = digit_version(mmdet.__version__)
 
 assert (mmcv_version >= digit_version(mmcv_minimum_version)
         and mmcv_version < digit_version(mmcv_maximum_version)), \
     f'MMCV=={mmcv.__version__} is used but incompatible. ' \
     f'Please install mmcv>={mmcv_minimum_version}, <{mmcv_maximum_version}.'
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/apis/inference.py` & `mmdet3d-1.1.1/mmdet3d/apis/inference.py`

 * *Files 16% similar despite different names*

```diff
@@ -7,17 +7,18 @@
 
 import mmengine
 import numpy as np
 import torch
 import torch.nn as nn
 from mmengine.config import Config
 from mmengine.dataset import Compose, pseudo_collate
+from mmengine.registry import init_default_scope
 from mmengine.runner import load_checkpoint
 
-from mmdet3d.registry import MODELS
+from mmdet3d.registry import DATASETS, MODELS
 from mmdet3d.structures import Box3DMode, Det3DDataSample, get_box_type
 from mmdet3d.structures.det3d_data_sample import SampleList
 
 
 def convert_SyncBN(config):
     """Convert config's naiveSyncBN to BN.
 
@@ -33,14 +34,15 @@
             else:
                 convert_SyncBN(config[item])
 
 
 def init_model(config: Union[str, Path, Config],
                checkpoint: Optional[str] = None,
                device: str = 'cuda:0',
+               palette: str = 'none',
                cfg_options: Optional[dict] = None):
     """Initialize a model from config file, which could be a 3D detector or a
     3D segmentor.
 
     Args:
         config (str, :obj:`Path`, or :obj:`mmengine.Config`): Config file path,
             :obj:`Path`, or the config object.
@@ -59,35 +61,50 @@
         raise TypeError('config must be a filename or Config object, '
                         f'but got {type(config)}')
     if cfg_options is not None:
         config.merge_from_dict(cfg_options)
 
     convert_SyncBN(config.model)
     config.model.train_cfg = None
+    init_default_scope(config.get('default_scope', 'mmdet3d'))
     model = MODELS.build(config.model)
 
     if checkpoint is not None:
         checkpoint = load_checkpoint(model, checkpoint, map_location='cpu')
         # save the dataset_meta in the model for convenience
         if 'dataset_meta' in checkpoint.get('meta', {}):
             # mmdet3d 1.x
             model.dataset_meta = checkpoint['meta']['dataset_meta']
         elif 'CLASSES' in checkpoint.get('meta', {}):
             # < mmdet3d 1.x
             classes = checkpoint['meta']['CLASSES']
-            model.dataset_meta = {'CLASSES': classes}
+            model.dataset_meta = {'classes': classes}
 
             if 'PALETTE' in checkpoint.get('meta', {}):  # 3D Segmentor
-                model.dataset_meta['PALETTE'] = checkpoint['meta']['PALETTE']
+                model.dataset_meta['palette'] = checkpoint['meta']['PALETTE']
         else:
             # < mmdet3d 1.x
-            model.dataset_meta = {'CLASSES': config.class_names}
+            model.dataset_meta = {'classes': config.class_names}
 
             if 'PALETTE' in checkpoint.get('meta', {}):  # 3D Segmentor
-                model.dataset_meta['PALETTE'] = checkpoint['meta']['PALETTE']
+                model.dataset_meta['palette'] = checkpoint['meta']['PALETTE']
+
+        test_dataset_cfg = deepcopy(config.test_dataloader.dataset)
+        # lazy init. We only need the metainfo.
+        test_dataset_cfg['lazy_init'] = True
+        metainfo = DATASETS.build(test_dataset_cfg).metainfo
+        cfg_palette = metainfo.get('palette', None)
+        if cfg_palette is not None:
+            model.dataset_meta['palette'] = cfg_palette
+        else:
+            if 'palette' not in model.dataset_meta:
+                warnings.warn(
+                    'palette does not exist, random is used by default. '
+                    'You can also set the palette to customize.')
+                model.dataset_meta['palette'] = 'random'
 
     model.cfg = config  # save the config in the model for convenience
     if device != 'cpu':
         torch.cuda.set_device(device)
     else:
         warnings.warn('Don\'t suggest using CPU device. '
                       'Some functions are not supported for now.')
@@ -170,36 +187,36 @@
         return results, data
 
 
 def inference_multi_modality_detector(model: nn.Module,
                                       pcds: Union[str, Sequence[str]],
                                       imgs: Union[str, Sequence[str]],
                                       ann_file: Union[str, Sequence[str]],
-                                      cam_type: str = 'CAM_FRONT'):
-    """Inference point cloud with the multi-modality detector.
+                                      cam_type: str = 'CAM2'):
+    """Inference point cloud with the multi-modality detector. Now we only
+    support multi-modality detector for KITTI and SUNRGBD datasets since the
+    multi-view image loading is not supported yet in this inference function.
 
     Args:
         model (nn.Module): The loaded detector.
         pcds (str, Sequence[str]):
             Either point cloud files or loaded point cloud.
         imgs (str, Sequence[str]):
            Either image files or loaded images.
         ann_file (str, Sequence[str]): Annotation files.
-        cam_type (str): Image of Camera chose to infer.
-            For kitti dataset, it should be 'CAM_2',
-            and for nuscenes dataset, it should be
-            'CAM_FRONT'. Defaults to 'CAM_FRONT'.
+        cam_type (str): Image of Camera chose to infer. When detector only uses
+            single-view image, we need to specify a camera view. For kitti
+            dataset, it should be 'CAM2'. For sunrgbd, it should be 'CAM0'.
+            When detector uses multi-view images, we should set it to 'all'.
 
     Returns:
         :obj:`Det3DDataSample` or list[:obj:`Det3DDataSample`]:
         If pcds is a list or tuple, the same length list type results
         will be returned, otherwise return the detection results directly.
     """
-
-    # TODO: We will support
     if isinstance(pcds, (list, tuple)):
         is_batch = True
         assert isinstance(imgs, (list, tuple))
         assert len(pcds) == len(imgs)
     else:
         pcds = [pcds]
         imgs = [imgs]
@@ -210,42 +227,63 @@
     # build the data pipeline
     test_pipeline = deepcopy(cfg.test_dataloader.dataset.pipeline)
     test_pipeline = Compose(test_pipeline)
     box_type_3d, box_mode_3d = \
         get_box_type(cfg.test_dataloader.dataset.box_type_3d)
 
     data_list = mmengine.load(ann_file)['data_list']
-    assert len(imgs) == len(data_list)
 
     data = []
     for index, pcd in enumerate(pcds):
         # get data info containing calib
-        img = imgs[index]
         data_info = data_list[index]
-        img_path = data_info['images'][cam_type]['img_path']
-
-        if osp.basename(img_path) != osp.basename(img):
-            raise ValueError(f'the info file of {img_path} is not provided.')
+        img = imgs[index]
 
-        # TODO: check the name consistency of
-        # image file and point cloud file
-        data_ = dict(
-            lidar_points=dict(lidar_path=pcd),
-            img_path=img,
-            box_type_3d=box_type_3d,
-            box_mode_3d=box_mode_3d)
+        if cam_type != 'all':
+            assert osp.isfile(img), f'{img} must be a file.'
+            img_path = data_info['images'][cam_type]['img_path']
+            if osp.basename(img_path) != osp.basename(img):
+                raise ValueError(
+                    f'the info file of {img_path} is not provided.')
+            data_ = dict(
+                lidar_points=dict(lidar_path=pcd),
+                img_path=img,
+                box_type_3d=box_type_3d,
+                box_mode_3d=box_mode_3d)
+            data_info['images'][cam_type]['img_path'] = img
+            if 'cam2img' in data_info['images'][cam_type]:
+                # The data annotation in SRUNRGBD dataset does not contain
+                # `cam2img`
+                data_['cam2img'] = np.array(
+                    data_info['images'][cam_type]['cam2img'])
+
+            # LiDAR to image conversion for KITTI dataset
+            if box_mode_3d == Box3DMode.LIDAR:
+                if 'lidar2img' in data_info['images'][cam_type]:
+                    data_['lidar2img'] = np.array(
+                        data_info['images'][cam_type]['lidar2img'])
+            # Depth to image conversion for SUNRGBD dataset
+            elif box_mode_3d == Box3DMode.DEPTH:
+                data_['depth2img'] = np.array(
+                    data_info['images'][cam_type]['depth2img'])
+        else:
+            assert osp.isdir(img), f'{img} must be a file directory'
+            for _, img_info in data_info['images'].items():
+                img_info['img_path'] = osp.join(img, img_info['img_path'])
+                assert osp.isfile(img_info['img_path']
+                                  ), f'{img_info["img_path"]} does not exist.'
+            data_ = dict(
+                lidar_points=dict(lidar_path=pcd),
+                images=data_info['images'],
+                box_type_3d=box_type_3d,
+                box_mode_3d=box_mode_3d)
 
-        # LiDAR to image conversion for KITTI dataset
-        if box_mode_3d == Box3DMode.LIDAR:
-            data_['lidar2img'] = np.array(
-                data_info['images'][cam_type]['lidar2img'])
-        # Depth to image conversion for SUNRGBD dataset
-        elif box_mode_3d == Box3DMode.DEPTH:
-            data_['depth2img'] = np.array(
-                data_info['images'][cam_type]['depth2img'])
+        if 'timestamp' in data_info:
+            # Using multi-sweeps need `timestamp`
+            data_['timestamp'] = data_info['timestamp']
 
         data_ = test_pipeline(data_)
         data.append(data_)
 
     collate_data = pseudo_collate(data)
 
     # forward the model
@@ -289,29 +327,31 @@
 
     # build the data pipeline
     test_pipeline = deepcopy(cfg.test_dataloader.dataset.pipeline)
     test_pipeline = Compose(test_pipeline)
     box_type_3d, box_mode_3d = \
         get_box_type(cfg.test_dataloader.dataset.box_type_3d)
 
-    data_list = mmengine.load(ann_file)
+    data_list = mmengine.load(ann_file)['data_list']
     assert len(imgs) == len(data_list)
 
     data = []
     for index, img in enumerate(imgs):
         # get data info containing calib
         data_info = data_list[index]
         img_path = data_info['images'][cam_type]['img_path']
         if osp.basename(img_path) != osp.basename(img):
             raise ValueError(f'the info file of {img_path} is not provided.')
 
         # replace the img_path in data_info with img
         data_info['images'][cam_type]['img_path'] = img
+        # avoid data_info['images'] has multiple keys anout camera views.
+        mono_img_info = {f'{cam_type}': data_info['images'][cam_type]}
         data_ = dict(
-            images=data_info['images'],
+            images=mono_img_info,
             box_type_3d=box_type_3d,
             box_mode_3d=box_mode_3d)
 
         data_ = test_pipeline(data_)
         data.append(data_)
 
     collate_data = pseudo_collate(data)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/datasets/__init__.py` & `mmdet3d-1.1.1/mmdet3d/datasets/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 from .lyft_dataset import LyftDataset
 from .nuscenes_dataset import NuScenesDataset
 # yapf: enable
 from .s3dis_dataset import S3DISDataset, S3DISSegDataset
 from .scannet_dataset import (ScanNetDataset, ScanNetInstanceSegDataset,
                               ScanNetSegDataset)
 from .seg3d_dataset import Seg3DDataset
-from .semantickitti_dataset import SemanticKITTIDataset
+from .semantickitti_dataset import SemanticKittiDataset
 from .sunrgbd_dataset import SUNRGBDDataset
 # yapf: disable
 from .transforms import (AffineResize, BackgroundPointsFilter, GlobalAlignment,
                          GlobalRotScaleTrans, IndoorPatchPointSample,
                          IndoorPointSample, LoadAnnotations3D,
                          LoadPointsFromDict, LoadPointsFromFile,
                          LoadPointsFromMultiSweeps, NormalizePointsColor,
@@ -29,13 +29,13 @@
     'KittiDataset', 'CBGSDataset', 'NuScenesDataset', 'LyftDataset',
     'ObjectSample', 'RandomFlip3D', 'ObjectNoise', 'GlobalRotScaleTrans',
     'PointShuffle', 'ObjectRangeFilter', 'PointsRangeFilter',
     'LoadPointsFromFile', 'S3DISSegDataset', 'S3DISDataset',
     'NormalizePointsColor', 'IndoorPatchPointSample', 'IndoorPointSample',
     'PointSample', 'LoadAnnotations3D', 'GlobalAlignment', 'SUNRGBDDataset',
     'ScanNetDataset', 'ScanNetSegDataset', 'ScanNetInstanceSegDataset',
-    'SemanticKITTIDataset', 'Det3DDataset', 'Seg3DDataset',
+    'SemanticKittiDataset', 'Det3DDataset', 'Seg3DDataset',
     'LoadPointsFromMultiSweeps', 'WaymoDataset', 'BackgroundPointsFilter',
     'VoxelBasedPointSampler', 'get_loading_pipeline', 'RandomDropPointsColor',
     'RandomJitterPoints', 'ObjectNameFilter', 'AffineResize',
     'RandomShiftScale', 'LoadPointsFromDict', 'Resize3D', 'RandomResize3D',
 ]
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/datasets/convert_utils.py` & `mmdet3d-1.1.1/mmdet3d/datasets/convert_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,18 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import copy
+import warnings
 from typing import List, Optional, Tuple, Union
 
 import numpy as np
 from nuscenes import NuScenes
 from nuscenes.utils.geometry_utils import view_points
 from pyquaternion import Quaternion
 from shapely.geometry import MultiPoint, box
+from shapely.geometry.polygon import Polygon
 
 from mmdet3d.structures import Box3DMode, CameraInstance3DBoxes, points_cam2img
 from mmdet3d.structures.ops import box_np_ops
 
 kitti_categories = ('Pedestrian', 'Cyclist', 'Car', 'Van', 'Truck',
                     'Person_sitting', 'Tram', 'Misc')
 
@@ -326,15 +328,15 @@
     gt_bboxes_3d = np.concatenate([loc, dims, rots[..., np.newaxis]],
                                   axis=1).astype(np.float32)
     # convert gt_bboxes_3d to velodyne coordinates
     gt_bboxes_3d = CameraInstance3DBoxes(gt_bboxes_3d).convert_to(
         Box3DMode.LIDAR, np.linalg.inv(rect @ lidar2cam0), correct_yaw=True)
     # convert gt_bboxes_3d to cam coordinates
     gt_bboxes_3d = gt_bboxes_3d.convert_to(
-        Box3DMode.CAM, rect @ lidar2cami, correct_yaw=True).tensor.numpy()
+        Box3DMode.CAM, rect @ lidar2cami, correct_yaw=True).numpy()
     converted_annos['location'] = gt_bboxes_3d[:, :3]
     converted_annos['dimensions'] = gt_bboxes_3d[:, 3:6]
     converted_annos['rotation_y'] = gt_bboxes_3d[:, 6]
     return converted_annos
 
 
 def post_process_coords(
@@ -354,23 +356,25 @@
         corners and the image canvas.
     """
     polygon_from_2d_box = MultiPoint(corner_coords).convex_hull
     img_canvas = box(0, 0, imsize[0], imsize[1])
 
     if polygon_from_2d_box.intersects(img_canvas):
         img_intersection = polygon_from_2d_box.intersection(img_canvas)
-        intersection_coords = np.array(
-            [coord for coord in img_intersection.exterior.coords])
-
-        min_x = min(intersection_coords[:, 0])
-        min_y = min(intersection_coords[:, 1])
-        max_x = max(intersection_coords[:, 0])
-        max_y = max(intersection_coords[:, 1])
-
-        return min_x, min_y, max_x, max_y
+        if isinstance(img_intersection, Polygon):
+            intersection_coords = np.array(
+                [coord for coord in img_intersection.exterior.coords])
+            min_x = min(intersection_coords[:, 0])
+            min_y = min(intersection_coords[:, 1])
+            max_x = max(intersection_coords[:, 0])
+            max_y = max(intersection_coords[:, 1])
+            return min_x, min_y, max_x, max_y
+        else:
+            warnings.warn('img_intersection is not an object of Polygon.')
+            return None
     else:
         return None
 
 
 def generate_record(ann_rec: dict, x1: float, y1: float, x2: float, y2: float,
                     dataset: str) -> Union[dict, None]:
     """Generate one 2D annotation record given various information on top of
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/datasets/det3d_dataset.py` & `mmdet3d-1.1.1/mmdet3d/datasets/det3d_dataset.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import copy
 import os
 from os import path as osp
 from typing import Callable, List, Optional, Set, Union
 
-import mmengine
 import numpy as np
 import torch
 from mmengine.dataset import BaseDataset
 from mmengine.logging import print_log
 from terminaltables import AsciiTable
 
 from mmdet3d.registry import DATASETS
@@ -57,16 +56,16 @@
             data pipeline will be dropped and a random example will be chosen
             in `__getitem__`. Defaults to True.
         test_mode (bool): Whether the dataset is in test mode.
             Defaults to False.
         load_eval_anns (bool): Whether to load annotations in test_mode,
             the annotation will be save in `eval_ann_infos`, which can be
             used in Evaluator. Defaults to True.
-        file_client_args (dict): Configuration of file client.
-            Defaults to dict(backend='disk').
+        backend_args (dict, optional): Arguments to instantiate the
+            corresponding backend. Defaults to None.
         show_ins_var (bool): For debug purpose. Whether to show variation
             of the number of instances before and after through pipeline.
             Defaults to False.
     """
 
     def __init__(self,
                  data_root: Optional[str] = None,
@@ -76,19 +75,18 @@
                  pipeline: List[Union[dict, Callable]] = [],
                  modality: dict = dict(use_lidar=True, use_camera=False),
                  default_cam_key: str = None,
                  box_type_3d: dict = 'LiDAR',
                  filter_empty_gt: bool = True,
                  test_mode: bool = False,
                  load_eval_anns: bool = True,
-                 file_client_args: dict = dict(backend='disk'),
+                 backend_args: Optional[dict] = None,
                  show_ins_var: bool = False,
                  **kwargs) -> None:
-        # init file client
-        self.file_client = mmengine.FileClient(**file_client_args)
+        self.backend_args = backend_args
         self.filter_empty_gt = filter_empty_gt
         self.load_eval_anns = load_eval_anns
         _default_modality_keys = ('use_lidar', 'use_camera')
         if modality is None:
             modality = dict()
 
         # Defaults to False if not specify
@@ -137,28 +135,29 @@
             test_mode=test_mode,
             **kwargs)
 
         # can be accessed by other component in runner
         self.metainfo['box_type_3d'] = box_type_3d
         self.metainfo['label_mapping'] = self.label_mapping
 
-        # used for showing variation of the number of instances before and
-        # after through the pipeline
-        self.show_ins_var = show_ins_var
-
-        # show statistics of this dataset
-        print_log('-' * 30, 'current')
-        print_log(f'The length of the dataset: {len(self)}', 'current')
-        content_show = [['category', 'number']]
-        for cat_name, num in self.num_ins_per_cat.items():
-            content_show.append([cat_name, num])
-        table = AsciiTable(content_show)
-        print_log(
-            f'The number of instances per category in the dataset:\n{table.table}',  # noqa: E501
-            'current')
+        if not kwargs.get('lazy_init', False):
+            # used for showing variation of the number of instances before and
+            # after through the pipeline
+            self.show_ins_var = show_ins_var
+
+            # show statistics of this dataset
+            print_log('-' * 30, 'current')
+            print_log(f'The length of the dataset: {len(self)}', 'current')
+            content_show = [['category', 'number']]
+            for cat_name, num in self.num_ins_per_cat.items():
+                content_show.append([cat_name, num])
+            table = AsciiTable(content_show)
+            print_log(
+                f'The number of instances per category in the dataset:\n{table.table}',  # noqa: E501
+                'current')
 
     def _remove_dontcare(self, ann_info: dict) -> dict:
         """Remove annotations that do not need to be cared.
 
         -1 indicates dontcare in MMDet3d.
 
         Args:
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/datasets/kitti2d_dataset.py` & `mmdet3d-1.1.1/mmdet3d/datasets/kitti2d_dataset.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/datasets/kitti_dataset.py` & `mmdet3d-1.1.1/mmdet3d/datasets/kitti_dataset.py`

 * *Files 3% similar despite different names*

```diff
@@ -50,15 +50,17 @@
         pcd_limit_range (List[float]): The range of point cloud used to filter
             invalid predicted boxes.
             Defaults to [0, -40, -3, 70.4, 40, 0.0].
     """
     # TODO: use full classes of kitti
     METAINFO = {
         'classes': ('Pedestrian', 'Cyclist', 'Car', 'Van', 'Truck',
-                    'Person_sitting', 'Tram', 'Misc')
+                    'Person_sitting', 'Tram', 'Misc'),
+        'palette': [(106, 0, 228), (119, 11, 32), (165, 42, 42), (0, 0, 192),
+                    (197, 226, 255), (0, 60, 100), (0, 0, 142), (255, 77, 255)]
     }
 
     def __init__(self,
                  data_root: str,
                  ann_file: str,
                  pipeline: List[Union[dict, Callable]] = [],
                  modality: dict = dict(use_lidar=True),
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/datasets/lyft_dataset.py` & `mmdet3d-1.1.1/mmdet3d/datasets/lyft_dataset.py`

 * *Files 4% similar despite different names*

```diff
@@ -40,15 +40,18 @@
         test_mode (bool): Whether the dataset is in test mode.
             Defaults to False.
     """
 
     METAINFO = {
         'classes':
         ('car', 'truck', 'bus', 'emergency_vehicle', 'other_vehicle',
-         'motorcycle', 'bicycle', 'pedestrian', 'animal')
+         'motorcycle', 'bicycle', 'pedestrian', 'animal'),
+        'palette': [(106, 0, 228), (119, 11, 32), (165, 42, 42), (0, 0, 192),
+                    (197, 226, 255), (0, 60, 100), (0, 0, 142), (255, 77, 255),
+                    (153, 69, 1)]
     }
 
     def __init__(self,
                  data_root: str,
                  ann_file: str,
                  pipeline: List[Union[dict, Callable]] = [],
                  modality: dict = dict(use_camera=False, use_lidar=True),
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/datasets/nuscenes_dataset.py` & `mmdet3d-1.1.1/mmdet3d/datasets/nuscenes_dataset.py`

 * *Files 2% similar despite different names*

```diff
@@ -56,15 +56,27 @@
             Defaults to False.
     """
     METAINFO = {
         'classes':
         ('car', 'truck', 'trailer', 'bus', 'construction_vehicle', 'bicycle',
          'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'),
         'version':
-        'v1.0-trainval'
+        'v1.0-trainval',
+        'palette': [
+            (255, 158, 0),  # Orange
+            (255, 99, 71),  # Tomato
+            (255, 140, 0),  # Darkorange
+            (255, 127, 80),  # Coral
+            (233, 150, 70),  # Darksalmon
+            (220, 20, 60),  # Crimson
+            (255, 61, 99),  # Red
+            (0, 0, 230),  # Blue
+            (47, 79, 79),  # Darkslategrey
+            (112, 128, 144),  # Slategrey
+        ]
     }
 
     def __init__(self,
                  data_root: str,
                  ann_file: str,
                  pipeline: List[Union[dict, Callable]] = [],
                  box_type_3d: str = 'LiDAR',
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/datasets/s3dis_dataset.py` & `mmdet3d-1.1.1/mmdet3d/datasets/s3dis_dataset.py`

 * *Files 2% similar despite different names*

```diff
@@ -48,15 +48,18 @@
         test_mode (bool): Whether the dataset is in test mode.
             Defaults to False.
     """
     METAINFO = {
         'classes': ('table', 'chair', 'sofa', 'bookcase', 'board'),
         # the valid ids of segmentation annotations
         'seg_valid_class_ids': (7, 8, 9, 10, 11),
-        'seg_all_class_ids': tuple(range(1, 14))  # possibly with 'stair' class
+        'seg_all_class_ids':
+        tuple(range(1, 14)),  # possibly with 'stair' class
+        'palette': [(170, 120, 200), (255, 0, 0), (200, 100, 100),
+                    (10, 200, 100), (200, 200, 200)]
     }
 
     def __init__(self,
                  data_root: str,
                  ann_file: str,
                  metainfo: Optional[dict] = None,
                  data_prefix: dict = dict(
@@ -71,15 +74,15 @@
                  **kwargs) -> None:
 
         # construct seg_label_mapping for semantic mask
         seg_max_cat_id = len(self.METAINFO['seg_all_class_ids'])
         seg_valid_cat_ids = self.METAINFO['seg_valid_class_ids']
         neg_label = len(seg_valid_cat_ids)
         seg_label_mapping = np.ones(
-            seg_max_cat_id + 1, dtype=np.int) * neg_label
+            seg_max_cat_id + 1, dtype=np.int64) * neg_label
         for cls_idx, cat_id in enumerate(seg_valid_cat_ids):
             seg_label_mapping[cat_id] = cls_idx
         self.seg_label_mapping = seg_label_mapping
 
         super().__init__(
             data_root=data_root,
             ann_file=ann_file,
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/datasets/scannet_dataset.py` & `mmdet3d-1.1.1/mmdet3d/datasets/scannet_dataset.py`

 * *Files 4% similar despite different names*

```diff
@@ -53,15 +53,21 @@
         ('cabinet', 'bed', 'chair', 'sofa', 'table', 'door', 'window',
          'bookshelf', 'picture', 'counter', 'desk', 'curtain', 'refrigerator',
          'showercurtrain', 'toilet', 'sink', 'bathtub', 'garbagebin'),
         # the valid ids of segmentation annotations
         'seg_valid_class_ids':
         (3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 16, 24, 28, 33, 34, 36, 39),
         'seg_all_class_ids':
-        tuple(range(1, 41))
+        tuple(range(1, 41)),
+        'palette': [(31, 119, 180), (255, 187, 120), (188, 189, 34),
+                    (140, 86, 75), (255, 152, 150), (214, 39, 40),
+                    (197, 176, 213), (148, 103, 189), (196, 156, 148),
+                    (23, 190, 207), (247, 182, 210), (219, 219, 141),
+                    (255, 127, 14), (158, 218, 229), (44, 160, 44),
+                    (112, 128, 144), (227, 119, 194), (82, 84, 163)]
     }
 
     def __init__(self,
                  data_root: str,
                  ann_file: str,
                  metainfo: Optional[dict] = None,
                  data_prefix: dict = dict(
@@ -76,15 +82,15 @@
                  **kwargs) -> None:
 
         # construct seg_label_mapping for semantic mask
         seg_max_cat_id = len(self.METAINFO['seg_all_class_ids'])
         seg_valid_cat_ids = self.METAINFO['seg_valid_class_ids']
         neg_label = len(seg_valid_cat_ids)
         seg_label_mapping = np.ones(
-            seg_max_cat_id + 1, dtype=np.int) * neg_label
+            seg_max_cat_id + 1, dtype=np.int64) * neg_label
         for cls_idx, cat_id in enumerate(seg_valid_cat_ids):
             seg_label_mapping[cat_id] = cls_idx
         self.seg_label_mapping = seg_label_mapping
 
         super().__init__(
             data_root=data_root,
             ann_file=ann_file,
@@ -327,21 +333,21 @@
                      pts_instance_mask='',
                      pts_semantic_mask=''),
                  pipeline: List[Union[dict, Callable]] = [],
                  modality: dict = dict(use_lidar=True, use_camera=False),
                  test_mode: bool = False,
                  ignore_index: Optional[int] = None,
                  scene_idxs: Optional[Union[np.ndarray, str]] = None,
-                 file_client_args: dict = dict(backend='disk'),
+                 backend_args: Optional[dict] = None,
                  **kwargs) -> None:
         super().__init__(
             data_root=data_root,
             ann_file=ann_file,
             metainfo=metainfo,
             pipeline=pipeline,
             data_prefix=data_prefix,
             modality=modality,
             test_mode=test_mode,
             ignore_index=ignore_index,
             scene_idxs=scene_idxs,
-            file_client_args=file_client_args,
+            backend_args=backend_args,
             **kwargs)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/datasets/seg3d_dataset.py` & `mmdet3d-1.1.1/mmdet3d/datasets/seg3d_dataset.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from os import path as osp
 from typing import Callable, List, Optional, Sequence, Union
 
-import mmengine
 import numpy as np
 from mmengine.dataset import BaseDataset
+from mmengine.fileio import get_local_path
 
 from mmdet3d.registry import DATASETS
 
 
 @DATASETS.register_module()
 class Seg3DDataset(BaseDataset):
     """Base Class for 3D semantic segmentation dataset.
@@ -45,16 +45,16 @@
         serialize_data (bool): Whether to hold memory using serialized objects,
             when enabled, data loader workers can use shared RAM from master
             process instead of making a copy.
             Defaults to False for 3D Segmentation datasets.
         load_eval_anns (bool): Whether to load annotations in test_mode,
             the annotation will be save in `eval_ann_infos`, which can be used
             in Evaluator. Defaults to True.
-        file_client_args (dict): Configuration of file client.
-            Defaults to dict(backend='disk').
+        backend_args (dict, optional): Arguments to instantiate the
+            corresponding backend. Defaults to None.
     """
     METAINFO = {
         'classes': None,  # names of all classes data used for the task
         'palette': None,  # official color for visualization
         'seg_valid_class_ids': None,  # class_ids used for training
         'seg_all_class_ids': None,  # all possible class_ids in loaded seg mask
     }
@@ -71,18 +71,17 @@
                  pipeline: List[Union[dict, Callable]] = [],
                  modality: dict = dict(use_lidar=True, use_camera=False),
                  ignore_index: Optional[int] = None,
                  scene_idxs: Optional[Union[str, np.ndarray]] = None,
                  test_mode: bool = False,
                  serialize_data: bool = False,
                  load_eval_anns: bool = True,
-                 file_client_args: dict = dict(backend='disk'),
+                 backend_args: Optional[dict] = None,
                  **kwargs) -> None:
-        # init file client
-        self.file_client = mmengine.FileClient(**file_client_args)
+        self.backend_args = backend_args
         self.modality = modality
         self.load_eval_anns = load_eval_anns
 
         # TODO: We maintain the ignore_index attributes,
         # but we may consider to remove it in the future.
         self.ignore_index = len(self.METAINFO['classes']) if \
             ignore_index is None else ignore_index
@@ -103,40 +102,34 @@
         # defined in dataset config.
         palette = metainfo.get('palette', None)
         updated_palette = self._update_palette(new_classes, palette)
 
         metainfo['palette'] = updated_palette
 
         # construct seg_label_mapping for semantic mask
-        seg_max_cat_id = len(self.METAINFO['seg_all_class_ids'])
-        seg_valid_cat_ids = self.METAINFO['seg_valid_class_ids']
-        neg_label = len(seg_valid_cat_ids)
-        seg_label_mapping = np.ones(
-            seg_max_cat_id + 1, dtype=np.int) * neg_label
-        for cls_idx, cat_id in enumerate(seg_valid_cat_ids):
-            seg_label_mapping[cat_id] = cls_idx
-        self.seg_label_mapping = seg_label_mapping
+        self.seg_label_mapping = self.get_seg_label_mapping(metainfo)
 
         super().__init__(
             ann_file=ann_file,
             metainfo=metainfo,
             data_root=data_root,
             data_prefix=data_prefix,
             pipeline=pipeline,
             test_mode=test_mode,
             serialize_data=serialize_data,
             **kwargs)
 
         self.metainfo['seg_label_mapping'] = self.seg_label_mapping
-        self.scene_idxs = self.get_scene_idxs(scene_idxs)
-        self.data_list = [self.data_list[i] for i in self.scene_idxs]
-
-        # set group flag for the sampler
-        if not self.test_mode:
-            self._set_group_flag()
+        if not kwargs.get('lazy_init', False):
+            self.scene_idxs = self.get_scene_idxs(scene_idxs)
+            self.data_list = [self.data_list[i] for i in self.scene_idxs]
+
+            # set group flag for the sampler
+            if not self.test_mode:
+                self._set_group_flag()
 
     def get_label_mapping(self,
                           new_classes: Optional[Sequence] = None) -> tuple:
         """Get label mapping.
 
         The ``label_mapping`` is a dictionary, its keys are the old label ids
         and its values are the new label ids, and is used for changing pixel
@@ -188,14 +181,37 @@
                 i: cat_name
                 for i, cat_name in enumerate(self.METAINFO['classes'])
             }
             valid_class_ids = self.METAINFO['seg_valid_class_ids']
 
         return label_mapping, label2cat, valid_class_ids
 
+    def get_seg_label_mapping(self, metainfo=None):
+        """Get segmentation label mapping.
+
+        The ``seg_label_mapping`` is an array, its indices are the old label
+        ids and its values are the new label ids, and is specifically used
+        for changing point labels in PointSegClassMapping.
+
+        Args:
+            metainfo (dict, optional): Meta information to set
+            seg_label_mapping. Defaults to None.
+
+        Returns:
+            tuple: The mapping from old classes to new classes.
+        """
+        seg_max_cat_id = len(self.METAINFO['seg_all_class_ids'])
+        seg_valid_cat_ids = self.METAINFO['seg_valid_class_ids']
+        neg_label = len(seg_valid_cat_ids)
+        seg_label_mapping = np.ones(
+            seg_max_cat_id + 1, dtype=np.int64) * neg_label
+        for cls_idx, cat_id in enumerate(seg_valid_cat_ids):
+            seg_label_mapping[cat_id] = cls_idx
+        return seg_label_mapping
+
     def _update_palette(self, new_classes: list, palette: Union[None,
                                                                 list]) -> list:
         """Update palette according to metainfo.
 
         If length of palette is equal to classes, just return the palette.
         If palette is not defined, it will randomly generate a palette.
         If classes is updated by customer, it will return the subset of
@@ -236,14 +252,17 @@
             all path has been converted to absolute path.
         """
         if self.modality['use_lidar']:
             info['lidar_points']['lidar_path'] = \
                 osp.join(
                     self.data_prefix.get('pts', ''),
                     info['lidar_points']['lidar_path'])
+            if 'num_pts_feats' in info['lidar_points']:
+                info['num_pts_feats'] = info['lidar_points']['num_pts_feats']
+            info['lidar_path'] = info['lidar_points']['lidar_path']
 
         if self.modality['use_camera']:
             for cam_id, img_info in info['images'].items():
                 if 'img_path' in img_info:
                     img_info['img_path'] = osp.join(
                         self.data_prefix.get('img', ''), img_info['img_path'])
 
@@ -263,14 +282,32 @@
 
         # 'eval_ann_info' will be updated in loading transforms
         if self.test_mode and self.load_eval_anns:
             info['eval_ann_info'] = dict()
 
         return info
 
+    def prepare_data(self, idx: int) -> dict:
+        """Get data processed by ``self.pipeline``.
+
+        Args:
+            idx (int): The index of ``data_info``.
+
+        Returns:
+            dict: Results passed through ``self.pipeline``.
+        """
+        if not self.test_mode:
+            data_info = self.get_data_info(idx)
+            # Pass the dataset to the pipeline during training to support mixed
+            # data augmentation, such as polarmix and lasermix.
+            data_info['dataset'] = self
+            return self.pipeline(data_info)
+        else:
+            return super().prepare_data(idx)
+
     def get_scene_idxs(self, scene_idxs: Union[None, str,
                                                np.ndarray]) -> np.ndarray:
         """Compute scene_idxs for data sampling.
 
         We sample more times for scenes with more points.
         """
         if self.test_mode:
@@ -279,15 +316,16 @@
 
         # we may need to re-sample different scenes according to scene_idxs
         # this is necessary for indoor scene segmentation such as ScanNet
         if scene_idxs is None:
             scene_idxs = np.arange(len(self))
         if isinstance(scene_idxs, str):
             scene_idxs = osp.join(self.data_root, scene_idxs)
-            with self.file_client.get_local_path(scene_idxs) as local_path:
+            with get_local_path(
+                    scene_idxs, backend_args=self.backend_args) as local_path:
                 scene_idxs = np.load(local_path)
         else:
             scene_idxs = np.array(scene_idxs)
 
         return scene_idxs.astype(np.int32)
 
     def _set_group_flag(self) -> None:
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/datasets/semantickitti_dataset.py` & `mmdet3d-1.1.1/mmdet3d/datasets/semantickitti_dataset.py`

 * *Files 21% similar despite different names*

```diff
@@ -4,28 +4,28 @@
 import numpy as np
 
 from mmdet3d.registry import DATASETS
 from .seg3d_dataset import Seg3DDataset
 
 
 @DATASETS.register_module()
-class SemanticKITTIDataset(Seg3DDataset):
-    r"""SemanticKITTI Dataset.
+class SemanticKittiDataset(Seg3DDataset):
+    r"""SemanticKitti Dataset.
 
     This class serves as the API for experiments on the SemanticKITTI Dataset
     Please refer to <http://www.semantic-kitti.org/dataset.html>`_
     for data downloading
 
     Args:
         data_root (str, optional): Path of dataset root. Defaults to None.
         ann_file (str): Path of annotation file. Defaults to ''.
         metainfo (dict, optional): Meta information for dataset, such as class
             information. Defaults to None.
         data_prefix (dict): Prefix for training data. Defaults to
-            dict(pts='points',
+            dict(pts='',
                  img='',
                  pts_instance_mask='',
                  pts_semantic_mask='').
         pipeline (List[dict]): Pipeline used for data processing.
             Defaults to [].
         modality (dict): Modality to specify the sensor data used as input,
             it usually has following keys:
@@ -40,30 +40,36 @@
         scene_idxs (np.ndarray or str, optional): Precomputed index to load
             data. For scenes with many points, we may sample it several times.
             Defaults to None.
         test_mode (bool): Whether the dataset is in test mode.
             Defaults to False.
     """
     METAINFO = {
-        'classes': ('unlabeled', 'car', 'bicycle', 'motorcycle', 'truck',
-                    'bus', 'person', 'bicyclist', 'motorcyclist', 'road',
-                    'parking', 'sidewalk', 'other-ground', 'building', 'fence',
-                    'vegetation', 'trunck', 'terrian', 'pole', 'traffic-sign'),
+        'classes': ('car', 'bicycle', 'motorcycle', 'truck', 'bus', 'person',
+                    'bicyclist', 'motorcyclist', 'road', 'parking', 'sidewalk',
+                    'other-ground', 'building', 'fence', 'vegetation',
+                    'trunck', 'terrian', 'pole', 'traffic-sign'),
+        'palette': [[100, 150, 245], [100, 230, 245], [30, 60, 150],
+                    [80, 30, 180], [100, 80, 250], [155, 30, 30],
+                    [255, 40, 200], [150, 30, 90], [255, 0, 255],
+                    [255, 150, 255], [75, 0, 75], [175, 0, 75], [255, 200, 0],
+                    [255, 120, 50], [0, 175, 0], [135, 60, 0], [150, 240, 80],
+                    [255, 240, 150], [255, 0, 0]],
         'seg_valid_class_ids':
-        tuple(range(20)),
+        tuple(range(19)),
         'seg_all_class_ids':
-        tuple(range(20))
+        tuple(range(19)),
     }
 
     def __init__(self,
                  data_root: Optional[str] = None,
                  ann_file: str = '',
                  metainfo: Optional[dict] = None,
                  data_prefix: dict = dict(
-                     pts='points',
+                     pts='',
                      img='',
                      pts_instance_mask='',
                      pts_semantic_mask=''),
                  pipeline: List[Union[dict, Callable]] = [],
                  modality: dict = dict(use_lidar=True, use_camera=False),
                  ignore_index: Optional[int] = None,
                  scene_idxs: Optional[Union[str, np.ndarray]] = None,
@@ -77,7 +83,13 @@
             data_prefix=data_prefix,
             pipeline=pipeline,
             modality=modality,
             ignore_index=ignore_index,
             scene_idxs=scene_idxs,
             test_mode=test_mode,
             **kwargs)
+
+    def get_seg_label_mapping(self, metainfo):
+        seg_label_mapping = np.zeros(metainfo['max_label'] + 1, dtype=np.int64)
+        for idx in metainfo['seg_label_mapping']:
+            seg_label_mapping[idx] = metainfo['seg_label_mapping'][idx]
+        return seg_label_mapping
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/datasets/sunrgbd_dataset.py` & `mmdet3d-1.1.1/mmdet3d/datasets/sunrgbd_dataset.py`

 * *Files 3% similar despite different names*

```diff
@@ -43,15 +43,19 @@
         filter_empty_gt (bool): Whether to filter empty GT.
             Defaults to True.
         test_mode (bool): Whether the dataset is in test mode.
             Defaults to False.
     """
     METAINFO = {
         'classes': ('bed', 'table', 'sofa', 'chair', 'toilet', 'desk',
-                    'dresser', 'night_stand', 'bookshelf', 'bathtub')
+                    'dresser', 'night_stand', 'bookshelf', 'bathtub'),
+        'palette': [(255, 187, 120), (255, 152, 150), (140, 86, 75),
+                    (188, 189, 34), (44, 160, 44), (247, 182, 210),
+                    (196, 156, 148), (23, 190, 207), (148, 103, 189),
+                    (227, 119, 194)]
     }
 
     def __init__(self,
                  data_root: str,
                  ann_file: str,
                  metainfo: Optional[dict] = None,
                  data_prefix: dict = dict(
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/datasets/transforms/__init__.py` & `mmdet3d-1.1.1/mmdet3d/datasets/transforms/__init__.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,32 +1,36 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from .dbsampler import DataBaseSampler
 from .formating import Pack3DDetInputs
-from .loading import (LoadAnnotations3D, LoadImageFromFileMono3D,
-                      LoadMultiViewImageFromFiles, LoadPointsFromDict,
-                      LoadPointsFromFile, LoadPointsFromMultiSweeps,
-                      NormalizePointsColor, PointSegClassMapping)
+from .loading import (LidarDet3DInferencerLoader, LoadAnnotations3D,
+                      LoadImageFromFileMono3D, LoadMultiViewImageFromFiles,
+                      LoadPointsFromDict, LoadPointsFromFile,
+                      LoadPointsFromMultiSweeps, MonoDet3DInferencerLoader,
+                      MultiModalityDet3DInferencerLoader, NormalizePointsColor,
+                      PointSegClassMapping)
 from .test_time_aug import MultiScaleFlipAug3D
 # yapf: disable
 from .transforms_3d import (AffineResize, BackgroundPointsFilter,
                             GlobalAlignment, GlobalRotScaleTrans,
                             IndoorPatchPointSample, IndoorPointSample,
-                            MultiViewWrapper, ObjectNameFilter, ObjectNoise,
-                            ObjectRangeFilter, ObjectSample,
+                            LaserMix, MultiViewWrapper, ObjectNameFilter,
+                            ObjectNoise, ObjectRangeFilter, ObjectSample,
                             PhotoMetricDistortion3D, PointSample, PointShuffle,
-                            PointsRangeFilter, RandomDropPointsColor,
+                            PointsRangeFilter, PolarMix, RandomDropPointsColor,
                             RandomFlip3D, RandomJitterPoints, RandomResize3D,
                             RandomShiftScale, Resize3D, VoxelBasedPointSampler)
 
 __all__ = [
     'ObjectSample', 'RandomFlip3D', 'ObjectNoise', 'GlobalRotScaleTrans',
     'PointShuffle', 'ObjectRangeFilter', 'PointsRangeFilter',
     'Pack3DDetInputs', 'LoadMultiViewImageFromFiles', 'LoadPointsFromFile',
     'DataBaseSampler', 'NormalizePointsColor', 'LoadAnnotations3D',
     'IndoorPointSample', 'PointSample', 'PointSegClassMapping',
     'MultiScaleFlipAug3D', 'LoadPointsFromMultiSweeps',
     'BackgroundPointsFilter', 'VoxelBasedPointSampler', 'GlobalAlignment',
     'IndoorPatchPointSample', 'LoadImageFromFileMono3D', 'ObjectNameFilter',
     'RandomDropPointsColor', 'RandomJitterPoints', 'AffineResize',
     'RandomShiftScale', 'LoadPointsFromDict', 'Resize3D', 'RandomResize3D',
-    'MultiViewWrapper', 'PhotoMetricDistortion3D'
+    'MultiViewWrapper', 'PhotoMetricDistortion3D', 'MonoDet3DInferencerLoader',
+    'LidarDet3DInferencerLoader', 'PolarMix', 'LaserMix',
+    'MultiModalityDet3DInferencerLoader'
 ]
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/datasets/transforms/data_augment_utils.py` & `mmdet3d-1.1.1/mmdet3d/datasets/transforms/data_augment_utils.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/datasets/transforms/dbsampler.py` & `mmdet3d-1.1.1/mmdet3d/datasets/transforms/dbsampler.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import copy
 import os
 from typing import List, Optional
 
 import mmengine
 import numpy as np
+from mmengine.fileio import get_local_path
 
 from mmdet3d.datasets.transforms import data_augment_utils
 from mmdet3d.registry import TRANSFORMS
 from mmdet3d.structures.ops import box_np_ops
 
 
 class BatchSampler:
@@ -87,47 +88,46 @@
         data_root (str): Path of groundtruth database.
         rate (float): Rate of actual sampled over maximum sampled number.
         prepare (dict): Name of preparation functions and the input value.
         sample_groups (dict): Sampled classes and numbers.
         classes (list[str], optional): List of classes. Defaults to None.
         points_loader (dict): Config of points loader. Defaults to
             dict(type='LoadPointsFromFile', load_dim=4, use_dim=[0, 1, 2, 3]).
-        file_client_args (dict): Arguments to instantiate a FileClient.
-            See :class:`mmengine.fileio.FileClient` for details.
-            Defaults to dict(backend='disk').
+        backend_args (dict, optional): Arguments to instantiate the
+            corresponding backend. Defaults to None.
     """
 
-    def __init__(
-        self,
-        info_path: str,
-        data_root: str,
-        rate: float,
-        prepare: dict,
-        sample_groups: dict,
-        classes: Optional[List[str]] = None,
-        points_loader: dict = dict(
-            type='LoadPointsFromFile',
-            coord_type='LIDAR',
-            load_dim=4,
-            use_dim=[0, 1, 2, 3]),
-        file_client_args: dict = dict(backend='disk')
-    ) -> None:
+    def __init__(self,
+                 info_path: str,
+                 data_root: str,
+                 rate: float,
+                 prepare: dict,
+                 sample_groups: dict,
+                 classes: Optional[List[str]] = None,
+                 points_loader: dict = dict(
+                     type='LoadPointsFromFile',
+                     coord_type='LIDAR',
+                     load_dim=4,
+                     use_dim=[0, 1, 2, 3],
+                     backend_args=None),
+                 backend_args: Optional[dict] = None) -> None:
         super().__init__()
         self.data_root = data_root
         self.info_path = info_path
         self.rate = rate
         self.prepare = prepare
         self.classes = classes
         self.cat2label = {name: i for i, name in enumerate(classes)}
         self.label2cat = {i: name for i, name in enumerate(classes)}
         self.points_loader = TRANSFORMS.build(points_loader)
-        self.file_client = mmengine.FileClient(**file_client_args)
+        self.backend_args = backend_args
 
         # load data base infos
-        with self.file_client.get_local_path(info_path) as local_path:
+        with get_local_path(
+                info_path, backend_args=self.backend_args) as local_path:
             # loading data from a file-like object needs file format
             db_infos = mmengine.load(open(local_path, 'rb'), file_format='pkl')
 
         # filter database infos
         from mmengine.logging import MMLogger
         logger: MMLogger = MMLogger.get_current_instance()
         for k, v in db_infos.items():
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/datasets/transforms/formating.py` & `mmdet3d-1.1.1/mmdet3d/datasets/transforms/formating.py`

 * *Files 12% similar despite different names*

```diff
@@ -143,23 +143,35 @@
         if 'points' in results:
             if isinstance(results['points'], BasePoints):
                 results['points'] = results['points'].tensor
 
         if 'img' in results:
             if isinstance(results['img'], list):
                 # process multiple imgs in single frame
-                imgs = [img.transpose(2, 0, 1) for img in results['img']]
-                imgs = np.ascontiguousarray(np.stack(imgs, axis=0))
-                results['img'] = to_tensor(imgs)
+                imgs = np.stack(results['img'], axis=0)
+                if imgs.flags.c_contiguous:
+                    imgs = to_tensor(imgs).permute(0, 3, 1, 2).contiguous()
+                else:
+                    imgs = to_tensor(
+                        np.ascontiguousarray(imgs.transpose(0, 3, 1, 2)))
+                results['img'] = imgs
             else:
                 img = results['img']
                 if len(img.shape) < 3:
                     img = np.expand_dims(img, -1)
-                results['img'] = to_tensor(
-                    np.ascontiguousarray(img.transpose(2, 0, 1)))
+                # To improve the computational speed by by 3-5 times, apply:
+                # `torch.permute()` rather than `np.transpose()`.
+                # Refer to https://github.com/open-mmlab/mmdetection/pull/9533
+                # for more details
+                if img.flags.c_contiguous:
+                    img = to_tensor(img).permute(2, 0, 1).contiguous()
+                else:
+                    img = to_tensor(
+                        np.ascontiguousarray(img.transpose(2, 0, 1)))
+                results['img'] = img
 
         for key in [
                 'proposals', 'gt_bboxes', 'gt_bboxes_ignore', 'gt_labels',
                 'gt_bboxes_labels', 'attr_labels', 'pts_instance_mask',
                 'pts_semantic_mask', 'centers_2d', 'depths', 'gt_labels_3d'
         ]:
             if key not in results:
@@ -179,19 +191,37 @@
             results['gt_seg_map'] = results['gt_seg_map'][None, ...]
 
         data_sample = Det3DDataSample()
         gt_instances_3d = InstanceData()
         gt_instances = InstanceData()
         gt_pts_seg = PointData()
 
-        img_metas = {}
+        data_metas = {}
         for key in self.meta_keys:
             if key in results:
-                img_metas[key] = results[key]
-        data_sample.set_metainfo(img_metas)
+                data_metas[key] = results[key]
+            elif 'images' in results:
+                if len(results['images'].keys()) == 1:
+                    cam_type = list(results['images'].keys())[0]
+                    # single-view image
+                    if key in results['images'][cam_type]:
+                        data_metas[key] = results['images'][cam_type][key]
+                else:
+                    # multi-view image
+                    img_metas = []
+                    cam_types = list(results['images'].keys())
+                    for cam_type in cam_types:
+                        if key in results['images'][cam_type]:
+                            img_metas.append(results['images'][cam_type][key])
+                    if len(img_metas) > 0:
+                        data_metas[key] = img_metas
+            elif 'lidar_points' in results:
+                if key in results['lidar_points']:
+                    data_metas[key] = results['lidar_points'][key]
+        data_sample.set_metainfo(data_metas)
 
         inputs = {}
         for key in self.keys:
             if key in results:
                 if key in self.INPUTS_KEYS:
                     inputs[key] = results[key]
                 elif key in self.INSTANCEDATA_3D_KEYS:
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/datasets/transforms/loading.py` & `mmdet3d-1.1.1/mmdet3d/datasets/transforms/loading.py`

 * *Files 26% similar despite different names*

```diff
@@ -4,51 +4,51 @@
 
 import mmcv
 import mmengine
 import numpy as np
 from mmcv.transforms import LoadImageFromFile
 from mmcv.transforms.base import BaseTransform
 from mmdet.datasets.transforms import LoadAnnotations
+from mmengine.fileio import get
 
 from mmdet3d.registry import TRANSFORMS
+from mmdet3d.structures.bbox_3d import get_box_type
 from mmdet3d.structures.points import BasePoints, get_points_type
 
 
 @TRANSFORMS.register_module()
 class LoadMultiViewImageFromFiles(BaseTransform):
     """Load multi channel images from a list of separate channel files.
 
     Expects results['img_filename'] to be a list of filenames.
 
     Args:
         to_float32 (bool): Whether to convert the img to float32.
             Defaults to False.
         color_type (str): Color type of the file. Defaults to 'unchanged'.
-        file_client_args (dict): Arguments to instantiate a FileClient.
-            See :class:`mmengine.fileio.FileClient` for details.
-            Defaults to dict(backend='disk').
+        backend_args (dict, optional): Arguments to instantiate the
+            corresponding backend. Defaults to None.
         num_views (int): Number of view in a frame. Defaults to 5.
         num_ref_frames (int): Number of frame in loading. Defaults to -1.
         test_mode (bool): Whether is test mode in loading. Defaults to False.
         set_default_scale (bool): Whether to set default scale.
             Defaults to True.
     """
 
     def __init__(self,
                  to_float32: bool = False,
                  color_type: str = 'unchanged',
-                 file_client_args: dict = dict(backend='disk'),
+                 backend_args: Optional[dict] = None,
                  num_views: int = 5,
                  num_ref_frames: int = -1,
                  test_mode: bool = False,
                  set_default_scale: bool = True) -> None:
         self.to_float32 = to_float32
         self.color_type = color_type
-        self.file_client_args = file_client_args.copy()
-        self.file_client = None
+        self.backend_args = backend_args
         self.num_views = num_views
         # num_ref_frames is used for multi-sweep loading
         self.num_ref_frames = num_ref_frames
         # when test_mode=False, we randomly select previous frames
         # otherwise, select the earliest one
         self.test_mode = test_mode
         self.set_default_scale = set_default_scale
@@ -158,20 +158,19 @@
             lidar2cam.append(cam_item['lidar2cam'])
         results['filename'] = filename
         results['cam2img'] = cam2img
         results['lidar2cam'] = lidar2cam
 
         results['ori_cam2img'] = copy.deepcopy(results['cam2img'])
 
-        if self.file_client is None:
-            self.file_client = mmengine.FileClient(**self.file_client_args)
-
         # img is of shape (h, w, c, num_views)
         # h and w can be different for different views
-        img_bytes = [self.file_client.get(name) for name in filename]
+        img_bytes = [
+            get(name, backend_args=self.backend_args) for name in filename
+        ]
         imgs = [
             mmcv.imfrombytes(img_byte, flag=self.color_type)
             for img_byte in img_bytes
         ]
         # handle the image with different shape
         img_shapes = np.stack([img.shape for img in imgs], axis=0)
         img_shape_max = np.max(img_shapes, axis=0)
@@ -189,18 +188,18 @@
         if self.to_float32:
             img = img.astype(np.float32)
 
         results['filename'] = filename
         # unravel to list, see `DefaultFormatBundle` in formating.py
         # which will transpose each image separately and then stack into array
         results['img'] = [img[..., i] for i in range(img.shape[-1])]
-        results['img_shape'] = img.shape
-        results['ori_shape'] = img.shape
+        results['img_shape'] = img.shape[:2]
+        results['ori_shape'] = img.shape[:2]
         # Set initial values for default meta_keys
-        results['pad_shape'] = img.shape
+        results['pad_shape'] = img.shape[:2]
         if self.set_default_scale:
             results['scale_factor'] = 1.0
         num_channels = 1 if len(img.shape) < 3 else img.shape[2]
         results['img_norm_cfg'] = dict(
             mean=np.zeros(num_channels, dtype=np.float32),
             std=np.ones(num_channels, dtype=np.float32),
             to_rgb=False)
@@ -247,80 +246,126 @@
             results['cam2img'] = results['images']['CAM2']['cam2img']
         elif len(list(results['images'].keys())) == 1:
             camera_type = list(results['images'].keys())[0]
             filename = results['images'][camera_type]['img_path']
             results['cam2img'] = results['images'][camera_type]['cam2img']
         else:
             raise NotImplementedError(
-                'Currently we only support load image from kitti and'
+                'Currently we only support load image from kitti and '
                 'nuscenes datasets')
 
-        img_bytes = self.file_client.get(filename)
-        img = mmcv.imfrombytes(
-            img_bytes, flag=self.color_type, backend=self.imdecode_backend)
+        try:
+            img_bytes = get(filename, backend_args=self.backend_args)
+            img = mmcv.imfrombytes(
+                img_bytes, flag=self.color_type, backend=self.imdecode_backend)
+        except Exception as e:
+            if self.ignore_empty:
+                return None
+            else:
+                raise e
         if self.to_float32:
             img = img.astype(np.float32)
 
         results['img'] = img
         results['img_shape'] = img.shape[:2]
         results['ori_shape'] = img.shape[:2]
 
         return results
 
 
 @TRANSFORMS.register_module()
+class LoadImageFromNDArray(LoadImageFromFile):
+    """Load an image from ``results['img']``.
+    Similar with :obj:`LoadImageFromFile`, but the image has been loaded as
+    :obj:`np.ndarray` in ``results['img']``. Can be used when loading image
+    from webcam.
+    Required Keys:
+    - img
+    Modified Keys:
+    - img
+    - img_path
+    - img_shape
+    - ori_shape
+    Args:
+        to_float32 (bool): Whether to convert the loaded image to a float32
+            numpy array. If set to False, the loaded image is an uint8 array.
+            Defaults to False.
+    """
+
+    def transform(self, results: dict) -> dict:
+        """Transform function to add image meta information.
+
+        Args:
+            results (dict): Result dict with Webcam read image in
+                ``results['img']``.
+        Returns:
+            dict: The dict contains loaded image and meta information.
+        """
+
+        img = results['img']
+        if self.to_float32:
+            img = img.astype(np.float32)
+
+        results['img_path'] = None
+        results['img'] = img
+        results['img_shape'] = img.shape[:2]
+        results['ori_shape'] = img.shape[:2]
+        return results
+
+
+@TRANSFORMS.register_module()
 class LoadPointsFromMultiSweeps(BaseTransform):
     """Load points from multiple sweeps.
 
     This is usually used for nuScenes dataset to utilize previous sweeps.
 
     Args:
         sweeps_num (int): Number of sweeps. Defaults to 10.
         load_dim (int): Dimension number of the loaded points. Defaults to 5.
         use_dim (list[int]): Which dimension to use. Defaults to [0, 1, 2, 4].
-        file_client_args (dict): Arguments to instantiate a FileClient.
-            See :class:`mmengine.fileio.FileClient` for details.
-            Defaults to dict(backend='disk').
+        backend_args (dict, optional): Arguments to instantiate the
+            corresponding backend. Defaults to None.
         pad_empty_sweeps (bool): Whether to repeat keyframe when
             sweeps is empty. Defaults to False.
         remove_close (bool): Whether to remove close points. Defaults to False.
         test_mode (bool): If `test_mode=True`, it will not randomly sample
             sweeps but select the nearest N frames. Defaults to False.
     """
 
     def __init__(self,
                  sweeps_num: int = 10,
                  load_dim: int = 5,
                  use_dim: List[int] = [0, 1, 2, 4],
-                 file_client_args: dict = dict(backend='disk'),
+                 backend_args: Optional[dict] = None,
                  pad_empty_sweeps: bool = False,
                  remove_close: bool = False,
                  test_mode: bool = False) -> None:
         self.load_dim = load_dim
         self.sweeps_num = sweeps_num
+        if isinstance(use_dim, int):
+            use_dim = list(range(use_dim))
+        assert max(use_dim) < load_dim, \
+            f'Expect all used dimensions < {load_dim}, got {use_dim}'
         self.use_dim = use_dim
-        self.file_client_args = file_client_args.copy()
-        self.file_client = None
+        self.backend_args = backend_args
         self.pad_empty_sweeps = pad_empty_sweeps
         self.remove_close = remove_close
         self.test_mode = test_mode
 
     def _load_points(self, pts_filename: str) -> np.ndarray:
         """Private function to load point clouds data.
 
         Args:
             pts_filename (str): Filename of point clouds data.
 
         Returns:
             np.ndarray: An array containing point clouds data.
         """
-        if self.file_client is None:
-            self.file_client = mmengine.FileClient(**self.file_client_args)
         try:
-            pts_bytes = self.file_client.get(pts_filename)
+            pts_bytes = get(pts_filename, backend_args=self.backend_args)
             points = np.frombuffer(pts_bytes, dtype=np.float32)
         except ConnectionError:
             mmengine.check_file_exist(pts_filename)
             if pts_filename.endswith('.npy'):
                 points = np.load(pts_filename)
             else:
                 points = np.fromfile(pts_filename, dtype=np.float32)
@@ -338,15 +383,15 @@
 
         Returns:
             np.ndarray | :obj:`BasePoints`: Points after removing.
         """
         if isinstance(points, np.ndarray):
             points_numpy = points
         elif isinstance(points, BasePoints):
-            points_numpy = points.tensor.numpy()
+            points_numpy = points.numpy()
         else:
             raise NotImplementedError
         x_filt = np.abs(points_numpy[:, 0]) < radius
         y_filt = np.abs(points_numpy[:, 1]) < radius
         not_close = np.logical_not(np.logical_and(x_filt, y_filt))
         return points[not_close]
 
@@ -530,57 +575,51 @@
         use_dim (list[int] | int): Which dimensions of the points to use.
             Defaults to [0, 1, 2]. For KITTI dataset, set use_dim=4
             or use_dim=[0, 1, 2, 3] to use the intensity dimension.
         shift_height (bool): Whether to use shifted height. Defaults to False.
         use_color (bool): Whether to use color features. Defaults to False.
         norm_intensity (bool): Whether to normlize the intensity. Defaults to
             False.
-        file_client_args (dict): Arguments to instantiate a FileClient.
-            See :class:`mmengine.fileio.FileClient` for details.
-            Defaults to dict(backend='disk').
+        backend_args (dict, optional): Arguments to instantiate the
+            corresponding backend. Defaults to None.
     """
 
-    def __init__(
-        self,
-        coord_type: str,
-        load_dim: int = 6,
-        use_dim: Union[int, List[int]] = [0, 1, 2],
-        shift_height: bool = False,
-        use_color: bool = False,
-        norm_intensity: bool = False,
-        file_client_args: dict = dict(backend='disk')
-    ) -> None:
+    def __init__(self,
+                 coord_type: str,
+                 load_dim: int = 6,
+                 use_dim: Union[int, List[int]] = [0, 1, 2],
+                 shift_height: bool = False,
+                 use_color: bool = False,
+                 norm_intensity: bool = False,
+                 backend_args: Optional[dict] = None) -> None:
         self.shift_height = shift_height
         self.use_color = use_color
         if isinstance(use_dim, int):
             use_dim = list(range(use_dim))
         assert max(use_dim) < load_dim, \
             f'Expect all used dimensions < {load_dim}, got {use_dim}'
         assert coord_type in ['CAMERA', 'LIDAR', 'DEPTH']
 
         self.coord_type = coord_type
         self.load_dim = load_dim
         self.use_dim = use_dim
         self.norm_intensity = norm_intensity
-        self.file_client_args = file_client_args.copy()
-        self.file_client = None
+        self.backend_args = backend_args
 
     def _load_points(self, pts_filename: str) -> np.ndarray:
         """Private function to load point clouds data.
 
         Args:
             pts_filename (str): Filename of point clouds data.
 
         Returns:
             np.ndarray: An array containing point clouds data.
         """
-        if self.file_client is None:
-            self.file_client = mmengine.FileClient(**self.file_client_args)
         try:
-            pts_bytes = self.file_client.get(pts_filename)
+            pts_bytes = get(pts_filename, backend_args=self.backend_args)
             points = np.frombuffer(pts_bytes, dtype=np.float32)
         except ConnectionError:
             mmengine.check_file_exist(pts_filename)
             if pts_filename.endswith('.npy'):
                 points = np.load(pts_filename)
             else:
                 points = np.fromfile(pts_filename, dtype=np.float32)
@@ -636,26 +675,67 @@
         return results
 
     def __repr__(self) -> str:
         """str: Return a string that describes the module."""
         repr_str = self.__class__.__name__ + '('
         repr_str += f'shift_height={self.shift_height}, '
         repr_str += f'use_color={self.use_color}, '
-        repr_str += f'file_client_args={self.file_client_args}, '
+        repr_str += f'backend_args={self.backend_args}, '
         repr_str += f'load_dim={self.load_dim}, '
         repr_str += f'use_dim={self.use_dim})'
         return repr_str
 
 
 @TRANSFORMS.register_module()
 class LoadPointsFromDict(LoadPointsFromFile):
     """Load Points From Dict."""
 
     def transform(self, results: dict) -> dict:
+        """Convert the type of points from ndarray to corresponding
+        `point_class`.
+
+        Args:
+            results (dict): input result. The value of key `points` is a
+                numpy array.
+
+        Returns:
+            dict: The processed results.
+        """
         assert 'points' in results
+        points = results['points']
+
+        if self.norm_intensity:
+            assert len(self.use_dim) >= 4, \
+                f'When using intensity norm, expect used dimensions >= 4, got {len(self.use_dim)}'  # noqa: E501
+            points[:, 3] = np.tanh(points[:, 3])
+        attribute_dims = None
+
+        if self.shift_height:
+            floor_height = np.percentile(points[:, 2], 0.99)
+            height = points[:, 2] - floor_height
+            points = np.concatenate(
+                [points[:, :3],
+                 np.expand_dims(height, 1), points[:, 3:]], 1)
+            attribute_dims = dict(height=3)
+
+        if self.use_color:
+            assert len(self.use_dim) >= 6
+            if attribute_dims is None:
+                attribute_dims = dict()
+            attribute_dims.update(
+                dict(color=[
+                    points.shape[1] - 3,
+                    points.shape[1] - 2,
+                    points.shape[1] - 1,
+                ]))
+
+        points_class = get_points_type(self.coord_type)
+        points = points_class(
+            points, points_dim=points.shape[-1], attribute_dims=attribute_dims)
+        results['points'] = points
         return results
 
 
 @TRANSFORMS.register_module()
 class LoadAnnotations3D(LoadAnnotations):
     """Load Annotations3D.
 
@@ -682,14 +762,16 @@
         - attr_labels (np.ndarray): Attribute labels of instances.
           Only when `with_attr_label` is True.
 
     - pts_instance_mask_path (str): Path of instance mask file.
       Only when `with_mask_3d` is True.
     - pts_semantic_mask_path (str): Path of semantic mask file.
       Only when `with_seg_3d` is True.
+    - pts_panoptic_mask_path (str): Path of panoptic mask file.
+      Only when both `with_panoptic_3d` is True.
 
     Added Keys:
 
     - gt_bboxes_3d (:obj:`LiDARInstance3DBoxes` |
       :obj:`DepthInstance3DBoxes` | :obj:`CameraInstance3DBoxes`):
       3D ground truth bboxes. Only when `with_bbox_3d` is True
     - gt_labels_3d (np.int64): Labels of ground truths.
@@ -719,52 +801,62 @@
         with_seg_3d (bool): Whether to load 3D semantic masks for points.
             Defaults to False.
         with_bbox (bool): Whether to load 2D boxes. Defaults to False.
         with_label (bool): Whether to load 2D labels. Defaults to False.
         with_mask (bool): Whether to load 2D instance masks. Defaults to False.
         with_seg (bool): Whether to load 2D semantic masks. Defaults to False.
         with_bbox_depth (bool): Whether to load 2.5D boxes. Defaults to False.
+        with_panoptic_3d (bool): Whether to load 3D panoptic masks for points.
+            Defaults to False.
         poly2mask (bool): Whether to convert polygon annotations to bitmasks.
             Defaults to True.
-        seg_3d_dtype (dtype): Dtype of 3D semantic masks. Defaults to int64.
-        file_client_args (dict): Arguments to instantiate a FileClient.
-            See :class:`mmengine.fileio.FileClient` for details.
-            Defaults to dict(backend='disk').
-    """
-
-    def __init__(
-        self,
-        with_bbox_3d: bool = True,
-        with_label_3d: bool = True,
-        with_attr_label: bool = False,
-        with_mask_3d: bool = False,
-        with_seg_3d: bool = False,
-        with_bbox: bool = False,
-        with_label: bool = False,
-        with_mask: bool = False,
-        with_seg: bool = False,
-        with_bbox_depth: bool = False,
-        poly2mask: bool = True,
-        seg_3d_dtype: np.dtype = np.int64,
-        file_client_args: dict = dict(backend='disk')
-    ) -> None:
+        seg_3d_dtype (str): String of dtype of 3D semantic masks.
+            Defaults to 'np.int64'.
+        seg_offset (int): The offset to split semantic and instance labels from
+            panoptic labels. Defaults to None.
+        dataset_type (str): Type of dataset used for splitting semantic and
+            instance labels. Defaults to None.
+        backend_args (dict, optional): Arguments to instantiate the
+            corresponding backend. Defaults to None.
+    """
+
+    def __init__(self,
+                 with_bbox_3d: bool = True,
+                 with_label_3d: bool = True,
+                 with_attr_label: bool = False,
+                 with_mask_3d: bool = False,
+                 with_seg_3d: bool = False,
+                 with_bbox: bool = False,
+                 with_label: bool = False,
+                 with_mask: bool = False,
+                 with_seg: bool = False,
+                 with_bbox_depth: bool = False,
+                 with_panoptic_3d: bool = False,
+                 poly2mask: bool = True,
+                 seg_3d_dtype: str = 'np.int64',
+                 seg_offset: int = None,
+                 dataset_type: str = None,
+                 backend_args: Optional[dict] = None) -> None:
         super().__init__(
             with_bbox=with_bbox,
             with_label=with_label,
             with_mask=with_mask,
             with_seg=with_seg,
             poly2mask=poly2mask,
-            file_client_args=file_client_args)
+            backend_args=backend_args)
         self.with_bbox_3d = with_bbox_3d
         self.with_bbox_depth = with_bbox_depth
         self.with_label_3d = with_label_3d
         self.with_attr_label = with_attr_label
         self.with_mask_3d = with_mask_3d
         self.with_seg_3d = with_seg_3d
-        self.seg_3d_dtype = seg_3d_dtype
+        self.with_panoptic_3d = with_panoptic_3d
+        self.seg_3d_dtype = eval(seg_3d_dtype)
+        self.seg_offset = seg_offset
+        self.dataset_type = dataset_type
 
     def _load_bboxes_3d(self, results: dict) -> dict:
         """Private function to move the 3D bounding box annotation from
         `ann_info` field to the root of `results`.
 
         Args:
             results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.
@@ -822,18 +914,17 @@
             results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.
 
         Returns:
             dict: The dict containing loaded 3D mask annotations.
         """
         pts_instance_mask_path = results['pts_instance_mask_path']
 
-        if self.file_client is None:
-            self.file_client = mmengine.FileClient(**self.file_client_args)
         try:
-            mask_bytes = self.file_client.get(pts_instance_mask_path)
+            mask_bytes = get(
+                pts_instance_mask_path, backend_args=self.backend_args)
             pts_instance_mask = np.frombuffer(mask_bytes, dtype=np.int64)
         except ConnectionError:
             mmengine.check_file_exist(pts_instance_mask_path)
             pts_instance_mask = np.fromfile(
                 pts_instance_mask_path, dtype=np.int64)
 
         results['pts_instance_mask'] = pts_instance_mask
@@ -849,32 +940,77 @@
             results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.
 
         Returns:
             dict: The dict containing the semantic segmentation annotations.
         """
         pts_semantic_mask_path = results['pts_semantic_mask_path']
 
-        if self.file_client is None:
-            self.file_client = mmengine.FileClient(**self.file_client_args)
         try:
-            mask_bytes = self.file_client.get(pts_semantic_mask_path)
+            mask_bytes = get(
+                pts_semantic_mask_path, backend_args=self.backend_args)
             # add .copy() to fix read-only bug
             pts_semantic_mask = np.frombuffer(
                 mask_bytes, dtype=self.seg_3d_dtype).copy()
         except ConnectionError:
             mmengine.check_file_exist(pts_semantic_mask_path)
             pts_semantic_mask = np.fromfile(
                 pts_semantic_mask_path, dtype=np.int64)
 
+        if self.dataset_type == 'semantickitti':
+            pts_semantic_mask = pts_semantic_mask.astype(np.int64)
+            pts_semantic_mask = pts_semantic_mask % self.seg_offset
+        # nuScenes loads semantic and panoptic labels from different files.
+
         results['pts_semantic_mask'] = pts_semantic_mask
+
         # 'eval_ann_info' will be passed to evaluator
         if 'eval_ann_info' in results:
             results['eval_ann_info']['pts_semantic_mask'] = pts_semantic_mask
         return results
 
+    def _load_panoptic_3d(self, results: dict) -> dict:
+        """Private function to load 3D panoptic segmentation annotations.
+
+        Args:
+            results (dict): Result dict from :obj:`mmdet3d.CustomDataset`.
+
+        Returns:
+            dict: The dict containing the panoptic segmentation annotations.
+        """
+        pts_panoptic_mask_path = results['pts_panoptic_mask_path']
+
+        try:
+            mask_bytes = get(
+                pts_panoptic_mask_path, backend_args=self.backend_args)
+            # add .copy() to fix read-only bug
+            pts_panoptic_mask = np.frombuffer(
+                mask_bytes, dtype=self.seg_3d_dtype).copy()
+        except ConnectionError:
+            mmengine.check_file_exist(pts_panoptic_mask_path)
+            pts_panoptic_mask = np.fromfile(
+                pts_panoptic_mask_path, dtype=np.int64)
+
+        if self.dataset_type == 'semantickitti':
+            pts_semantic_mask = pts_panoptic_mask.astype(np.int64)
+            pts_semantic_mask = pts_semantic_mask % self.seg_offset
+        elif self.dataset_type == 'nuscenes':
+            pts_semantic_mask = pts_semantic_mask // self.seg_offset
+
+        results['pts_semantic_mask'] = pts_semantic_mask
+
+        # We can directly take panoptic labels as instance ids.
+        pts_instance_mask = pts_panoptic_mask.astype(np.int64)
+        results['pts_instance_mask'] = pts_instance_mask
+
+        # 'eval_ann_info' will be passed to evaluator
+        if 'eval_ann_info' in results:
+            results['eval_ann_info']['pts_semantic_mask'] = pts_semantic_mask
+            results['eval_ann_info']['pts_instance_mask'] = pts_instance_mask
+        return results
+
     def _load_bboxes(self, results: dict) -> None:
         """Private function to load bounding box annotations.
 
         The only difference is it remove the proceess for
         `ignore_flag`
 
         Args:
@@ -912,30 +1048,272 @@
             results = self._load_bboxes_3d(results)
         if self.with_bbox_depth:
             results = self._load_bboxes_depth(results)
         if self.with_label_3d:
             results = self._load_labels_3d(results)
         if self.with_attr_label:
             results = self._load_attr_labels(results)
+        if self.with_panoptic_3d:
+            results = self._load_panoptic_3d(results)
         if self.with_mask_3d:
             results = self._load_masks_3d(results)
         if self.with_seg_3d:
             results = self._load_semantic_seg_3d(results)
-
         return results
 
     def __repr__(self) -> str:
         """str: Return a string that describes the module."""
         indent_str = '    '
         repr_str = self.__class__.__name__ + '(\n'
         repr_str += f'{indent_str}with_bbox_3d={self.with_bbox_3d}, '
         repr_str += f'{indent_str}with_label_3d={self.with_label_3d}, '
         repr_str += f'{indent_str}with_attr_label={self.with_attr_label}, '
         repr_str += f'{indent_str}with_mask_3d={self.with_mask_3d}, '
         repr_str += f'{indent_str}with_seg_3d={self.with_seg_3d}, '
+        repr_str += f'{indent_str}with_panoptic_3d={self.with_panoptic_3d}, '
         repr_str += f'{indent_str}with_bbox={self.with_bbox}, '
         repr_str += f'{indent_str}with_label={self.with_label}, '
         repr_str += f'{indent_str}with_mask={self.with_mask}, '
         repr_str += f'{indent_str}with_seg={self.with_seg}, '
         repr_str += f'{indent_str}with_bbox_depth={self.with_bbox_depth}, '
         repr_str += f'{indent_str}poly2mask={self.poly2mask})'
+        repr_str += f'{indent_str}seg_offset={self.seg_offset})'
+
         return repr_str
+
+
+@TRANSFORMS.register_module()
+class LidarDet3DInferencerLoader(BaseTransform):
+    """Load point cloud in the Inferencer's pipeline.
+
+    Added keys:
+      - points
+      - timestamp
+      - axis_align_matrix
+      - box_type_3d
+      - box_mode_3d
+    """
+
+    def __init__(self, coord_type='LIDAR', **kwargs) -> None:
+        super().__init__()
+        self.from_file = TRANSFORMS.build(
+            dict(type='LoadPointsFromFile', coord_type=coord_type, **kwargs))
+        self.from_ndarray = TRANSFORMS.build(
+            dict(type='LoadPointsFromDict', coord_type=coord_type, **kwargs))
+        self.box_type_3d, self.box_mode_3d = get_box_type(coord_type)
+
+    def transform(self, single_input: dict) -> dict:
+        """Transform function to add image meta information.
+        Args:
+            single_input (dict): Single input.
+
+        Returns:
+            dict: The dict contains loaded image and meta information.
+        """
+        assert 'points' in single_input, "key 'points' must be in input dict"
+        if isinstance(single_input['points'], str):
+            inputs = dict(
+                lidar_points=dict(lidar_path=single_input['points']),
+                timestamp=1,
+                # for ScanNet demo we need axis_align_matrix
+                axis_align_matrix=np.eye(4),
+                box_type_3d=self.box_type_3d,
+                box_mode_3d=self.box_mode_3d)
+        elif isinstance(single_input['points'], np.ndarray):
+            inputs = dict(
+                points=single_input['points'],
+                timestamp=1,
+                # for ScanNet demo we need axis_align_matrix
+                axis_align_matrix=np.eye(4),
+                box_type_3d=self.box_type_3d,
+                box_mode_3d=self.box_mode_3d)
+        else:
+            raise ValueError('Unsupported input points type: '
+                             f"{type(single_input['points'])}")
+
+        if 'points' in inputs:
+            return self.from_ndarray(inputs)
+        return self.from_file(inputs)
+
+
+@TRANSFORMS.register_module()
+class MonoDet3DInferencerLoader(BaseTransform):
+    """Load an image from ``results['images']['CAMX']['img']``. Similar with
+    :obj:`LoadImageFromFileMono3D`, but the image has been loaded as
+    :obj:`np.ndarray` in ``results['images']['CAMX']['img']``.
+
+    Added keys:
+      - img
+      - cam2img
+      - box_type_3d
+      - box_mode_3d
+
+    """
+
+    def __init__(self, **kwargs) -> None:
+        super().__init__()
+        self.from_file = TRANSFORMS.build(
+            dict(type='LoadImageFromFileMono3D', **kwargs))
+        self.from_ndarray = TRANSFORMS.build(
+            dict(type='LoadImageFromNDArray', **kwargs))
+
+    def transform(self, single_input: dict) -> dict:
+        """Transform function to add image meta information.
+
+        Args:
+            single_input (dict): Result dict with Webcam read image in
+                ``results['images']['CAMX']['img']``.
+        Returns:
+            dict: The dict contains loaded image and meta information.
+        """
+        box_type_3d, box_mode_3d = get_box_type('camera')
+        assert 'calib' in single_input and 'img' in single_input, \
+            "key 'calib' and 'img' must be in input dict"
+        if isinstance(single_input['calib'], str):
+            calib_path = single_input['calib']
+            with open(calib_path, 'r') as f:
+                lines = f.readlines()
+            cam2img = np.array([
+                float(info) for info in lines[0].split(' ')[0:16]
+            ]).reshape([4, 4])
+        elif isinstance(single_input['calib'], np.ndarray):
+            cam2img = single_input['calib']
+        else:
+            raise ValueError('Unsupported input calib type: '
+                             f"{type(single_input['calib'])}")
+
+        if isinstance(single_input['img'], str):
+            inputs = dict(
+                images=dict(
+                    CAM_FRONT=dict(
+                        img_path=single_input['img'], cam2img=cam2img)),
+                box_mode_3d=box_mode_3d,
+                box_type_3d=box_type_3d)
+        elif isinstance(single_input['img'], np.ndarray):
+            inputs = dict(
+                img=single_input['img'],
+                cam2img=cam2img,
+                box_type_3d=box_type_3d,
+                box_mode_3d=box_mode_3d)
+        else:
+            raise ValueError('Unsupported input image type: '
+                             f"{type(single_input['img'])}")
+
+        if 'img' in inputs:
+            return self.from_ndarray(inputs)
+        return self.from_file(inputs)
+
+
+@TRANSFORMS.register_module()
+class MultiModalityDet3DInferencerLoader(BaseTransform):
+    """Load point cloud and image in the Inferencer's pipeline.
+
+    Added keys:
+      - points
+      - img
+      - cam2img
+      - lidar2cam
+      - lidar2img
+      - timestamp
+      - axis_align_matrix
+      - box_type_3d
+      - box_mode_3d
+    """
+
+    def __init__(self, load_point_args: dict, load_img_args: dict) -> None:
+        super().__init__()
+        self.points_from_file = TRANSFORMS.build(
+            dict(type='LoadPointsFromFile', **load_point_args))
+        self.points_from_ndarray = TRANSFORMS.build(
+            dict(type='LoadPointsFromDict', **load_point_args))
+        coord_type = load_point_args['coord_type']
+        self.box_type_3d, self.box_mode_3d = get_box_type(coord_type)
+
+        self.imgs_from_file = TRANSFORMS.build(
+            dict(type='LoadImageFromFile', **load_img_args))
+        self.imgs_from_ndarray = TRANSFORMS.build(
+            dict(type='LoadImageFromNDArray', **load_img_args))
+
+    def transform(self, single_input: dict) -> dict:
+        """Transform function to add image meta information.
+        Args:
+            single_input (dict): Single input.
+
+        Returns:
+            dict: The dict contains loaded image, point cloud and meta
+            information.
+        """
+        assert 'points' in single_input and 'img' in single_input and \
+            'calib' in single_input, "key 'points', 'img' and 'calib' must be "
+        f'in input dict, but got {single_input}'
+        if isinstance(single_input['points'], str):
+            inputs = dict(
+                lidar_points=dict(lidar_path=single_input['points']),
+                timestamp=1,
+                # for ScanNet demo we need axis_align_matrix
+                axis_align_matrix=np.eye(4),
+                box_type_3d=self.box_type_3d,
+                box_mode_3d=self.box_mode_3d)
+        elif isinstance(single_input['points'], np.ndarray):
+            inputs = dict(
+                points=single_input['points'],
+                timestamp=1,
+                # for ScanNet demo we need axis_align_matrix
+                axis_align_matrix=np.eye(4),
+                box_type_3d=self.box_type_3d,
+                box_mode_3d=self.box_mode_3d)
+        else:
+            raise ValueError('Unsupported input points type: '
+                             f"{type(single_input['points'])}")
+
+        if 'points' in inputs:
+            points_inputs = self.points_from_ndarray(inputs)
+        else:
+            points_inputs = self.points_from_file(inputs)
+
+        multi_modality_inputs = points_inputs
+
+        box_type_3d, box_mode_3d = get_box_type('lidar')
+        if isinstance(single_input['calib'], str):
+            calib = mmengine.load(single_input['calib'])
+
+        elif isinstance(single_input['calib'], dict):
+            calib = single_input['calib']
+        else:
+            raise ValueError('Unsupported input calib type: '
+                             f"{type(single_input['calib'])}")
+
+        cam2img = np.asarray(calib['cam2img'], dtype=np.float32)
+        lidar2cam = np.asarray(calib['lidar2cam'], dtype=np.float32)
+        if 'lidar2cam' in calib:
+            lidar2img = np.asarray(calib['lidar2img'], dtype=np.float32)
+        else:
+            lidar2img = cam2img @ lidar2cam
+
+        if isinstance(single_input['img'], str):
+            inputs = dict(
+                img_path=single_input['img'],
+                cam2img=cam2img,
+                lidar2img=lidar2img,
+                lidar2cam=lidar2cam,
+                box_mode_3d=box_mode_3d,
+                box_type_3d=box_type_3d)
+        elif isinstance(single_input['img'], np.ndarray):
+            inputs = dict(
+                img=single_input['img'],
+                cam2img=cam2img,
+                lidar2img=lidar2img,
+                lidar2cam=lidar2cam,
+                box_type_3d=box_type_3d,
+                box_mode_3d=box_mode_3d)
+        else:
+            raise ValueError('Unsupported input image type: '
+                             f"{type(single_input['img'])}")
+
+        if isinstance(single_input['img'], np.ndarray):
+            imgs_inputs = self.imgs_from_ndarray(inputs)
+        else:
+            imgs_inputs = self.imgs_from_file(inputs)
+
+        multi_modality_inputs.update(imgs_inputs)
+
+        return multi_modality_inputs
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/datasets/transforms/test_time_aug.py` & `mmdet3d-1.1.1/mmdet3d/datasets/transforms/test_time_aug.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/datasets/transforms/transforms_3d.py` & `mmdet3d-1.1.1/mmdet3d/datasets/transforms/transforms_3d.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,19 +1,20 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import random
 import warnings
-from typing import List, Optional, Tuple, Union
+from typing import List, Optional, Sequence, Tuple, Union
 
 import cv2
 import mmcv
 import numpy as np
+import torch
 from mmcv.transforms import BaseTransform, Compose, RandomResize, Resize
 from mmdet.datasets.transforms import (PhotoMetricDistortion, RandomCrop,
                                        RandomFlip)
-from mmengine import is_tuple_of
+from mmengine import is_list_of, is_tuple_of
 
 from mmdet3d.models.task_modules import VoxelGenerator
 from mmdet3d.registry import TRANSFORMS
 from mmdet3d.structures import (CameraInstance3DBoxes, DepthInstance3DBoxes,
                                 LiDARInstance3DBoxes)
 from mmdet3d.structures.ops import box_np_ops
 from mmdet3d.structures.points import BasePoints
@@ -186,15 +187,23 @@
 
         Add the override feature that if 'flip' is already in results, use it
         to do the augmentation.
         """
         if 'flip' not in results:
             cur_dir = self._choose_direction()
         else:
-            cur_dir = results['flip_direction']
+            # `flip_direction` works only when `flip` is True.
+            # For example, in `MultiScaleFlipAug3D`, `flip_direction` is
+            # 'horizontal' but `flip` is False.
+            if results['flip']:
+                assert 'flip_direction' in results, 'flip and flip_direction '
+                'must exist simultaneously'
+                cur_dir = results['flip_direction']
+            else:
+                cur_dir = None
         if cur_dir is None:
             results['flip'] = False
             results['flip_direction'] = None
         else:
             results['flip'] = True
             results['flip_direction'] = cur_dir
             self._flip(results)
@@ -403,35 +412,34 @@
         # change to float for blending operation
         points = input_dict['points']
         if self.sample_2d:
             img = input_dict['img']
             gt_bboxes_2d = input_dict['gt_bboxes']
             # Assume for now 3D & 2D bboxes are the same
             sampled_dict = self.db_sampler.sample_all(
-                gt_bboxes_3d.tensor.numpy(),
+                gt_bboxes_3d.numpy(),
                 gt_labels_3d,
                 gt_bboxes_2d=gt_bboxes_2d,
                 img=img)
         else:
             sampled_dict = self.db_sampler.sample_all(
-                gt_bboxes_3d.tensor.numpy(),
+                gt_bboxes_3d.numpy(),
                 gt_labels_3d,
                 img=None,
                 ground_plane=ground_plane)
 
         if sampled_dict is not None:
             sampled_gt_bboxes_3d = sampled_dict['gt_bboxes_3d']
             sampled_points = sampled_dict['points']
             sampled_gt_labels = sampled_dict['gt_labels_3d']
 
             gt_labels_3d = np.concatenate([gt_labels_3d, sampled_gt_labels],
                                           axis=0)
             gt_bboxes_3d = gt_bboxes_3d.new_box(
-                np.concatenate(
-                    [gt_bboxes_3d.tensor.numpy(), sampled_gt_bboxes_3d]))
+                np.concatenate([gt_bboxes_3d.numpy(), sampled_gt_bboxes_3d]))
 
             points = self.remove_points_in_boxes(points, sampled_gt_bboxes_3d)
             # check the points dimension
             points = points.cat([sampled_points, points])
 
             if self.sample_2d:
                 sampled_gt_bboxes_2d = sampled_dict['gt_bboxes_2d']
@@ -502,16 +510,16 @@
             dict: Results after adding noise to each object,
             'points', 'gt_bboxes_3d' keys are updated in the result dict.
         """
         gt_bboxes_3d = input_dict['gt_bboxes_3d']
         points = input_dict['points']
 
         # TODO: this is inplace operation
-        numpy_box = gt_bboxes_3d.tensor.numpy()
-        numpy_points = points.tensor.numpy()
+        numpy_box = gt_bboxes_3d.numpy()
+        numpy_points = points.numpy()
 
         noise_per_object_v3_(
             numpy_box,
             numpy_points,
             rotation_perturb=self.rot_range,
             center_noise_std=self.translation_std,
             global_random_rot_range=self.global_rot_range,
@@ -873,15 +881,15 @@
         gt_labels_3d = input_dict['gt_labels_3d']
         mask = gt_bboxes_3d.in_range_bev(bev_range)
         gt_bboxes_3d = gt_bboxes_3d[mask]
         # mask is a torch tensor but gt_labels_3d is still numpy array
         # using mask to index gt_labels_3d will cause bug when
         # len(gt_labels_3d) == 1, where mask=1 will be interpreted
         # as gt_labels_3d[1] and cause out of index error
-        gt_labels_3d = gt_labels_3d[mask.numpy().astype(np.bool)]
+        gt_labels_3d = gt_labels_3d[mask.numpy().astype(bool)]
 
         # limit rad to [-pi, pi]
         gt_bboxes_3d.limit_yaw(offset=0.5, period=2 * np.pi)
         input_dict['gt_bboxes_3d'] = gt_bboxes_3d
         input_dict['gt_labels_3d'] = gt_labels_3d
 
         return input_dict
@@ -976,15 +984,15 @@
 
         Returns:
             dict: Results after filtering, 'gt_bboxes_3d', 'gt_labels_3d'
             keys are updated in the result dict.
         """
         gt_labels_3d = input_dict['gt_labels_3d']
         gt_bboxes_mask = np.array([n in self.labels for n in gt_labels_3d],
-                                  dtype=np.bool_)
+                                  dtype=bool)
         input_dict['gt_bboxes_3d'] = input_dict['gt_bboxes_3d'][gt_bboxes_mask]
         input_dict['gt_labels_3d'] = input_dict['gt_labels_3d'][gt_bboxes_mask]
 
         return input_dict
 
     def __repr__(self) -> str:
         """str: Return a string that describes the module."""
@@ -1027,38 +1035,45 @@
         self.num_points = num_points
         self.sample_range = sample_range
         self.replace = replace
 
     def _points_random_sampling(
         self,
         points: BasePoints,
-        num_samples: int,
+        num_samples: Union[int, float],
         sample_range: Optional[float] = None,
         replace: bool = False,
         return_choices: bool = False
     ) -> Union[Tuple[BasePoints, np.ndarray], BasePoints]:
         """Points random sampling.
 
         Sample points to a certain number.
 
         Args:
             points (:obj:`BasePoints`): 3D Points.
-            num_samples (int): Number of samples to be sampled.
+            num_samples (int, float): Number of samples to be sampled. If
+                float, we sample random fraction of points from num_points
+                to 100%.
             sample_range (float, optional): Indicating the range where the
                 points will be sampled. Defaults to None.
             replace (bool): Sampling with or without replacement.
                 Defaults to False.
             return_choices (bool): Whether return choice. Defaults to False.
 
         Returns:
             tuple[:obj:`BasePoints`, np.ndarray] | :obj:`BasePoints`:
 
                 - points (:obj:`BasePoints`): 3D Points.
                 - choices (np.ndarray, optional): The generated random samples.
         """
+        if isinstance(num_samples, float):
+            assert num_samples < 1
+            num_samples = int(
+                np.random.uniform(self.num_points, 1.) * points.shape[0])
+
         if not replace:
             replace = (points.shape[0] < num_samples)
         point_range = range(len(points))
         if sample_range is not None and not replace:
             # Only sampling the near points when len(points) >= num_samples
             dist = np.linalg.norm(points.coord.numpy(), axis=1)
             far_inds = np.where(dist >= sample_range)[0]
@@ -1534,15 +1549,15 @@
         original_dim = points.shape[1]
 
         # TODO: process instance and semantic mask while _max_num_points
         # is larger than 1
         # Extend points with seg and mask fields
         map_fields2dim = []
         start_dim = original_dim
-        points_numpy = points.tensor.numpy()
+        points_numpy = points.numpy()
         extra_channel = [points_numpy]
         for idx, key in enumerate(results['pts_mask_fields']):
             map_fields2dim.append((key, idx + start_dim))
             extra_channel.append(results[key][..., None])
 
         start_dim += len(results['pts_mask_fields'])
         for idx, key in enumerate(results['pts_seg_fields']):
@@ -1949,15 +1964,15 @@
 
     Required Keys:
 
     - img
     - gt_bboxes (np.float32) (optional)
     - gt_bboxes_labels (np.int64) (optional)
     - gt_masks (BitmapMasks | PolygonMasks) (optional)
-    - gt_ignore_flags (np.bool) (optional)
+    - gt_ignore_flags (bool) (optional)
     - gt_seg_map (np.uint8) (optional)
 
     Modified Keys:
 
     - img
     - img_shape
     - gt_bboxes (optional)
@@ -2348,7 +2363,320 @@
                     else:
                         input_dict[key].append(process_dict[key])
 
         for key in self.collected_keys:
             if len(input_dict[key]) == 0:
                 input_dict.pop(key)
         return input_dict
+
+
+@TRANSFORMS.register_module()
+class PolarMix(BaseTransform):
+    """PolarMix data augmentation.
+
+    The polarmix transform steps are as follows:
+
+        1. Another random point cloud is picked by dataset.
+        2. Exchange sectors of two point clouds that are cut with certain
+           azimuth angles.
+        3. Cut point instances from picked point cloud, rotate them by multiple
+           azimuth angles, and paste the cut and rotated instances.
+
+    Required Keys:
+
+    - points (:obj:`BasePoints`)
+    - pts_semantic_mask (np.int64)
+    - dataset (:obj:`BaseDataset`)
+
+    Modified Keys:
+
+    - points (:obj:`BasePoints`)
+    - pts_semantic_mask (np.int64)
+
+    Args:
+        instance_classes (List[int]): Semantic masks which represent the
+            instance.
+        swap_ratio (float): Swap ratio of two point cloud. Defaults to 0.5.
+        rotate_paste_ratio (float): Rotate paste ratio. Defaults to 1.0.
+        pre_transform (Sequence[dict], optional): Sequence of transform object
+            or config dict to be composed. Defaults to None.
+        prob (float): The transformation probability. Defaults to 1.0.
+    """
+
+    def __init__(self,
+                 instance_classes: List[int],
+                 swap_ratio: float = 0.5,
+                 rotate_paste_ratio: float = 1.0,
+                 pre_transform: Optional[Sequence[dict]] = None,
+                 prob: float = 1.0) -> None:
+        assert is_list_of(instance_classes, int), \
+            'instance_classes should be a list of int'
+        self.instance_classes = instance_classes
+        self.swap_ratio = swap_ratio
+        self.rotate_paste_ratio = rotate_paste_ratio
+
+        self.prob = prob
+        if pre_transform is None:
+            self.pre_transform = None
+        else:
+            self.pre_transform = Compose(pre_transform)
+
+    def polar_mix_transform(self, input_dict: dict, mix_results: dict) -> dict:
+        """PolarMix transform function.
+
+        Args:
+            input_dict (dict): Result dict from loading pipeline.
+            mix_results (dict): Mixed dict picked from dataset.
+
+        Returns:
+            dict: output dict after transformation.
+        """
+        mix_points = mix_results['points']
+        mix_pts_semantic_mask = mix_results['pts_semantic_mask']
+
+        points = input_dict['points']
+        pts_semantic_mask = input_dict['pts_semantic_mask']
+
+        # 1. swap point cloud
+        if np.random.random() < self.swap_ratio:
+            start_angle = (np.random.random() - 1) * np.pi  # -pi~0
+            end_angle = start_angle + np.pi
+            # calculate horizontal angle for each point
+            yaw = -torch.atan2(points.coord[:, 1], points.coord[:, 0])
+            mix_yaw = -torch.atan2(mix_points.coord[:, 1], mix_points.coord[:,
+                                                                            0])
+
+            # select points in sector
+            idx = (yaw <= start_angle) | (yaw >= end_angle)
+            mix_idx = (mix_yaw > start_angle) & (mix_yaw < end_angle)
+
+            # swap
+            points = points.cat([points[idx], mix_points[mix_idx]])
+            pts_semantic_mask = np.concatenate(
+                (pts_semantic_mask[idx.numpy()],
+                 mix_pts_semantic_mask[mix_idx.numpy()]),
+                axis=0)
+
+        # 2. rotate-pasting
+        if np.random.random() < self.rotate_paste_ratio:
+            # extract instance points
+            instance_points, instance_pts_semantic_mask = [], []
+            for instance_class in self.instance_classes:
+                mix_idx = mix_pts_semantic_mask == instance_class
+                instance_points.append(mix_points[mix_idx])
+                instance_pts_semantic_mask.append(
+                    mix_pts_semantic_mask[mix_idx])
+            instance_points = mix_points.cat(instance_points)
+            instance_pts_semantic_mask = np.concatenate(
+                instance_pts_semantic_mask, axis=0)
+
+            # rotate-copy
+            copy_points = [instance_points]
+            copy_pts_semantic_mask = [instance_pts_semantic_mask]
+            angle_list = [
+                np.random.random() * np.pi * 2 / 3,
+                (np.random.random() + 1) * np.pi * 2 / 3
+            ]
+            for angle in angle_list:
+                new_points = instance_points.clone()
+                new_points.rotate(angle)
+                copy_points.append(new_points)
+                copy_pts_semantic_mask.append(instance_pts_semantic_mask)
+            copy_points = instance_points.cat(copy_points)
+            copy_pts_semantic_mask = np.concatenate(
+                copy_pts_semantic_mask, axis=0)
+
+            points = points.cat([points, copy_points])
+            pts_semantic_mask = np.concatenate(
+                (pts_semantic_mask, copy_pts_semantic_mask), axis=0)
+
+        input_dict['points'] = points
+        input_dict['pts_semantic_mask'] = pts_semantic_mask
+        return input_dict
+
+    def transform(self, input_dict: dict) -> dict:
+        """PolarMix transform function.
+
+        Args:
+            input_dict (dict): Result dict from loading pipeline.
+
+        Returns:
+            dict: output dict after transformation.
+        """
+        if np.random.rand() > self.prob:
+            return input_dict
+
+        assert 'dataset' in input_dict, \
+            '`dataset` is needed to pass through PolarMix, while not found.'
+        dataset = input_dict['dataset']
+
+        # get index of other point cloud
+        index = np.random.randint(0, len(dataset))
+
+        mix_results = dataset.get_data_info(index)
+
+        if self.pre_transform is not None:
+            # pre_transform may also require dataset
+            mix_results.update({'dataset': dataset})
+            # before polarmix need to go through
+            # the necessary pre_transform
+            mix_results = self.pre_transform(mix_results)
+            mix_results.pop('dataset')
+
+        input_dict = self.polar_mix_transform(input_dict, mix_results)
+
+        return input_dict
+
+    def __repr__(self) -> str:
+        """str: Return a string that describes the module."""
+        repr_str = self.__class__.__name__
+        repr_str += f'(instance_classes={self.instance_classes}, '
+        repr_str += f'swap_ratio={self.swap_ratio}, '
+        repr_str += f'rotate_paste_ratio={self.rotate_paste_ratio}, '
+        repr_str += f'pre_transform={self.pre_transform}, '
+        repr_str += f'prob={self.prob})'
+        return repr_str
+
+
+@TRANSFORMS.register_module()
+class LaserMix(BaseTransform):
+    """LaserMix data augmentation.
+
+    The lasermix transform steps are as follows:
+
+        1. Another random point cloud is picked by dataset.
+        2. Divide the point cloud into several regions according to pitch
+           angles and combine the areas crossly.
+
+    Required Keys:
+
+    - points (:obj:`BasePoints`)
+    - pts_semantic_mask (np.int64)
+    - dataset (:obj:`BaseDataset`)
+
+    Modified Keys:
+
+    - points (:obj:`BasePoints`)
+    - pts_semantic_mask (np.int64)
+
+    Args:
+        num_areas (List[int]): A list of area numbers will be divided into.
+        pitch_angles (Sequence[float]): Pitch angles used to divide areas.
+        pre_transform (Sequence[dict], optional): Sequence of transform object
+            or config dict to be composed. Defaults to None.
+        prob (float): The transformation probability. Defaults to 1.0.
+    """
+
+    def __init__(self,
+                 num_areas: List[int],
+                 pitch_angles: Sequence[float],
+                 pre_transform: Optional[Sequence[dict]] = None,
+                 prob: float = 1.0) -> None:
+        assert is_list_of(num_areas, int), \
+            'num_areas should be a list of int.'
+        self.num_areas = num_areas
+
+        assert len(pitch_angles) == 2, \
+            'The length of pitch_angles should be 2, ' \
+            f'but got {len(pitch_angles)}.'
+        assert pitch_angles[1] > pitch_angles[0], \
+            'pitch_angles[1] should be larger than pitch_angles[0].'
+        self.pitch_angles = pitch_angles
+
+        self.prob = prob
+        if pre_transform is None:
+            self.pre_transform = None
+        else:
+            self.pre_transform = Compose(pre_transform)
+
+    def laser_mix_transform(self, input_dict: dict, mix_results: dict) -> dict:
+        """LaserMix transform function.
+
+        Args:
+            input_dict (dict): Result dict from loading pipeline.
+            mix_results (dict): Mixed dict picked from dataset.
+
+        Returns:
+            dict: output dict after transformation.
+        """
+        mix_points = mix_results['points']
+        mix_pts_semantic_mask = mix_results['pts_semantic_mask']
+
+        points = input_dict['points']
+        pts_semantic_mask = input_dict['pts_semantic_mask']
+
+        rho = torch.sqrt(points.coord[:, 0]**2 + points.coord[:, 1]**2)
+        pitch = torch.atan2(points.coord[:, 2], rho)
+        pitch = torch.clamp(pitch, self.pitch_angles[0] + 1e-5,
+                            self.pitch_angles[1] - 1e-5)
+
+        mix_rho = torch.sqrt(mix_points.coord[:, 0]**2 +
+                             mix_points.coord[:, 1]**2)
+        mix_pitch = torch.atan2(mix_points.coord[:, 2], mix_rho)
+        mix_pitch = torch.clamp(mix_pitch, self.pitch_angles[0] + 1e-5,
+                                self.pitch_angles[1] - 1e-5)
+
+        num_areas = np.random.choice(self.num_areas, size=1)[0]
+        angle_list = np.linspace(self.pitch_angles[1], self.pitch_angles[0],
+                                 num_areas + 1)
+        out_points = []
+        out_pts_semantic_mask = []
+        for i in range(num_areas):
+            # convert angle to radian
+            start_angle = angle_list[i + 1] / 180 * np.pi
+            end_angle = angle_list[i] / 180 * np.pi
+            if i % 2 == 0:  # pick from original point cloud
+                idx = (pitch > start_angle) & (pitch <= end_angle)
+                out_points.append(points[idx])
+                out_pts_semantic_mask.append(pts_semantic_mask[idx.numpy()])
+            else:  # pickle from mixed point cloud
+                idx = (mix_pitch > start_angle) & (mix_pitch <= end_angle)
+                out_points.append(mix_points[idx])
+                out_pts_semantic_mask.append(
+                    mix_pts_semantic_mask[idx.numpy()])
+        out_points = points.cat(out_points)
+        out_pts_semantic_mask = np.concatenate(out_pts_semantic_mask, axis=0)
+        input_dict['points'] = out_points
+        input_dict['pts_semantic_mask'] = out_pts_semantic_mask
+        return input_dict
+
+    def transform(self, input_dict: dict) -> dict:
+        """LaserMix transform function.
+
+        Args:
+            input_dict (dict): Result dict from loading pipeline.
+
+        Returns:
+            dict: output dict after transformation.
+        """
+        if np.random.rand() > self.prob:
+            return input_dict
+
+        assert 'dataset' in input_dict, \
+            '`dataset` is needed to pass through LaserMix, while not found.'
+        dataset = input_dict['dataset']
+
+        # get index of other point cloud
+        index = np.random.randint(0, len(dataset))
+
+        mix_results = dataset.get_data_info(index)
+
+        if self.pre_transform is not None:
+            # pre_transform may also require dataset
+            mix_results.update({'dataset': dataset})
+            # before lasermix need to go through
+            # the necessary pre_transform
+            mix_results = self.pre_transform(mix_results)
+            mix_results.pop('dataset')
+
+        input_dict = self.laser_mix_transform(input_dict, mix_results)
+
+        return input_dict
+
+    def __repr__(self) -> str:
+        """str: Return a string that describes the module."""
+        repr_str = self.__class__.__name__
+        repr_str += f'(num_areas={self.num_areas}, '
+        repr_str += f'pitch_angles={self.pitch_angles}, '
+        repr_str += f'pre_transform={self.pre_transform}, '
+        repr_str += f'prob={self.prob})'
+        return repr_str
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/datasets/utils.py` & `mmdet3d-1.1.1/mmdet3d/datasets/utils.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/datasets/waymo_dataset.py` & `mmdet3d-1.1.1/mmdet3d/datasets/waymo_dataset.py`

 * *Files 2% similar despite different names*

```diff
@@ -23,18 +23,18 @@
     Args:
         data_root (str): Path of dataset root.
         ann_file (str): Path of annotation file.
         data_prefix (dict): data prefix for point cloud and
             camera data dict. Defaults to dict(
                                     pts='velodyne',
                                     CAM_FRONT='image_0',
-                                    CAM_FRONT_RIGHT='image_1',
-                                    CAM_FRONT_LEFT='image_2',
-                                    CAM_SIDE_RIGHT='image_3',
-                                    CAM_SIDE_LEFT='image_4')
+                                    CAM_FRONT_LEFT='image_1',
+                                    CAM_FRONT_RIGHT='image_2',
+                                    CAM_SIDE_LEFT='image_3',
+                                    CAM_SIDE_RIGHT='image_4')
         pipeline (List[dict]): Pipeline used for data processing.
             Defaults to [].
         modality (dict): Modality to specify the sensor data used
             as input. Defaults to dict(use_lidar=True).
         default_cam_key (str): Default camera key for lidar2img
             association. Defaults to 'CAM_FRONT'.
         box_type_3d (str): Type of 3D box of this dataset.
@@ -64,26 +64,33 @@
             used to filter invalid predicted boxes.
             Defaults to [-85, -85, -5, 85, 85, 5].
         cam_sync_instances (bool): If use the camera sync label
             supported from waymo version 1.3.1. Defaults to False.
         load_interval (int): load frame interval. Defaults to 1.
         max_sweeps (int): max sweep for each frame. Defaults to 0.
     """
-    METAINFO = {'classes': ('Car', 'Pedestrian', 'Cyclist')}
+    METAINFO = {
+        'classes': ('Car', 'Pedestrian', 'Cyclist'),
+        'palette': [
+            (0, 120, 255),  # Waymo Blue
+            (0, 232, 157),  # Waymo Green
+            (255, 205, 85)  # Amber
+        ]
+    }
 
     def __init__(self,
                  data_root: str,
                  ann_file: str,
                  data_prefix: dict = dict(
                      pts='velodyne',
                      CAM_FRONT='image_0',
-                     CAM_FRONT_RIGHT='image_1',
-                     CAM_FRONT_LEFT='image_2',
-                     CAM_SIDE_RIGHT='image_3',
-                     CAM_SIDE_LEFT='image_4'),
+                     CAM_FRONT_LEFT='image_1',
+                     CAM_FRONT_RIGHT='image_2',
+                     CAM_SIDE_LEFT='image_3',
+                     CAM_SIDE_RIGHT='image_4'),
                  pipeline: List[Union[dict, Callable]] = [],
                  modality: dict = dict(use_lidar=True),
                  default_cam_key: str = 'CAM_FRONT',
                  box_type_3d: str = 'LiDAR',
                  load_type: str = 'frame_based',
                  filter_empty_gt: bool = True,
                  test_mode: bool = False,
@@ -95,15 +102,15 @@
         self.load_interval = load_interval
         # set loading mode for different task settings
         self.cam_sync_instances = cam_sync_instances
         # construct self.cat_ids for vision-only anns parsing
         self.cat_ids = range(len(self.METAINFO['classes']))
         self.cat2label = {cat_id: i for i, cat_id in enumerate(self.cat_ids)}
         self.max_sweeps = max_sweeps
-        # we do not provide file_client_args to custom_3d init
+        # we do not provide backend_args to custom_3d init
         # because we want disk loading for info
         # while ceph loading for Prediction2Waymo
         super().__init__(
             data_root=data_root,
             ann_file=ann_file,
             pipeline=pipeline,
             modality=modality,
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/engine/hooks/benchmark_hook.py` & `mmdet3d-1.1.1/mmdet3d/engine/hooks/benchmark_hook.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/engine/hooks/disable_object_sample_hook.py` & `mmdet3d-1.1.1/mmdet3d/engine/hooks/disable_object_sample_hook.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from mmengine.dataset import BaseDataset
 from mmengine.hooks import Hook
 from mmengine.model import is_model_wrapper
 from mmengine.runner import Runner
 
 from mmdet3d.datasets.transforms import ObjectSample
 from mmdet3d.registry import HOOKS
 
@@ -31,15 +32,19 @@
         train_loader = runner.train_dataloader
         model = runner.model
         # TODO: refactor after mmengine using model wrapper
         if is_model_wrapper(model):
             model = model.module
         if epoch == self.disable_after_epoch:
             runner.logger.info('Disable ObjectSample')
-            for transform in runner.train_dataloader.dataset.pipeline.transforms:  # noqa: E501
+            dataset = runner.train_dataloader.dataset
+            # handle dataset wrapper
+            if not isinstance(dataset, BaseDataset):
+                dataset = dataset.dataset
+            for transform in dataset.pipeline.transforms:  # noqa: E501
                 if isinstance(transform, ObjectSample):
                     assert hasattr(transform, 'disabled')
                     transform.disabled = True
             # The dataset pipeline cannot be updated when persistent_workers
             # is True, so we need to force the dataloader's multi-process
             # restart. This is a very hacky approach.
             if hasattr(train_loader, 'persistent_workers'
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/evaluation/__init__.py` & `mmdet3d-1.1.1/mmdet3d/evaluation/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -2,22 +2,24 @@
 from mmdet3d.evaluation.functional.kitti_utils import (do_eval, eval_class,
                                                        kitti_eval,
                                                        kitti_eval_coco_style)
 from .functional import (aggregate_predictions, average_precision,
                          eval_det_cls, eval_map_recall, fast_hist, get_acc,
                          get_acc_cls, get_classwise_aps, get_single_class_aps,
                          indoor_eval, instance_seg_eval, load_lyft_gts,
-                         load_lyft_predictions, lyft_eval, per_class_iou,
-                         rename_gt, seg_eval)
+                         load_lyft_predictions, lyft_eval, panoptic_seg_eval,
+                         per_class_iou, rename_gt, seg_eval)
 from .metrics import (IndoorMetric, InstanceSegMetric, KittiMetric, LyftMetric,
-                      NuScenesMetric, SegMetric, WaymoMetric)
+                      NuScenesMetric, PanopticSegMetric, SegMetric,
+                      WaymoMetric)
 
 __all__ = [
     'kitti_eval_coco_style', 'kitti_eval', 'indoor_eval', 'lyft_eval',
     'seg_eval', 'instance_seg_eval', 'average_precision', 'eval_det_cls',
     'eval_map_recall', 'indoor_eval', 'aggregate_predictions', 'rename_gt',
     'instance_seg_eval', 'load_lyft_gts', 'load_lyft_predictions', 'lyft_eval',
     'get_classwise_aps', 'get_single_class_aps', 'fast_hist', 'per_class_iou',
     'get_acc', 'get_acc_cls', 'seg_eval', 'KittiMetric', 'NuScenesMetric',
     'IndoorMetric', 'LyftMetric', 'SegMetric', 'InstanceSegMetric',
-    'WaymoMetric', 'eval_class', 'do_eval'
+    'WaymoMetric', 'eval_class', 'do_eval', 'PanopticSegMetric',
+    'panoptic_seg_eval'
 ]
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/__init__.py` & `mmdet3d-1.1.1/mmdet3d/evaluation/functional/__init__.py`

 * *Files 9% similar despite different names*

```diff
@@ -2,18 +2,19 @@
 from .indoor_eval import (average_precision, eval_det_cls, eval_map_recall,
                           indoor_eval)
 from .instance_seg_eval import (aggregate_predictions, instance_seg_eval,
                                 rename_gt)
 from .kitti_utils import do_eval, kitti_eval, kitti_eval_coco_style
 from .lyft_eval import (get_classwise_aps, get_single_class_aps, load_lyft_gts,
                         load_lyft_predictions, lyft_eval)
+from .panoptic_seg_eval import panoptic_seg_eval
 from .scannet_utils import evaluate_matches, scannet_eval
 from .seg_eval import fast_hist, get_acc, get_acc_cls, per_class_iou, seg_eval
 
 __all__ = [
     'average_precision', 'eval_det_cls', 'eval_map_recall', 'indoor_eval',
     'aggregate_predictions', 'rename_gt', 'instance_seg_eval', 'load_lyft_gts',
     'load_lyft_predictions', 'lyft_eval', 'get_classwise_aps',
     'get_single_class_aps', 'fast_hist', 'per_class_iou', 'get_acc',
     'get_acc_cls', 'seg_eval', 'kitti_eval', 'kitti_eval_coco_style',
-    'scannet_eval', 'evaluate_matches', 'do_eval'
+    'scannet_eval', 'evaluate_matches', 'do_eval', 'panoptic_seg_eval'
 ]
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/indoor_eval.py` & `mmdet3d-1.1.1/mmdet3d/evaluation/functional/indoor_eval.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/instance_seg_eval.py` & `mmdet3d-1.1.1/mmdet3d/evaluation/functional/instance_seg_eval.py`

 * *Files 0% similar despite different names*

```diff
@@ -25,15 +25,15 @@
         score = score.clone().numpy()
         info = dict()
         n_instances = mask.max() + 1
         for i in range(n_instances):
             # match pred_instance['filename'] from assign_instances_for_scan
             file_name = f'{id}_{i}'
             info[file_name] = dict()
-            info[file_name]['mask'] = (mask == i).astype(np.int)
+            info[file_name]['mask'] = (mask == i).astype(np.int64)
             info[file_name]['label_id'] = valid_class_ids[label[i]]
             info[file_name]['conf'] = score[i]
         infos.append(info)
     return infos
 
 
 def rename_gt(gt_semantic_masks, gt_instance_masks, valid_class_ids):
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/kitti_utils/eval.py` & `mmdet3d-1.1.1/mmdet3d/evaluation/functional/kitti_utils/eval.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/kitti_utils/rotate_iou.py` & `mmdet3d-1.1.1/mmdet3d/evaluation/functional/kitti_utils/rotate_iou.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/lyft_eval.py` & `mmdet3d-1.1.1/mmdet3d/evaluation/functional/lyft_eval.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/scannet_utils/evaluate_semantic_instance.py` & `mmdet3d-1.1.1/mmdet3d/evaluation/functional/scannet_utils/evaluate_semantic_instance.py`

 * *Files 1% similar despite different names*

```diff
@@ -21,16 +21,15 @@
     """
     overlaps = options['overlaps']
     min_region_sizes = [options['min_region_sizes'][0]]
     dist_threshes = [options['distance_threshes'][0]]
     dist_confs = [options['distance_confs'][0]]
 
     # results: class x overlap
-    ap = np.zeros((len(dist_threshes), len(class_labels), len(overlaps)),
-                  np.float)
+    ap = np.zeros((len(dist_threshes), len(class_labels), len(overlaps)))
     for di, (min_region_size, distance_thresh, distance_conf) in enumerate(
             zip(min_region_sizes, dist_threshes, dist_confs)):
         for oi, overlap_th in enumerate(overlaps):
             pred_visited = {}
             for m in matches:
                 for label_name in class_labels:
                     for p in matches[m]['pred'][label_name]:
@@ -55,15 +54,15 @@
                     if gt_instances:
                         has_gt = True
                     if pred_instances:
                         has_pred = True
 
                     cur_true = np.ones(len(gt_instances))
                     cur_score = np.ones(len(gt_instances)) * (-float('inf'))
-                    cur_match = np.zeros(len(gt_instances), dtype=np.bool)
+                    cur_match = np.zeros(len(gt_instances), dtype=bool)
                     # collect matches
                     for (gti, gt) in enumerate(gt_instances):
                         found_match = False
                         for pred in gt['matched_pred']:
                             # greedy assignments
                             if pred_visited[pred['filename']]:
                                 continue
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/scannet_utils/util_3d.py` & `mmdet3d-1.1.1/mmdet3d/evaluation/functional/scannet_utils/util_3d.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/seg_eval.py` & `mmdet3d-1.1.1/mmdet3d/evaluation/functional/seg_eval.py`

 * *Files 7% similar despite different names*

```diff
@@ -84,25 +84,28 @@
         dict[str, float]: Dict of results.
     """
     assert len(seg_preds) == len(gt_labels)
     num_classes = len(label2cat)
 
     hist_list = []
     for i in range(len(gt_labels)):
-        gt_seg = gt_labels[i].astype(np.int)
-        pred_seg = seg_preds[i].astype(np.int)
+        gt_seg = gt_labels[i].astype(np.int64)
+        pred_seg = seg_preds[i].astype(np.int64)
 
         # filter out ignored points
         pred_seg[gt_seg == ignore_index] = -1
         gt_seg[gt_seg == ignore_index] = -1
 
         # calculate one instance result
         hist_list.append(fast_hist(pred_seg, gt_seg, num_classes))
 
     iou = per_class_iou(sum(hist_list))
+    # if ignore_index is in iou, replace it with nan
+    if ignore_index < len(iou):
+        iou[ignore_index] = np.nan
     miou = np.nanmean(iou)
     acc = get_acc(sum(hist_list))
     acc_cls = get_acc_cls(sum(hist_list))
 
     header = ['classes']
     for i in range(len(label2cat)):
         header.append(label2cat[i])
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/evaluation/functional/waymo_utils/prediction_to_waymo.py` & `mmdet3d-1.1.1/mmdet3d/evaluation/functional/waymo_utils/prediction_to_waymo.py`

 * *Files 2% similar despite different names*

```diff
@@ -37,16 +37,16 @@
             in waymo format (.bin files).
         waymo_results_final_path (str): Path to save combined
             predictions in waymo format (.bin file), like 'a/b/c.bin'.
         prefix (str): Prefix of filename. In general, 0 for training, 1 for
             validation and 2 for testing.
         classes (dict): A list of class name.
         workers (str): Number of parallel processes. Defaults to 2.
-        file_client_args (str): File client for reading gt in waymo format.
-            Defaults to ``dict(backend='disk')``.
+        backend_args (dict, optional): Arguments to instantiate the
+            corresponding backend. Defaults to None.
         from_kitti_format (bool, optional): Whether the reuslts are kitti
             format. Defaults to False.
         idx2metainfo (Optional[dict], optional): The mapping from sample_idx to
             metainfo. The metainfo must contain the keys: 'idx2contextname' and
             'idx2timestamp'. Defaults to None.
     """
 
@@ -54,26 +54,26 @@
                  results: List[dict],
                  waymo_tfrecords_dir: str,
                  waymo_results_save_dir: str,
                  waymo_results_final_path: str,
                  prefix: str,
                  classes: dict,
                  workers: int = 2,
-                 file_client_args: dict = dict(backend='disk'),
+                 backend_args: Optional[dict] = None,
                  from_kitti_format: bool = False,
                  idx2metainfo: Optional[dict] = None):
 
         self.results = results
         self.waymo_tfrecords_dir = waymo_tfrecords_dir
         self.waymo_results_save_dir = waymo_results_save_dir
         self.waymo_results_final_path = waymo_results_final_path
         self.prefix = prefix
         self.classes = classes
         self.workers = int(workers)
-        self.file_client_args = file_client_args
+        self.backend_args = backend_args
         self.from_kitti_format = from_kitti_format
         if idx2metainfo is not None:
             self.idx2metainfo = idx2metainfo
             # If ``fast_eval``, the metainfo does not need to be read from
             # original data online. It's preprocessed offline.
             self.fast_eval = True
         else:
@@ -110,20 +110,20 @@
             if int(tf.__version__.split('.')[0]) < 2:
                 tf.enable_eager_execution()
 
         self.create_folder()
 
     def get_file_names(self):
         """Get file names of waymo raw data."""
-        if 'path_mapping' in self.file_client_args:
-            for path in self.file_client_args['path_mapping'].keys():
+        if 'path_mapping' in self.backend_args:
+            for path in self.backend_args['path_mapping'].keys():
                 if path in self.waymo_tfrecords_dir:
                     self.waymo_tfrecords_dir = \
                         self.waymo_tfrecords_dir.replace(
-                            path, self.file_client_args['path_mapping'][path])
+                            path, self.backend_args['path_mapping'][path])
             from petrel_client.client import Client
             client = Client()
             contents = client.list(self.waymo_tfrecords_dir)
             self.waymo_tfrecord_pathnames = list()
             for content in sorted(list(contents)):
                 if content.endswith('tfrecord'):
                     self.waymo_tfrecord_pathnames.append(
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/evaluation/metrics/__init__.py` & `mmdet3d-1.1.1/mmdet3d/evaluation/metrics/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from .indoor_metric import IndoorMetric  # noqa: F401,F403
 from .instance_seg_metric import InstanceSegMetric  # noqa: F401,F403
 from .kitti_metric import KittiMetric  # noqa: F401,F403
 from .lyft_metric import LyftMetric  # noqa: F401,F403
 from .nuscenes_metric import NuScenesMetric  # noqa: F401,F403
+from .panoptic_seg_metric import PanopticSegMetric  # noqa: F401,F403
 from .seg_metric import SegMetric  # noqa: F401,F403
 from .waymo_metric import WaymoMetric  # noqa: F401,F403
 
 __all__ = [
     'KittiMetric', 'NuScenesMetric', 'IndoorMetric', 'LyftMetric', 'SegMetric',
-    'InstanceSegMetric', 'WaymoMetric'
+    'InstanceSegMetric', 'WaymoMetric', 'PanopticSegMetric'
 ]
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/evaluation/metrics/indoor_metric.py` & `mmdet3d-1.1.1/mmdet3d/evaluation/metrics/indoor_metric.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from collections import OrderedDict
-from typing import Dict, List, Optional, Sequence
+from typing import Dict, List, Optional, Sequence, Union
 
 import numpy as np
 from mmdet.evaluation import eval_map
 from mmengine.evaluator import BaseMetric
 from mmengine.logging import MMLogger
 
 from mmdet3d.evaluation import indoor_eval
@@ -13,45 +13,42 @@
 
 
 @METRICS.register_module()
 class IndoorMetric(BaseMetric):
     """Indoor scene evaluation metric.
 
     Args:
-        iou_thr (list[float]): List of iou threshold when calculate the
-            metric. Defaults to  [0.25, 0.5].
-        collect_device (str, optional): Device name used for collecting
-            results from different ranks during distributed training.
-            Must be 'cpu' or 'gpu'. Defaults to 'cpu'.
-        prefix (str): The prefix that will be added in the metric
+        iou_thr (float or List[float]): List of iou threshold when calculate
+            the metric. Defaults to [0.25, 0.5].
+        collect_device (str): Device name used for collecting results from
+            different ranks during distributed training. Must be 'cpu' or
+            'gpu'. Defaults to 'cpu'.
+        prefix (str, optional): The prefix that will be added in the metric
             names to disambiguate homonymous metrics of different evaluators.
-            If prefix is not provided in the argument, self.default_prefix
-            will be used instead. Default: None
+            If prefix is not provided in the argument, self.default_prefix will
+            be used instead. Defaults to None.
     """
 
     def __init__(self,
                  iou_thr: List[float] = [0.25, 0.5],
                  collect_device: str = 'cpu',
-                 prefix: Optional[str] = None,
-                 **kwargs):
+                 prefix: Optional[str] = None) -> None:
         super(IndoorMetric, self).__init__(
             prefix=prefix, collect_device=collect_device)
-        self.iou_thr = iou_thr
+        self.iou_thr = [iou_thr] if isinstance(iou_thr, float) else iou_thr
 
     def process(self, data_batch: dict, data_samples: Sequence[dict]) -> None:
         """Process one batch of data samples and predictions.
 
-        The processed results should be stored in ``self.results``,
-        which will be used to compute the metrics when all batches
-        have been processed.
+        The processed results should be stored in ``self.results``, which will
+        be used to compute the metrics when all batches have been processed.
 
         Args:
             data_batch (dict): A batch of data from the dataloader.
-            data_samples (Sequence[dict]): A batch of outputs from
-                the model.
+            data_samples (Sequence[dict]): A batch of outputs from the model.
         """
         for data_sample in data_samples:
             pred_3d = data_sample['pred_instances_3d']
             eval_ann_info = data_sample['eval_ann_info']
             cpu_pred_3d = dict()
             for k, v in pred_3d.items():
                 if hasattr(v, 'to'):
@@ -94,45 +91,42 @@
 
 
 @METRICS.register_module()
 class Indoor2DMetric(BaseMetric):
     """indoor 2d predictions evaluation metric.
 
     Args:
-        iou_thr (list[float]): List of iou threshold when calculate the
-            metric. Defaults to  [0.5].
-        collect_device (str, optional): Device name used for collecting
-            results from different ranks during distributed training.
-            Must be 'cpu' or 'gpu'. Defaults to 'cpu'.
-        prefix (str): The prefix that will be added in the metric
+        iou_thr (float or List[float]): List of iou threshold when calculate
+            the metric. Defaults to [0.5].
+        collect_device (str): Device name used for collecting results from
+            different ranks during distributed training. Must be 'cpu' or
+            'gpu'. Defaults to 'cpu'.
+        prefix (str, optional): The prefix that will be added in the metric
             names to disambiguate homonymous metrics of different evaluators.
-            If prefix is not provided in the argument, self.default_prefix
-            will be used instead. Default: None
+            If prefix is not provided in the argument, self.default_prefix will
+            be used instead. Defaults to None.
     """
 
     def __init__(self,
-                 iou_thr: List[float] = [0.5],
+                 iou_thr: Union[float, List[float]] = [0.5],
                  collect_device: str = 'cpu',
-                 prefix: Optional[str] = None,
-                 **kwargs):
+                 prefix: Optional[str] = None):
         super(Indoor2DMetric, self).__init__(
             prefix=prefix, collect_device=collect_device)
-        self.iou_thr = iou_thr
+        self.iou_thr = [iou_thr] if isinstance(iou_thr, float) else iou_thr
 
     def process(self, data_batch: dict, data_samples: Sequence[dict]) -> None:
         """Process one batch of data samples and predictions.
 
-        The processed results should be stored in ``self.results``,
-        which will be used to compute the metrics when all batches
-        have been processed.
+        The processed results should be stored in ``self.results``, which will
+        be used to compute the metrics when all batches have been processed.
 
         Args:
             data_batch (dict): A batch of data from the dataloader.
-            predictions (Sequence[dict]): A batch of outputs from
-                the model.
+            data_samples (Sequence[dict]): A batch of outputs from the model.
         """
         for data_sample in data_samples:
             pred = data_sample['pred_instances']
             eval_ann_info = data_sample['eval_ann_info']
             ann = dict(
                 labels=eval_ann_info['gt_bboxes_labels'],
                 bboxes=eval_ann_info['gt_bboxes'])
@@ -159,17 +153,15 @@
         Returns:
             Dict[str, float]: The computed metrics. The keys are the names of
             the metrics, and the values are corresponding results.
         """
         logger: MMLogger = MMLogger.get_current_instance()
         annotations, preds = zip(*results)
         eval_results = OrderedDict()
-        iou_thr_2d = (self.iou_thr) if isinstance(self.iou_thr,
-                                                  float) else self.iou_thr
-        for iou_thr_2d_single in iou_thr_2d:
+        for iou_thr_2d_single in self.iou_thr:
             mean_ap, _ = eval_map(
                 preds,
                 annotations,
                 scale_ranges=None,
                 iou_thr=iou_thr_2d_single,
                 dataset=self.dataset_meta['classes'],
                 logger=logger)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/evaluation/metrics/instance_seg_metric.py` & `mmdet3d-1.1.1/mmdet3d/evaluation/metrics/instance_seg_metric.py`

 * *Files 6% similar despite different names*

```diff
@@ -9,41 +9,38 @@
 
 
 @METRICS.register_module()
 class InstanceSegMetric(BaseMetric):
     """3D instance segmentation evaluation metric.
 
     Args:
-        collect_device (str, optional): Device name used for collecting
-            results from different ranks during distributed training.
-            Must be 'cpu' or 'gpu'. Defaults to 'cpu'.
-        prefix (str): The prefix that will be added in the metric
+        collect_device (str): Device name used for collecting results from
+            different ranks during distributed training. Must be 'cpu' or
+            'gpu'. Defaults to 'cpu'.
+        prefix (str, optional): The prefix that will be added in the metric
             names to disambiguate homonymous metrics of different evaluators.
-            If prefix is not provided in the argument, self.default_prefix
-            will be used instead. Default: None
+            If prefix is not provided in the argument, self.default_prefix will
+            be used instead. Defaults to None.
     """
 
     def __init__(self,
                  collect_device: str = 'cpu',
-                 prefix: Optional[str] = None,
-                 **kwargs):
+                 prefix: Optional[str] = None):
         super(InstanceSegMetric, self).__init__(
             prefix=prefix, collect_device=collect_device)
 
     def process(self, data_batch: dict, data_samples: Sequence[dict]) -> None:
         """Process one batch of data samples and predictions.
 
-        The processed results should be stored in ``self.results``,
-        which will be used to compute the metrics when all batches
-        have been processed.
+        The processed results should be stored in ``self.results``, which will
+        be used to compute the metrics when all batches have been processed.
 
         Args:
             data_batch (dict): A batch of data from the dataloader.
-            data_samples (Sequence[dict]): A batch of outputs from
-                the model.
+            data_samples (Sequence[dict]): A batch of outputs from the model.
         """
         for data_sample in data_samples:
             pred_3d = data_sample['pred_pts_seg']
             eval_ann_info = data_sample['eval_ann_info']
             cpu_pred_3d = dict()
             for k, v in pred_3d.items():
                 if hasattr(v, 'to'):
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/evaluation/metrics/kitti_metric.py` & `mmdet3d-1.1.1/mmdet3d/evaluation/metrics/kitti_metric.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import tempfile
 from os import path as osp
-from typing import Dict, List, Optional, Sequence, Union
+from typing import Dict, List, Optional, Sequence, Tuple, Union
 
 import mmengine
 import numpy as np
 import torch
 from mmengine import load
 from mmengine.evaluator import BaseMetric
 from mmengine.logging import MMLogger, print_log
@@ -18,79 +18,77 @@
 
 @METRICS.register_module()
 class KittiMetric(BaseMetric):
     """Kitti evaluation metric.
 
     Args:
         ann_file (str): Annotation file path.
-        metric (str | list[str]): Metrics to be evaluated.
-            Default to 'bbox'.
-        pcd_limit_range (list): The range of point cloud used to
-            filter invalid predicted boxes.
-            Default to [0, -40, -3, 70.4, 40, 0.0].
+        metric (str or List[str]): Metrics to be evaluated. Defaults to 'bbox'.
+        pcd_limit_range (List[float]): The range of point cloud used to filter
+            invalid predicted boxes. Defaults to [0, -40, -3, 70.4, 40, 0.0].
         prefix (str, optional): The prefix that will be added in the metric
             names to disambiguate homonymous metrics of different evaluators.
-            If prefix is not provided in the argument, self.default_prefix
-            will be used instead. Defaults to None.
-        pklfile_prefix (str, optional): The prefix of pkl files, including
-            the file path and the prefix of filename, e.g., "a/b/prefix".
-            If not specified, a temp file will be created. Default: None.
-        default_cam_key (str, optional): The default camera for lidar to
-            camear conversion. By default, KITTI: CAM2, Waymo: CAM_FRONT
+            If prefix is not provided in the argument, self.default_prefix will
+            be used instead. Defaults to None.
+        pklfile_prefix (str, optional): The prefix of pkl files, including the
+            file path and the prefix of filename, e.g., "a/b/prefix". If not
+            specified, a temp file will be created. Defaults to None.
+        default_cam_key (str): The default camera for lidar to camera
+            conversion. By default, KITTI: 'CAM2', Waymo: 'CAM_FRONT'.
+            Defaults to 'CAM2'.
         format_only (bool): Format the output results without perform
-            evaluation. It is useful when you want to format the result
-            to a specific format and submit it to the test server.
+            evaluation. It is useful when you want to format the result to a
+            specific format and submit it to the test server.
             Defaults to False.
-        submission_prefix (str, optional): The prefix of submission data.
-            If not specified, the submission data will not be generated.
-            Default: None.
-        collect_device (str): Device name used for collecting results
-            from different ranks during distributed training. Must be 'cpu' or
+        submission_prefix (str, optional): The prefix of submission data. If
+            not specified, the submission data will not be generated.
+            Defaults to None.
+        collect_device (str): Device name used for collecting results from
+            different ranks during distributed training. Must be 'cpu' or
             'gpu'. Defaults to 'cpu'.
+        backend_args (dict, optional): Arguments to instantiate the
+            corresponding backend. Defaults to None.
     """
 
     def __init__(self,
                  ann_file: str,
                  metric: Union[str, List[str]] = 'bbox',
-                 pred_box_type_3d: str = 'LiDAR',
                  pcd_limit_range: List[float] = [0, -40, -3, 70.4, 40, 0.0],
                  prefix: Optional[str] = None,
-                 pklfile_prefix: str = None,
+                 pklfile_prefix: Optional[str] = None,
                  default_cam_key: str = 'CAM2',
                  format_only: bool = False,
-                 submission_prefix: str = None,
+                 submission_prefix: Optional[str] = None,
                  collect_device: str = 'cpu',
-                 file_client_args: dict = dict(backend='disk')):
+                 backend_args: Optional[dict] = None) -> None:
         self.default_prefix = 'Kitti metric'
         super(KittiMetric, self).__init__(
             collect_device=collect_device, prefix=prefix)
         self.pcd_limit_range = pcd_limit_range
         self.ann_file = ann_file
         self.pklfile_prefix = pklfile_prefix
         self.format_only = format_only
         if self.format_only:
-            assert submission_prefix is not None, 'submission_prefix must be'
-            'not None when format_only is True, otherwise the result files'
-            'will be saved to a temp directory which will be cleaned up at'
+            assert submission_prefix is not None, 'submission_prefix must be '
+            'not None when format_only is True, otherwise the result files '
+            'will be saved to a temp directory which will be cleaned up at '
             'the end.'
 
         self.submission_prefix = submission_prefix
-        self.pred_box_type_3d = pred_box_type_3d
-        self.default_cam_key = default_cam_key
-        self.file_client_args = file_client_args
         self.default_cam_key = default_cam_key
+        self.backend_args = backend_args
 
         allowed_metrics = ['bbox', 'img_bbox', 'mAP', 'LET_mAP']
         self.metrics = metric if isinstance(metric, list) else [metric]
         for metric in self.metrics:
             if metric not in allowed_metrics:
                 raise KeyError("metric should be one of 'bbox', 'img_bbox', "
-                               'but got {metric}.')
+                               f'but got {metric}.')
 
-    def convert_annos_to_kitti_annos(self, data_infos: dict) -> list:
+    def convert_annos_to_kitti_annos(self, data_infos: dict) -> List[dict]:
         """Convert loading annotations to Kitti annotations.
 
         Args:
             data_infos (dict): Data infos including metainfo and annotations
                 loaded from ann_file.
 
         Returns:
@@ -143,65 +141,63 @@
                         kitti_annos[name] = np.array(kitti_annos[name])
                 data_annos[i]['kitti_annos'] = kitti_annos
         return data_annos
 
     def process(self, data_batch: dict, data_samples: Sequence[dict]) -> None:
         """Process one batch of data samples and predictions.
 
-        The processed results should be stored in ``self.results``,
-        which will be used to compute the metrics when all batches
-        have been processed.
+        The processed results should be stored in ``self.results``, which will
+        be used to compute the metrics when all batches have been processed.
 
         Args:
             data_batch (dict): A batch of data from the dataloader.
-            data_samples (Sequence[dict]): A batch of outputs from
-                the model.
+            data_samples (Sequence[dict]): A batch of outputs from the model.
         """
 
         for data_sample in data_samples:
             result = dict()
             pred_3d = data_sample['pred_instances_3d']
             pred_2d = data_sample['pred_instances']
             for attr_name in pred_3d:
                 pred_3d[attr_name] = pred_3d[attr_name].to('cpu')
             result['pred_instances_3d'] = pred_3d
             for attr_name in pred_2d:
                 pred_2d[attr_name] = pred_2d[attr_name].to('cpu')
             result['pred_instances'] = pred_2d
             sample_idx = data_sample['sample_idx']
             result['sample_idx'] = sample_idx
-        self.results.append(result)
+            self.results.append(result)
 
-    def compute_metrics(self, results: list) -> Dict[str, float]:
+    def compute_metrics(self, results: List[dict]) -> Dict[str, float]:
         """Compute the metrics from processed results.
 
         Args:
-            results (list): The processed results of the whole dataset.
+            results (List[dict]): The processed results of the whole dataset.
 
         Returns:
             Dict[str, float]: The computed metrics. The keys are the names of
             the metrics, and the values are corresponding results.
         """
         logger: MMLogger = MMLogger.get_current_instance()
         self.classes = self.dataset_meta['classes']
 
         # load annotations
-        pkl_infos = load(self.ann_file, file_client_args=self.file_client_args)
+        pkl_infos = load(self.ann_file, backend_args=self.backend_args)
         self.data_infos = self.convert_annos_to_kitti_annos(pkl_infos)
         result_dict, tmp_dir = self.format_results(
             results,
             pklfile_prefix=self.pklfile_prefix,
             submission_prefix=self.submission_prefix,
             classes=self.classes)
 
         metric_dict = {}
 
         if self.format_only:
-            logger.info('results are saved in '
-                        f'{osp.dirname(self.submission_prefix)}')
+            logger.info(
+                f'results are saved in {osp.dirname(self.submission_prefix)}')
             return metric_dict
 
         gt_annos = [
             self.data_infos[result['sample_idx']]['kitti_annos']
             for result in results
         ]
 
@@ -216,141 +212,141 @@
                 metric_dict[result] = ap_dict[result]
 
         if tmp_dir is not None:
             tmp_dir.cleanup()
         return metric_dict
 
     def kitti_evaluate(self,
-                       results_dict: List[dict],
+                       results_dict: dict,
                        gt_annos: List[dict],
-                       metric: str = None,
-                       classes: List[str] = None,
-                       logger: MMLogger = None) -> dict:
+                       metric: Optional[str] = None,
+                       classes: Optional[List[str]] = None,
+                       logger: Optional[MMLogger] = None) -> Dict[str, float]:
         """Evaluation in KITTI protocol.
 
         Args:
             results_dict (dict): Formatted results of the dataset.
-            gt_annos (list[dict]): Contain gt information of each sample.
-            metric (str, optional): Metrics to be evaluated.
-                Default: None.
-            logger (MMLogger, optional): Logger used for printing
-                related information during evaluation. Default: None.
-            classes (list[String], optional): A list of class name. Defaults
-                to None.
+            gt_annos (List[dict]): Contain gt information of each sample.
+            metric (str, optional): Metrics to be evaluated. Defaults to None.
+            classes (List[str], optional): A list of class name.
+                Defaults to None.
+            logger (MMLogger, optional): Logger used for printing related
+                information during evaluation. Defaults to None.
 
         Returns:
-            dict[str, float]: Results of each evaluation metric.
+            Dict[str, float]: Results of each evaluation metric.
         """
         ap_dict = dict()
         for name in results_dict:
             if name == 'pred_instances' or metric == 'img_bbox':
                 eval_types = ['bbox']
             else:
                 eval_types = ['bbox', 'bev', '3d']
             ap_result_str, ap_dict_ = kitti_eval(
                 gt_annos, results_dict[name], classes, eval_types=eval_types)
             for ap_type, ap in ap_dict_.items():
-                ap_dict[f'{name}/{ap_type}'] = float('{:.4f}'.format(ap))
+                ap_dict[f'{name}/{ap_type}'] = float(f'{ap:.4f}')
 
             print_log(f'Results of {name}:\n' + ap_result_str, logger=logger)
 
         return ap_dict
 
-    def format_results(self,
-                       results: List[dict],
-                       pklfile_prefix: str = None,
-                       submission_prefix: str = None,
-                       classes: List[str] = None):
+    def format_results(
+        self,
+        results: List[dict],
+        pklfile_prefix: Optional[str] = None,
+        submission_prefix: Optional[str] = None,
+        classes: Optional[List[str]] = None
+    ) -> Tuple[dict, Union[tempfile.TemporaryDirectory, None]]:
         """Format the results to pkl file.
 
         Args:
-            results (list[dict]): Testing results of the
-                dataset.
+            results (List[dict]): Testing results of the dataset.
             pklfile_prefix (str, optional): The prefix of pkl files. It
                 includes the file path and the prefix of filename, e.g.,
                 "a/b/prefix". If not specified, a temp file will be created.
-                Default: None.
+                Defaults to None.
             submission_prefix (str, optional): The prefix of submitted files.
                 It includes the file path and the prefix of filename, e.g.,
                 "a/b/prefix". If not specified, a temp file will be created.
-                Default: None.
-            classes (list[String], optional): A list of class name. Defaults
-                to None.
+                Defaults to None.
+            classes (List[str], optional): A list of class name.
+                Defaults to None.
 
         Returns:
-            tuple: (result_dict, tmp_dir), result_dict is a dict containing
-                the formatted result, tmp_dir is the temporal directory created
-                for saving json files when jsonfile_prefix is not specified.
+            tuple: (result_dict, tmp_dir), result_dict is a dict containing the
+            formatted result, tmp_dir is the temporal directory created for
+            saving json files when jsonfile_prefix is not specified.
         """
         if pklfile_prefix is None:
             tmp_dir = tempfile.TemporaryDirectory()
             pklfile_prefix = osp.join(tmp_dir.name, 'results')
         else:
             tmp_dir = None
         result_dict = dict()
-        sample_id_list = [result['sample_idx'] for result in results]
+        sample_idx_list = [result['sample_idx'] for result in results]
         for name in results[0]:
             if submission_prefix is not None:
                 submission_prefix_ = osp.join(submission_prefix, name)
             else:
                 submission_prefix_ = None
             if pklfile_prefix is not None:
                 pklfile_prefix_ = osp.join(pklfile_prefix, name) + '.pkl'
             else:
                 pklfile_prefix_ = None
             if 'pred_instances' in name and '3d' in name and name[
                     0] != '_' and results[0][name]:
                 net_outputs = [result[name] for result in results]
                 result_list_ = self.bbox2result_kitti(net_outputs,
-                                                      sample_id_list, classes,
+                                                      sample_idx_list, classes,
                                                       pklfile_prefix_,
                                                       submission_prefix_)
                 result_dict[name] = result_list_
             elif name == 'pred_instances' and name[0] != '_' and results[0][
                     name]:
                 net_outputs = [result[name] for result in results]
                 result_list_ = self.bbox2result_kitti2d(
-                    net_outputs, sample_id_list, classes, pklfile_prefix_,
+                    net_outputs, sample_idx_list, classes, pklfile_prefix_,
                     submission_prefix_)
                 result_dict[name] = result_list_
         return result_dict, tmp_dir
 
-    def bbox2result_kitti(self,
-                          net_outputs: list,
-                          sample_id_list: list,
-                          class_names: list,
-                          pklfile_prefix: str = None,
-                          submission_prefix: str = None):
+    def bbox2result_kitti(
+            self,
+            net_outputs: List[dict],
+            sample_idx_list: List[int],
+            class_names: List[str],
+            pklfile_prefix: Optional[str] = None,
+            submission_prefix: Optional[str] = None) -> List[dict]:
         """Convert 3D detection results to kitti format for evaluation and test
         submission.
 
         Args:
-            net_outputs (list[dict]): List of array storing the
-                inferenced bounding boxes and scores.
-            sample_id_list (list[int]): List of input sample id.
-            class_names (list[String]): A list of class names.
+            net_outputs (List[dict]): List of dict storing the inferenced
+                bounding boxes and scores.
+            sample_idx_list (List[int]): List of input sample idx.
+            class_names (List[str]): A list of class names.
             pklfile_prefix (str, optional): The prefix of pkl file.
                 Defaults to None.
             submission_prefix (str, optional): The prefix of submission file.
                 Defaults to None.
 
         Returns:
-            list[dict]: A list of dictionaries with the kitti format.
+            List[dict]: A list of dictionaries with the kitti format.
         """
         assert len(net_outputs) == len(self.data_infos), \
             'invalid list length of network outputs'
         if submission_prefix is not None:
             mmengine.mkdir_or_exist(submission_prefix)
 
         det_annos = []
         print('\nConverting 3D prediction to KITTI format')
         for idx, pred_dicts in enumerate(
                 mmengine.track_iter_progress(net_outputs)):
-            annos = []
-            sample_idx = sample_id_list[idx]
+            sample_idx = sample_idx_list[idx]
             info = self.data_infos[sample_idx]
             # Here default used 'CAM2' to compute metric. If you want to
             # use another camera, please modify it.
             image_shape = (info['images'][self.default_cam_key]['height'],
                            info['images'][self.default_cam_key]['width'])
             box_dict = self.convert_valid_bboxes(pred_dicts, info)
             anno = {
@@ -389,28 +385,26 @@
                     anno['bbox'].append(bbox)
                     anno['dimensions'].append(box[3:6])
                     anno['location'].append(box[:3])
                     anno['rotation_y'].append(box[6])
                     anno['score'].append(score)
 
                 anno = {k: np.stack(v) for k, v in anno.items()}
-                annos.append(anno)
             else:
                 anno = {
                     'name': np.array([]),
                     'truncated': np.array([]),
                     'occluded': np.array([]),
                     'alpha': np.array([]),
                     'bbox': np.zeros([0, 4]),
                     'dimensions': np.zeros([0, 3]),
                     'location': np.zeros([0, 3]),
                     'rotation_y': np.array([]),
                     'score': np.array([]),
                 }
-                annos.append(anno)
 
             if submission_prefix is not None:
                 curr_file = f'{submission_prefix}/{sample_idx:06d}.txt'
                 with open(curr_file, 'w') as f:
                     bbox = anno['bbox']
                     loc = anno['location']
                     dims = anno['dimensions']  # lhw -> hwl
@@ -424,69 +418,69 @@
                                 bbox[idx][0], bbox[idx][1], bbox[idx][2],
                                 bbox[idx][3], dims[idx][1], dims[idx][2],
                                 dims[idx][0], loc[idx][0], loc[idx][1],
                                 loc[idx][2], anno['rotation_y'][idx],
                                 anno['score'][idx]),
                             file=f)
 
-            annos[-1]['sample_id'] = np.array(
-                [sample_idx] * len(annos[-1]['score']), dtype=np.int64)
+            anno['sample_idx'] = np.array(
+                [sample_idx] * len(anno['score']), dtype=np.int64)
 
-            det_annos += annos
+            det_annos.append(anno)
 
         if pklfile_prefix is not None:
             if not pklfile_prefix.endswith(('.pkl', '.pickle')):
                 out = f'{pklfile_prefix}.pkl'
             else:
                 out = pklfile_prefix
             mmengine.dump(det_annos, out)
             print(f'Result is saved to {out}.')
 
         return det_annos
 
-    def bbox2result_kitti2d(self,
-                            net_outputs: list,
-                            sample_id_list,
-                            class_names: list,
-                            pklfile_prefix: str = None,
-                            submission_prefix: str = None):
+    def bbox2result_kitti2d(
+            self,
+            net_outputs: List[dict],
+            sample_idx_list: List[int],
+            class_names: List[str],
+            pklfile_prefix: Optional[str] = None,
+            submission_prefix: Optional[str] = None) -> List[dict]:
         """Convert 2D detection results to kitti format for evaluation and test
         submission.
 
         Args:
-            net_outputs (list[dict]): List of array storing the
-                inferenced bounding boxes and scores.
-            sample_id_list (list[int]): List of input sample id.
-            class_names (list[String]): A list of class names.
+            net_outputs (List[dict]): List of dict storing the inferenced
+                bounding boxes and scores.
+            sample_idx_list (List[int]): List of input sample idx.
+            class_names (List[str]): A list of class names.
             pklfile_prefix (str, optional): The prefix of pkl file.
                 Defaults to None.
             submission_prefix (str, optional): The prefix of submission file.
                 Defaults to None.
 
         Returns:
-            list[dict]: A list of dictionaries have the kitti format
+            List[dict]: A list of dictionaries with the kitti format.
         """
         assert len(net_outputs) == len(self.data_infos), \
             'invalid list length of network outputs'
         det_annos = []
         print('\nConverting 2D prediction to KITTI format')
         for i, bboxes_per_sample in enumerate(
                 mmengine.track_iter_progress(net_outputs)):
-            annos = []
             anno = dict(
                 name=[],
                 truncated=[],
                 occluded=[],
                 alpha=[],
                 bbox=[],
                 dimensions=[],
                 location=[],
                 rotation_y=[],
                 score=[])
-            sample_idx = sample_id_list[i]
+            sample_idx = sample_idx_list[i]
 
             num_example = 0
             bbox = bboxes_per_sample['bboxes']
             for i in range(bbox.shape[0]):
                 anno['name'].append(class_names[int(
                     bboxes_per_sample['labels'][i])])
                 anno['truncated'].append(0.0)
@@ -500,48 +494,46 @@
                 anno['location'].append(
                     np.ones(shape=[3], dtype=np.float32) * (-1000.0))
                 anno['rotation_y'].append(0.0)
                 anno['score'].append(bboxes_per_sample['scores'][i])
                 num_example += 1
 
             if num_example == 0:
-                annos.append(
-                    dict(
-                        name=np.array([]),
-                        truncated=np.array([]),
-                        occluded=np.array([]),
-                        alpha=np.array([]),
-                        bbox=np.zeros([0, 4]),
-                        dimensions=np.zeros([0, 3]),
-                        location=np.zeros([0, 3]),
-                        rotation_y=np.array([]),
-                        score=np.array([]),
-                    ))
+                anno = dict(
+                    name=np.array([]),
+                    truncated=np.array([]),
+                    occluded=np.array([]),
+                    alpha=np.array([]),
+                    bbox=np.zeros([0, 4]),
+                    dimensions=np.zeros([0, 3]),
+                    location=np.zeros([0, 3]),
+                    rotation_y=np.array([]),
+                    score=np.array([]),
+                )
             else:
                 anno = {k: np.stack(v) for k, v in anno.items()}
-                annos.append(anno)
 
-            annos[-1]['sample_id'] = np.array(
+            anno['sample_idx'] = np.array(
                 [sample_idx] * num_example, dtype=np.int64)
-            det_annos += annos
+            det_annos.append(anno)
 
         if pklfile_prefix is not None:
             if not pklfile_prefix.endswith(('.pkl', '.pickle')):
                 out = f'{pklfile_prefix}.pkl'
             else:
                 out = pklfile_prefix
             mmengine.dump(det_annos, out)
             print(f'Result is saved to {out}.')
 
         if submission_prefix is not None:
             # save file in submission format
             mmengine.mkdir_or_exist(submission_prefix)
             print(f'Saving KITTI submission to {submission_prefix}')
             for i, anno in enumerate(det_annos):
-                sample_idx = sample_id_list[i]
+                sample_idx = sample_idx_list[i]
                 cur_det_file = f'{submission_prefix}/{sample_idx:06d}.txt'
                 with open(cur_det_file, 'w') as f:
                     bbox = anno['bbox']
                     loc = anno['location']
                     dims = anno['dimensions'][::-1]  # lhw -> hwl
                     for idx in range(len(bbox)):
                         print(
@@ -556,36 +548,36 @@
                                 anno['score'][idx]),
                             file=f,
                         )
             print(f'Result is saved to {submission_prefix}')
 
         return det_annos
 
-    def convert_valid_bboxes(self, box_dict: dict, info: dict):
+    def convert_valid_bboxes(self, box_dict: dict, info: dict) -> dict:
         """Convert the predicted boxes into valid ones.
 
         Args:
             box_dict (dict): Box dictionaries to be converted.
 
-                - boxes_3d (:obj:`LiDARInstance3DBoxes`): 3D bounding boxes.
-                - scores_3d (torch.Tensor): Scores of boxes.
-                - labels_3d (torch.Tensor): Class labels of boxes.
+                - bboxes_3d (:obj:`BaseInstance3DBoxes`): 3D bounding boxes.
+                - scores_3d (Tensor): Scores of boxes.
+                - labels_3d (Tensor): Class labels of boxes.
             info (dict): Data info.
 
         Returns:
             dict: Valid predicted boxes.
 
-                - bbox (np.ndarray): 2D bounding boxes.
-                - box3d_camera (np.ndarray): 3D bounding boxes in
-                    camera coordinate.
-                - box3d_lidar (np.ndarray): 3D bounding boxes in
-                    LiDAR coordinate.
-                - scores (np.ndarray): Scores of boxes.
-                - label_preds (np.ndarray): Class label predictions.
-                - sample_idx (int): Sample index.
+            - bbox (np.ndarray): 2D bounding boxes.
+            - box3d_camera (np.ndarray): 3D bounding boxes in
+              camera coordinate.
+            - box3d_lidar (np.ndarray): 3D bounding boxes in
+              LiDAR coordinate.
+            - scores (np.ndarray): Scores of boxes.
+            - label_preds (np.ndarray): Class label predictions.
+            - sample_idx (int): Sample index.
         """
         # TODO: refactor this function
         box_preds = box_dict['bboxes_3d']
         scores = box_dict['scores_3d']
         labels = box_dict['labels_3d']
         sample_idx = info['sample_idx']
         box_preds.limit_yaw(offset=0.5, period=np.pi * 2)
@@ -638,21 +630,21 @@
         else:
             valid_inds = valid_cam_inds
 
         if valid_inds.sum() > 0:
             return dict(
                 bbox=box_2d_preds[valid_inds, :].numpy(),
                 pred_box_type_3d=type(box_preds),
-                box3d_camera=box_preds_camera[valid_inds].tensor.numpy(),
-                box3d_lidar=box_preds_lidar[valid_inds].tensor.numpy(),
+                box3d_camera=box_preds_camera[valid_inds].numpy(),
+                box3d_lidar=box_preds_lidar[valid_inds].numpy(),
                 scores=scores[valid_inds].numpy(),
                 label_preds=labels[valid_inds].numpy(),
                 sample_idx=sample_idx)
         else:
             return dict(
                 bbox=np.zeros([0, 4]),
                 pred_box_type_3d=type(box_preds),
                 box3d_camera=np.zeros([0, 7]),
                 box3d_lidar=np.zeros([0, 7]),
                 scores=np.zeros([0]),
-                label_preds=np.zeros([0, 4]),
+                label_preds=np.zeros([0]),
                 sample_idx=sample_idx)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/evaluation/metrics/lyft_metric.py` & `mmdet3d-1.1.1/mmdet3d/evaluation/metrics/lyft_metric.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,8 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-import logging
 import os
 import tempfile
 from os import path as osp
 from typing import Dict, List, Optional, Sequence, Tuple, Union
 
 import mmengine
 import numpy as np
@@ -22,164 +21,183 @@
 @METRICS.register_module()
 class LyftMetric(BaseMetric):
     """Lyft evaluation metric.
 
     Args:
         data_root (str): Path of dataset root.
         ann_file (str): Path of annotation file.
-        metric (str | list[str]): Metrics to be evaluated.
-            Default to 'bbox'.
-        modality (dict): Modality to specify the sensor data used
-            as input. Defaults to dict(use_camera=False, use_lidar=True).
+        metric (str or List[str]): Metrics to be evaluated. Defaults to 'bbox'.
+        modality (dict): Modality to specify the sensor data used as input.
+            Defaults to dict(use_camera=False, use_lidar=True).
         prefix (str, optional): The prefix that will be added in the metric
             names to disambiguate homonymous metrics of different evaluators.
-            If prefix is not provided in the argument, self.default_prefix
-            will be used instead. Defaults to None.
-        jsonfile_prefix (str, optional): The prefix of json files including
-            the file path and the prefix of filename, e.g., "a/b/prefix".
-            If not specified, a temp file will be created. Default: None.
-        csv_savepath (str, optional): The path for saving csv files.
-            It includes the file path and the csv filename,
-            e.g., "a/b/filename.csv". If not specified,
-            the result will not be converted to csv file.
-        collect_device (str): Device name used for collecting results
-            from different ranks during distributed training. Must be 'cpu' or
+            If prefix is not provided in the argument, self.default_prefix will
+            be used instead. Defaults to None.
+        jsonfile_prefix (str, optional): The prefix of json files including the
+            file path and the prefix of filename, e.g., "a/b/prefix". If not
+            specified, a temp file will be created. Defaults to None.
+        format_only (bool): Format the output results without perform
+            evaluation. It is useful when you want to format the result to a
+            specific format and submit it to the test server.
+            Defaults to False.
+        csv_savepath (str, optional): The path for saving csv files. It
+            includes the file path and the csv filename, e.g.,
+            "a/b/filename.csv". If not specified, the result will not be
+            converted to csv file. Defaults to None.
+        collect_device (str): Device name used for collecting results from
+            different ranks during distributed training. Must be 'cpu' or
             'gpu'. Defaults to 'cpu'.
+        backend_args (dict, optional): Arguments to instantiate the
+            corresponding backend. Defaults to None.
     """
 
-    def __init__(
-        self,
-        data_root: str,
-        ann_file: str,
-        metric: Union[str, List[str]] = 'bbox',
-        modality=dict(
-            use_camera=False,
-            use_lidar=True,
-        ),
-        prefix: Optional[str] = None,
-        jsonfile_prefix: str = None,
-        csv_savepath: str = None,
-        collect_device: str = 'cpu',
-        file_client_args: dict = dict(backend='disk')
-    ) -> None:
+    def __init__(self,
+                 data_root: str,
+                 ann_file: str,
+                 metric: Union[str, List[str]] = 'bbox',
+                 modality=dict(
+                     use_camera=False,
+                     use_lidar=True,
+                 ),
+                 prefix: Optional[str] = None,
+                 jsonfile_prefix: str = None,
+                 format_only: bool = False,
+                 csv_savepath: str = None,
+                 collect_device: str = 'cpu',
+                 backend_args: Optional[dict] = None) -> None:
         self.default_prefix = 'Lyft metric'
         super(LyftMetric, self).__init__(
             collect_device=collect_device, prefix=prefix)
         self.ann_file = ann_file
         self.data_root = data_root
         self.modality = modality
         self.jsonfile_prefix = jsonfile_prefix
-        self.file_client_args = file_client_args
+        self.format_only = format_only
+        if self.format_only:
+            assert csv_savepath is not None, 'csv_savepath must be not None '
+            'when format_only is True, otherwise the result files will be '
+            'saved to a temp directory which will be cleaned up at the end.'
+
+        self.backend_args = backend_args
         self.csv_savepath = csv_savepath
         self.metrics = metric if isinstance(metric, list) else [metric]
 
     def process(self, data_batch: dict, data_samples: Sequence[dict]) -> None:
         """Process one batch of data samples and data_samples.
 
-        The processed results should be stored in ``self.results``,
-        which will be used to compute the metrics when all batches
-        have been processed.
+        The processed results should be stored in ``self.results``, which will
+        be used to compute the metrics when all batches have been processed.
 
         Args:
             data_batch (dict): A batch of data from the dataloader.
-            data_samples (Sequence[dict]): A batch of outputs from
-                the model.
+            data_samples (Sequence[dict]): A batch of outputs from the model.
         """
         for data_sample in data_samples:
             result = dict()
             pred_3d = data_sample['pred_instances_3d']
             pred_2d = data_sample['pred_instances']
             for attr_name in pred_3d:
                 pred_3d[attr_name] = pred_3d[attr_name].to('cpu')
             result['pred_instances_3d'] = pred_3d
             for attr_name in pred_2d:
                 pred_2d[attr_name] = pred_2d[attr_name].to('cpu')
             result['pred_instances'] = pred_2d
             sample_idx = data_sample['sample_idx']
             result['sample_idx'] = sample_idx
-        self.results.append(result)
+            self.results.append(result)
 
-    def compute_metrics(self, results: list) -> Dict[str, float]:
+    def compute_metrics(self, results: List[dict]) -> Dict[str, float]:
         """Compute the metrics from processed results.
 
         Args:
-            results (list): The processed results of each batch.
+            results (List[dict]): The processed results of the whole dataset.
 
         Returns:
             Dict[str, float]: The computed metrics. The keys are the names of
             the metrics, and the values are corresponding results.
         """
         logger: MMLogger = MMLogger.get_current_instance()
 
         classes = self.dataset_meta['classes']
         self.version = self.dataset_meta['version']
-        # load annotations
 
+        # load annotations
         self.data_infos = load(
-            self.ann_file, file_client_args=self.file_client_args)['data_list']
+            osp.join(self.data_root, self.ann_file),
+            backend_args=self.backend_args)['data_list']
         result_dict, tmp_dir = self.format_results(results, classes,
-                                                   self.jsonfile_prefix)
+                                                   self.jsonfile_prefix,
+                                                   self.csv_savepath)
 
         metric_dict = {}
+
+        if self.format_only:
+            logger.info(
+                f'results are saved in {osp.dirname(self.csv_savepath)}')
+            return metric_dict
+
         for metric in self.metrics:
             ap_dict = self.lyft_evaluate(
                 result_dict, metric=metric, logger=logger)
             for result in ap_dict:
                 metric_dict[result] = ap_dict[result]
 
         if tmp_dir is not None:
             tmp_dir.cleanup()
         return metric_dict
 
-    def format_results(self,
-                       results: List[dict],
-                       classes: List[str] = None,
-                       jsonfile_prefix: str = None,
-                       csv_savepath: str = None) -> Tuple:
+    def format_results(
+        self,
+        results: List[dict],
+        classes: Optional[List[str]] = None,
+        jsonfile_prefix: Optional[str] = None,
+        csv_savepath: Optional[str] = None
+    ) -> Tuple[dict, Union[tempfile.TemporaryDirectory, None]]:
         """Format the results to json (standard format for COCO evaluation).
 
         Args:
-            results (list[dict]): Testing results of the dataset.
-            classes (list[String], optional): A list of class name. Defaults
-                to None.
+            results (List[dict]): Testing results of the dataset.
+            classes (List[str], optional): A list of class name.
+                Defaults to None.
             jsonfile_prefix (str, optional): The prefix of json files. It
                 includes the file path and the prefix of filename, e.g.,
                 "a/b/prefix". If not specified, a temp file will be created.
-                Default: None.
-            csv_savepath (str, optional): The path for saving csv files.
-                It includes the file path and the csv filename,
-                e.g., "a/b/filename.csv". If not specified,
-                the result will not be converted to csv file.
+                Defaults to None.
+            csv_savepath (str, optional): The path for saving csv files. It
+                includes the file path and the csv filename, e.g.,
+                "a/b/filename.csv". If not specified, the result will not be
+                converted to csv file. Defaults to None.
 
         Returns:
-            tuple: Returns (result_dict, tmp_dir), where `result_dict` is a
-                dict containing the json filepaths, `tmp_dir` is the temporal
-                directory created for saving json files when
-                `jsonfile_prefix` is not specified.
+            tuple: Returns (result_dict, tmp_dir), where ``result_dict`` is a
+            dict containing the json filepaths, ``tmp_dir`` is the temporal
+            directory created for saving json files when ``jsonfile_prefix`` is
+            not specified.
         """
         assert isinstance(results, list), 'results must be a list'
 
         if jsonfile_prefix is None:
             tmp_dir = tempfile.TemporaryDirectory()
             jsonfile_prefix = osp.join(tmp_dir.name, 'results')
         else:
             tmp_dir = None
         result_dict = dict()
-        sample_id_list = [result['sample_idx'] for result in results]
+        sample_idx_list = [result['sample_idx'] for result in results]
 
         for name in results[0]:
             if 'pred' in name and '3d' in name and name[0] != '_':
                 print(f'\nFormating bboxes of {name}')
                 # format result of model output in Det3dDataSample,
                 # include 'pred_instances_3d','pts_pred_instances_3d',
                 # 'img_pred_instances_3d'
                 results_ = [out[name] for out in results]
                 tmp_file_ = osp.join(jsonfile_prefix, name)
-                result_dict[name] = self._format_bbox(results_, sample_id_list,
-                                                      classes, tmp_file_)
+                result_dict[name] = self._format_bbox(results_,
+                                                      sample_idx_list, classes,
+                                                      tmp_file_)
         if csv_savepath is not None:
             if 'pred_instances_3d' in result_dict:
                 self.json2csv(result_dict['pred_instances_3d'], csv_savepath)
             elif 'pts_pred_instances_3d' in result_dict:
                 self.json2csv(result_dict['pts_pred_instances_3d'],
                               csv_savepath)
         return result_dict, tmp_dir
@@ -219,40 +237,41 @@
             pred_list[idx] = prediction_str
         df = pd.DataFrame({'Id': Id_list, 'PredictionString': pred_list})
         mmengine.mkdir_or_exist(os.path.dirname(csv_savepath))
         df.to_csv(csv_savepath, index=False)
 
     def _format_bbox(self,
                      results: List[dict],
-                     sample_id_list: List[int],
-                     classes: List[str] = None,
-                     jsonfile_prefix: str = None) -> str:
+                     sample_idx_list: List[int],
+                     classes: Optional[List[str]] = None,
+                     jsonfile_prefix: Optional[str] = None) -> str:
         """Convert the results to the standard format.
 
         Args:
-            results (list[dict]): Testing results of the dataset.
-            sample_id_list (list[int]): List of result sample id.
-            classes (list[String], optional): A list of class name. Defaults
-                to None.
+            results (List[dict]): Testing results of the dataset.
+            sample_idx_list (List[int]): List of result sample idx.
+            classes (List[str], optional): A list of class name.
+                Defaults to None.
             jsonfile_prefix (str, optional): The prefix of the output jsonfile.
-                You can specify the output directory/filename by
-                modifying the jsonfile_prefix. Default: None.
+                You can specify the output directory/filename by modifying the
+                jsonfile_prefix. Defaults to None.
 
         Returns:
             str: Path of the output json file.
         """
         lyft_annos = {}
 
         print('Start to convert detection format...')
         for i, det in enumerate(mmengine.track_iter_progress(results)):
             annos = []
             boxes = output_to_lyft_box(det)
-            sample_id = sample_id_list[i]
-            sample_token = self.data_infos[sample_id]['token']
-            boxes = lidar_lyft_box_to_global(self.data_infos[sample_id], boxes)
+            sample_idx = sample_idx_list[i]
+            sample_token = self.data_infos[sample_idx]['token']
+            boxes = lidar_lyft_box_to_global(self.data_infos[sample_idx],
+                                             boxes)
             for i, box in enumerate(boxes):
                 name = classes[box.label]
                 lyft_anno = dict(
                     sample_token=sample_token,
                     translation=box.center.tolist(),
                     size=box.wlh.tolist(),
                     rotation=box.orientation.elements.tolist(),
@@ -270,54 +289,49 @@
         print('Results writes to', res_path)
         mmengine.dump(lyft_submissions, res_path)
         return res_path
 
     def lyft_evaluate(self,
                       result_dict: dict,
                       metric: str = 'bbox',
-                      logger: logging.Logger = None) -> dict:
+                      logger: Optional[MMLogger] = None) -> Dict[str, float]:
         """Evaluation in Lyft protocol.
 
         Args:
             result_dict (dict): Formatted results of the dataset.
-            metric (str): Metrics to be evaluated.
-                Default: 'bbox'.
-            classes (list[String], optional): A list of class name. Defaults
-                to None.
-            logger (MMLogger, optional): Logger used for printing
-                related information during evaluation. Default: None.
+            metric (str): Metrics to be evaluated. Defaults to 'bbox'.
+            logger (MMLogger, optional): Logger used for printing related
+                information during evaluation. Defaults to None.
 
         Returns:
-            dict[str, float]: Evaluation results.
+            Dict[str, float]: Evaluation results.
         """
         metric_dict = dict()
         for name in result_dict:
-            print('Evaluating bboxes of {}'.format(name))
+            print(f'Evaluating bboxes of {name}')
             ret_dict = self._evaluate_single(
                 result_dict[name], logger=logger, result_name=name)
-        metric_dict.update(ret_dict)
+            metric_dict.update(ret_dict)
         return metric_dict
 
     def _evaluate_single(self,
                          result_path: str,
                          logger: MMLogger = None,
                          result_name: str = 'pts_bbox') -> dict:
         """Evaluation for a single model in Lyft protocol.
 
         Args:
             result_path (str): Path of the result file.
-            logger (logging.Logger | str, optional): Logger used for printing
-                related information during evaluation. Default: None.
-            metric (str): Metric name used for evaluation.
-                Default: 'bbox'.
+            logger (MMLogger, optional): Logger used for printing related
+                information during evaluation. Defaults to None.
             result_name (str): Result name in the metric prefix.
-                Default: 'pts_bbox'.
+                Defaults to 'pts_bbox'.
 
         Returns:
-            dict: Dictionary of evaluation details.
+            Dict[str, float]: Dictionary of evaluation details.
         """
         output_dir = osp.join(*osp.split(result_path)[:-1])
         lyft = Lyft(
             data_path=osp.join(self.data_root, self.version),
             json_path=osp.join(self.data_root, self.version, self.version),
             verbose=True)
         eval_set_map = {
@@ -341,17 +355,17 @@
 def output_to_lyft_box(detection: dict) -> List[LyftBox]:
     """Convert the output to the box class in the Lyft.
 
     Args:
         detection (dict): Detection results.
 
     Returns:
-        list[:obj:`LyftBox`]: List of standard LyftBoxes.
+        List[:obj:`LyftBox`]: List of standard LyftBoxes.
     """
-    bbox3d = detection['bbox_3d']
+    bbox3d = detection['bboxes_3d']
     scores = detection['scores_3d'].numpy()
     labels = detection['labels_3d'].numpy()
 
     box_gravity_center = bbox3d.gravity_center.numpy()
     box_dims = bbox3d.dims.numpy()
     box_yaw = bbox3d.yaw.numpy()
 
@@ -372,21 +386,21 @@
 
 
 def lidar_lyft_box_to_global(info: dict,
                              boxes: List[LyftBox]) -> List[LyftBox]:
     """Convert the box from ego to global coordinate.
 
     Args:
-        info (dict): Info for a specific sample data, including the
-            calibration information.
-        boxes (list[:obj:`LyftBox`]): List of predicted LyftBoxes.
+        info (dict): Info for a specific sample data, including the calibration
+            information.
+        boxes (List[:obj:`LyftBox`]): List of predicted LyftBoxes.
 
     Returns:
-        list: List of standard LyftBoxes in the global
-            coordinate.
+        List[:obj:`LyftBox`]: List of standard LyftBoxes in the global
+        coordinate.
     """
     box_list = []
     for box in boxes:
         # Move box to ego vehicle coord system
         lidar2ego = np.array(info['lidar_points']['lidar2ego'])
         box.rotate(Quaternion(matrix=lidar2ego, rtol=1e-05, atol=1e-07))
         box.translate(lidar2ego[:3, 3])
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/evaluation/metrics/nuscenes_metric.py` & `mmdet3d-1.1.1/mmdet3d/evaluation/metrics/nuscenes_metric.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,8 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-import logging
 import tempfile
 from os import path as osp
 from typing import Dict, List, Optional, Sequence, Tuple, Union
 
 import mmengine
 import numpy as np
 import pyquaternion
@@ -24,30 +23,35 @@
 @METRICS.register_module()
 class NuScenesMetric(BaseMetric):
     """Nuscenes evaluation metric.
 
     Args:
         data_root (str): Path of dataset root.
         ann_file (str): Path of annotation file.
-        metric (str | list[str]): Metrics to be evaluated.
-            Default to 'bbox'.
-        modality (dict): Modality to specify the sensor data used
-            as input. Defaults to dict(use_camera=False, use_lidar=True).
+        metric (str or List[str]): Metrics to be evaluated. Defaults to 'bbox'.
+        modality (dict): Modality to specify the sensor data used as input.
+            Defaults to dict(use_camera=False, use_lidar=True).
         prefix (str, optional): The prefix that will be added in the metric
             names to disambiguate homonymous metrics of different evaluators.
-            If prefix is not provided in the argument, self.default_prefix
-            will be used instead. Defaults to None.
-        jsonfile_prefix (str, optional): The prefix of json files including
-            the file path and the prefix of filename, e.g., "a/b/prefix".
-            If not specified, a temp file will be created. Default: None.
+            If prefix is not provided in the argument, self.default_prefix will
+            be used instead. Defaults to None.
+        format_only (bool): Format the output results without perform
+            evaluation. It is useful when you want to format the result to a
+            specific format and submit it to the test server.
+            Defaults to False.
+        jsonfile_prefix (str, optional): The prefix of json files including the
+            file path and the prefix of filename, e.g., "a/b/prefix".
+            If not specified, a temp file will be created. Defaults to None.
         eval_version (str): Configuration version of evaluation.
-            Defaults to  'detection_cvpr_2019'.
-        collect_device (str): Device name used for collecting results
-            from different ranks during distributed training. Must be 'cpu' or
+            Defaults to 'detection_cvpr_2019'.
+        collect_device (str): Device name used for collecting results from
+            different ranks during distributed training. Must be 'cpu' or
             'gpu'. Defaults to 'cpu'.
+        backend_args (dict, optional): Arguments to instantiate the
+            corresponding backend. Defaults to None.
     """
     NameMapping = {
         'movable_object.barrier': 'barrier',
         'vehicle.bicycle': 'bicycle',
         'vehicle.bus.bendy': 'bus',
         'vehicle.bus.rigid': 'bus',
         'vehicle.car': 'car',
@@ -78,145 +82,153 @@
         'trans_err': 'mATE',
         'scale_err': 'mASE',
         'orient_err': 'mAOE',
         'vel_err': 'mAVE',
         'attr_err': 'mAAE'
     }
 
-    def __init__(
-        self,
-        data_root: str,
-        ann_file: str,
-        metric: Union[str, List[str]] = 'bbox',
-        modality: Dict = dict(use_camera=False, use_lidar=True),
-        prefix: Optional[str] = None,
-        jsonfile_prefix: Optional[str] = None,
-        eval_version: str = 'detection_cvpr_2019',
-        collect_device: str = 'cpu',
-        file_client_args: dict = dict(backend='disk')
-    ) -> None:
+    def __init__(self,
+                 data_root: str,
+                 ann_file: str,
+                 metric: Union[str, List[str]] = 'bbox',
+                 modality: dict = dict(use_camera=False, use_lidar=True),
+                 prefix: Optional[str] = None,
+                 format_only: bool = False,
+                 jsonfile_prefix: Optional[str] = None,
+                 eval_version: str = 'detection_cvpr_2019',
+                 collect_device: str = 'cpu',
+                 backend_args: Optional[dict] = None) -> None:
         self.default_prefix = 'NuScenes metric'
         super(NuScenesMetric, self).__init__(
             collect_device=collect_device, prefix=prefix)
         if modality is None:
             modality = dict(
                 use_camera=False,
                 use_lidar=True,
             )
         self.ann_file = ann_file
         self.data_root = data_root
         self.modality = modality
+        self.format_only = format_only
+        if self.format_only:
+            assert jsonfile_prefix is not None, 'jsonfile_prefix must be not '
+            'None when format_only is True, otherwise the result files will '
+            'be saved to a temp directory which will be cleanup at the end.'
+
         self.jsonfile_prefix = jsonfile_prefix
-        self.file_client_args = file_client_args
+        self.backend_args = backend_args
 
         self.metrics = metric if isinstance(metric, list) else [metric]
 
         self.eval_version = eval_version
         self.eval_detection_configs = config_factory(self.eval_version)
 
     def process(self, data_batch: dict, data_samples: Sequence[dict]) -> None:
         """Process one batch of data samples and predictions.
 
-        The processed results should be stored in ``self.results``,
-        which will be used to compute the metrics when all batches
-        have been processed.
+        The processed results should be stored in ``self.results``, which will
+        be used to compute the metrics when all batches have been processed.
 
         Args:
             data_batch (dict): A batch of data from the dataloader.
-            data_samples (Sequence[dict]): A batch of outputs from
-                the model.
+            data_samples (Sequence[dict]): A batch of outputs from the model.
         """
         for data_sample in data_samples:
             result = dict()
             pred_3d = data_sample['pred_instances_3d']
             pred_2d = data_sample['pred_instances']
             for attr_name in pred_3d:
                 pred_3d[attr_name] = pred_3d[attr_name].to('cpu')
             result['pred_instances_3d'] = pred_3d
             for attr_name in pred_2d:
                 pred_2d[attr_name] = pred_2d[attr_name].to('cpu')
             result['pred_instances'] = pred_2d
             sample_idx = data_sample['sample_idx']
             result['sample_idx'] = sample_idx
-        self.results.append(result)
+            self.results.append(result)
 
-    def compute_metrics(self, results: list) -> Dict[str, float]:
+    def compute_metrics(self, results: List[dict]) -> Dict[str, float]:
         """Compute the metrics from processed results.
 
         Args:
-            results (list): The processed results of each batch.
+            results (List[dict]): The processed results of each batch.
 
         Returns:
             Dict[str, float]: The computed metrics. The keys are the names of
             the metrics, and the values are corresponding results.
         """
         logger: MMLogger = MMLogger.get_current_instance()
 
         classes = self.dataset_meta['classes']
         self.version = self.dataset_meta['version']
         # load annotations
         self.data_infos = load(
-            self.ann_file, file_client_args=self.file_client_args)['data_list']
+            self.ann_file, backend_args=self.backend_args)['data_list']
         result_dict, tmp_dir = self.format_results(results, classes,
                                                    self.jsonfile_prefix)
 
         metric_dict = {}
+
+        if self.format_only:
+            logger.info(
+                f'results are saved in {osp.basename(self.jsonfile_prefix)}')
+            return metric_dict
+
         for metric in self.metrics:
             ap_dict = self.nus_evaluate(
                 result_dict, classes=classes, metric=metric, logger=logger)
             for result in ap_dict:
                 metric_dict[result] = ap_dict[result]
 
         if tmp_dir is not None:
             tmp_dir.cleanup()
         return metric_dict
 
     def nus_evaluate(self,
                      result_dict: dict,
                      metric: str = 'bbox',
-                     classes: List[str] = None,
-                     logger: logging.Logger = None) -> dict:
+                     classes: Optional[List[str]] = None,
+                     logger: Optional[MMLogger] = None) -> Dict[str, float]:
         """Evaluation in Nuscenes protocol.
 
         Args:
             result_dict (dict): Formatted results of the dataset.
-            metric (str): Metrics to be evaluated.
-                Default: None.
-            classes (list[String], optional): A list of class name. Defaults
-                to None.
-            logger (MMLogger, optional): Logger used for printing
-                related information during evaluation. Default: None.
+            metric (str): Metrics to be evaluated. Defaults to 'bbox'.
+            classes (List[str], optional): A list of class name.
+                Defaults to None.
+            logger (MMLogger, optional): Logger used for printing related
+                information during evaluation. Defaults to None.
 
         Returns:
-            dict[str, float]: Results of each evaluation metric.
+            Dict[str, float]: Results of each evaluation metric.
         """
         metric_dict = dict()
         for name in result_dict:
             print(f'Evaluating bboxes of {name}')
             ret_dict = self._evaluate_single(
                 result_dict[name], classes=classes, result_name=name)
-        metric_dict.update(ret_dict)
+            metric_dict.update(ret_dict)
         return metric_dict
 
-    def _evaluate_single(self,
-                         result_path: str,
-                         classes: List[None] = None,
-                         result_name: str = 'pred_instances_3d') -> dict:
+    def _evaluate_single(
+            self,
+            result_path: str,
+            classes: Optional[List[str]] = None,
+            result_name: str = 'pred_instances_3d') -> Dict[str, float]:
         """Evaluation for a single model in nuScenes protocol.
 
         Args:
             result_path (str): Path of the result file.
-                Default: 'bbox'.
-            classes (list[String], optional): A list of class name. Defaults
-                to None.
+            classes (List[str], optional): A list of class name.
+                Defaults to None.
             result_name (str): Result name in the metric prefix.
-                Default: 'pred_instances_3d'.
+                Defaults to 'pred_instances_3d'.
 
         Returns:
-            dict: Dictionary of evaluation details.
+            Dict[str, float]: Dictionary of evaluation details.
         """
         from nuscenes import NuScenes
         from nuscenes.eval.detection.evaluate import NuScenesEval
 
         output_dir = osp.join(*osp.split(result_path)[:-1])
         nusc = NuScenes(
             version=self.version, dataroot=self.data_root, verbose=False)
@@ -235,80 +247,82 @@
 
         # record metrics
         metrics = mmengine.load(osp.join(output_dir, 'metrics_summary.json'))
         detail = dict()
         metric_prefix = f'{result_name}_NuScenes'
         for name in classes:
             for k, v in metrics['label_aps'][name].items():
-                val = float('{:.4f}'.format(v))
+                val = float(f'{v:.4f}')
                 detail[f'{metric_prefix}/{name}_AP_dist_{k}'] = val
             for k, v in metrics['label_tp_errors'][name].items():
-                val = float('{:.4f}'.format(v))
+                val = float(f'{v:.4f}')
                 detail[f'{metric_prefix}/{name}_{k}'] = val
             for k, v in metrics['tp_errors'].items():
-                val = float('{:.4f}'.format(v))
+                val = float(f'{v:.4f}')
                 detail[f'{metric_prefix}/{self.ErrNameMapping[k]}'] = val
 
         detail[f'{metric_prefix}/NDS'] = metrics['nd_score']
         detail[f'{metric_prefix}/mAP'] = metrics['mean_ap']
         return detail
 
-    def format_results(self,
-                       results: List[dict],
-                       classes: List[str] = None,
-                       jsonfile_prefix: str = None) -> Tuple:
+    def format_results(
+        self,
+        results: List[dict],
+        classes: Optional[List[str]] = None,
+        jsonfile_prefix: Optional[str] = None
+    ) -> Tuple[dict, Union[tempfile.TemporaryDirectory, None]]:
         """Format the mmdet3d results to standard NuScenes json file.
 
         Args:
-            results (list[dict]): Testing results of the dataset.
-            classes (list[String], optional): A list of class name. Defaults
-                to None.
+            results (List[dict]): Testing results of the dataset.
+            classes (List[str], optional): A list of class name.
+                Defaults to None.
             jsonfile_prefix (str, optional): The prefix of json files. It
                 includes the file path and the prefix of filename, e.g.,
                 "a/b/prefix". If not specified, a temp file will be created.
-                Default: None.
+                Defaults to None.
 
         Returns:
-            tuple: Returns (result_dict, tmp_dir), where `result_dict` is a
-                dict containing the json filepaths, `tmp_dir` is the temporal
-                directory created for saving json files when
-                `jsonfile_prefix` is not specified.
+            tuple: Returns (result_dict, tmp_dir), where ``result_dict`` is a
+            dict containing the json filepaths, ``tmp_dir`` is the temporal
+            directory created for saving json files when ``jsonfile_prefix`` is
+            not specified.
         """
         assert isinstance(results, list), 'results must be a list'
 
         if jsonfile_prefix is None:
             tmp_dir = tempfile.TemporaryDirectory()
             jsonfile_prefix = osp.join(tmp_dir.name, 'results')
         else:
             tmp_dir = None
         result_dict = dict()
-        sample_id_list = [result['sample_idx'] for result in results]
+        sample_idx_list = [result['sample_idx'] for result in results]
 
         for name in results[0]:
             if 'pred' in name and '3d' in name and name[0] != '_':
                 print(f'\nFormating bboxes of {name}')
                 results_ = [out[name] for out in results]
                 tmp_file_ = osp.join(jsonfile_prefix, name)
                 box_type_3d = type(results_[0]['bboxes_3d'])
                 if box_type_3d == LiDARInstance3DBoxes:
                     result_dict[name] = self._format_lidar_bbox(
-                        results_, sample_id_list, classes, tmp_file_)
+                        results_, sample_idx_list, classes, tmp_file_)
                 elif box_type_3d == CameraInstance3DBoxes:
                     result_dict[name] = self._format_camera_bbox(
-                        results_, sample_id_list, classes, tmp_file_)
+                        results_, sample_idx_list, classes, tmp_file_)
 
         return result_dict, tmp_dir
 
-    def get_attr_name(self, attr_idx, label_name):
+    def get_attr_name(self, attr_idx: int, label_name: str) -> str:
         """Get attribute from predicted index.
 
         This is a workaround to predict attribute when the predicted velocity
-        is not reliable. We map the predicted attribute index to the one
-        in the attribute set. If it is consistent with the category, we will
-        keep it. Otherwise, we will use the default attribute.
+        is not reliable. We map the predicted attribute index to the one in the
+        attribute set. If it is consistent with the category, we will keep it.
+        Otherwise, we will use the default attribute.
 
         Args:
             attr_idx (int): Attribute index.
             label_name (str): Predicted category name.
 
         Returns:
             str: Predicted attribute name.
@@ -343,24 +357,27 @@
             else:
                 return self.DefaultAttribute[label_name]
         else:
             return self.DefaultAttribute[label_name]
 
     def _format_camera_bbox(self,
                             results: List[dict],
-                            sample_id_list: List[int],
-                            classes: List[str] = None,
-                            jsonfile_prefix: str = None) -> str:
+                            sample_idx_list: List[int],
+                            classes: Optional[List[str]] = None,
+                            jsonfile_prefix: Optional[str] = None) -> str:
         """Convert the results to the standard format.
 
         Args:
-            results (list[dict]): Testing results of the dataset.
-            jsonfile_prefix (str): The prefix of the output jsonfile.
-                You can specify the output directory/filename by
-                modifying the jsonfile_prefix. Default: None.
+            results (List[dict]): Testing results of the dataset.
+            sample_idx_list (List[int]): List of result sample idx.
+            classes (List[str], optional): A list of class name.
+                Defaults to None.
+            jsonfile_prefix (str, optional): The prefix of the output jsonfile.
+                You can specify the output directory/filename by modifying the
+                jsonfile_prefix. Defaults to None.
 
         Returns:
             str: Path of the output json file.
         """
         nusc_annos = {}
 
         print('Start to convert detection format...')
@@ -375,37 +392,37 @@
             'CAM_BACK_RIGHT',
         ]
 
         CAM_NUM = 6
 
         for i, det in enumerate(mmengine.track_iter_progress(results)):
 
-            sample_id = sample_id_list[i]
+            sample_idx = sample_idx_list[i]
 
-            frame_sample_id = sample_id // CAM_NUM
-            camera_type_id = sample_id % CAM_NUM
+            frame_sample_idx = sample_idx // CAM_NUM
+            camera_type_id = sample_idx % CAM_NUM
 
             if camera_type_id == 0:
                 boxes_per_frame = []
                 attrs_per_frame = []
 
             # need to merge results from images of the same sample
             annos = []
             boxes, attrs = output_to_nusc_box(det)
-            sample_token = self.data_infos[frame_sample_id]['token']
+            sample_token = self.data_infos[frame_sample_idx]['token']
             camera_type = camera_types[camera_type_id]
             boxes, attrs = cam_nusc_box_to_global(
-                self.data_infos[frame_sample_id], boxes, attrs, classes,
+                self.data_infos[frame_sample_idx], boxes, attrs, classes,
                 self.eval_detection_configs, camera_type)
             boxes_per_frame.extend(boxes)
             attrs_per_frame.extend(attrs)
             # Remove redundant predictions caused by overlap of images
-            if (sample_id + 1) % CAM_NUM != 0:
+            if (sample_idx + 1) % CAM_NUM != 0:
                 continue
-            boxes = global_nusc_box_to_cam(self.data_infos[frame_sample_id],
+            boxes = global_nusc_box_to_cam(self.data_infos[frame_sample_idx],
                                            boxes_per_frame, classes,
                                            self.eval_detection_configs)
             cam_boxes3d, scores, labels = nusc_box_to_cam_box3d(boxes)
             # box nms 3d over 6 images in a frame
             # TODO: move this global setting into config
             nms_cfg = dict(
                 use_rotate_nms=True,
@@ -428,15 +445,15 @@
                 nms_cfg.max_per_frame,
                 nms_cfg,
                 mlvl_attr_scores=attrs)
             cam_boxes3d = CameraInstance3DBoxes(boxes3d, box_dim=9)
             det = bbox3d2result(cam_boxes3d, scores, labels, attrs)
             boxes, attrs = output_to_nusc_box(det)
             boxes, attrs = cam_nusc_box_to_global(
-                self.data_infos[frame_sample_id], boxes, attrs, classes,
+                self.data_infos[frame_sample_idx], boxes, attrs, classes,
                 self.eval_detection_configs)
 
             for i, box in enumerate(boxes):
                 name = classes[box.label]
                 attr = self.get_attr_name(attrs[i], name)
                 nusc_anno = dict(
                     sample_token=sample_token,
@@ -457,47 +474,47 @@
         nusc_submissions = {
             'meta': self.modality,
             'results': nusc_annos,
         }
 
         mmengine.mkdir_or_exist(jsonfile_prefix)
         res_path = osp.join(jsonfile_prefix, 'results_nusc.json')
-        print('Results writes to', res_path)
+        print(f'Results writes to {res_path}')
         mmengine.dump(nusc_submissions, res_path)
         return res_path
 
     def _format_lidar_bbox(self,
                            results: List[dict],
-                           sample_id_list: List[int],
-                           classes: List[str] = None,
-                           jsonfile_prefix: str = None) -> str:
+                           sample_idx_list: List[int],
+                           classes: Optional[List[str]] = None,
+                           jsonfile_prefix: Optional[str] = None) -> str:
         """Convert the results to the standard format.
 
         Args:
-            results (list[dict]): Testing results of the dataset.
-            sample_id_list (list[int]): List of result sample id.
-            classes (list[String], optional): A list of class name. Defaults
-                to None.
+            results (List[dict]): Testing results of the dataset.
+            sample_idx_list (List[int]): List of result sample idx.
+            classes (List[str], optional): A list of class name.
+                Defaults to None.
             jsonfile_prefix (str, optional): The prefix of the output jsonfile.
-                You can specify the output directory/filename by
-                modifying the jsonfile_prefix. Default: None.
+                You can specify the output directory/filename by modifying the
+                jsonfile_prefix. Defaults to None.
 
         Returns:
             str: Path of the output json file.
         """
         nusc_annos = {}
 
         print('Start to convert detection format...')
         for i, det in enumerate(mmengine.track_iter_progress(results)):
             annos = []
             boxes, attrs = output_to_nusc_box(det)
-            sample_id = sample_id_list[i]
-            sample_token = self.data_infos[sample_id]['token']
-            boxes = lidar_nusc_box_to_global(self.data_infos[sample_id], boxes,
-                                             classes,
+            sample_idx = sample_idx_list[i]
+            sample_token = self.data_infos[sample_idx]['token']
+            boxes = lidar_nusc_box_to_global(self.data_infos[sample_idx],
+                                             boxes, classes,
                                              self.eval_detection_configs)
             for i, box in enumerate(boxes):
                 name = classes[box.label]
                 if np.sqrt(box.velocity[0]**2 + box.velocity[1]**2) > 0.2:
                     if name in [
                             'car',
                             'construction_vehicle',
@@ -531,46 +548,48 @@
             nusc_annos[sample_token] = annos
         nusc_submissions = {
             'meta': self.modality,
             'results': nusc_annos,
         }
         mmengine.mkdir_or_exist(jsonfile_prefix)
         res_path = osp.join(jsonfile_prefix, 'results_nusc.json')
-        print('Results writes to', res_path)
+        print(f'Results writes to {res_path}')
         mmengine.dump(nusc_submissions, res_path)
         return res_path
 
 
-def output_to_nusc_box(detection: dict) -> List[NuScenesBox]:
+def output_to_nusc_box(
+        detection: dict) -> Tuple[List[NuScenesBox], Union[np.ndarray, None]]:
     """Convert the output to the box class in the nuScenes.
 
     Args:
         detection (dict): Detection results.
 
             - bboxes_3d (:obj:`BaseInstance3DBoxes`): Detection bbox.
             - scores_3d (torch.Tensor): Detection scores.
             - labels_3d (torch.Tensor): Predicted box labels.
 
     Returns:
-        list[:obj:`NuScenesBox`]: List of standard NuScenesBoxes.
+        Tuple[List[:obj:`NuScenesBox`], np.ndarray or None]: List of standard
+        NuScenesBoxes and attribute labels.
     """
     bbox3d = detection['bboxes_3d']
     scores = detection['scores_3d'].numpy()
     labels = detection['labels_3d'].numpy()
     attrs = None
     if 'attr_labels' in detection:
         attrs = detection['attr_labels'].numpy()
 
     box_gravity_center = bbox3d.gravity_center.numpy()
     box_dims = bbox3d.dims.numpy()
     box_yaw = bbox3d.yaw.numpy()
 
     box_list = []
 
-    if type(bbox3d) == LiDARInstance3DBoxes:
+    if isinstance(bbox3d, LiDARInstance3DBoxes):
         # our LiDAR coordinate system -> nuScenes box coordinate system
         nus_box_dims = box_dims[:, [1, 0, 2]]
         for i in range(len(bbox3d)):
             quat = pyquaternion.Quaternion(axis=[0, 0, 1], radians=box_yaw[i])
             velocity = (*bbox3d.tensor[i, 7:9], 0.0)
             # velo_val = np.linalg.norm(box3d[i, 7:9])
             # velo_ori = box3d[i, 6]
@@ -580,15 +599,15 @@
                 box_gravity_center[i],
                 nus_box_dims[i],
                 quat,
                 label=labels[i],
                 score=scores[i],
                 velocity=velocity)
             box_list.append(box)
-    elif type(bbox3d) == CameraInstance3DBoxes:
+    elif isinstance(bbox3d, CameraInstance3DBoxes):
         # our Camera coordinate system -> nuScenes box coordinate system
         # convert the dim/rot to nuscbox convention
         nus_box_dims = box_dims[:, [2, 0, 1]]
         nus_box_yaw = -box_yaw
         for i in range(len(bbox3d)):
             q1 = pyquaternion.Quaternion(
                 axis=[0, 0, 1], radians=nus_box_yaw[i])
@@ -601,35 +620,35 @@
                 quat,
                 label=labels[i],
                 score=scores[i],
                 velocity=velocity)
             box_list.append(box)
     else:
         raise NotImplementedError(
-            f'Do not support convert {type(bbox3d)} bboxes'
+            f'Do not support convert {type(bbox3d)} bboxes '
             'to standard NuScenesBoxes.')
 
     return box_list, attrs
 
 
 def lidar_nusc_box_to_global(
         info: dict, boxes: List[NuScenesBox], classes: List[str],
         eval_configs: DetectionConfig) -> List[NuScenesBox]:
     """Convert the box from ego to global coordinate.
 
     Args:
-        info (dict): Info for a specific sample data, including the
-            calibration information.
-        boxes (list[:obj:`NuScenesBox`]): List of predicted NuScenesBoxes.
-        classes (list[str]): Mapped classes in the evaluation.
-        eval_configs (object): Evaluation configuration object.
+        info (dict): Info for a specific sample data, including the calibration
+            information.
+        boxes (List[:obj:`NuScenesBox`]): List of predicted NuScenesBoxes.
+        classes (List[str]): Mapped classes in the evaluation.
+        eval_configs (:obj:`DetectionConfig`): Evaluation configuration object.
 
     Returns:
-        list: List of standard NuScenesBoxes in the global
-            coordinate.
+        List[:obj:`DetectionConfig`]: List of standard NuScenesBoxes in the
+        global coordinate.
     """
     box_list = []
     for box in boxes:
         # Move box to ego vehicle coord system
         lidar2ego = np.array(info['lidar_points']['lidar2ego'])
         box.rotate(
             pyquaternion.Quaternion(matrix=lidar2ego, rtol=1e-05, atol=1e-07))
@@ -648,33 +667,33 @@
         box_list.append(box)
     return box_list
 
 
 def cam_nusc_box_to_global(
     info: dict,
     boxes: List[NuScenesBox],
-    attrs: List[str],
+    attrs: np.ndarray,
     classes: List[str],
     eval_configs: DetectionConfig,
     camera_type: str = 'CAM_FRONT',
-) -> List[NuScenesBox]:
+) -> Tuple[List[NuScenesBox], List[int]]:
     """Convert the box from camera to global coordinate.
 
     Args:
-        info (dict): Info for a specific sample data, including the
-            calibration information.
-        boxes (list[:obj:`NuScenesBox`]): List of predicted NuScenesBoxes.
-        attrs (list[str]): List of attributes.
-        camera_type (str): Type of camera.
-        classes (list[str]): Mapped classes in the evaluation.
-        eval_configs (object): Evaluation configuration object.
+        info (dict): Info for a specific sample data, including the calibration
+            information.
+        boxes (List[:obj:`NuScenesBox`]): List of predicted NuScenesBoxes.
+        attrs (np.ndarray): Predicted attributes.
+        classes (List[str]): Mapped classes in the evaluation.
+        eval_configs (:obj:`DetectionConfig`): Evaluation configuration object.
+        camera_type (str): Type of camera. Defaults to 'CAM_FRONT'.
 
     Returns:
-        list: List of standard NuScenesBoxes in the global
-            coordinate.
+        Tuple[List[:obj:`NuScenesBox`], List[int]]: List of standard
+        NuScenesBoxes in the global coordinate and attribute label.
     """
     box_list = []
     attr_list = []
     for (box, attr) in zip(boxes, attrs):
         # Move box to ego vehicle coord system
         cam2ego = np.array(info['images'][camera_type]['cam2ego'])
         box.rotate(
@@ -698,23 +717,23 @@
 
 def global_nusc_box_to_cam(info: dict, boxes: List[NuScenesBox],
                            classes: List[str],
                            eval_configs: DetectionConfig) -> List[NuScenesBox]:
     """Convert the box from global to camera coordinate.
 
     Args:
-        info (dict): Info for a specific sample data, including the
-            calibration information.
-        boxes (list[:obj:`NuScenesBox`]): List of predicted NuScenesBoxes.
-        classes (list[str]): Mapped classes in the evaluation.
-        eval_configs (object): Evaluation configuration object.
+        info (dict): Info for a specific sample data, including the calibration
+            information.
+        boxes (List[:obj:`NuScenesBox`]): List of predicted NuScenesBoxes.
+        classes (List[str]): Mapped classes in the evaluation.
+        eval_configs (:obj:`DetectionConfig`): Evaluation configuration object.
 
     Returns:
-        list: List of standard NuScenesBoxes in the global
-            coordinate.
+        List[:obj:`NuScenesBox`]: List of standard NuScenesBoxes in camera
+        coordinate.
     """
     box_list = []
     for box in boxes:
         # Move box to ego vehicle coord system
         ego2global = np.array(info['ego2global'])
         box.translate(-ego2global[:3, 3])
         box.rotate(
@@ -732,23 +751,25 @@
         box.rotate(
             pyquaternion.Quaternion(matrix=cam2ego, rtol=1e-05,
                                     atol=1e-07).inverse)
         box_list.append(box)
     return box_list
 
 
-def nusc_box_to_cam_box3d(boxes: List[NuScenesBox]):
+def nusc_box_to_cam_box3d(
+    boxes: List[NuScenesBox]
+) -> Tuple[CameraInstance3DBoxes, torch.Tensor, torch.Tensor]:
     """Convert boxes from :obj:`NuScenesBox` to :obj:`CameraInstance3DBoxes`.
 
     Args:
-        boxes (list[:obj:`NuScenesBox`]): List of predicted NuScenesBoxes.
+        boxes (:obj:`List[NuScenesBox]`): List of predicted NuScenesBoxes.
 
     Returns:
-        tuple (:obj:`CameraInstance3DBoxes` | torch.Tensor | torch.Tensor):
-            Converted 3D bounding boxes, scores and labels.
+        Tuple[:obj:`CameraInstance3DBoxes`, torch.Tensor, torch.Tensor]:
+        Converted 3D bounding boxes, scores and labels.
     """
     locs = torch.Tensor([b.center for b in boxes]).view(-1, 3)
     dims = torch.Tensor([b.wlh for b in boxes]).view(-1, 3)
     rots = torch.Tensor([b.orientation.yaw_pitch_roll[0]
                          for b in boxes]).view(-1, 1)
     velocity = torch.Tensor([b.velocity[0::2] for b in boxes]).view(-1, 2)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/evaluation/metrics/seg_metric.py` & `mmdet3d-1.1.1/mmdet3d/evaluation/metrics/seg_metric.py`

 * *Files 2% similar despite different names*

```diff
@@ -83,23 +83,23 @@
         if submission_prefix is None:
             tmp_dir = tempfile.TemporaryDirectory()
             submission_prefix = osp.join(tmp_dir.name, 'results')
         mmcv.mkdir_or_exist(submission_prefix)
         ignore_index = self.dataset_meta['ignore_index']
         # need to map network output to original label idx
         cat2label = np.zeros(len(self.dataset_meta['label2cat'])).astype(
-            np.int)
+            np.int64)
         for original_label, output_idx in self.dataset_meta['label2cat'].items(
         ):
             if output_idx != ignore_index:
                 cat2label[output_idx] = original_label
 
         for i, (eval_ann, result) in enumerate(results):
             sample_idx = eval_ann['point_cloud']['lidar_idx']
-            pred_sem_mask = result['semantic_mask'].numpy().astype(np.int)
+            pred_sem_mask = result['semantic_mask'].numpy().astype(np.int64)
             pred_label = cat2label[pred_sem_mask]
             curr_file = f'{submission_prefix}/{sample_idx}.txt'
             np.savetxt(curr_file, pred_label, fmt='%d')
 
     def compute_metrics(self, results: list) -> Dict[str, float]:
         """Compute the metrics from processed results.
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/evaluation/metrics/waymo_metric.py` & `mmdet3d-1.1.1/mmdet3d/evaluation/metrics/waymo_metric.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import tempfile
 from os import path as osp
-from typing import Dict, List, Optional, Union
+from typing import Dict, List, Optional, Tuple, Union
 
 import mmengine
 import numpy as np
 import torch
 from mmengine import Config, load
 from mmengine.logging import MMLogger, print_log
 
@@ -20,108 +20,119 @@
 @METRICS.register_module()
 class WaymoMetric(KittiMetric):
     """Waymo evaluation metric.
 
     Args:
         ann_file (str): The path of the annotation file in kitti format.
         waymo_bin_file (str): The path of the annotation file in waymo format.
-        data_root (str): Path of dataset root.
-                         Used for storing waymo evaluation programs.
-        split (str): The split of the evaluation set.
-        metric (str | list[str]): Metrics to be evaluated.
-            Default to 'mAP'.
-        pcd_limit_range (list): The range of point cloud used to
-            filter invalid predicted boxes.
-            Default to [0, -40, -3, 70.4, 40, 0.0].
+        data_root (str): Path of dataset root. Used for storing waymo
+            evaluation programs.
+        split (str): The split of the evaluation set. Defaults to 'training'.
+        metric (str or List[str]): Metrics to be evaluated. Defaults to 'mAP'.
+        pcd_limit_range (List[float]): The range of point cloud used to filter
+            invalid predicted boxes. Defaults to [-85, -85, -5, 85, 85, 5].
+        convert_kitti_format (bool): Whether to convert the results to kitti
+            format. Now, in order to be compatible with camera-based methods,
+            defaults to True.
         prefix (str, optional): The prefix that will be added in the metric
             names to disambiguate homonymous metrics of different evaluators.
-            If prefix is not provided in the argument, self.default_prefix
-            will be used instead. Defaults to None.
-        convert_kitti_format (bool, optional): Whether convert the reuslts to
-            kitti format. Now, in order to be compatible with camera-based
-            methods, defaults to True.
-        pklfile_prefix (str, optional): The prefix of pkl files, including
-            the file path and the prefix of filename, e.g., "a/b/prefix".
-            If not specified, a temp file will be created. Default: None.
-        submission_prefix (str, optional): The prefix of submission data.
-            If not specified, the submission data will not be generated.
-            Default: None.
-        load_type (str, optional): Type of loading mode during training.
+            If prefix is not provided in the argument, self.default_prefix will
+            be used instead. Defaults to None.
+        format_only (bool): Format the output results without perform
+            evaluation. It is useful when you want to format the result to a
+            specific format and submit it to the test server.
+            Defaults to False.
+        pklfile_prefix (str, optional): The prefix of pkl files, including the
+            file path and the prefix of filename, e.g., "a/b/prefix". If not
+            specified, a temp file will be created. Defaults to None.
+        submission_prefix (str, optional): The prefix of submission data. If
+            not specified, the submission data will not be generated.
+            Defaults to None.
+        load_type (str): Type of loading mode during training.
 
             - 'frame_based': Load all of the instances in the frame.
             - 'mv_image_based': Load all of the instances in the frame and need
-                to convert to the FOV-based data type to support image-based
-                detector.
-            - 'fov_image_base': Only load the instances inside the default cam,
-                and need to convert to the FOV-based data type to support
-                image-based detector.
-        default_cam_key (str, optional): The default camera for lidar to
-            camear conversion. By default, KITTI: CAM2, Waymo: CAM_FRONT
-        use_pred_sample_idx (bool, optional): In formating results, use the
-            sample index from the prediction or from the load annoataitons.
-            By default, KITTI: True, Waymo: False, Waymo has a conversion
-            process, which needs to use the sample id from load annotation.
-        collect_device (str): Device name used for collecting results
-            from different ranks during distributed training. Must be 'cpu' or
+              to convert to the FOV-based data type to support image-based
+              detector.
+            - 'fov_image_based': Only load the instances inside the default cam
+              and need to convert to the FOV-based data type to support image-
+              based detector.
+        default_cam_key (str): The default camera for lidar to camera
+            conversion. By default, KITTI: 'CAM2', Waymo: 'CAM_FRONT'.
+            Defaults to 'CAM_FRONT'.
+        use_pred_sample_idx (bool): In formating results, use the sample index
+            from the prediction or from the load annotations. By default,
+            KITTI: True, Waymo: False, Waymo has a conversion process, which
+            needs to use the sample idx from load annotation.
+            Defaults to False.
+        collect_device (str): Device name used for collecting results from
+            different ranks during distributed training. Must be 'cpu' or
             'gpu'. Defaults to 'cpu'.
-        file_client_args (dict): file client for reading gt in waymo format.
-            Defaults to ``dict(backend='disk')``.
-        idx2metainfo (Optional[str], optional): The file path of the metainfo
-            in waymmo. It stores the mapping from sample_idx to metainfo.
-            The metainfo must contain the keys: 'idx2contextname' and
-            'idx2timestamp'. Defaults to None.
+        backend_args (dict, optional): Arguments to instantiate the
+            corresponding backend. Defaults to None.
+        idx2metainfo (str, optional): The file path of the metainfo in waymo.
+            It stores the mapping from sample_idx to metainfo. The metainfo
+            must contain the keys: 'idx2contextname' and 'idx2timestamp'.
+            Defaults to None.
     """
     num_cams = 5
 
     def __init__(self,
                  ann_file: str,
                  waymo_bin_file: str,
                  data_root: str,
                  split: str = 'training',
                  metric: Union[str, List[str]] = 'mAP',
                  pcd_limit_range: List[float] = [-85, -85, -5, 85, 85, 5],
                  convert_kitti_format: bool = True,
                  prefix: Optional[str] = None,
-                 pklfile_prefix: str = None,
-                 submission_prefix: str = None,
+                 format_only: bool = False,
+                 pklfile_prefix: Optional[str] = None,
+                 submission_prefix: Optional[str] = None,
                  load_type: str = 'frame_based',
                  default_cam_key: str = 'CAM_FRONT',
                  use_pred_sample_idx: bool = False,
                  collect_device: str = 'cpu',
-                 file_client_args: dict = dict(backend='disk'),
-                 idx2metainfo: Optional[str] = None):
+                 backend_args: Optional[dict] = None,
+                 idx2metainfo: Optional[str] = None) -> None:
         self.waymo_bin_file = waymo_bin_file
         self.data_root = data_root
         self.split = split
         self.load_type = load_type
         self.use_pred_sample_idx = use_pred_sample_idx
         self.convert_kitti_format = convert_kitti_format
 
         if idx2metainfo is not None:
             self.idx2metainfo = mmengine.load(idx2metainfo)
         else:
             self.idx2metainfo = None
 
-        super().__init__(
+        super(WaymoMetric, self).__init__(
             ann_file=ann_file,
             metric=metric,
             pcd_limit_range=pcd_limit_range,
             prefix=prefix,
             pklfile_prefix=pklfile_prefix,
             submission_prefix=submission_prefix,
             default_cam_key=default_cam_key,
             collect_device=collect_device,
-            file_client_args=file_client_args)
+            backend_args=backend_args)
+        self.format_only = format_only
+        if self.format_only:
+            assert pklfile_prefix is not None, 'pklfile_prefix must be not '
+            'None when format_only is True, otherwise the result files will '
+            'be saved to a temp directory which will be cleaned up at the end.'
+
         self.default_prefix = 'Waymo metric'
 
-    def compute_metrics(self, results: list) -> Dict[str, float]:
+    def compute_metrics(self, results: List[dict]) -> Dict[str, float]:
         """Compute the metrics from processed results.
 
         Args:
-            results (list): The processed results of the whole dataset.
+            results (List[dict]): The processed results of the whole dataset.
 
         Returns:
             Dict[str, float]: The computed metrics. The keys are the names of
             the metrics, and the values are corresponding results.
         """
         logger: MMLogger = MMLogger.get_current_instance()
         self.classes = self.dataset_meta['classes']
@@ -151,15 +162,15 @@
                             cam_key]
                     else:
                         camera_info['instances'] = []
                     camera_info['ego2global'] = info['ego2global']
                     if 'image_sweeps' in info:
                         camera_info['image_sweeps'] = info['image_sweeps']
 
-                    # TODO check if need to modify the sample id
+                    # TODO check if need to modify the sample idx
                     # TODO check when will use it except for evaluation.
                     camera_info['sample_idx'] = info['sample_idx']
                     new_data_infos.append(camera_info)
             self.data_infos = new_data_infos
 
         if self.pklfile_prefix is None:
             eval_tmp_dir = tempfile.TemporaryDirectory()
@@ -171,40 +182,46 @@
         result_dict, tmp_dir = self.format_results(
             results,
             pklfile_prefix=pklfile_prefix,
             submission_prefix=self.submission_prefix,
             classes=self.classes)
 
         metric_dict = {}
+
+        if self.format_only:
+            logger.info('results are saved in '
+                        f'{osp.dirname(self.pklfile_prefix)}')
+            return metric_dict
+
         for metric in self.metrics:
             ap_dict = self.waymo_evaluate(
                 pklfile_prefix, metric=metric, logger=logger)
             metric_dict.update(ap_dict)
         if eval_tmp_dir is not None:
             eval_tmp_dir.cleanup()
 
         if tmp_dir is not None:
             tmp_dir.cleanup()
         return metric_dict
 
     def waymo_evaluate(self,
                        pklfile_prefix: str,
-                       metric: str = None,
-                       logger: MMLogger = None) -> dict:
+                       metric: Optional[str] = None,
+                       logger: Optional[MMLogger] = None) -> Dict[str, float]:
         """Evaluation in Waymo protocol.
 
         Args:
             pklfile_prefix (str): The location that stored the prediction
                 results.
-            metric (str): Metric to be evaluated. Defaults to None.
-            logger (MMLogger, optional): Logger used for printing
-                related information during evaluation. Default: None.
+            metric (str, optional): Metric to be evaluated. Defaults to None.
+            logger (MMLogger, optional): Logger used for printing related
+                information during evaluation. Defaults to None.
 
         Returns:
-            dict[str, float]: Results of each evaluation metric.
+            Dict[str, float]: Results of each evaluation metric.
         """
 
         import subprocess
 
         if metric == 'mAP':
             eval_str = 'mmdet3d/evaluation/functional/waymo_utils/' + \
                 f'compute_detection_metrics_main {pklfile_prefix}.bin ' + \
@@ -234,16 +251,14 @@
                 'Overall/L1 mAP': 0,
                 'Overall/L1 mAPH': 0,
                 'Overall/L2 mAP': 0,
                 'Overall/L2 mAPH': 0
             }
             mAP_splits = ret_texts.split('mAP ')
             mAPH_splits = ret_texts.split('mAPH ')
-            mAP_splits = ret_texts.split('mAP ')
-            mAPH_splits = ret_texts.split('mAPH ')
             for idx, key in enumerate(ap_dict.keys()):
                 split_idx = int(idx / 2) + 1
                 if idx % 2 == 0:  # mAP
                     ap_dict[key] = float(mAP_splits[split_idx].split(']')[0])
                 else:  # mAPH
                     ap_dict[key] = float(mAPH_splits[split_idx].split(']')[0])
             ap_dict['Overall/L1 mAP'] = \
@@ -303,39 +318,40 @@
                 (ap_dict['Vehicle mAP'] + ap_dict['Pedestrian mAP'] +
                     ap_dict['Cyclist mAP']) / 3
             ap_dict['Overall mAPH'] = \
                 (ap_dict['Vehicle mAPH'] + ap_dict['Pedestrian mAPH'] +
                     ap_dict['Cyclist mAPH']) / 3
         return ap_dict
 
-    def format_results(self,
-                       results: List[dict],
-                       pklfile_prefix: str = None,
-                       submission_prefix: str = None,
-                       classes: List[str] = None):
+    def format_results(
+        self,
+        results: List[dict],
+        pklfile_prefix: Optional[str] = None,
+        submission_prefix: Optional[str] = None,
+        classes: Optional[List[str]] = None
+    ) -> Tuple[dict, Union[tempfile.TemporaryDirectory, None]]:
         """Format the results to bin file.
 
         Args:
-            results (list[dict]): Testing results of the
-                dataset.
+            results (List[dict]): Testing results of the dataset.
             pklfile_prefix (str, optional): The prefix of pkl files. It
                 includes the file path and the prefix of filename, e.g.,
                 "a/b/prefix". If not specified, a temp file will be created.
-                Default: None.
+                Defaults to None.
             submission_prefix (str, optional): The prefix of submitted files.
                 It includes the file path and the prefix of filename, e.g.,
                 "a/b/prefix". If not specified, a temp file will be created.
-                Default: None.
-            classes (list[String], optional): A list of class name. Defaults
-                to None.
+                Defaults to None.
+            classes (List[str], optional): A list of class name.
+                Defaults to None.
 
         Returns:
-            tuple: (result_dict, tmp_dir), result_dict is a dict containing
-                the formatted result, tmp_dir is the temporal directory created
-                for saving json files when jsonfile_prefix is not specified.
+            tuple: (result_dict, tmp_dir), result_dict is a dict containing the
+            formatted result, tmp_dir is the temporal directory created for
+            saving json files when jsonfile_prefix is not specified.
         """
         waymo_save_tmp_dir = tempfile.TemporaryDirectory()
         waymo_results_save_dir = waymo_save_tmp_dir.name
         waymo_results_final_path = f'{pklfile_prefix}.bin'
 
         if self.convert_kitti_format:
             results_kitti_format, tmp_dir = super().format_results(
@@ -365,32 +381,33 @@
         converter = Prediction2Waymo(
             final_results,
             waymo_tfrecords_dir,
             waymo_results_save_dir,
             waymo_results_final_path,
             prefix,
             classes,
-            file_client_args=self.file_client_args,
+            backend_args=self.backend_args,
             from_kitti_format=self.convert_kitti_format,
             idx2metainfo=self.idx2metainfo)
         converter.convert()
         waymo_save_tmp_dir.cleanup()
 
         return final_results, waymo_save_tmp_dir
 
     def merge_multi_view_boxes(self, box_dict_per_frame: List[dict],
-                               cam0_info: dict):
+                               cam0_info: dict) -> dict:
         """Merge bounding boxes predicted from multi-view images.
+
         Args:
-            box_dict_per_frame (list[dict]): The results of prediction
-                for each camera.
-            cam2_info (dict): store the sample id for the given frame.
+            box_dict_per_frame (List[dict]): The results of prediction for each
+                camera.
+            cam0_info (dict): Store the sample idx for the given frame.
 
         Returns:
-            merged_box_dict (dict), store the merge results
+            dict: Merged results.
         """
         box_dict = dict()
         # convert list[dict] to dict[list]
         for key in box_dict_per_frame[0].keys():
             box_dict[key] = list()
             for cam_idx in range(self.num_cams):
                 box_dict[key].append(box_dict_per_frame[cam_idx][key])
@@ -432,53 +449,53 @@
         lidar2cam = cam0_info['images'][self.default_cam_key]['lidar2img']
         lidar2cam = np.array(lidar2cam).astype(np.float32)
         box_preds_camera = box_preds_lidar.convert_to(
             Box3DMode.CAM, lidar2cam, correct_yaw=True)
         # Note: bbox is meaningless in final evaluation, set to 0
         merged_box_dict = dict(
             bbox=np.zeros([box_preds_lidar.tensor.shape[0], 4]),
-            box3d_camera=box_preds_camera.tensor.numpy(),
-            box3d_lidar=box_preds_lidar.tensor.numpy(),
+            box3d_camera=box_preds_camera.numpy(),
+            box3d_lidar=box_preds_lidar.numpy(),
             scores=scores.numpy(),
             label_preds=labels.numpy(),
             sample_idx=box_dict['sample_idx'],
         )
         return merged_box_dict
 
-    def bbox2result_kitti(self,
-                          net_outputs: list,
-                          sample_id_list: list,
-                          class_names: list,
-                          pklfile_prefix: str = None,
-                          submission_prefix: str = None):
+    def bbox2result_kitti(
+            self,
+            net_outputs: List[dict],
+            sample_idx_list: List[int],
+            class_names: List[str],
+            pklfile_prefix: Optional[str] = None,
+            submission_prefix: Optional[str] = None) -> List[dict]:
         """Convert 3D detection results to kitti format for evaluation and test
         submission.
 
         Args:
-            net_outputs (list[dict]): List of array storing the
-                inferenced bounding boxes and scores.
-            sample_id_list (list[int]): List of input sample id.
-            class_names (list[String]): A list of class names.
+            net_outputs (List[dict]): List of dict storing the inferenced
+                bounding boxes and scores.
+            sample_idx_list (List[int]): List of input sample idx.
+            class_names (List[str]): A list of class names.
             pklfile_prefix (str, optional): The prefix of pkl file.
                 Defaults to None.
             submission_prefix (str, optional): The prefix of submission file.
                 Defaults to None.
 
         Returns:
-            list[dict]: A list of dictionaries with the kitti format.
+            List[dict]: A list of dictionaries with the kitti format.
         """
         if submission_prefix is not None:
             mmengine.mkdir_or_exist(submission_prefix)
 
         det_annos = []
         print('\nConverting prediction to KITTI format')
         for idx, pred_dicts in enumerate(
                 mmengine.track_iter_progress(net_outputs)):
-            annos = []
-            sample_idx = sample_id_list[idx]
+            sample_idx = sample_idx_list[idx]
             info = self.data_infos[sample_idx]
 
             if self.load_type == 'mv_image_based':
                 if idx % self.num_cams == 0:
                     box_dict_per_frame = []
                     cam0_key = list(info['images'].keys())[0]
                     cam0_info = info
@@ -532,28 +549,26 @@
                     anno['bbox'].append(bbox)
                     anno['dimensions'].append(box[3:6])
                     anno['location'].append(box[:3])
                     anno['rotation_y'].append(box[6])
                     anno['score'].append(score)
 
                 anno = {k: np.stack(v) for k, v in anno.items()}
-                annos.append(anno)
             else:
                 anno = {
                     'name': np.array([]),
                     'truncated': np.array([]),
                     'occluded': np.array([]),
                     'alpha': np.array([]),
                     'bbox': np.zeros([0, 4]),
                     'dimensions': np.zeros([0, 3]),
                     'location': np.zeros([0, 3]),
                     'rotation_y': np.array([]),
                     'score': np.array([]),
                 }
-                annos.append(anno)
 
             if submission_prefix is not None:
                 curr_file = f'{submission_prefix}/{sample_idx:06d}.txt'
                 with open(curr_file, 'w') as f:
                     bbox = anno['bbox']
                     loc = anno['location']
                     dims = anno['dimensions']  # lhw -> hwl
@@ -573,52 +588,51 @@
             if self.use_pred_sample_idx:
                 save_sample_idx = sample_idx
             else:
                 # use the sample idx in the info file
                 # In waymo validation sample_idx in prediction is 000xxx
                 # but in info file it is 1000xxx
                 save_sample_idx = box_dict['sample_idx']
-            annos[-1]['sample_idx'] = np.array(
-                [save_sample_idx] * len(annos[-1]['score']), dtype=np.int64)
+            anno['sample_idx'] = np.array(
+                [save_sample_idx] * len(anno['score']), dtype=np.int64)
 
-            det_annos += annos
+            det_annos.append(anno)
 
         if pklfile_prefix is not None:
             if not pklfile_prefix.endswith(('.pkl', '.pickle')):
                 out = f'{pklfile_prefix}.pkl'
             else:
                 out = pklfile_prefix
             mmengine.dump(det_annos, out)
             print(f'Result is saved to {out}.')
 
         return det_annos
 
-    def convert_valid_bboxes(self, box_dict: dict, info: dict):
+    def convert_valid_bboxes(self, box_dict: dict, info: dict) -> dict:
         """Convert the predicted boxes into valid ones. Should handle the
         load_model (frame_based, mv_image_based, fov_image_based), separately.
 
         Args:
             box_dict (dict): Box dictionaries to be converted.
 
-                - bboxes_3d (:obj:`LiDARInstance3DBoxes`): 3D bounding boxes.
-                - scores_3d (torch.Tensor): Scores of boxes.
-                - labels_3d (torch.Tensor): Class labels of boxes.
+                - bboxes_3d (:obj:`BaseInstance3DBoxes`): 3D bounding boxes.
+                - scores_3d (Tensor): Scores of boxes.
+                - labels_3d (Tensor): Class labels of boxes.
             info (dict): Data info.
 
         Returns:
             dict: Valid predicted boxes.
 
-                - bbox (np.ndarray): 2D bounding boxes.
-                - box3d_camera (np.ndarray): 3D bounding boxes in
-                    camera coordinate.
-                - box3d_lidar (np.ndarray): 3D bounding boxes in
-                    LiDAR coordinate.
-                - scores (np.ndarray): Scores of boxes.
-                - label_preds (np.ndarray): Class label predictions.
-                - sample_idx (int): Sample index.
+            - bbox (np.ndarray): 2D bounding boxes.
+            - box3d_camera (np.ndarray): 3D bounding boxes in camera
+              coordinate.
+            - box3d_lidar (np.ndarray): 3D bounding boxes in LiDAR coordinate.
+            - scores (np.ndarray): Scores of boxes.
+            - label_preds (np.ndarray): Class label predictions.
+            - sample_idx (int): Sample index.
         """
         # TODO: refactor this function
         box_preds = box_dict['bboxes_3d']
         scores = box_dict['scores_3d']
         labels = box_dict['labels_3d']
         sample_idx = info['sample_idx']
         box_preds.limit_yaw(offset=0.5, period=np.pi * 2)
@@ -669,23 +683,23 @@
                           (box_2d_preds[:, 2] > 0) & (box_2d_preds[:, 3] > 0))
         # check box_preds_lidar
         if self.load_type in ['frame_based']:
             limit_range = box_preds.tensor.new_tensor(self.pcd_limit_range)
             valid_pcd_inds = ((box_preds_lidar.center > limit_range[:3]) &
                               (box_preds_lidar.center < limit_range[3:]))
             valid_inds = valid_pcd_inds.all(-1)
-        if self.load_type in ['mv_image_based', 'fov_image_based']:
+        elif self.load_type in ['mv_image_based', 'fov_image_based']:
             valid_inds = valid_cam_inds
 
         if valid_inds.sum() > 0:
             return dict(
                 bbox=box_2d_preds[valid_inds, :].numpy(),
                 pred_box_type_3d=type(box_preds),
-                box3d_camera=box_preds_camera[valid_inds].tensor.numpy(),
-                box3d_lidar=box_preds_lidar[valid_inds].tensor.numpy(),
+                box3d_camera=box_preds_camera[valid_inds].numpy(),
+                box3d_lidar=box_preds_lidar[valid_inds].numpy(),
                 scores=scores[valid_inds].numpy(),
                 label_preds=labels[valid_inds].numpy(),
                 sample_idx=sample_idx)
         else:
             return dict(
                 bbox=np.zeros([0, 4]),
                 pred_box_type_3d=type(box_preds),
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/__init__.py` & `mmdet3d-1.1.1/mmdet3d/models/__init__.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/backbones/__init__.py` & `mmdet3d-1.1.1/mmdet3d/models/backbones/__init__.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,17 +1,21 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from mmdet.models.backbones import SSDVGG, HRNet, ResNet, ResNetV1d, ResNeXt
 
+from .cylinder3d import Asymm3DSpconv
 from .dgcnn import DGCNNBackbone
 from .dla import DLANet
 from .mink_resnet import MinkResNet
+from .minkunet_backbone import MinkUNetBackbone
 from .multi_backbone import MultiBackbone
 from .nostem_regnet import NoStemRegNet
 from .pointnet2_sa_msg import PointNet2SAMSG
 from .pointnet2_sa_ssg import PointNet2SASSG
 from .second import SECOND
+from .spvcnn_backone import MinkUNetBackboneV2, SPVCNNBackbone
 
 __all__ = [
     'ResNet', 'ResNetV1d', 'ResNeXt', 'SSDVGG', 'HRNet', 'NoStemRegNet',
     'SECOND', 'DGCNNBackbone', 'PointNet2SASSG', 'PointNet2SAMSG',
-    'MultiBackbone', 'DLANet', 'MinkResNet'
+    'MultiBackbone', 'DLANet', 'MinkResNet', 'Asymm3DSpconv',
+    'MinkUNetBackbone', 'SPVCNNBackbone', 'MinkUNetBackboneV2'
 ]
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/backbones/base_pointnet.py` & `mmdet3d-1.1.1/mmdet3d/models/backbones/base_pointnet.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,29 +1,34 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import warnings
 from abc import ABCMeta
+from typing import Optional, Tuple
 
 from mmengine.model import BaseModule
+from torch import Tensor
+
+from mmdet3d.utils import OptMultiConfig
 
 
 class BasePointNet(BaseModule, metaclass=ABCMeta):
     """Base class for PointNet."""
 
-    def __init__(self, init_cfg=None, pretrained=None):
+    def __init__(self,
+                 init_cfg: OptMultiConfig = None,
+                 pretrained: Optional[str] = None):
         super(BasePointNet, self).__init__(init_cfg)
-        self.fp16_enabled = False
         assert not (init_cfg and pretrained), \
             'init_cfg and pretrained cannot be setting at the same time'
         if isinstance(pretrained, str):
             warnings.warn('DeprecationWarning: pretrained is a deprecated, '
                           'please use "init_cfg" instead')
             self.init_cfg = dict(type='Pretrained', checkpoint=pretrained)
 
     @staticmethod
-    def _split_point_feats(points):
+    def _split_point_feats(points: Tensor) -> Tuple[Tensor, Optional[Tensor]]:
         """Split coordinates and features of input points.
 
         Args:
             points (torch.Tensor): Point coordinates with features,
                 with shape (B, N, 3 + input_feature_dim).
 
         Returns:
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/backbones/dgcnn.py` & `mmdet3d-1.1.1/mmdet3d/models/backbones/dgcnn.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,17 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Sequence, Union
+
 from mmengine.model import BaseModule
+from torch import Tensor
 from torch import nn as nn
 
 from mmdet3d.models.layers import DGCNNFAModule, DGCNNGFModule
 from mmdet3d.registry import MODELS
+from mmdet3d.utils import ConfigType, OptMultiConfig
 
 
 @MODELS.register_module()
 class DGCNNBackbone(BaseModule):
     """Backbone network for DGCNN.
 
     Args:
@@ -26,22 +30,23 @@
         act_cfg (dict, optional): Config of activation layer.
             Defaults to dict(type='ReLU').
         init_cfg (dict, optional): Initialization config.
             Defaults to None.
     """
 
     def __init__(self,
-                 in_channels,
-                 num_samples=(20, 20, 20),
-                 knn_modes=('D-KNN', 'F-KNN', 'F-KNN'),
-                 radius=(None, None, None),
-                 gf_channels=((64, 64), (64, 64), (64, )),
-                 fa_channels=(1024, ),
-                 act_cfg=dict(type='ReLU'),
-                 init_cfg=None):
+                 in_channels: int,
+                 num_samples: Sequence[int] = (20, 20, 20),
+                 knn_modes: Sequence[str] = ('D-KNN', 'F-KNN', 'F-KNN'),
+                 radius: Sequence[Union[float, None]] = (None, None, None),
+                 gf_channels: Sequence[Sequence[int]] = ((64, 64), (64, 64),
+                                                         (64, )),
+                 fa_channels: Sequence[int] = (1024, ),
+                 act_cfg: ConfigType = dict(type='ReLU'),
+                 init_cfg: OptMultiConfig = None):
         super().__init__(init_cfg=init_cfg)
         self.num_gf = len(gf_channels)
 
         assert len(num_samples) == len(knn_modes) == len(radius) == len(
             gf_channels), 'Num_samples, knn_modes, radius and gf_channels \
             should have the same length.'
 
@@ -67,15 +72,15 @@
         fa_in_channel = sum(skip_channel_list[1:])
         cur_fa_mlps = list(fa_channels)
         cur_fa_mlps = [fa_in_channel] + cur_fa_mlps
 
         self.FA_module = DGCNNFAModule(
             mlp_channels=cur_fa_mlps, act_cfg=act_cfg)
 
-    def forward(self, points):
+    def forward(self, points: Tensor) -> dict:
         """Forward pass.
 
         Args:
             points (torch.Tensor): point coordinates with features,
                 with shape (B, N, in_channels).
 
         Returns:
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/backbones/dla.py` & `mmdet3d-1.1.1/mmdet3d/models/backbones/dla.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,19 +1,22 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import warnings
+from typing import List, Optional, Sequence, Tuple
 
 import torch
 from mmcv.cnn import build_conv_layer, build_norm_layer
 from mmengine.model import BaseModule
-from torch import nn
+from torch import Tensor, nn
 
 from mmdet3d.registry import MODELS
+from mmdet3d.utils import ConfigType, OptConfigType, OptMultiConfig
 
 
-def dla_build_norm_layer(cfg, num_features):
+def dla_build_norm_layer(cfg: ConfigType,
+                         num_features: int) -> Tuple[str, nn.Module]:
     """Build normalization layer specially designed for DLANet.
 
     Args:
         cfg (dict): The norm layer config, which should contain:
 
             - type (str): Layer type.
             - layer args: Args needed to instantiate a norm layer.
@@ -49,21 +52,21 @@
         stride (int, optional): Conv stride. Default: 1.
         dilation (int, optional): Conv dilation. Default: 1.
         init_cfg (dict, optional): Initialization config.
             Default: None.
     """
 
     def __init__(self,
-                 in_channels,
-                 out_channels,
-                 norm_cfg,
-                 conv_cfg,
-                 stride=1,
-                 dilation=1,
-                 init_cfg=None):
+                 in_channels: int,
+                 out_channels: int,
+                 norm_cfg: ConfigType,
+                 conv_cfg: ConfigType,
+                 stride: int = 1,
+                 dilation: int = 1,
+                 init_cfg: OptMultiConfig = None):
         super(BasicBlock, self).__init__(init_cfg)
         self.conv1 = build_conv_layer(
             conv_cfg,
             in_channels,
             out_channels,
             3,
             stride=stride,
@@ -80,15 +83,15 @@
             stride=1,
             padding=dilation,
             dilation=dilation,
             bias=False)
         self.norm2 = dla_build_norm_layer(norm_cfg, out_channels)[1]
         self.stride = stride
 
-    def forward(self, x, identity=None):
+    def forward(self, x: Tensor, identity: Optional[Tensor] = None) -> Tensor:
         """Forward function."""
 
         if identity is None:
             identity = x
         out = self.conv1(x)
         out = self.norm1(out)
         out = self.relu(out)
@@ -113,35 +116,35 @@
         kernel_size (int): Size of convolution kernel.
         add_identity (bool): Whether to add identity in root.
         init_cfg (dict, optional): Initialization config.
             Default: None.
     """
 
     def __init__(self,
-                 in_channels,
-                 out_channels,
-                 norm_cfg,
-                 conv_cfg,
-                 kernel_size,
-                 add_identity,
-                 init_cfg=None):
+                 in_channels: int,
+                 out_channels: int,
+                 norm_cfg: ConfigType,
+                 conv_cfg: ConfigType,
+                 kernel_size: int,
+                 add_identity: bool,
+                 init_cfg: OptMultiConfig = None):
         super(Root, self).__init__(init_cfg)
         self.conv = build_conv_layer(
             conv_cfg,
             in_channels,
             out_channels,
             1,
             stride=1,
             padding=(kernel_size - 1) // 2,
             bias=False)
         self.norm = dla_build_norm_layer(norm_cfg, out_channels)[1]
         self.relu = nn.ReLU(inplace=True)
         self.add_identity = add_identity
 
-    def forward(self, feat_list):
+    def forward(self, feat_list: List[Tensor]) -> Tensor:
         """Forward function.
 
         Args:
             feat_list (list[torch.Tensor]): Output features from
                 multiple layers.
         """
         children = feat_list
@@ -177,27 +180,27 @@
         add_identity (bool, optional): Whether to add
             identity in root. Default: False.
         init_cfg (dict, optional): Initialization config.
             Default: None.
     """
 
     def __init__(self,
-                 levels,
-                 block,
-                 in_channels,
-                 out_channels,
-                 norm_cfg,
-                 conv_cfg,
-                 stride=1,
-                 level_root=False,
-                 root_dim=None,
-                 root_kernel_size=1,
-                 dilation=1,
-                 add_identity=False,
-                 init_cfg=None):
+                 levels: int,
+                 block: nn.Module,
+                 in_channels: int,
+                 out_channels: int,
+                 norm_cfg: ConfigType,
+                 conv_cfg: ConfigType,
+                 stride: int = 1,
+                 level_root: bool = False,
+                 root_dim: Optional[int] = None,
+                 root_kernel_size: int = 1,
+                 dilation: int = 1,
+                 add_identity: bool = False,
+                 init_cfg: OptMultiConfig = None):
         super(Tree, self).__init__(init_cfg)
         if root_dim is None:
             root_dim = 2 * out_channels
         if level_root:
             root_dim += in_channels
         if levels == 1:
             self.root = Root(root_dim, out_channels, norm_cfg, conv_cfg,
@@ -254,15 +257,18 @@
                     in_channels,
                     out_channels,
                     1,
                     stride=1,
                     bias=False),
                 dla_build_norm_layer(norm_cfg, out_channels)[1])
 
-    def forward(self, x, identity=None, children=None):
+    def forward(self,
+                x: Tensor,
+                identity: Optional[Tensor] = None,
+                children: Optional[List[Tensor]] = None) -> Tensor:
         children = [] if children is None else children
         bottom = self.downsample(x) if self.downsample else x
         identity = self.project(bottom) if self.project else bottom
         if self.level_root:
             children.append(bottom)
         x1 = self.tree1(x, identity)
         if self.levels == 1:
@@ -298,24 +304,25 @@
             config dict. Default: None
     """
     arch_settings = {
         34: (BasicBlock, (1, 1, 1, 2, 2, 1), (16, 32, 64, 128, 256, 512)),
     }
 
     def __init__(self,
-                 depth,
-                 in_channels=3,
-                 out_indices=(0, 1, 2, 3, 4, 5),
-                 frozen_stages=-1,
-                 norm_cfg=None,
-                 conv_cfg=None,
-                 layer_with_level_root=(False, True, True, True),
-                 with_identity_root=False,
-                 pretrained=None,
-                 init_cfg=None):
+                 depth: int,
+                 in_channels: int = 3,
+                 out_indices: Sequence[int] = (0, 1, 2, 3, 4, 5),
+                 frozen_stages: int = -1,
+                 norm_cfg: OptConfigType = None,
+                 conv_cfg: OptConfigType = None,
+                 layer_with_level_root: Sequence[bool] = (False, True, True,
+                                                          True),
+                 with_identity_root: bool = False,
+                 pretrained: Optional[str] = None,
+                 init_cfg: OptMultiConfig = None):
         super(DLANet, self).__init__(init_cfg)
         if depth not in self.arch_settings:
             raise KeyError(f'invalida depth {depth} for DLA')
 
         assert not (init_cfg and pretrained), \
             'init_cfg and pretrained cannot be setting at the same time'
         if isinstance(pretrained, str):
@@ -376,21 +383,21 @@
                 add_identity=with_identity_root)
             layer_name = f'level{i}'
             self.add_module(layer_name, dla_layer)
 
         self._freeze_stages()
 
     def _make_conv_level(self,
-                         in_channels,
-                         out_channels,
-                         num_convs,
-                         norm_cfg,
-                         conv_cfg,
-                         stride=1,
-                         dilation=1):
+                         in_channels: int,
+                         out_channels: int,
+                         num_convs: int,
+                         norm_cfg: ConfigType,
+                         conv_cfg: ConfigType,
+                         stride: int = 1,
+                         dilation: int = 1) -> nn.Sequential:
         """Conv modules.
 
         Args:
             in_channels (int): Input feature channel.
             out_channels (int): Output feature channel.
             num_convs (int): Number of Conv module.
             norm_cfg (dict): Dictionary to construct and config
@@ -414,15 +421,15 @@
                     dilation=dilation),
                 dla_build_norm_layer(norm_cfg, out_channels)[1],
                 nn.ReLU(inplace=True)
             ])
             in_channels = out_channels
         return nn.Sequential(*modules)
 
-    def _freeze_stages(self):
+    def _freeze_stages(self) -> None:
         if self.frozen_stages >= 0:
             self.base_layer.eval()
             for param in self.base_layer.parameters():
                 param.requires_grad = False
 
             for i in range(2):
                 m = getattr(self, f'level{i}')
@@ -432,15 +439,15 @@
 
         for i in range(1, self.frozen_stages + 1):
             m = getattr(self, f'level{i+1}')
             m.eval()
             for param in m.parameters():
                 param.requires_grad = False
 
-    def forward(self, x):
+    def forward(self, x: Tensor) -> Tuple[Tensor, ...]:
         outs = []
         x = self.base_layer(x)
         for i in range(self.num_levels):
             x = getattr(self, 'level{}'.format(i))(x)
             if i in self.out_indices:
                 outs.append(x)
         return tuple(outs)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/backbones/mink_resnet.py` & `mmdet3d-1.1.1/mmdet3d/models/backbones/mink_resnet.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,24 +1,28 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 # Follow https://github.com/NVIDIA/MinkowskiEngine/blob/master/examples/resnet.py # noqa
 # and mmcv.cnn.ResNet
+from typing import List, Union
+
 try:
     import MinkowskiEngine as ME
+    from MinkowskiEngine import SparseTensor
     from MinkowskiEngine.modules.resnet_block import BasicBlock, Bottleneck
 except ImportError:
     # blocks are used in the static part of MinkResNet
-    ME = BasicBlock = Bottleneck = None
+    ME = BasicBlock = Bottleneck = SparseTensor = None
 
 import torch.nn as nn
+from mmengine.model import BaseModule
 
 from mmdet3d.registry import MODELS
 
 
 @MODELS.register_module()
-class MinkResNet(nn.Module):
+class MinkResNet(BaseModule):
     r"""Minkowski ResNet backbone. See `4D Spatio-Temporal ConvNets
     <https://arxiv.org/abs/1904.08755>`_ for more details.
 
     Args:
         depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.
         in_channels (int): Number of input channels, 3 for RGB.
         num_stages (int): Resnet stages. Defaults to 4.
@@ -29,20 +33,23 @@
         18: (BasicBlock, (2, 2, 2, 2)),
         34: (BasicBlock, (3, 4, 6, 3)),
         50: (Bottleneck, (3, 4, 6, 3)),
         101: (Bottleneck, (3, 4, 23, 3)),
         152: (Bottleneck, (3, 8, 36, 3))
     }
 
-    def __init__(self, depth, in_channels, num_stages=4, pool=True):
+    def __init__(self,
+                 depth: int,
+                 in_channels: int,
+                 num_stages: int = 4,
+                 pool: bool = True):
         super(MinkResNet, self).__init__()
         if ME is None:
             raise ImportError(
-                'Please follow `getting_started.md` to install MinkowskiEngine.`'  # noqa: E501
-            )
+                'Please follow `get_started.md` to install MinkowskiEngine.`')
         if depth not in self.arch_settings:
             raise KeyError(f'invalid depth {depth} for resnet')
         assert 4 >= num_stages >= 1
         block, stage_blocks = self.arch_settings[depth]
         stage_blocks = stage_blocks[:num_stages]
         self.num_stages = num_stages
         self.pool = pool
@@ -53,30 +60,43 @@
         # May be BatchNorm is better, but we follow original implementation.
         self.norm1 = ME.MinkowskiInstanceNorm(self.inplanes)
         self.relu = ME.MinkowskiReLU(inplace=True)
         if self.pool:
             self.maxpool = ME.MinkowskiMaxPooling(
                 kernel_size=2, stride=2, dimension=3)
 
-        for i, num_blocks in enumerate(stage_blocks):
+        for i in range(len(stage_blocks)):
             setattr(
                 self, f'layer{i + 1}',
                 self._make_layer(block, 64 * 2**i, stage_blocks[i], stride=2))
 
     def init_weights(self):
+        """Initialize weights."""
         for m in self.modules():
             if isinstance(m, ME.MinkowskiConvolution):
                 ME.utils.kaiming_normal_(
                     m.kernel, mode='fan_out', nonlinearity='relu')
 
             if isinstance(m, ME.MinkowskiBatchNorm):
                 nn.init.constant_(m.bn.weight, 1)
                 nn.init.constant_(m.bn.bias, 0)
 
-    def _make_layer(self, block, planes, blocks, stride):
+    def _make_layer(self, block: Union[BasicBlock, Bottleneck], planes: int,
+                    blocks: int, stride: int) -> nn.Module:
+        """Make single level of residual blocks.
+
+        Args:
+            block (BasicBlock | Bottleneck): Residual block class.
+            planes (int): Number of convolution filters.
+            blocks (int): Number of blocks in the layers.
+            stride (int): Stride of the first convolutional layer.
+
+        Returns:
+            nn.Module: With residual blocks.
+        """
         downsample = None
         if stride != 1 or self.inplanes != planes * block.expansion:
             downsample = nn.Sequential(
                 ME.MinkowskiConvolution(
                     self.inplanes,
                     planes * block.expansion,
                     kernel_size=1,
@@ -88,19 +108,19 @@
             block(
                 self.inplanes,
                 planes,
                 stride=stride,
                 downsample=downsample,
                 dimension=3))
         self.inplanes = planes * block.expansion
-        for i in range(1, blocks):
+        for _ in range(1, blocks):
             layers.append(block(self.inplanes, planes, stride=1, dimension=3))
         return nn.Sequential(*layers)
 
-    def forward(self, x):
+    def forward(self, x: SparseTensor) -> List[SparseTensor]:
         """Forward pass of ResNet.
 
         Args:
             x (ME.SparseTensor): Input sparse tensor.
 
         Returns:
             list[ME.SparseTensor]: Output sparse tensors.
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/backbones/multi_backbone.py` & `mmdet3d-1.1.1/mmdet3d/models/backbones/multi_backbone.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,17 +1,19 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import copy
 import warnings
+from typing import Dict, List, Optional, Sequence, Tuple, Union
 
 import torch
 from mmcv.cnn import ConvModule
 from mmengine.model import BaseModule
-from torch import nn as nn
+from torch import Tensor, nn
 
 from mmdet3d.registry import MODELS
+from mmdet3d.utils import ConfigType, OptMultiConfig
 
 
 @MODELS.register_module()
 class MultiBackbone(BaseModule):
     """MultiBackbone with different configs.
 
     Args:
@@ -23,24 +25,25 @@
         norm_cfg (dict): Config dict of normalization layers.
         act_cfg (dict): Config dict of activation layers.
         suffixes (list): A list of suffixes to rename the return dict
             for each backbone.
     """
 
     def __init__(self,
-                 num_streams,
-                 backbones,
-                 aggregation_mlp_channels=None,
-                 conv_cfg=dict(type='Conv1d'),
-                 norm_cfg=dict(type='BN1d', eps=1e-5, momentum=0.01),
-                 act_cfg=dict(type='ReLU'),
-                 suffixes=('net0', 'net1'),
-                 init_cfg=None,
-                 pretrained=None,
-                 **kwargs):
+                 num_streams: int,
+                 backbones: Union[List[dict], Dict],
+                 aggregation_mlp_channels: Optional[Sequence[int]] = None,
+                 conv_cfg: ConfigType = dict(type='Conv1d'),
+                 norm_cfg: ConfigType = dict(
+                     type='BN1d', eps=1e-5, momentum=0.01),
+                 act_cfg: ConfigType = dict(type='ReLU'),
+                 suffixes: Tuple[str] = ('net0', 'net1'),
+                 init_cfg: OptMultiConfig = None,
+                 pretrained: Optional[str] = None,
+                 **kwargs) -> None:
         super().__init__(init_cfg=init_cfg)
         assert isinstance(backbones, dict) or isinstance(backbones, list)
         if isinstance(backbones, dict):
             backbones_list = []
             for ind in range(num_streams):
                 backbones_list.append(copy.deepcopy(backbones))
             backbones = backbones_list
@@ -85,15 +88,15 @@
         assert not (init_cfg and pretrained), \
             'init_cfg and pretrained cannot be setting at the same time'
         if isinstance(pretrained, str):
             warnings.warn('DeprecationWarning: pretrained is a deprecated, '
                           'please use "init_cfg" instead')
             self.init_cfg = dict(type='Pretrained', checkpoint=pretrained)
 
-    def forward(self, points):
+    def forward(self, points: Tensor) -> dict:
         """Forward pass.
 
         Args:
             points (torch.Tensor): point coordinates with features,
                 with shape (B, N, 3 + input_feature_dim).
 
         Returns:
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/backbones/nostem_regnet.py` & `mmdet3d-1.1.1/mmdet3d/models/backbones/nostem_regnet.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,11 +1,16 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Tuple
+
+import torch.nn as nn
 from mmdet.models.backbones import RegNet
+from torch import Tensor
 
 from mmdet3d.registry import MODELS
+from mmdet3d.utils import OptMultiConfig
 
 
 @MODELS.register_module()
 class NoStemRegNet(RegNet):
     """RegNet backbone without Stem for 3D detection.
 
     More details can be found in `paper <https://arxiv.org/abs/2003.13678>`_ .
@@ -55,23 +60,27 @@
         ...     print(tuple(level_out.shape))
         (1, 96, 8, 8)
         (1, 192, 4, 4)
         (1, 432, 2, 2)
         (1, 1008, 1, 1)
     """
 
-    def __init__(self, arch, init_cfg=None, **kwargs):
+    def __init__(self,
+                 arch: dict,
+                 init_cfg: OptMultiConfig = None,
+                 **kwargs) -> None:
         super(NoStemRegNet, self).__init__(arch, init_cfg=init_cfg, **kwargs)
 
-    def _make_stem_layer(self, in_channels, base_channels):
+    def _make_stem_layer(self, in_channels: int,
+                         base_channels: int) -> nn.Module:
         """Override the original function that do not initialize a stem layer
         since 3D detector's voxel encoder works like a stem layer."""
         return
 
-    def forward(self, x):
+    def forward(self, x: Tensor) -> Tuple[Tensor, ...]:
         """Forward function of backbone.
 
         Args:
             x (torch.Tensor): Features in shape (N, C, H, W).
 
         Returns:
             tuple[torch.Tensor]: Multi-scale features.
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/backbones/pointnet2_sa_msg.py` & `mmdet3d-1.1.1/mmdet3d/models/backbones/pointnet2_sa_msg.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,16 +1,23 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Tuple
+
 import torch
 from mmcv.cnn import ConvModule
-from torch import nn as nn
+from torch import Tensor, nn
 
 from mmdet3d.models.layers.pointnet_modules import build_sa_module
 from mmdet3d.registry import MODELS
+from mmdet3d.utils import OptConfigType
 from .base_pointnet import BasePointNet
 
+ThreeTupleIntType = Tuple[Tuple[Tuple[int, int, int]]]
+TwoTupleIntType = Tuple[Tuple[int, int, int]]
+TwoTupleStrType = Tuple[Tuple[str]]
+
 
 @MODELS.register_module()
 class PointNet2SAMSG(BasePointNet):
     """PointNet2 with Multi-scale grouping.
 
     Args:
         in_channels (int): Input channels of point cloud.
@@ -18,15 +25,15 @@
             module samples.
         radii (tuple[float]): Sampling radii of each SA module.
         num_samples (tuple[int]): The number of samples for ball
             query in each SA module.
         sa_channels (tuple[tuple[int]]): Out channels of each mlp in SA module.
         aggregation_channels (tuple[int]): Out channels of aggregation
             multi-scale grouping features.
-        fps_mods (tuple[int]): Mod of FPS for each SA module.
+        fps_mods Sequence[Tuple[str]]: Mod of FPS for each SA module.
         fps_sample_range_lists (tuple[tuple[int]]): The number of sampling
             points which each SA module samples.
         dilated_group (tuple[bool]): Whether to use dilated ball query for
         out_indices (Sequence[int]): Output from which stages.
         norm_cfg (dict): Config of normalization layer.
         sa_cfg (dict): Config of set abstraction module, which may contain
             the following keys and values:
@@ -34,34 +41,45 @@
             - pool_mod (str): Pool method ('max' or 'avg') for SA modules.
             - use_xyz (bool): Whether to use xyz as a part of features.
             - normalize_xyz (bool): Whether to normalize xyz with radii in
               each SA module.
     """
 
     def __init__(self,
-                 in_channels,
-                 num_points=(2048, 1024, 512, 256),
-                 radii=((0.2, 0.4, 0.8), (0.4, 0.8, 1.6), (1.6, 3.2, 4.8)),
-                 num_samples=((32, 32, 64), (32, 32, 64), (32, 32, 32)),
-                 sa_channels=(((16, 16, 32), (16, 16, 32), (32, 32, 64)),
-                              ((64, 64, 128), (64, 64, 128), (64, 96, 128)),
-                              ((128, 128, 256), (128, 192, 256), (128, 256,
-                                                                  256))),
-                 aggregation_channels=(64, 128, 256),
-                 fps_mods=(('D-FPS'), ('FS'), ('F-FPS', 'D-FPS')),
-                 fps_sample_range_lists=((-1), (-1), (512, -1)),
-                 dilated_group=(True, True, True),
-                 out_indices=(2, ),
-                 norm_cfg=dict(type='BN2d'),
-                 sa_cfg=dict(
+                 in_channels: int,
+                 num_points: Tuple[int] = (2048, 1024, 512, 256),
+                 radii: Tuple[Tuple[float, float, float]] = (
+                     (0.2, 0.4, 0.8),
+                     (0.4, 0.8, 1.6),
+                     (1.6, 3.2, 4.8),
+                 ),
+                 num_samples: TwoTupleIntType = ((32, 32, 64), (32, 32, 64),
+                                                 (32, 32, 32)),
+                 sa_channels: ThreeTupleIntType = (((16, 16, 32), (16, 16, 32),
+                                                    (32, 32, 64)),
+                                                   ((64, 64, 128),
+                                                    (64, 64, 128), (64, 96,
+                                                                    128)),
+                                                   ((128, 128, 256),
+                                                    (128, 192, 256), (128, 256,
+                                                                      256))),
+                 aggregation_channels: Tuple[int] = (64, 128, 256),
+                 fps_mods: TwoTupleStrType = (('D-FPS'), ('FS'), ('F-FPS',
+                                                                  'D-FPS')),
+                 fps_sample_range_lists: TwoTupleIntType = ((-1), (-1), (512,
+                                                                         -1)),
+                 dilated_group: Tuple[bool] = (True, True, True),
+                 out_indices: Tuple[int] = (2, ),
+                 norm_cfg: dict = dict(type='BN2d'),
+                 sa_cfg: dict = dict(
                      type='PointSAModuleMSG',
                      pool_mod='max',
                      use_xyz=True,
                      normalize_xyz=False),
-                 init_cfg=None):
+                 init_cfg: OptConfigType = None):
         super().__init__(init_cfg=init_cfg)
         self.num_sa = len(sa_channels)
         self.out_indices = out_indices
         assert max(out_indices) < self.num_sa
         assert len(num_points) == len(radii) == len(num_samples) == len(
             sa_channels)
         if aggregation_channels is not None:
@@ -119,15 +137,15 @@
                         cur_aggregation_channel,
                         conv_cfg=dict(type='Conv1d'),
                         norm_cfg=dict(type='BN1d'),
                         kernel_size=1,
                         bias=True))
                 sa_in_channel = cur_aggregation_channel
 
-    def forward(self, points):
+    def forward(self, points: Tensor):
         """Forward pass.
 
         Args:
             points (torch.Tensor): point coordinates with features,
                 with shape (B, N, 3 + input_feature_dim).
 
         Returns:
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/backbones/pointnet2_sa_ssg.py` & `mmdet3d-1.1.1/mmdet3d/models/backbones/pointnet2_sa_ssg.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,13 +1,16 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Dict, List, Sequence
+
 import torch
-from torch import nn as nn
+from torch import Tensor, nn
 
 from mmdet3d.models.layers import PointFPModule, build_sa_module
 from mmdet3d.registry import MODELS
+from mmdet3d.utils import ConfigType, OptMultiConfig
 from .base_pointnet import BasePointNet
 
 
 @MODELS.register_module()
 class PointNet2SASSG(BasePointNet):
     """PointNet2 with Single-scale grouping.
 
@@ -27,28 +30,31 @@
             - pool_mod (str): Pool method ('max' or 'avg') for SA modules.
             - use_xyz (bool): Whether to use xyz as a part of features.
             - normalize_xyz (bool): Whether to normalize xyz with radii in
               each SA module.
     """
 
     def __init__(self,
-                 in_channels,
-                 num_points=(2048, 1024, 512, 256),
-                 radius=(0.2, 0.4, 0.8, 1.2),
-                 num_samples=(64, 32, 16, 16),
-                 sa_channels=((64, 64, 128), (128, 128, 256), (128, 128, 256),
-                              (128, 128, 256)),
-                 fp_channels=((256, 256), (256, 256)),
-                 norm_cfg=dict(type='BN2d'),
-                 sa_cfg=dict(
+                 in_channels: int,
+                 num_points: Sequence[int] = (2048, 1024, 512, 256),
+                 radius: Sequence[float] = (0.2, 0.4, 0.8, 1.2),
+                 num_samples: Sequence[int] = (64, 32, 16, 16),
+                 sa_channels: Sequence[Sequence[int]] = ((64, 64, 128),
+                                                         (128, 128, 256),
+                                                         (128, 128, 256),
+                                                         (128, 128, 256)),
+                 fp_channels: Sequence[Sequence[int]] = ((256, 256), (256,
+                                                                      256)),
+                 norm_cfg: ConfigType = dict(type='BN2d'),
+                 sa_cfg: ConfigType = dict(
                      type='PointSAModule',
                      pool_mod='max',
                      use_xyz=True,
                      normalize_xyz=True),
-                 init_cfg=None):
+                 init_cfg: OptMultiConfig = None):
         super().__init__(init_cfg=init_cfg)
         self.num_sa = len(sa_channels)
         self.num_fp = len(fp_channels)
 
         assert len(num_points) == len(radius) == len(num_samples) == len(
             sa_channels)
         assert len(sa_channels) >= len(fp_channels)
@@ -81,15 +87,15 @@
             cur_fp_mlps = list(fp_channels[fp_index])
             cur_fp_mlps = [fp_source_channel + fp_target_channel] + cur_fp_mlps
             self.FP_modules.append(PointFPModule(mlp_channels=cur_fp_mlps))
             if fp_index != len(fp_channels) - 1:
                 fp_source_channel = cur_fp_mlps[-1]
                 fp_target_channel = skip_channel_list.pop()
 
-    def forward(self, points):
+    def forward(self, points: Tensor) -> Dict[str, List[Tensor]]:
         """Forward pass.
 
         Args:
             points (torch.Tensor): point coordinates with features,
                 with shape (B, N, 3 + input_feature_dim).
 
         Returns:
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/backbones/second.py` & `mmdet3d-1.1.1/mmdet3d/models/backbones/second.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,15 +1,18 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import warnings
+from typing import Optional, Sequence, Tuple
 
 from mmcv.cnn import build_conv_layer, build_norm_layer
 from mmengine.model import BaseModule
+from torch import Tensor
 from torch import nn as nn
 
 from mmdet3d.registry import MODELS
+from mmdet3d.utils import ConfigType, OptMultiConfig
 
 
 @MODELS.register_module()
 class SECOND(BaseModule):
     """Backbone network for SECOND/PointPillars/PartA2/MVXNet.
 
     Args:
@@ -18,22 +21,23 @@
         layer_nums (list[int]): Number of layers in each stage.
         layer_strides (list[int]): Strides of each stage.
         norm_cfg (dict): Config dict of normalization layers.
         conv_cfg (dict): Config dict of convolutional layers.
     """
 
     def __init__(self,
-                 in_channels=128,
-                 out_channels=[128, 128, 256],
-                 layer_nums=[3, 5, 5],
-                 layer_strides=[2, 2, 2],
-                 norm_cfg=dict(type='BN', eps=1e-3, momentum=0.01),
-                 conv_cfg=dict(type='Conv2d', bias=False),
-                 init_cfg=None,
-                 pretrained=None):
+                 in_channels: int = 128,
+                 out_channels: Sequence[int] = [128, 128, 256],
+                 layer_nums: Sequence[int] = [3, 5, 5],
+                 layer_strides: Sequence[int] = [2, 2, 2],
+                 norm_cfg: ConfigType = dict(
+                     type='BN', eps=1e-3, momentum=0.01),
+                 conv_cfg: ConfigType = dict(type='Conv2d', bias=False),
+                 init_cfg: OptMultiConfig = None,
+                 pretrained: Optional[str] = None) -> None:
         super(SECOND, self).__init__(init_cfg=init_cfg)
         assert len(layer_strides) == len(layer_nums)
         assert len(out_channels) == len(layer_nums)
 
         in_filters = [in_channels, *out_channels[:-1]]
         # note that when stride > 1, conv2d with same padding isn't
         # equal to pad-conv2d. we should use pad-conv2d.
@@ -71,15 +75,15 @@
         if isinstance(pretrained, str):
             warnings.warn('DeprecationWarning: pretrained is a deprecated, '
                           'please use "init_cfg" instead')
             self.init_cfg = dict(type='Pretrained', checkpoint=pretrained)
         else:
             self.init_cfg = dict(type='Kaiming', layer='Conv2d')
 
-    def forward(self, x):
+    def forward(self, x: Tensor) -> Tuple[Tensor, ...]:
         """Forward function.
 
         Args:
             x (torch.Tensor): Input with shape (N, C, H, W).
 
         Returns:
             tuple[torch.Tensor]: Multi-scale features.
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/data_preprocessors/data_preprocessor.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/mvx_two_stage.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,387 +1,407 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-import math
-from numbers import Number
-from typing import Dict, List, Optional, Sequence, Union
+import copy
+from typing import Dict, List, Optional, Sequence
 
-import numpy as np
 import torch
-from mmcv.ops import Voxelization
-from mmdet.models import DetDataPreprocessor
-from mmengine.model import stack_batch
-from mmengine.utils import is_list_of
-from torch.nn import functional as F
+from mmengine.structures import InstanceData
+from torch import Tensor
 
 from mmdet3d.registry import MODELS
-from mmdet3d.utils import OptConfigType
-from .utils import multiview_img_stack_batch
+from mmdet3d.structures import Det3DDataSample
+from .base import Base3DDetector
 
 
 @MODELS.register_module()
-class Det3DDataPreprocessor(DetDataPreprocessor):
-    """Points / Image pre-processor for point clouds / vision-only / multi-
-    modality 3D detection tasks.
-
-    It provides the data pre-processing as follows
-
-    - Collate and move image and point cloud data to the target device.
-
-    - 1) For image data:
-    - Pad images in inputs to the maximum size of current batch with defined
-      ``pad_value``. The padding size can be divisible by a defined
-      ``pad_size_divisor``.
-    - Stack images in inputs to batch_imgs.
-    - Convert images in inputs from bgr to rgb if the shape of input is
-      (3, H, W).
-    - Normalize images in inputs with defined std and mean.
-    - Do batch augmentations during training.
-
-    - 2) For point cloud data:
-    - If no voxelization, directly return list of point cloud data.
-    - If voxelization is applied, voxelize point cloud according to
-      ``voxel_type`` and obtain ``voxels``.
+class MVXTwoStageDetector(Base3DDetector):
+    """Base class of Multi-modality VoxelNet.
 
     Args:
-        voxel (bool): Whether to apply voxelization to point cloud.
-            Defaults to False.
-        voxel_type (str): Voxelization type. Two voxelization types are
-            provided: 'hard' and 'dynamic', respectively for hard
-            voxelization and dynamic voxelization. Defaults to 'hard'.
-        voxel_layer (dict or :obj:`ConfigDict`, optional): Voxelization layer
-            config. Defaults to None.
-        mean (Sequence[Number], optional): The pixel mean of R, G, B channels.
+        pts_voxel_encoder (dict, optional): Point voxelization
+            encoder layer. Defaults to None.
+        pts_middle_encoder (dict, optional): Middle encoder layer
+            of points cloud modality. Defaults to None.
+        pts_fusion_layer (dict, optional): Fusion layer.
             Defaults to None.
-        std (Sequence[Number], optional): The pixel standard deviation of
-            R, G, B channels. Defaults to None.
-        pad_size_divisor (int): The size of padded image should be
-            divisible by ``pad_size_divisor``. Defaults to 1.
-        pad_value (Number): The padded pixel value. Defaults to 0.
-        pad_mask (bool): Whether to pad instance masks. Defaults to False.
-        mask_pad_value (int): The padded pixel value for instance masks.
-            Defaults to 0.
-        pad_seg (bool): Whether to pad semantic segmentation maps.
-            Defaults to False.
-        seg_pad_value (int): The padded pixel value for semantic
-            segmentation maps. Defaults to 255.
-        bgr_to_rgb (bool): Whether to convert image from BGR to RGB.
-            Defaults to False.
-        rgb_to_bgr (bool): Whether to convert image from RGB to BGR.
-            Defaults to False.
-        boxtype2tensor (bool): Whether to keep the ``BaseBoxes`` type of
-            bboxes data or not. Defaults to True.
-        batch_augments (List[dict], optional): Batch-level augmentations.
+        img_backbone (dict, optional): Backbone of extracting
+            images feature. Defaults to None.
+        pts_backbone (dict, optional): Backbone of extracting
+            points features. Defaults to None.
+        img_neck (dict, optional): Neck of extracting
+            image features. Defaults to None.
+        pts_neck (dict, optional): Neck of extracting
+            points features. Defaults to None.
+        pts_bbox_head (dict, optional): Bboxes head of
+            point cloud modality. Defaults to None.
+        img_roi_head (dict, optional): RoI head of image
+            modality. Defaults to None.
+        img_rpn_head (dict, optional): RPN head of image
+            modality. Defaults to None.
+        train_cfg (dict, optional): Train config of model.
             Defaults to None.
+        test_cfg (dict, optional): Train config of model.
+            Defaults to None.
+        init_cfg (dict, optional): Initialize config of
+            model. Defaults to None.
+        data_preprocessor (dict or ConfigDict, optional): The pre-process
+            config of :class:`Det3DDataPreprocessor`. Defaults to None.
     """
 
     def __init__(self,
-                 voxel: bool = False,
-                 voxel_type: str = 'hard',
-                 voxel_layer: OptConfigType = None,
-                 mean: Sequence[Number] = None,
-                 std: Sequence[Number] = None,
-                 pad_size_divisor: int = 1,
-                 pad_value: Union[float, int] = 0,
-                 pad_mask: bool = False,
-                 mask_pad_value: int = 0,
-                 pad_seg: bool = False,
-                 seg_pad_value: int = 255,
-                 bgr_to_rgb: bool = False,
-                 rgb_to_bgr: bool = False,
-                 boxtype2tensor: bool = True,
-                 batch_augments: Optional[List[dict]] = None) -> None:
-        super(Det3DDataPreprocessor, self).__init__(
-            mean=mean,
-            std=std,
-            pad_size_divisor=pad_size_divisor,
-            pad_value=pad_value,
-            pad_mask=pad_mask,
-            mask_pad_value=mask_pad_value,
-            pad_seg=pad_seg,
-            seg_pad_value=seg_pad_value,
-            bgr_to_rgb=bgr_to_rgb,
-            rgb_to_bgr=rgb_to_bgr,
-            batch_augments=batch_augments)
-        self.voxel = voxel
-        self.voxel_type = voxel_type
-        if voxel:
-            self.voxel_layer = Voxelization(**voxel_layer)
-
-    def forward(self,
-                data: Union[dict, List[dict]],
-                training: bool = False) -> Union[dict, List[dict]]:
-        """Perform normalization, padding and bgr2rgb conversion based on
-        ``BaseDataPreprocessor``.
+                 pts_voxel_encoder: Optional[dict] = None,
+                 pts_middle_encoder: Optional[dict] = None,
+                 pts_fusion_layer: Optional[dict] = None,
+                 img_backbone: Optional[dict] = None,
+                 pts_backbone: Optional[dict] = None,
+                 img_neck: Optional[dict] = None,
+                 pts_neck: Optional[dict] = None,
+                 pts_bbox_head: Optional[dict] = None,
+                 img_roi_head: Optional[dict] = None,
+                 img_rpn_head: Optional[dict] = None,
+                 train_cfg: Optional[dict] = None,
+                 test_cfg: Optional[dict] = None,
+                 init_cfg: Optional[dict] = None,
+                 data_preprocessor: Optional[dict] = None,
+                 **kwargs):
+        super(MVXTwoStageDetector, self).__init__(
+            init_cfg=init_cfg, data_preprocessor=data_preprocessor, **kwargs)
+
+        if pts_voxel_encoder:
+            self.pts_voxel_encoder = MODELS.build(pts_voxel_encoder)
+        if pts_middle_encoder:
+            self.pts_middle_encoder = MODELS.build(pts_middle_encoder)
+        if pts_backbone:
+            self.pts_backbone = MODELS.build(pts_backbone)
+        if pts_fusion_layer:
+            self.pts_fusion_layer = MODELS.build(pts_fusion_layer)
+        if pts_neck is not None:
+            self.pts_neck = MODELS.build(pts_neck)
+        if pts_bbox_head:
+            pts_train_cfg = train_cfg.pts if train_cfg else None
+            pts_bbox_head.update(train_cfg=pts_train_cfg)
+            pts_test_cfg = test_cfg.pts if test_cfg else None
+            pts_bbox_head.update(test_cfg=pts_test_cfg)
+            self.pts_bbox_head = MODELS.build(pts_bbox_head)
+
+        if img_backbone:
+            self.img_backbone = MODELS.build(img_backbone)
+        if img_neck is not None:
+            self.img_neck = MODELS.build(img_neck)
+        if img_rpn_head is not None:
+            self.img_rpn_head = MODELS.build(img_rpn_head)
+        if img_roi_head is not None:
+            self.img_roi_head = MODELS.build(img_roi_head)
+
+        self.train_cfg = train_cfg
+        self.test_cfg = test_cfg
+
+    @property
+    def with_img_shared_head(self):
+        """bool: Whether the detector has a shared head in image branch."""
+        return hasattr(self,
+                       'img_shared_head') and self.img_shared_head is not None
+
+    @property
+    def with_pts_bbox(self):
+        """bool: Whether the detector has a 3D box head."""
+        return hasattr(self,
+                       'pts_bbox_head') and self.pts_bbox_head is not None
+
+    @property
+    def with_img_bbox(self):
+        """bool: Whether the detector has a 2D image box head."""
+        return hasattr(self,
+                       'img_bbox_head') and self.img_bbox_head is not None
+
+    @property
+    def with_img_backbone(self):
+        """bool: Whether the detector has a 2D image backbone."""
+        return hasattr(self, 'img_backbone') and self.img_backbone is not None
+
+    @property
+    def with_pts_backbone(self):
+        """bool: Whether the detector has a 3D backbone."""
+        return hasattr(self, 'pts_backbone') and self.pts_backbone is not None
+
+    @property
+    def with_fusion(self):
+        """bool: Whether the detector has a fusion layer."""
+        return hasattr(self,
+                       'pts_fusion_layer') and self.fusion_layer is not None
+
+    @property
+    def with_img_neck(self):
+        """bool: Whether the detector has a neck in image branch."""
+        return hasattr(self, 'img_neck') and self.img_neck is not None
+
+    @property
+    def with_pts_neck(self):
+        """bool: Whether the detector has a neck in 3D detector branch."""
+        return hasattr(self, 'pts_neck') and self.pts_neck is not None
+
+    @property
+    def with_img_rpn(self):
+        """bool: Whether the detector has a 2D RPN in image detector branch."""
+        return hasattr(self, 'img_rpn_head') and self.img_rpn_head is not None
+
+    @property
+    def with_img_roi_head(self):
+        """bool: Whether the detector has a RoI Head in image branch."""
+        return hasattr(self, 'img_roi_head') and self.img_roi_head is not None
+
+    @property
+    def with_voxel_encoder(self):
+        """bool: Whether the detector has a voxel encoder."""
+        return hasattr(self,
+                       'voxel_encoder') and self.voxel_encoder is not None
+
+    @property
+    def with_middle_encoder(self):
+        """bool: Whether the detector has a middle encoder."""
+        return hasattr(self,
+                       'middle_encoder') and self.middle_encoder is not None
+
+    def _forward(self):
+        pass
+
+    def extract_img_feat(self, img: Tensor, input_metas: List[dict]) -> dict:
+        """Extract features of images."""
+        if self.with_img_backbone and img is not None:
+            input_shape = img.shape[-2:]
+            # update real input shape of each single img
+            for img_meta in input_metas:
+                img_meta.update(input_shape=input_shape)
+
+            if img.dim() == 5 and img.size(0) == 1:
+                img.squeeze_()
+            elif img.dim() == 5 and img.size(0) > 1:
+                B, N, C, H, W = img.size()
+                img = img.view(B * N, C, H, W)
+            img_feats = self.img_backbone(img)
+        else:
+            return None
+        if self.with_img_neck:
+            img_feats = self.img_neck(img_feats)
+        return img_feats
+
+    def extract_pts_feat(
+            self,
+            voxel_dict: Dict[str, Tensor],
+            points: Optional[List[Tensor]] = None,
+            img_feats: Optional[Sequence[Tensor]] = None,
+            batch_input_metas: Optional[List[dict]] = None
+    ) -> Sequence[Tensor]:
+        """Extract features of points.
 
         Args:
-            data (dict or List[dict]): Data from dataloader.
-                The dict contains the whole batch data, when it is
-                a list[dict], the list indicate test time augmentation.
-            training (bool): Whether to enable training time augmentation.
-                Defaults to False.
+            voxel_dict(Dict[str, Tensor]): Dict of voxelization infos.
+            points (List[tensor], optional):  Point cloud of multiple inputs.
+            img_feats (list[Tensor], tuple[tensor], optional): Features from
+                image backbone.
+            batch_input_metas (list[dict], optional): The meta information
+                of multiple samples. Defaults to True.
 
         Returns:
-            dict or List[dict]: Data in the same format as the model input.
+            Sequence[tensor]: points features of multiple inputs
+            from backbone or neck.
         """
-        if isinstance(data, list):
-            num_augs = len(data)
-            aug_batch_data = []
-            for aug_id in range(num_augs):
-                single_aug_batch_data = self.simple_process(
-                    data[aug_id], training)
-                aug_batch_data.append(single_aug_batch_data)
-            return aug_batch_data
+        if not self.with_pts_bbox:
+            return None
+        voxel_features = self.pts_voxel_encoder(voxel_dict['voxels'],
+                                                voxel_dict['num_points'],
+                                                voxel_dict['coors'], img_feats,
+                                                batch_input_metas)
+        batch_size = voxel_dict['coors'][-1, 0] + 1
+        x = self.pts_middle_encoder(voxel_features, voxel_dict['coors'],
+                                    batch_size)
+        x = self.pts_backbone(x)
+        if self.with_pts_neck:
+            x = self.pts_neck(x)
+        return x
+
+    def extract_feat(self, batch_inputs_dict: dict,
+                     batch_input_metas: List[dict]) -> tuple:
+        """Extract features from images and points.
 
-        else:
-            return self.simple_process(data, training)
+        Args:
+            batch_inputs_dict (dict): Dict of batch inputs. It
+                contains
 
-    def simple_process(self, data: dict, training: bool = False) -> dict:
-        """Perform normalization, padding and bgr2rgb conversion for img data
-        based on ``BaseDataPreprocessor``, and voxelize point cloud if `voxel`
-        is set to be True.
+                - points (List[tensor]):  Point cloud of multiple inputs.
+                - imgs (tensor): Image tensor with shape (B, C, H, W).
+            batch_input_metas (list[dict]): Meta information of multiple inputs
+                in a batch.
 
+        Returns:
+             tuple: Two elements in tuple arrange as
+             image features and point cloud features.
+        """
+        voxel_dict = batch_inputs_dict.get('voxels', None)
+        imgs = batch_inputs_dict.get('imgs', None)
+        points = batch_inputs_dict.get('points', None)
+        img_feats = self.extract_img_feat(imgs, batch_input_metas)
+        pts_feats = self.extract_pts_feat(
+            voxel_dict,
+            points=points,
+            img_feats=img_feats,
+            batch_input_metas=batch_input_metas)
+        return (img_feats, pts_feats)
+
+    def loss(self, batch_inputs_dict: Dict[List, torch.Tensor],
+             batch_data_samples: List[Det3DDataSample],
+             **kwargs) -> List[Det3DDataSample]:
+        """
         Args:
-            data (dict): Data sampled from dataloader.
-            training (bool): Whether to enable training time augmentation.
-                Defaults to False.
+            batch_inputs_dict (dict): The model input dict which include
+                'points' and `imgs` keys.
+
+                - points (list[torch.Tensor]): Point cloud of each sample.
+                - imgs (torch.Tensor): Tensor of batch images, has shape
+                  (B, C, H ,W)
+            batch_data_samples (List[:obj:`Det3DDataSample`]): The Data
+                Samples. It usually includes information such as
+                `gt_instance_3d`, .
 
         Returns:
-            dict: Data in the same format as the model input.
+            dict[str, Tensor]: A dictionary of loss components.
+
         """
-        if 'img' in data['inputs']:
-            batch_pad_shape = self._get_pad_shape(data)
 
-        data = self.collate_data(data)
-        inputs, data_samples = data['inputs'], data['data_samples']
-        batch_inputs = dict()
-
-        if 'points' in inputs:
-            batch_inputs['points'] = inputs['points']
-
-            if self.voxel:
-                voxel_dict = self.voxelize(inputs['points'])
-                batch_inputs['voxels'] = voxel_dict
-
-        if 'imgs' in inputs:
-            imgs = inputs['imgs']
-
-            if data_samples is not None:
-                # NOTE the batched image size information may be useful, e.g.
-                # in DETR, this is needed for the construction of masks, which
-                # is then used for the transformer_head.
-                batch_input_shape = tuple(imgs[0].size()[-2:])
-                for data_sample, pad_shape in zip(data_samples,
-                                                  batch_pad_shape):
-                    data_sample.set_metainfo({
-                        'batch_input_shape': batch_input_shape,
-                        'pad_shape': pad_shape
-                    })
-
-                if hasattr(self, 'boxtype2tensor') and self.boxtype2tensor:
-                    from mmdet.models.utils.misc import \
-                        samplelist_boxtype2tensor
-                    samplelist_boxtype2tensor(data_samples)
-                elif hasattr(self, 'boxlist2tensor') and self.boxlist2tensor:
-                    from mmdet.models.utils.misc import \
-                        samplelist_boxlist2tensor
-                    samplelist_boxlist2tensor(data_samples)
-                if self.pad_mask:
-                    self.pad_gt_masks(data_samples)
-
-                if self.pad_seg:
-                    self.pad_gt_sem_seg(data_samples)
-
-            if training and self.batch_augments is not None:
-                for batch_aug in self.batch_augments:
-                    imgs, data_samples = batch_aug(imgs, data_samples)
-            batch_inputs['imgs'] = imgs
-
-        return {'inputs': batch_inputs, 'data_samples': data_samples}
-
-    def preprocess_img(self, _batch_img: torch.Tensor) -> torch.Tensor:
-        # channel transform
-        if self._channel_conversion:
-            _batch_img = _batch_img[[2, 1, 0], ...]
-        # Convert to float after channel conversion to ensure
-        # efficiency
-        _batch_img = _batch_img.float()
-        # Normalization.
-        if self._enable_normalize:
-            if self.mean.shape[0] == 3:
-                assert _batch_img.dim() == 3 and _batch_img.shape[0] == 3, (
-                    'If the mean has 3 values, the input tensor '
-                    'should in shape of (3, H, W), but got the '
-                    f'tensor with shape {_batch_img.shape}')
-            _batch_img = (_batch_img - self.mean) / self.std
-        return _batch_img
-
-    def collate_data(self, data: dict) -> dict:
-        """Copying data to the target device and Performs normalization,
-        padding and bgr2rgb conversion and stack based on
-        ``BaseDataPreprocessor``.
+        batch_input_metas = [item.metainfo for item in batch_data_samples]
+        img_feats, pts_feats = self.extract_feat(batch_inputs_dict,
+                                                 batch_input_metas)
+        losses = dict()
+        if pts_feats:
+            losses_pts = self.pts_bbox_head.loss(pts_feats, batch_data_samples,
+                                                 **kwargs)
+            losses.update(losses_pts)
+        if img_feats:
+            losses_img = self.loss_imgs(img_feats, batch_data_samples)
+            losses.update(losses_img)
+        return losses
+
+    def loss_imgs(self, x: List[Tensor],
+                  batch_data_samples: List[Det3DDataSample], **kwargs):
+        """Forward function for image branch.
 
-        Collates the data sampled from dataloader into a list of dict and
-        list of labels, and then copies tensor to the target device.
+        This function works similar to the forward function of Faster R-CNN.
 
         Args:
-            data (dict): Data sampled from dataloader.
+            x (list[torch.Tensor]): Image features of shape (B, C, H, W)
+                of multiple levels.
+            batch_data_samples (List[:obj:`Det3DDataSample`]): The Data
+                Samples. It usually includes information such as
+                `gt_instance_3d`, .
 
         Returns:
-            dict: Data in the same format as the model input.
+            dict: Losses of each branch.
         """
-        data = self.cast_data(data)  # type: ignore
+        losses = dict()
+        # RPN forward and loss
+        if self.with_img_rpn:
+            proposal_cfg = self.test_cfg.rpn
+            rpn_data_samples = copy.deepcopy(batch_data_samples)
+            # set cat_id of gt_labels to 0 in RPN
+            for data_sample in rpn_data_samples:
+                data_sample.gt_instances.labels = \
+                    torch.zeros_like(data_sample.gt_instances.labels)
+            rpn_losses, rpn_results_list = self.img_rpn_head.loss_and_predict(
+                x, rpn_data_samples, proposal_cfg=proposal_cfg, **kwargs)
+            # avoid get same name with roi_head loss
+            keys = rpn_losses.keys()
+            for key in keys:
+                if 'loss' in key and 'rpn' not in key:
+                    rpn_losses[f'rpn_{key}'] = rpn_losses.pop(key)
+            losses.update(rpn_losses)
 
-        if 'img' in data['inputs']:
-            _batch_imgs = data['inputs']['img']
-            # Process data with `pseudo_collate`.
-            if is_list_of(_batch_imgs, torch.Tensor):
-                batch_imgs = []
-                img_dim = _batch_imgs[0].dim()
-                for _batch_img in _batch_imgs:
-                    if img_dim == 3:  # standard img
-                        _batch_img = self.preprocess_img(_batch_img)
-                    elif img_dim == 4:
-                        _batch_img = [
-                            self.preprocess_img(_img) for _img in _batch_img
-                        ]
-
-                        _batch_img = torch.stack(_batch_img, dim=0)
-
-                    batch_imgs.append(_batch_img)
-
-                # Pad and stack Tensor.
-                if img_dim == 3:
-                    batch_imgs = stack_batch(batch_imgs, self.pad_size_divisor,
-                                             self.pad_value)
-                elif img_dim == 4:
-                    batch_imgs = multiview_img_stack_batch(
-                        batch_imgs, self.pad_size_divisor, self.pad_value)
-
-            # Process data with `default_collate`.
-            elif isinstance(_batch_imgs, torch.Tensor):
-                assert _batch_imgs.dim() == 4, (
-                    'The input of `ImgDataPreprocessor` should be a NCHW '
-                    'tensor or a list of tensor, but got a tensor with '
-                    f'shape: {_batch_imgs.shape}')
-                if self._channel_conversion:
-                    _batch_imgs = _batch_imgs[:, [2, 1, 0], ...]
-                # Convert to float after channel conversion to ensure
-                # efficiency
-                _batch_imgs = _batch_imgs.float()
-                if self._enable_normalize:
-                    _batch_imgs = (_batch_imgs - self.mean) / self.std
-                h, w = _batch_imgs.shape[2:]
-                target_h = math.ceil(
-                    h / self.pad_size_divisor) * self.pad_size_divisor
-                target_w = math.ceil(
-                    w / self.pad_size_divisor) * self.pad_size_divisor
-                pad_h = target_h - h
-                pad_w = target_w - w
-                batch_imgs = F.pad(_batch_imgs, (0, pad_w, 0, pad_h),
-                                   'constant', self.pad_value)
-            else:
-                raise TypeError(
-                    'Output of `cast_data` should be a list of dict '
-                    'or a tuple with inputs and data_samples, but got'
-                    f'{type(data)}: {data}')
-
-            data['inputs']['imgs'] = batch_imgs
-
-        data.setdefault('data_samples', None)
-
-        return data
-
-    def _get_pad_shape(self, data: dict) -> List[tuple]:
-        """Get the pad_shape of each image based on data and
-        pad_size_divisor."""
-        # rewrite `_get_pad_shape` for obtaining image inputs.
-        _batch_inputs = data['inputs']['img']
-        # Process data with `pseudo_collate`.
-        if is_list_of(_batch_inputs, torch.Tensor):
-            batch_pad_shape = []
-            for ori_input in _batch_inputs:
-                if ori_input.dim() == 4:
-                    # mean multiview input, select one of the
-                    # image to calculate the pad shape
-                    ori_input = ori_input[0]
-                pad_h = int(
-                    np.ceil(ori_input.shape[1] /
-                            self.pad_size_divisor)) * self.pad_size_divisor
-                pad_w = int(
-                    np.ceil(ori_input.shape[2] /
-                            self.pad_size_divisor)) * self.pad_size_divisor
-                batch_pad_shape.append((pad_h, pad_w))
-        # Process data with `default_collate`.
-        elif isinstance(_batch_inputs, torch.Tensor):
-            assert _batch_inputs.dim() == 4, (
-                'The input of `ImgDataPreprocessor` should be a NCHW tensor '
-                'or a list of tensor, but got a tensor with shape: '
-                f'{_batch_inputs.shape}')
-            pad_h = int(
-                np.ceil(_batch_inputs.shape[1] /
-                        self.pad_size_divisor)) * self.pad_size_divisor
-            pad_w = int(
-                np.ceil(_batch_inputs.shape[2] /
-                        self.pad_size_divisor)) * self.pad_size_divisor
-            batch_pad_shape = [(pad_h, pad_w)] * _batch_inputs.shape[0]
         else:
-            raise TypeError('Output of `cast_data` should be a list of dict '
-                            'or a tuple with inputs and data_samples, but got '
-                            f'{type(data)}: {data}')
-        return batch_pad_shape
-
-    @torch.no_grad()
-    def voxelize(self, points: List[torch.Tensor]) -> Dict[str, torch.Tensor]:
-        """Apply voxelization to point cloud.
+            if 'proposals' in batch_data_samples[0]:
+                # use pre-defined proposals in InstanceData
+                # for the second stage
+                # to extract ROI features.
+                rpn_results_list = [
+                    data_sample.proposals for data_sample in batch_data_samples
+                ]
+            else:
+                rpn_results_list = None
+        # bbox head forward and loss
+        if self.with_img_bbox:
+            roi_losses = self.img_roi_head.loss(x, rpn_results_list,
+                                                batch_data_samples, **kwargs)
+            losses.update(roi_losses)
+        return losses
+
+    def predict_imgs(self,
+                     x: List[Tensor],
+                     batch_data_samples: List[Det3DDataSample],
+                     rescale: bool = True,
+                     **kwargs) -> InstanceData:
+        """Predict results from a batch of inputs and data samples with post-
+        processing.
 
         Args:
-            points (List[Tensor]): Point cloud in one data batch.
+            x (List[Tensor]): Image features from FPN.
+            batch_data_samples (List[:obj:`Det3DDataSample`]): The Data
+                Samples. It usually includes information such as
+                `gt_instance`, `gt_panoptic_seg` and `gt_sem_seg`.
+            rescale (bool): Whether to rescale the results.
+                Defaults to True.
+        """
 
-        Returns:
-            Dict[str, Tensor]: Voxelization information.
+        if batch_data_samples[0].get('proposals', None) is None:
+            rpn_results_list = self.img_rpn_head.predict(
+                x, batch_data_samples, rescale=False)
+        else:
+            rpn_results_list = [
+                data_sample.proposals for data_sample in batch_data_samples
+            ]
+        results_list = self.img_roi_head.predict(
+            x, rpn_results_list, batch_data_samples, rescale=rescale, **kwargs)
+        return results_list
+
+    def predict(self, batch_inputs_dict: Dict[str, Optional[Tensor]],
+                batch_data_samples: List[Det3DDataSample],
+                **kwargs) -> List[Det3DDataSample]:
+        """Forward of testing.
 
-            - voxels (Tensor): Features of voxels, shape is MxNxC for hard
-              voxelization, NxC for dynamic voxelization.
-            - coors (Tensor): Coordinates of voxels, shape is Nx(1+NDim),
-              where 1 represents the batch index.
-            - num_points (Tensor, optional): Number of points in each voxel.
-            - voxel_centers (Tensor, optional): Centers of voxels.
-        """
+        Args:
+            batch_inputs_dict (dict): The model input dict which include
+                'points' keys.
 
-        voxel_dict = dict()
+                - points (list[torch.Tensor]): Point cloud of each sample.
+            batch_data_samples (List[:obj:`Det3DDataSample`]): The Data
+                Samples. It usually includes information such as
+                `gt_instance_3d`.
 
-        if self.voxel_type == 'hard':
-            voxels, coors, num_points, voxel_centers = [], [], [], []
-            for i, res in enumerate(points):
-                res_voxels, res_coors, res_num_points = self.voxel_layer(res)
-                res_voxel_centers = (
-                    res_coors[:, [2, 1, 0]] + 0.5) * res_voxels.new_tensor(
-                        self.voxel_layer.voxel_size) + res_voxels.new_tensor(
-                            self.voxel_layer.point_cloud_range[0:3])
-                res_coors = F.pad(res_coors, (1, 0), mode='constant', value=i)
-                voxels.append(res_voxels)
-                coors.append(res_coors)
-                num_points.append(res_num_points)
-                voxel_centers.append(res_voxel_centers)
-
-            voxels = torch.cat(voxels, dim=0)
-            coors = torch.cat(coors, dim=0)
-            num_points = torch.cat(num_points, dim=0)
-            voxel_centers = torch.cat(voxel_centers, dim=0)
-
-            voxel_dict['num_points'] = num_points
-            voxel_dict['voxel_centers'] = voxel_centers
-        elif self.voxel_type == 'dynamic':
-            coors = []
-            # dynamic voxelization only provide a coors mapping
-            for i, res in enumerate(points):
-                res_coors = self.voxel_layer(res)
-                res_coors = F.pad(res_coors, (1, 0), mode='constant', value=i)
-                coors.append(res_coors)
-            voxels = torch.cat(points, dim=0)
-            coors = torch.cat(coors, dim=0)
+        Returns:
+            list[:obj:`Det3DDataSample`]: Detection results of the
+            input sample. Each Det3DDataSample usually contain
+            'pred_instances_3d'. And the ``pred_instances_3d`` usually
+            contains following keys.
+
+            - scores_3d (Tensor): Classification scores, has a shape
+                (num_instances, )
+            - labels_3d (Tensor): Labels of bboxes, has a shape
+                (num_instances, ).
+            - bbox_3d (:obj:`BaseInstance3DBoxes`): Prediction of bboxes,
+                contains a tensor with shape (num_instances, 7).
+        """
+        batch_input_metas = [item.metainfo for item in batch_data_samples]
+        img_feats, pts_feats = self.extract_feat(batch_inputs_dict,
+                                                 batch_input_metas)
+        if pts_feats and self.with_pts_bbox:
+            results_list_3d = self.pts_bbox_head.predict(
+                pts_feats, batch_data_samples, **kwargs)
         else:
-            raise ValueError(f'Invalid voxelization type {self.voxel_type}')
+            results_list_3d = None
 
-        voxel_dict['voxels'] = voxels
-        voxel_dict['coors'] = coors
+        if img_feats and self.with_img_bbox:
+            # TODO check this for camera modality
+            results_list_2d = self.predict_imgs(img_feats, batch_data_samples,
+                                                **kwargs)
+        else:
+            results_list_2d = None
 
-        return voxel_dict
+        detsamples = self.add_pred_to_datasample(batch_data_samples,
+                                                 results_list_3d,
+                                                 results_list_2d)
+        return detsamples
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/data_preprocessors/utils.py` & `mmdet3d-1.1.1/mmdet3d/models/data_preprocessors/utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,47 +1,45 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from typing import List, Union
 
 import torch
 import torch.nn.functional as F
+from torch import Tensor
 
 
-def multiview_img_stack_batch(
-        tensor_list: List[torch.Tensor],
-        pad_size_divisor: int = 1,
-        pad_value: Union[int, float] = 0) -> torch.Tensor:
-    """
-    Compared to the stack_batch in mmengine.model.utils,
+def multiview_img_stack_batch(tensor_list: List[Tensor],
+                              pad_size_divisor: int = 1,
+                              pad_value: Union[int, float] = 0) -> Tensor:
+    """Compared to the ``stack_batch`` in `mmengine.model.utils`,
     multiview_img_stack_batch further handle the multiview images.
-    see diff of padded_sizes[:, :-2] = 0 vs padded_sizes[:, 0] = 0 in line 47
-    Stack multiple tensors to form a batch and pad the tensor to the max
-    shape use the right bottom padding mode in these images. If
+
+    See diff of padded_sizes[:, :-2] = 0 vs padded_sizes[:, 0] = 0 in line 47.
+
+    Stack multiple tensors to form a batch and pad the tensor to the max shape
+    use the right bottom padding mode in these images. If
     ``pad_size_divisor > 0``, add padding to ensure the shape of each dim is
     divisible by ``pad_size_divisor``.
 
     Args:
         tensor_list (List[Tensor]): A list of tensors with the same dim.
-        pad_size_divisor (int): If ``pad_size_divisor > 0``, add padding
-            to ensure the shape of each dim is divisible by
-            ``pad_size_divisor``. This depends on the model, and many
-            models need to be divisible by 32. Defaults to 1.
+        pad_size_divisor (int): If ``pad_size_divisor > 0``, add padding to
+            ensure the shape of each dim is divisible by ``pad_size_divisor``.
+            This depends on the model, and many models need to be divisible by
+            32. Defaults to 1.
         pad_value (int or float): The padding value. Defaults to 0.
 
     Returns:
         Tensor: The n dim tensor.
     """
-    assert isinstance(
-        tensor_list,
-        list), f'Expected input type to be list, but got {type(tensor_list)}'
+    assert isinstance(tensor_list, list), \
+        f'Expected input type to be list, but got {type(tensor_list)}'
     assert tensor_list, '`tensor_list` could not be an empty list'
-    assert len({
-        tensor.ndim
-        for tensor in tensor_list
-    }) == 1, ('Expected the dimensions of all tensors must be the same, '
-              f'but got {[tensor.ndim for tensor in tensor_list]}')
+    assert len({tensor.ndim for tensor in tensor_list}) == 1, \
+        'Expected the dimensions of all tensors must be the same, ' \
+        f'but got {[tensor.ndim for tensor in tensor_list]}'
 
     dim = tensor_list[0].dim()
     num_img = len(tensor_list)
     all_sizes: torch.Tensor = torch.Tensor(
         [tensor.shape for tensor in tensor_list])
     max_sizes = torch.ceil(
         torch.max(all_sizes, dim=0)[0] / pad_size_divisor) * pad_size_divisor
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/decode_heads/decode_head.py` & `mmdet3d-1.1.1/mmdet3d/models/decode_heads/decode_head.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from abc import ABCMeta, abstractmethod
-from typing import List
+from typing import Dict, List
 
 import torch
 from mmengine.model import BaseModule, normal_init
 from torch import Tensor
 from torch import nn as nn
 
 from mmdet3d.registry import MODELS
 from mmdet3d.structures.det3d_data_sample import SampleList
-from mmdet3d.utils.typing_utils import ConfigType
+from mmdet3d.utils.typing_utils import ConfigType, OptMultiConfig
 
 
 class Base3DDecodeHead(BaseModule, metaclass=ABCMeta):
     """Base class for BaseDecodeHead.
 
     1. The ``init_weights`` method is used to initialize decode_head's
     model parameters. After segmentor initialization, ``init_weights``
@@ -38,104 +38,114 @@
 
     predict(): forward() -> predict_by_feat()
 
     Args:
         channels (int): Channels after modules, before conv_seg.
         num_classes (int): Number of classes.
         dropout_ratio (float): Ratio of dropout layer. Defaults to 0.5.
-        conv_cfg (dict): Config of conv layers.
+        conv_cfg (dict or :obj:`ConfigDict`): Config of conv layers.
             Defaults to dict(type='Conv1d').
-        norm_cfg (dict): Config of norm layers.
+        norm_cfg (dict or :obj:`ConfigDict`): Config of norm layers.
             Defaults to dict(type='BN1d').
-        act_cfg (dict): Config of activation layers.
+        act_cfg (dict or :obj:`ConfigDict`): Config of activation layers.
             Defaults to dict(type='ReLU').
-        loss_decode (dict): Config of decode loss.
-            Defaults to dict(type='CrossEntropyLoss').
-        ignore_index (int): The label index to be ignored.
-            When using masked BCE loss, ignore_index should be set to None.
-            Defaults to 255.
-        init_cfg (dict or list[dict], optional): Initialization config dict.
-            Defaults to None.
+        loss_decode (dict or :obj:`ConfigDict`): Config of decode loss.
+            Defaults to dict(type='mmdet.CrossEntropyLoss', use_sigmoid=False,
+            class_weight=None, loss_weight=1.0).
+        conv_seg_kernel_size (int): The kernel size used in conv_seg.
+            Defaults to 1.
+        ignore_index (int): The label index to be ignored. When using masked
+            BCE loss, ignore_index should be set to None. Defaults to 255.
+        init_cfg (dict or :obj:`ConfigDict` or list[dict or :obj:`ConfigDict`],
+            optional): Initialization config dict. Defaults to None.
     """
 
     def __init__(self,
-                 channels,
-                 num_classes,
-                 dropout_ratio=0.5,
-                 conv_cfg=dict(type='Conv1d'),
-                 norm_cfg=dict(type='BN1d'),
-                 act_cfg=dict(type='ReLU'),
-                 loss_decode=dict(
+                 channels: int,
+                 num_classes: int,
+                 dropout_ratio: float = 0.5,
+                 conv_cfg: ConfigType = dict(type='Conv1d'),
+                 norm_cfg: ConfigType = dict(type='BN1d'),
+                 act_cfg: ConfigType = dict(type='ReLU'),
+                 loss_decode: ConfigType = dict(
                      type='mmdet.CrossEntropyLoss',
                      use_sigmoid=False,
                      class_weight=None,
                      loss_weight=1.0),
-                 ignore_index=255,
-                 init_cfg=None) -> None:
+                 conv_seg_kernel_size: int = 1,
+                 ignore_index: int = 255,
+                 init_cfg: OptMultiConfig = None) -> None:
         super(Base3DDecodeHead, self).__init__(init_cfg=init_cfg)
         self.channels = channels
         self.num_classes = num_classes
         self.dropout_ratio = dropout_ratio
         self.conv_cfg = conv_cfg
         self.norm_cfg = norm_cfg
         self.act_cfg = act_cfg
         self.loss_decode = MODELS.build(loss_decode)
         self.ignore_index = ignore_index
 
-        self.conv_seg = nn.Conv1d(channels, num_classes, kernel_size=1)
+        self.conv_seg = self.build_conv_seg(
+            channels=channels,
+            num_classes=num_classes,
+            kernel_size=conv_seg_kernel_size)
         if dropout_ratio > 0:
             self.dropout = nn.Dropout(dropout_ratio)
         else:
             self.dropout = None
 
-    def init_weights(self):
+    def init_weights(self) -> None:
         """Initialize weights of classification layer."""
         super().init_weights()
         normal_init(self.conv_seg, mean=0, std=0.01)
 
     @abstractmethod
-    def forward(self, feats_dict: dict):
+    def forward(self, feats_dict: dict) -> Tensor:
         """Placeholder of forward function."""
         pass
 
+    def build_conv_seg(self, channels: int, num_classes: int,
+                       kernel_size: int) -> nn.Module:
+        """Build Convolutional Segmentation Layers."""
+        return nn.Conv1d(channels, num_classes, kernel_size=kernel_size)
+
     def cls_seg(self, feat: Tensor) -> Tensor:
         """Classify each points."""
         if self.dropout is not None:
             feat = self.dropout(feat)
         output = self.conv_seg(feat)
         return output
 
-    def loss(self, inputs: List[Tensor], batch_data_samples: SampleList,
-             train_cfg: ConfigType) -> dict:
+    def loss(self, inputs: dict, batch_data_samples: SampleList,
+             train_cfg: ConfigType) -> Dict[str, Tensor]:
         """Forward function for training.
 
         Args:
-            inputs (list[torch.Tensor]): List of multi-level point features.
-            batch_data_samples (List[:obj:`Det3DDataSample`]): The seg
-                data samples. It usually includes information such
-                as `metainfo` and `gt_pts_seg`.
-            train_cfg (dict): The training config.
+            inputs (dict): Feature dict from backbone.
+            batch_data_samples (List[:obj:`Det3DDataSample`]): The seg data
+                samples. It usually includes information such as `metainfo` and
+                `gt_pts_seg`.
+            train_cfg (dict or :obj:`ConfigDict`): The training config.
 
         Returns:
-            dict[str, Tensor]: a dictionary of loss components
+            Dict[str, Tensor]: A dictionary of loss components.
         """
         seg_logits = self.forward(inputs)
         losses = self.loss_by_feat(seg_logits, batch_data_samples)
         return losses
 
-    def predict(self, inputs: List[Tensor], batch_input_metas: List[dict],
-                test_cfg: ConfigType) -> List[Tensor]:
+    def predict(self, inputs: dict, batch_input_metas: List[dict],
+                test_cfg: ConfigType) -> Tensor:
         """Forward function for testing.
 
         Args:
-            inputs (list[Tensor]): List of multi-level point features.
-            batch_data_samples (List[:obj:`Det3DDataSample`]): The seg
-                data samples. It usually includes information such
-                as `metainfo` and `gt_pts_seg`.
-            test_cfg (dict): The testing config.
+            inputs (dict): Feature dict from backbone.
+            batch_input_metas (List[dict]): Meta information of a batch of
+                samples.
+            test_cfg (dict or :obj:`ConfigDict`): The testing config.
 
         Returns:
             Tensor: Output segmentation map.
         """
         seg_logits = self.forward(inputs)
 
         return seg_logits
@@ -144,22 +154,25 @@
         gt_semantic_segs = [
             data_sample.gt_pts_seg.pts_semantic_mask
             for data_sample in batch_data_samples
         ]
         return torch.stack(gt_semantic_segs, dim=0)
 
     def loss_by_feat(self, seg_logit: Tensor,
-                     batch_data_samples: SampleList) -> dict:
+                     batch_data_samples: SampleList) -> Dict[str, Tensor]:
         """Compute semantic segmentation loss.
 
         Args:
-            seg_logit (torch.Tensor): Predicted per-point segmentation logits
-                of shape [B, num_classes, N].
-            batch_data_samples (List[:obj:`Det3DDataSample`]): The seg
-                data samples. It usually includes information such
-                as `metainfo` and `gt_pts_seg`.
+            seg_logit (Tensor): Predicted per-point segmentation logits of
+                shape [B, num_classes, N].
+            batch_data_samples (List[:obj:`Det3DDataSample`]): The seg data
+                samples. It usually includes information such as `metainfo` and
+                `gt_pts_seg`.
+
+        Returns:
+            Dict[str, Tensor]: A dictionary of loss components.
         """
         seg_label = self._stack_batch_gt(batch_data_samples)
         loss = dict()
         loss['loss_sem_seg'] = self.loss_decode(
             seg_logit, seg_label, ignore_index=self.ignore_index)
         return loss
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/decode_heads/dgcnn_head.py` & `mmdet3d-1.1.1/mmdet3d/models/decode_heads/dgcnn_head.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-from typing import Tuple
+from typing import Sequence
 
 from mmcv.cnn.bricks import ConvModule
 from torch import Tensor
 
 from mmdet3d.models.layers import DGCNNFPModule
 from mmdet3d.registry import MODELS
 from .decode_head import Base3DDecodeHead
@@ -14,19 +14,20 @@
     r"""DGCNN decoder head.
 
     Decoder head used in `DGCNN <https://arxiv.org/abs/1801.07829>`_.
     Refer to the
     `reimplementation code <https://github.com/AnTao97/dgcnn.pytorch>`_.
 
     Args:
-        fp_channels (tuple[int], optional): Tuple of mlp channels in feature
+        fp_channels (Sequence[int]): Tuple of mlp channels in feature
             propagation (FP) modules. Defaults to (1216, 512).
     """
 
-    def __init__(self, fp_channels: Tuple = (1216, 512), **kwargs) -> None:
+    def __init__(self, fp_channels: Sequence[int] = (1216, 512),
+                 **kwargs) -> None:
         super(DGCNNHead, self).__init__(**kwargs)
 
         self.FP_module = DGCNNFPModule(
             mlp_channels=fp_channels, act_cfg=self.act_cfg)
 
         # https://github.com/charlesq34/pointnet2/blob/master/models/pointnet2_sem_seg.py#L40
         self.pre_seg_conv = ConvModule(
@@ -41,28 +42,28 @@
     def _extract_input(self, feat_dict: dict) -> Tensor:
         """Extract inputs from features dictionary.
 
         Args:
             feat_dict (dict): Feature dict from backbone.
 
         Returns:
-            torch.Tensor: points for decoder.
+            torch.Tensor: Points for decoder.
         """
         fa_points = feat_dict['fa_points']
 
         return fa_points
 
     def forward(self, feat_dict: dict) -> Tensor:
         """Forward pass.
 
         Args:
             feat_dict (dict): Feature dict from backbone.
 
         Returns:
-            torch.Tensor: Segmentation map of shape [B, num_classes, N].
+            Tensor: Segmentation map of shape [B, num_classes, N].
         """
         fa_points = self._extract_input(feat_dict)
 
         fp_points = self.FP_module(fa_points)
         fp_points = fp_points.transpose(1, 2).contiguous()
         output = self.pre_seg_conv(fp_points)
         output = self.cls_seg(output)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/decode_heads/paconv_head.py` & `mmdet3d-1.1.1/mmdet3d/models/decode_heads/paconv_head.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-from typing import Tuple
+from typing import Sequence
 
 from mmcv.cnn.bricks import ConvModule
 from torch import Tensor
 
 from mmdet3d.registry import MODELS
 from mmdet3d.utils.typing_utils import ConfigType
 from .pointnet2_head import PointNet2Head
@@ -13,24 +13,27 @@
 class PAConvHead(PointNet2Head):
     r"""PAConv decoder head.
 
     Decoder head used in `PAConv <https://arxiv.org/abs/2103.14635>`_.
     Refer to the `official code <https://github.com/CVMI-Lab/PAConv>`_.
 
     Args:
-        fp_channels (tuple[tuple[int]]): Tuple of mlp channels in FP modules.
-        fp_norm_cfg (dict): Config of norm layers used in FP modules.
-            Default: dict(type='BN2d').
+        fp_channels (Sequence[Sequence[int]]): Tuple of mlp channels in FP
+            modules. Defaults to ((768, 256, 256), (384, 256, 256),
+            (320, 256, 128), (128 + 6, 128, 128, 128)).
+        fp_norm_cfg (dict or :obj:`ConfigDict`): Config of norm layers used in
+            FP modules. Defaults to dict(type='BN2d').
     """
 
     def __init__(self,
-                 fp_channels: Tuple[Tuple[int]] = ((768, 256, 256),
-                                                   (384, 256, 256), (320, 256,
-                                                                     128),
-                                                   (128 + 6, 128, 128, 128)),
+                 fp_channels: Sequence[Sequence[int]] = ((768, 256, 256),
+                                                         (384, 256, 256),
+                                                         (320, 256,
+                                                          128), (128 + 6, 128,
+                                                                 128, 128)),
                  fp_norm_cfg: ConfigType = dict(type='BN2d'),
                  **kwargs) -> None:
         super(PAConvHead, self).__init__(
             fp_channels=fp_channels, fp_norm_cfg=fp_norm_cfg, **kwargs)
 
         # https://github.com/CVMI-Lab/PAConv/blob/main/scene_seg/model/pointnet2/pointnet2_paconv_seg.py#L53
         # PointNet++'s decoder conv has bias while PAConv's doesn't have
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/decode_heads/pointnet2_head.py` & `mmdet3d-1.1.1/mmdet3d/models/decode_heads/pointnet2_head.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-from typing import Tuple
+from typing import List, Sequence, Tuple
 
 from mmcv.cnn.bricks import ConvModule
 from torch import Tensor
 from torch import nn as nn
 
 from mmdet3d.models.layers import PointFPModule
 from mmdet3d.registry import MODELS
@@ -15,24 +15,26 @@
 class PointNet2Head(Base3DDecodeHead):
     r"""PointNet2 decoder head.
 
     Decoder head used in `PointNet++ <https://arxiv.org/abs/1706.02413>`_.
     Refer to the `official code <https://github.com/charlesq34/pointnet2>`_.
 
     Args:
-        fp_channels (tuple[tuple[int]]): Tuple of mlp channels in FP modules.
-        fp_norm_cfg (dict): Config of norm layers used in FP modules.
-            Default: dict(type='BN2d').
+        fp_channels (Sequence[Sequence[int]]): Tuple of mlp channels in FP
+            modules. Defaults to ((768, 256, 256), (384, 256, 256),
+            (320, 256, 128), (128, 128, 128, 128)).
+        fp_norm_cfg (dict or :obj:`ConfigDict`): Config of norm layers used
+            in FP modules. Defaults to dict(type='BN2d').
     """
 
     def __init__(self,
-                 fp_channels: Tuple[Tuple[int]] = ((768, 256, 256),
-                                                   (384, 256, 256), (320, 256,
-                                                                     128),
-                                                   (128, 128, 128, 128)),
+                 fp_channels: Sequence[Sequence[int]] = ((768, 256, 256),
+                                                         (384, 256, 256),
+                                                         (320, 256, 128),
+                                                         (128, 128, 128, 128)),
                  fp_norm_cfg: ConfigType = dict(type='BN2d'),
                  **kwargs) -> None:
         super(PointNet2Head, self).__init__(**kwargs)
 
         self.num_fp = len(fp_channels)
         self.FP_modules = nn.ModuleList()
         for cur_fp_mlps in fp_channels:
@@ -45,38 +47,39 @@
             self.channels,
             kernel_size=1,
             bias=True,
             conv_cfg=self.conv_cfg,
             norm_cfg=self.norm_cfg,
             act_cfg=self.act_cfg)
 
-    def _extract_input(self, feat_dict: dict) -> Tensor:
+    def _extract_input(self,
+                       feat_dict: dict) -> Tuple[List[Tensor], List[Tensor]]:
         """Extract inputs from features dictionary.
 
         Args:
             feat_dict (dict): Feature dict from backbone.
 
         Returns:
-            list[torch.Tensor]: Coordinates of multiple levels of points.
-            list[torch.Tensor]: Features of multiple levels of points.
+            Tuple[List[Tensor], List[Tensor]]: Coordinates and features of
+            multiple levels of points.
         """
         sa_xyz = feat_dict['sa_xyz']
         sa_features = feat_dict['sa_features']
         assert len(sa_xyz) == len(sa_features)
 
         return sa_xyz, sa_features
 
     def forward(self, feat_dict: dict) -> Tensor:
         """Forward pass.
 
         Args:
             feat_dict (dict): Feature dict from backbone.
 
         Returns:
-            torch.Tensor: Segmentation map of shape [B, num_classes, N].
+            Tensor: Segmentation map of shape [B, num_classes, N].
         """
         sa_xyz, sa_features = self._extract_input(feat_dict)
 
         # https://github.com/charlesq34/pointnet2/blob/master/models/pointnet2_sem_seg.py#L24
         sa_features[0] = None
 
         fp_feature = sa_features[-1]
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/__init__.py` & `mmdet3d-1.1.1/mmdet3d/models/dense_heads/__init__.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/anchor3d_head.py` & `mmdet3d-1.1.1/mmdet3d/models/dense_heads/anchor3d_head.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import warnings
 from typing import List, Tuple
 
 import numpy as np
 import torch
 from mmdet.models.utils import multi_apply
+from mmdet.utils.memory import cast_tensor_type
+from mmengine.runner import amp
 from torch import Tensor
 from torch import nn as nn
 
 from mmdet3d.models.task_modules import PseudoSampler
 from mmdet3d.models.test_time_augs import merge_aug_bboxes_3d
 from mmdet3d.registry import MODELS, TASK_UTILS
 from mmdet3d.utils.typing_utils import (ConfigType, InstanceList,
@@ -88,15 +90,14 @@
         self.assigner_per_size = assigner_per_size
         self.assign_per_class = assign_per_class
         self.dir_offset = dir_offset
         self.dir_limit_offset = dir_limit_offset
         warnings.warn(
             'dir_offset and dir_limit_offset will be depressed and be '
             'incorporated into box coder in the future')
-        self.fp16_enabled = False
 
         # build anchor generator
         self.prior_generator = TASK_UTILS.build(anchor_generator)
         # In 3D detection, the anchor stride is connected with anchor size
         self.num_anchors = self.prior_generator.num_base_anchors
         # build box coder
         self.bbox_coder = TASK_UTILS.build(bbox_coder)
@@ -108,15 +109,14 @@
             'mmdet.FocalLoss', 'mmdet.GHMC'
         ]
         if not self.use_sigmoid_cls:
             self.num_classes += 1
         self.loss_cls = MODELS.build(loss_cls)
         self.loss_bbox = MODELS.build(loss_bbox)
         self.loss_dir = MODELS.build(loss_dir)
-        self.fp16_enabled = False
 
         self._init_layers()
         self._init_assigner_sampler()
 
         if init_cfg is None:
             self.init_cfg = dict(
                 type='Normal',
@@ -407,21 +407,22 @@
         (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list,
          dir_targets_list, dir_weights_list, num_total_pos,
          num_total_neg) = cls_reg_targets
         num_total_samples = (
             num_total_pos + num_total_neg if self.sampling else num_total_pos)
 
         # num_total_samples = None
-        losses_cls, losses_bbox, losses_dir = multi_apply(
-            self._loss_by_feat_single,
-            cls_scores,
-            bbox_preds,
-            dir_cls_preds,
-            labels_list,
-            label_weights_list,
-            bbox_targets_list,
-            bbox_weights_list,
-            dir_targets_list,
-            dir_weights_list,
-            num_total_samples=num_total_samples)
+        with amp.autocast(enabled=False):
+            losses_cls, losses_bbox, losses_dir = multi_apply(
+                self._loss_by_feat_single,
+                cast_tensor_type(cls_scores, dst_type=torch.float32),
+                cast_tensor_type(bbox_preds, dst_type=torch.float32),
+                cast_tensor_type(dir_cls_preds, dst_type=torch.float32),
+                labels_list,
+                label_weights_list,
+                bbox_targets_list,
+                bbox_weights_list,
+                dir_targets_list,
+                dir_weights_list,
+                num_total_samples=num_total_samples)
         return dict(
             loss_cls=losses_cls, loss_bbox=losses_bbox, loss_dir=losses_dir)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/anchor_free_mono3d_head.py` & `mmdet3d-1.1.1/mmdet3d/models/dense_heads/anchor_free_mono3d_head.py`

 * *Files 0% similar despite different names*

```diff
@@ -172,15 +172,14 @@
             else:
                 self.out_channels.append(-1)
         self.dir_branch = dir_branch
         self.train_cfg = train_cfg
         self.test_cfg = test_cfg
         self.conv_cfg = conv_cfg
         self.norm_cfg = norm_cfg
-        self.fp16_enabled = False
         self.background_label = (
             num_classes if background_label is None else background_label)
         # background_label should be either 0 or num_classes
         assert (self.background_label == 0
                 or self.background_label == num_classes)
         self.pred_attrs = pred_attrs
         self.attr_background_label = -1
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/base_3d_dense_head.py` & `mmdet3d-1.1.1/mmdet3d/models/dense_heads/base_3d_dense_head.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/base_conv_bbox_head.py` & `mmdet3d-1.1.1/mmdet3d/models/dense_heads/base_conv_bbox_head.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/base_mono3d_dense_head.py` & `mmdet3d-1.1.1/mmdet3d/models/dense_heads/base_mono3d_dense_head.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/centerpoint_head.py` & `mmdet3d-1.1.1/mmdet3d/models/dense_heads/centerpoint_head.py`

 * *Files 0% similar despite different names*

```diff
@@ -313,15 +313,14 @@
         self.num_classes = num_classes
         self.norm_bbox = norm_bbox
 
         self.loss_cls = MODELS.build(loss_cls)
         self.loss_bbox = MODELS.build(loss_bbox)
         self.bbox_coder = TASK_UTILS.build(bbox_coder)
         self.num_anchor_per_locs = [n for n in num_classes]
-        self.fp16_enabled = False
 
         # a shared convolution
         self.shared_conv = ConvModule(
             in_channels,
             share_conv_channel,
             kernel_size=3,
             padding=1,
@@ -467,15 +466,15 @@
         gt_labels_3d = gt_instances_3d.labels_3d
         gt_bboxes_3d = gt_instances_3d.bboxes_3d
         device = gt_labels_3d.device
         gt_bboxes_3d = torch.cat(
             (gt_bboxes_3d.gravity_center, gt_bboxes_3d.tensor[:, 3:]),
             dim=1).to(device)
         max_objs = self.train_cfg['max_objs'] * self.train_cfg['dense_reg']
-        grid_size = torch.tensor(self.train_cfg['grid_size'])
+        grid_size = torch.tensor(self.train_cfg['grid_size']).to(device)
         pc_range = torch.tensor(self.train_cfg['point_cloud_range'])
         voxel_size = torch.tensor(self.train_cfg['voxel_size'])
 
         feature_map_size = grid_size[:2] // self.train_cfg['out_size_factor']
 
         # reorganize the gt_dict by tasks
         task_masks = []
@@ -515,24 +514,24 @@
             mask = gt_bboxes_3d.new_zeros((max_objs), dtype=torch.uint8)
 
             num_objs = min(task_boxes[idx].shape[0], max_objs)
 
             for k in range(num_objs):
                 cls_id = task_classes[idx][k] - 1
 
-                width = task_boxes[idx][k][3]
-                length = task_boxes[idx][k][4]
-                width = width / voxel_size[0] / self.train_cfg[
+                length = task_boxes[idx][k][3]
+                width = task_boxes[idx][k][4]
+                length = length / voxel_size[0] / self.train_cfg[
                     'out_size_factor']
-                length = length / voxel_size[1] / self.train_cfg[
+                width = width / voxel_size[1] / self.train_cfg[
                     'out_size_factor']
 
                 if width > 0 and length > 0:
                     radius = gaussian_radius(
-                        (length, width),
+                        (width, length),
                         min_overlap=self.train_cfg['gaussian_overlap'])
                     radius = max(self.train_cfg['min_radius'], int(radius))
 
                     # be really careful for the coordinate system of
                     # your box annotation.
                     x, y, z = task_boxes[idx][k][0], task_boxes[idx][k][
                         1], task_boxes[idx][k][2]
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/fcaf3d_head.py` & `mmdet3d-1.1.1/mmdet3d/models/dense_heads/fcaf3d_head.py`

 * *Files 1% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 # Adapted from https://github.com/SamsungLabs/fcaf3d/blob/master/mmdet3d/models/dense_heads/fcaf3d_neck_with_head.py # noqa
 from typing import List, Optional, Tuple
 
 try:
     import MinkowskiEngine as ME
     from MinkowskiEngine import SparseTensor
 except ImportError:
-    # Please follow getting_started.md to install MinkowskiEngine.
+    # Please follow get_started.md to install MinkowskiEngine.
     ME = SparseTensor = None
     pass
 
 import torch
 from mmcv.cnn import Scale
 from mmcv.ops import nms3d, nms3d_normal
 from mmdet.utils import reduce_mean
@@ -30,15 +30,15 @@
 
     Actually here we store both the sparse 3D FPN and a head. The neck and
     the head can not be simply separated as pruning score on the i-th level
     of FPN requires classification scores from i+1-th level of the head.
 
     Args:
         num_classes (int): Number of classes.
-        in_channels (int): Number of channels in input tensors.
+        in_channels (tuple(int)): Number of channels in input tensors.
         out_channels (int): Number of channels in the neck output tensors.
         num_reg_outs (int): Number of regression layer channels.
         voxel_size (float): Voxel size in meters.
         pts_prune_threshold (int): Pruning threshold on each feature level.
         pts_assign_threshold (int): Box to location assigner parameter.
             Assigner selects the maximum feature level with more locations
             inside the box than pts_assign_threshold.
@@ -55,15 +55,15 @@
         test_cfg (dict, optional): Config for test stage. Defaults to None.
         init_cfg (dict, optional): Config for weight initialization.
             Defaults to None.
     """
 
     def __init__(self,
                  num_classes: int,
-                 in_channels: int,
+                 in_channels: Tuple[int],
                  out_channels: int,
                  num_reg_outs: int,
                  voxel_size: float,
                  pts_prune_threshold: int,
                  pts_assign_threshold: int,
                  pts_center_threshold: int,
                  center_loss: dict = dict(
@@ -72,16 +72,15 @@
                  cls_loss: dict = dict(type='mmdet.FocalLoss'),
                  train_cfg: Optional[dict] = None,
                  test_cfg: Optional[dict] = None,
                  init_cfg: Optional[dict] = None):
         super(FCAF3DHead, self).__init__(init_cfg)
         if ME is None:
             raise ImportError(
-                'Please follow `getting_started.md` to install MinkowskiEngine.`'  # noqa: E501
-            )
+                'Please follow `get_started.md` to install MinkowskiEngine.`')
         self.voxel_size = voxel_size
         self.pts_prune_threshold = pts_prune_threshold
         self.pts_assign_threshold = pts_assign_threshold
         self.pts_center_threshold = pts_center_threshold
         self.center_loss = MODELS.build(center_loss)
         self.bbox_loss = MODELS.build(bbox_loss)
         self.cls_loss = MODELS.build(cls_loss)
@@ -336,16 +335,16 @@
                 levels. The second list contains predictions in a mini-batch.
             points (list[list[Tensor]]): Final location coordinates for all
                 scenes. The first list contains predictions from different
                 levels. The second list contains predictions in a mini-batch.
             batch_gt_instances_3d (list[:obj:`InstanceData`]): Batch of
                 gt_instance_3d.  It usually includes ``bboxes_3d```
                 `labels_3d````depths````centers_2d`` and attributes.
-            batch_img_metas (list[dict]): Meta information of each image, e.g.,
-                image size, scaling factor, etc.
+            batch_input_metas (list[dict]): Meta information of each input,
+                e.g., image size, scaling factor, etc.
             batch_gt_instances_ignore (list[:obj:`InstanceData`], optional):
                 Batch of gt_instances_ignore. It includes ``bboxes`` attribute
                 data that is ignored during training and testing.
                 Defaults to None.
 
         Returns:
             dict: Centerness, bbox, and classification losses.
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/fcos_mono3d_head.py` & `mmdet3d-1.1.1/mmdet3d/models/dense_heads/fcos_mono3d_head.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/free_anchor3d_head.py` & `mmdet3d-1.1.1/mmdet3d/models/dense_heads/free_anchor3d_head.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/groupfree3d_head.py` & `mmdet3d-1.1.1/mmdet3d/models/dense_heads/groupfree3d_head.py`

 * *Files 1% similar despite different names*

```diff
@@ -210,16 +210,14 @@
         self.num_dir_bins = self.bbox_coder.num_dir_bins
 
         # Initial object candidate sampling
         self.gsample_module = GeneralSamplingModule()
         self.fps_module = Points_Sampler([self.num_proposal])
         self.points_obj_cls = PointsObjClsModule(self.in_channels)
 
-        self.fp16_enabled = False
-
         # initial candidate prediction
         self.conv_pred = BaseConvBboxHead(
             **pred_layer_cfg,
             num_cls_out_channels=self._get_cls_out_channels(),
             num_reg_out_channels=self._get_reg_out_channels())
 
         # query proj and key proj
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/imvoxel_head.py` & `mmdet3d-1.1.1/mmdet3d/models/dense_heads/imvoxel_head.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/monoflex_head.py` & `mmdet3d-1.1.1/mmdet3d/models/dense_heads/monoflex_head.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/parta2_rpn_head.py` & `mmdet3d-1.1.1/mmdet3d/models/dense_heads/parta2_rpn_head.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/pgd_head.py` & `mmdet3d-1.1.1/mmdet3d/models/dense_heads/pgd_head.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/point_rpn_head.py` & `mmdet3d-1.1.1/mmdet3d/models/dense_heads/point_rpn_head.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/shape_aware_head.py` & `mmdet3d-1.1.1/mmdet3d/models/dense_heads/shape_aware_head.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/smoke_mono3d_head.py` & `mmdet3d-1.1.1/mmdet3d/models/dense_heads/smoke_mono3d_head.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/ssd_3d_head.py` & `mmdet3d-1.1.1/mmdet3d/models/dense_heads/ssd_3d_head.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/train_mixins.py` & `mmdet3d-1.1.1/mmdet3d/models/dense_heads/train_mixins.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/dense_heads/vote_head.py` & `mmdet3d-1.1.1/mmdet3d/models/dense_heads/vote_head.py`

 * *Files 0% similar despite different names*

```diff
@@ -95,15 +95,14 @@
 
         self.bbox_coder = TASK_UTILS.build(bbox_coder)
         self.num_sizes = self.bbox_coder.num_sizes
         self.num_dir_bins = self.bbox_coder.num_dir_bins
 
         self.vote_module = VoteModule(**vote_module_cfg)
         self.vote_aggregation = build_sa_module(vote_aggregation_cfg)
-        self.fp16_enabled = False
 
         # Bbox classification and regression
         self.conv_pred = BaseConvBboxHead(
             **pred_layer_cfg,
             num_cls_out_channels=self._get_cls_out_channels(),
             num_reg_out_channels=self._get_reg_out_channels())
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/__init__.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/__init__.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/base.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/base.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/centerpoint.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/centerpoint.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/dfm.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/dfm.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/dynamic_voxelnet.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/dynamic_voxelnet.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/fcos_mono3d.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/fcos_mono3d.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/groupfree3dnet.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/groupfree3dnet.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/h3dnet.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/h3dnet.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/imvotenet.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/imvotenet.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/imvoxelnet.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/imvoxelnet.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/mink_single_stage.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/mink_single_stage.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 
 import torch
 from torch import Tensor
 
 try:
     import MinkowskiEngine as ME
 except ImportError:
-    # Please follow getting_started.md to install MinkowskiEngine.
+    # Please follow get_started.md to install MinkowskiEngine.
     ME = None
     pass
 
 from mmdet3d.registry import MODELS
 from mmdet3d.utils import ConfigType, OptConfigType, OptMultiConfig
 from .single_stage import SingleStage3DDetector
 
@@ -55,16 +55,15 @@
             bbox_head=bbox_head,
             train_cfg=train_cfg,
             test_cfg=test_cfg,
             data_preprocessor=data_preprocessor,
             init_cfg=init_cfg)
         if ME is None:
             raise ImportError(
-                'Please follow `getting_started.md` to install MinkowskiEngine.`'  # noqa: E501
-            )
+                'Please follow `get_started.md` to install MinkowskiEngine.`')
         self.voxel_size = bbox_head['voxel_size']
 
     def extract_feat(
         self, batch_inputs_dict: Dict[str, Tensor]
     ) -> Union[Tuple[torch.Tensor], Dict[str, Tensor]]:
         """Directly extract features from the backbone+neck.
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/multiview_dfm.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/multiview_dfm.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/mvx_faster_rcnn.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/mvx_faster_rcnn.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/mvx_two_stage.py` & `mmdet3d-1.1.1/mmdet3d/models/roi_heads/part_aggregation_roi_head.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,407 +1,379 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-import copy
-from typing import Dict, List, Optional, Sequence
+from typing import Dict, List, Tuple
 
-import torch
-from mmengine.structures import InstanceData
+from mmdet.models.task_modules import AssignResult, SamplingResult
+from mmengine import ConfigDict
 from torch import Tensor
+from torch.nn import functional as F
 
 from mmdet3d.registry import MODELS
-from mmdet3d.structures import Det3DDataSample
-from .base import Base3DDetector
+from mmdet3d.structures import bbox3d2roi
+from mmdet3d.utils import InstanceList
+from ...structures.det3d_data_sample import SampleList
+from .base_3droi_head import Base3DRoIHead
 
 
 @MODELS.register_module()
-class MVXTwoStageDetector(Base3DDetector):
-    """Base class of Multi-modality VoxelNet.
+class PartAggregationROIHead(Base3DRoIHead):
+    """Part aggregation roi head for PartA2.
 
     Args:
-        pts_voxel_encoder (dict, optional): Point voxelization
-            encoder layer. Defaults to None.
-        pts_middle_encoder (dict, optional): Middle encoder layer
-            of points cloud modality. Defaults to None.
-        pts_fusion_layer (dict, optional): Fusion layer.
-            Defaults to None.
-        img_backbone (dict, optional): Backbone of extracting
-            images feature. Defaults to None.
-        pts_backbone (dict, optional): Backbone of extracting
-            points features. Defaults to None.
-        img_neck (dict, optional): Neck of extracting
-            image features. Defaults to None.
-        pts_neck (dict, optional): Neck of extracting
-            points features. Defaults to None.
-        pts_bbox_head (dict, optional): Bboxes head of
-            point cloud modality. Defaults to None.
-        img_roi_head (dict, optional): RoI head of image
-            modality. Defaults to None.
-        img_rpn_head (dict, optional): RPN head of image
-            modality. Defaults to None.
-        train_cfg (dict, optional): Train config of model.
-            Defaults to None.
-        test_cfg (dict, optional): Train config of model.
-            Defaults to None.
-        init_cfg (dict, optional): Initialize config of
-            model. Defaults to None.
-        data_preprocessor (dict or ConfigDict, optional): The pre-process
-            config of :class:`Det3DDataPreprocessor`. Defaults to None.
+        semantic_head (ConfigDict): Config of semantic head.
+        num_classes (int): The number of classes.
+        seg_roi_extractor (ConfigDict): Config of seg_roi_extractor.
+        bbox_roi_extractor (ConfigDict): Config of part_roi_extractor.
+        bbox_head (ConfigDict): Config of bbox_head.
+        train_cfg (ConfigDict): Training config.
+        test_cfg (ConfigDict): Testing config.
     """
 
     def __init__(self,
-                 pts_voxel_encoder: Optional[dict] = None,
-                 pts_middle_encoder: Optional[dict] = None,
-                 pts_fusion_layer: Optional[dict] = None,
-                 img_backbone: Optional[dict] = None,
-                 pts_backbone: Optional[dict] = None,
-                 img_neck: Optional[dict] = None,
-                 pts_neck: Optional[dict] = None,
-                 pts_bbox_head: Optional[dict] = None,
-                 img_roi_head: Optional[dict] = None,
-                 img_rpn_head: Optional[dict] = None,
-                 train_cfg: Optional[dict] = None,
-                 test_cfg: Optional[dict] = None,
-                 init_cfg: Optional[dict] = None,
-                 data_preprocessor: Optional[dict] = None,
-                 **kwargs):
-        super(MVXTwoStageDetector, self).__init__(
-            init_cfg=init_cfg, data_preprocessor=data_preprocessor, **kwargs)
-
-        if pts_voxel_encoder:
-            self.pts_voxel_encoder = MODELS.build(pts_voxel_encoder)
-        if pts_middle_encoder:
-            self.pts_middle_encoder = MODELS.build(pts_middle_encoder)
-        if pts_backbone:
-            self.pts_backbone = MODELS.build(pts_backbone)
-        if pts_fusion_layer:
-            self.pts_fusion_layer = MODELS.build(pts_fusion_layer)
-        if pts_neck is not None:
-            self.pts_neck = MODELS.build(pts_neck)
-        if pts_bbox_head:
-            pts_train_cfg = train_cfg.pts if train_cfg else None
-            pts_bbox_head.update(train_cfg=pts_train_cfg)
-            pts_test_cfg = test_cfg.pts if test_cfg else None
-            pts_bbox_head.update(test_cfg=pts_test_cfg)
-            self.pts_bbox_head = MODELS.build(pts_bbox_head)
-
-        if img_backbone:
-            self.img_backbone = MODELS.build(img_backbone)
-        if img_neck is not None:
-            self.img_neck = MODELS.build(img_neck)
-        if img_rpn_head is not None:
-            self.img_rpn_head = MODELS.build(img_rpn_head)
-        if img_roi_head is not None:
-            self.img_roi_head = MODELS.build(img_roi_head)
+                 semantic_head: dict,
+                 num_classes: int = 3,
+                 seg_roi_extractor: dict = None,
+                 bbox_head: dict = None,
+                 bbox_roi_extractor: dict = None,
+                 train_cfg: dict = None,
+                 test_cfg: dict = None,
+                 init_cfg: dict = None) -> None:
+        super(PartAggregationROIHead, self).__init__(
+            bbox_head=bbox_head,
+            bbox_roi_extractor=bbox_roi_extractor,
+            train_cfg=train_cfg,
+            test_cfg=test_cfg,
+            init_cfg=init_cfg)
+        self.num_classes = num_classes
+        assert semantic_head is not None
+        self.init_seg_head(seg_roi_extractor, semantic_head)
+
+    def init_seg_head(self, seg_roi_extractor: dict,
+                      semantic_head: dict) -> None:
+        """Initialize semantic head and seg roi extractor.
 
-        self.train_cfg = train_cfg
-        self.test_cfg = test_cfg
-
-    @property
-    def with_img_shared_head(self):
-        """bool: Whether the detector has a shared head in image branch."""
-        return hasattr(self,
-                       'img_shared_head') and self.img_shared_head is not None
-
-    @property
-    def with_pts_bbox(self):
-        """bool: Whether the detector has a 3D box head."""
-        return hasattr(self,
-                       'pts_bbox_head') and self.pts_bbox_head is not None
-
-    @property
-    def with_img_bbox(self):
-        """bool: Whether the detector has a 2D image box head."""
-        return hasattr(self,
-                       'img_bbox_head') and self.img_bbox_head is not None
-
-    @property
-    def with_img_backbone(self):
-        """bool: Whether the detector has a 2D image backbone."""
-        return hasattr(self, 'img_backbone') and self.img_backbone is not None
-
-    @property
-    def with_pts_backbone(self):
-        """bool: Whether the detector has a 3D backbone."""
-        return hasattr(self, 'pts_backbone') and self.pts_backbone is not None
+        Args:
+            seg_roi_extractor (dict): Config of seg
+                roi extractor.
+            semantic_head (dict): Config of semantic head.
+        """
+        self.semantic_head = MODELS.build(semantic_head)
+        self.seg_roi_extractor = MODELS.build(seg_roi_extractor)
 
     @property
-    def with_fusion(self):
-        """bool: Whether the detector has a fusion layer."""
+    def with_semantic(self):
+        """bool: whether the head has semantic branch"""
         return hasattr(self,
-                       'pts_fusion_layer') and self.fusion_layer is not None
+                       'semantic_head') and self.semantic_head is not None
 
-    @property
-    def with_img_neck(self):
-        """bool: Whether the detector has a neck in image branch."""
-        return hasattr(self, 'img_neck') and self.img_neck is not None
+    def _bbox_forward_train(self, feats_dict: Dict, voxels_dict: Dict,
+                            sampling_results: List[SamplingResult]) -> Dict:
+        """Forward training function of roi_extractor and bbox_head.
 
-    @property
-    def with_pts_neck(self):
-        """bool: Whether the detector has a neck in 3D detector branch."""
-        return hasattr(self, 'pts_neck') and self.pts_neck is not None
-
-    @property
-    def with_img_rpn(self):
-        """bool: Whether the detector has a 2D RPN in image detector branch."""
-        return hasattr(self, 'img_rpn_head') and self.img_rpn_head is not None
-
-    @property
-    def with_img_roi_head(self):
-        """bool: Whether the detector has a RoI Head in image branch."""
-        return hasattr(self, 'img_roi_head') and self.img_roi_head is not None
-
-    @property
-    def with_voxel_encoder(self):
-        """bool: Whether the detector has a voxel encoder."""
-        return hasattr(self,
-                       'voxel_encoder') and self.voxel_encoder is not None
-
-    @property
-    def with_middle_encoder(self):
-        """bool: Whether the detector has a middle encoder."""
-        return hasattr(self,
-                       'middle_encoder') and self.middle_encoder is not None
+        Args:
+            feats_dict (dict): Contains features from the first stage.
+            voxels_dict (dict): Contains information of voxels.
+            sampling_results (:obj:`SamplingResult`): Sampled results used
+                for training.
 
-    def _forward(self):
-        pass
+        Returns:
+            dict: Forward results including losses and predictions.
+        """
+        rois = bbox3d2roi([res.bboxes for res in sampling_results])
+        bbox_results = self._bbox_forward(feats_dict, voxels_dict, rois)
 
-    def extract_img_feat(self, img: Tensor, input_metas: List[dict]) -> dict:
-        """Extract features of images."""
-        if self.with_img_backbone and img is not None:
-            input_shape = img.shape[-2:]
-            # update real input shape of each single img
-            for img_meta in input_metas:
-                img_meta.update(input_shape=input_shape)
-
-            if img.dim() == 5 and img.size(0) == 1:
-                img.squeeze_()
-            elif img.dim() == 5 and img.size(0) > 1:
-                B, N, C, H, W = img.size()
-                img = img.view(B * N, C, H, W)
-            img_feats = self.img_backbone(img)
-        else:
-            return None
-        if self.with_img_neck:
-            img_feats = self.img_neck(img_feats)
-        return img_feats
-
-    def extract_pts_feat(
-            self,
-            voxel_dict: Dict[str, Tensor],
-            points: Optional[List[Tensor]] = None,
-            img_feats: Optional[Sequence[Tensor]] = None,
-            batch_input_metas: Optional[List[dict]] = None
-    ) -> Sequence[Tensor]:
-        """Extract features of points.
+        bbox_targets = self.bbox_head.get_targets(sampling_results,
+                                                  self.train_cfg)
+        loss_bbox = self.bbox_head.loss(bbox_results['cls_score'],
+                                        bbox_results['bbox_pred'], rois,
+                                        *bbox_targets)
+
+        bbox_results.update(loss_bbox=loss_bbox)
+        return bbox_results
+
+    def _assign_and_sample(
+            self, rpn_results_list: InstanceList,
+            batch_gt_instances_3d: InstanceList,
+            batch_gt_instances_ignore: InstanceList) -> List[SamplingResult]:
+        """Assign and sample proposals for training.
 
         Args:
-            voxel_dict(Dict[str, Tensor]): Dict of voxelization infos.
-            points (List[tensor], optional):  Point cloud of multiple inputs.
-            img_feats (list[Tensor], tuple[tensor], optional): Features from
-                image backbone.
-            batch_input_metas (list[dict], optional): The meta information
-                of multiple samples. Defaults to True.
+            rpn_results_list (List[:obj:`InstanceData`]): Detection results
+                of rpn head.
+            batch_gt_instances_3d (list[:obj:`InstanceData`]): Batch of
+                gt_instances. It usually includes ``bboxes_3d`` and
+                ``labels_3d`` attributes.
+            batch_gt_instances_ignore (list): Ignore instances of gt bboxes.
 
         Returns:
-            Sequence[tensor]: points features of multiple inputs
-            from backbone or neck.
+            list[:obj:`SamplingResult`]: Sampled results of each training
+                sample.
         """
-        if not self.with_pts_bbox:
-            return None
-        voxel_features = self.pts_voxel_encoder(voxel_dict['voxels'],
-                                                voxel_dict['num_points'],
-                                                voxel_dict['coors'], img_feats,
-                                                batch_input_metas)
-        batch_size = voxel_dict['coors'][-1, 0] + 1
-        x = self.pts_middle_encoder(voxel_features, voxel_dict['coors'],
-                                    batch_size)
-        x = self.pts_backbone(x)
-        if self.with_pts_neck:
-            x = self.pts_neck(x)
-        return x
-
-    def extract_feat(self, batch_inputs_dict: dict,
-                     batch_input_metas: List[dict]) -> tuple:
-        """Extract features from images and points.
+        sampling_results = []
+        # bbox assign
+        for batch_idx in range(len(rpn_results_list)):
+            cur_proposal_list = rpn_results_list[batch_idx]
+            cur_boxes = cur_proposal_list['bboxes_3d']
+            cur_labels_3d = cur_proposal_list['labels_3d']
+            cur_gt_instances_3d = batch_gt_instances_3d[batch_idx]
+            cur_gt_instances_ignore = batch_gt_instances_ignore[batch_idx]
+            cur_gt_instances_3d.bboxes_3d = cur_gt_instances_3d.\
+                bboxes_3d.tensor
+            cur_gt_bboxes = cur_gt_instances_3d.bboxes_3d.to(cur_boxes.device)
+            cur_gt_labels = cur_gt_instances_3d.labels_3d
+
+            batch_num_gts = 0
+            # 0 is bg
+            batch_gt_indis = cur_gt_labels.new_full((len(cur_boxes), ), 0)
+            batch_max_overlaps = cur_boxes.tensor.new_zeros(len(cur_boxes))
+            # -1 is bg
+            batch_gt_labels = cur_gt_labels.new_full((len(cur_boxes), ), -1)
+
+            # each class may have its own assigner
+            if isinstance(self.bbox_assigner, list):
+                for i, assigner in enumerate(self.bbox_assigner):
+                    gt_per_cls = (cur_gt_labels == i)
+                    pred_per_cls = (cur_labels_3d == i)
+                    cur_assign_res = assigner.assign(
+                        cur_proposal_list[pred_per_cls],
+                        cur_gt_instances_3d[gt_per_cls],
+                        cur_gt_instances_ignore)
+                    # gather assign_results in different class into one result
+                    batch_num_gts += cur_assign_res.num_gts
+                    # gt inds (1-based)
+                    gt_inds_arange_pad = gt_per_cls.nonzero(
+                        as_tuple=False).view(-1) + 1
+                    # pad 0 for indice unassigned
+                    gt_inds_arange_pad = F.pad(
+                        gt_inds_arange_pad, (1, 0), mode='constant', value=0)
+                    # pad -1 for indice ignore
+                    gt_inds_arange_pad = F.pad(
+                        gt_inds_arange_pad, (1, 0), mode='constant', value=-1)
+                    # convert to 0~gt_num+2 for indices
+                    gt_inds_arange_pad += 1
+                    # now 0 is bg, >1 is fg in batch_gt_indis
+                    batch_gt_indis[pred_per_cls] = gt_inds_arange_pad[
+                        cur_assign_res.gt_inds + 1] - 1
+                    batch_max_overlaps[
+                        pred_per_cls] = cur_assign_res.max_overlaps
+                    batch_gt_labels[pred_per_cls] = cur_assign_res.labels
+
+                assign_result = AssignResult(batch_num_gts, batch_gt_indis,
+                                             batch_max_overlaps,
+                                             batch_gt_labels)
+            else:  # for single class
+                assign_result = self.bbox_assigner.assign(
+                    cur_proposal_list, cur_gt_instances_3d,
+                    cur_gt_instances_ignore)
+            # sample boxes
+            sampling_result = self.bbox_sampler.sample(assign_result,
+                                                       cur_boxes.tensor,
+                                                       cur_gt_bboxes,
+                                                       cur_gt_labels)
+            sampling_results.append(sampling_result)
+        return sampling_results
+
+    def _semantic_forward_train(self, feats_dict: dict, voxel_dict: dict,
+                                batch_gt_instances_3d: InstanceList) -> Dict:
+        """Train semantic head.
 
         Args:
-            batch_inputs_dict (dict): Dict of batch inputs. It
-                contains
-
-                - points (List[tensor]):  Point cloud of multiple inputs.
-                - imgs (tensor): Image tensor with shape (B, C, H, W).
-            batch_input_metas (list[dict]): Meta information of multiple inputs
-                in a batch.
+            feats_dict (dict): Contains features from the first stage.
+            voxel_dict (dict): Contains information of voxels.
+            batch_gt_instances_3d (list[:obj:`InstanceData`]): Batch of
+                gt_instances. It usually includes ``bboxes_3d`` and
+                ``labels_3d`` attributes.
 
         Returns:
-             tuple: Two elements in tuple arrange as
-             image features and point cloud features.
+            dict: Segmentation results including losses
         """
-        voxel_dict = batch_inputs_dict.get('voxels', None)
-        imgs = batch_inputs_dict.get('imgs', None)
-        points = batch_inputs_dict.get('points', None)
-        img_feats = self.extract_img_feat(imgs, batch_input_metas)
-        pts_feats = self.extract_pts_feat(
-            voxel_dict,
-            points=points,
-            img_feats=img_feats,
-            batch_input_metas=batch_input_metas)
-        return (img_feats, pts_feats)
-
-    def loss(self, batch_inputs_dict: Dict[List, torch.Tensor],
-             batch_data_samples: List[Det3DDataSample],
-             **kwargs) -> List[Det3DDataSample]:
-        """
-        Args:
-            batch_inputs_dict (dict): The model input dict which include
-                'points' and `imgs` keys.
+        semantic_results = self.semantic_head(feats_dict['seg_features'])
+        semantic_targets = self.semantic_head.get_targets(
+            voxel_dict, batch_gt_instances_3d)
+        loss_semantic = self.semantic_head.loss(semantic_results,
+                                                semantic_targets)
+        semantic_results.update(loss_semantic=loss_semantic)
+        return semantic_results
+
+    def predict(self,
+                feats_dict: Dict,
+                rpn_results_list: InstanceList,
+                batch_data_samples: SampleList,
+                rescale: bool = False,
+                **kwargs) -> InstanceList:
+        """Perform forward propagation of the roi head and predict detection
+        results on the features of the upstream network.
 
-                - points (list[torch.Tensor]): Point cloud of each sample.
-                - imgs (torch.Tensor): Tensor of batch images, has shape
-                  (B, C, H ,W)
+        Args:
+            feats_dict (dict): Contains features from the first stage.
+            rpn_results_list (List[:obj:`InstanceData`]): Detection results
+                of rpn head.
             batch_data_samples (List[:obj:`Det3DDataSample`]): The Data
-                Samples. It usually includes information such as
-                `gt_instance_3d`, .
+                samples. It usually includes information such as
+                `gt_instance_3d`, `gt_panoptic_seg_3d` and `gt_sem_seg_3d`.
+            rescale (bool): If True, return boxes in original image space.
+                Defaults to False.
 
         Returns:
-            dict[str, Tensor]: A dictionary of loss components.
+            list[:obj:`InstanceData`]: Detection results of each sample
+            after the post process.
+            Each item usually contains following keys.
 
+            - scores_3d (Tensor): Classification scores, has a shape
+              (num_instances, )
+            - labels_3d (Tensor): Labels of bboxes, has a shape
+              (num_instances, ).
+            - bboxes_3d (BaseInstance3DBoxes): Prediction of bboxes,
+              contains a tensor with shape (num_instances, C), where
+              C >= 7.
         """
+        assert self.with_bbox, 'Bbox head must be implemented in PartA2.'
+        assert self.with_semantic, 'Semantic head must be implemented' \
+                                   ' in PartA2.'
+
+        batch_input_metas = [
+            data_samples.metainfo for data_samples in batch_data_samples
+        ]
+        voxels_dict = feats_dict.pop('voxels_dict')
+        # TODO: Split predict semantic and bbox
+        results_list = self.predict_bbox(feats_dict, voxels_dict,
+                                         batch_input_metas, rpn_results_list,
+                                         self.test_cfg)
+        return results_list
 
-        batch_input_metas = [item.metainfo for item in batch_data_samples]
-        img_feats, pts_feats = self.extract_feat(batch_inputs_dict,
-                                                 batch_input_metas)
-        losses = dict()
-        if pts_feats:
-            losses_pts = self.pts_bbox_head.loss(pts_feats, batch_data_samples,
-                                                 **kwargs)
-            losses.update(losses_pts)
-        if img_feats:
-            losses_img = self.loss_imgs(img_feats, batch_data_samples)
-            losses.update(losses_img)
-        return losses
+    def predict_bbox(self, feats_dict: Dict, voxel_dict: Dict,
+                     batch_input_metas: List[dict],
+                     rpn_results_list: InstanceList,
+                     test_cfg: ConfigDict) -> InstanceList:
+        """Perform forward propagation of the bbox head and predict detection
+        results on the features of the upstream network.
 
-    def loss_imgs(self, x: List[Tensor],
-                  batch_data_samples: List[Det3DDataSample], **kwargs):
-        """Forward function for image branch.
+        Args:
+            feats_dict (dict): Contains features from the first stage.
+            voxel_dict (dict): Contains information of voxels.
+            batch_input_metas (list[dict], Optional): Batch image meta info.
+                Defaults to None.
+            rpn_results_list (List[:obj:`InstanceData`]): Detection results
+                of rpn head.
+            test_cfg (Config): Test config.
 
-        This function works similar to the forward function of Faster R-CNN.
+        Returns:
+            list[:obj:`InstanceData`]: Detection results of each sample
+            after the post process.
+            Each item usually contains following keys.
+
+            - scores_3d (Tensor): Classification scores, has a shape
+              (num_instances, )
+            - labels_3d (Tensor): Labels of bboxes, has a shape
+              (num_instances, ).
+            - bboxes_3d (BaseInstance3DBoxes): Prediction of bboxes,
+              contains a tensor with shape (num_instances, C), where
+              C >= 7.
+        """
+        semantic_results = self.semantic_head(feats_dict['seg_features'])
+        feats_dict.update(semantic_results)
+        rois = bbox3d2roi(
+            [res['bboxes_3d'].tensor for res in rpn_results_list])
+        labels_3d = [res['labels_3d'] for res in rpn_results_list]
+        cls_preds = [res['cls_preds'] for res in rpn_results_list]
+        bbox_results = self._bbox_forward(feats_dict, voxel_dict, rois)
+
+        bbox_list = self.bbox_head.get_results(rois, bbox_results['cls_score'],
+                                               bbox_results['bbox_pred'],
+                                               labels_3d, cls_preds,
+                                               batch_input_metas, test_cfg)
+        return bbox_list
+
+    def _bbox_forward(self, feats_dict: Dict, voxel_dict: Dict,
+                      rois: Tensor) -> Dict:
+        """Forward function of roi_extractor and bbox_head used in both
+        training and testing.
 
         Args:
-            x (list[torch.Tensor]): Image features of shape (B, C, H, W)
-                of multiple levels.
-            batch_data_samples (List[:obj:`Det3DDataSample`]): The Data
-                Samples. It usually includes information such as
-                `gt_instance_3d`, .
+            feats_dict (dict): Contains features from the first stage.
+            voxel_dict (dict): Contains information of voxels.
+            rois (Tensor): Roi boxes.
 
         Returns:
-            dict: Losses of each branch.
+            dict: Contains predictions of bbox_head and
+                features of roi_extractor.
         """
-        losses = dict()
-        # RPN forward and loss
-        if self.with_img_rpn:
-            proposal_cfg = self.test_cfg.rpn
-            rpn_data_samples = copy.deepcopy(batch_data_samples)
-            # set cat_id of gt_labels to 0 in RPN
-            for data_sample in rpn_data_samples:
-                data_sample.gt_instances.labels = \
-                    torch.zeros_like(data_sample.gt_instances.labels)
-            rpn_losses, rpn_results_list = self.img_rpn_head.loss_and_predict(
-                x, rpn_data_samples, proposal_cfg=proposal_cfg, **kwargs)
-            # avoid get same name with roi_head loss
-            keys = rpn_losses.keys()
-            for key in keys:
-                if 'loss' in key and 'rpn' not in key:
-                    rpn_losses[f'rpn_{key}'] = rpn_losses.pop(key)
-            losses.update(rpn_losses)
-
-        else:
-            if 'proposals' in batch_data_samples[0]:
-                # use pre-defined proposals in InstanceData
-                # for the second stage
-                # to extract ROI features.
-                rpn_results_list = [
-                    data_sample.proposals for data_sample in batch_data_samples
-                ]
-            else:
-                rpn_results_list = None
-        # bbox head forward and loss
-        if self.with_img_bbox:
-            roi_losses = self.img_roi_head.loss(x, rpn_results_list,
-                                                batch_data_samples, **kwargs)
-            losses.update(roi_losses)
-        return losses
-
-    def predict_imgs(self,
-                     x: List[Tensor],
-                     batch_data_samples: List[Det3DDataSample],
-                     rescale: bool = True,
-                     **kwargs) -> InstanceData:
-        """Predict results from a batch of inputs and data samples with post-
-        processing.
+        pooled_seg_feats = self.seg_roi_extractor(feats_dict['seg_features'],
+                                                  voxel_dict['voxel_centers'],
+                                                  voxel_dict['coors'][...,
+                                                                      0], rois)
+        pooled_part_feats = self.bbox_roi_extractor(
+            feats_dict['part_feats'], voxel_dict['voxel_centers'],
+            voxel_dict['coors'][..., 0], rois)
+        cls_score, bbox_pred = self.bbox_head(pooled_seg_feats,
+                                              pooled_part_feats)
+
+        bbox_results = dict(
+            cls_score=cls_score,
+            bbox_pred=bbox_pred,
+            pooled_seg_feats=pooled_seg_feats,
+            pooled_part_feats=pooled_part_feats)
+        return bbox_results
+
+    def loss(self, feats_dict: Dict, rpn_results_list: InstanceList,
+             batch_data_samples: SampleList, **kwargs) -> dict:
+        """Perform forward propagation and loss calculation of the detection
+        roi on the features of the upstream network.
 
         Args:
-            x (List[Tensor]): Image features from FPN.
+            feats_dict (dict): Contains features from the first stage.
+            rpn_results_list (List[:obj:`InstanceData`]): Detection results
+                of rpn head.
             batch_data_samples (List[:obj:`Det3DDataSample`]): The Data
-                Samples. It usually includes information such as
-                `gt_instance`, `gt_panoptic_seg` and `gt_sem_seg`.
-            rescale (bool): Whether to rescale the results.
-                Defaults to True.
+                samples. It usually includes information such as
+                `gt_instance_3d`, `gt_panoptic_seg_3d` and `gt_sem_seg_3d`.
+
+        Returns:
+            dict[str, Tensor]: A dictionary of loss components
         """
+        assert len(rpn_results_list) == len(batch_data_samples)
+        losses = dict()
+        batch_gt_instances_3d = []
+        batch_gt_instances_ignore = []
+        voxels_dict = feats_dict.pop('voxels_dict')
+        for data_sample in batch_data_samples:
+            batch_gt_instances_3d.append(data_sample.gt_instances_3d)
+            if 'ignored_instances' in data_sample:
+                batch_gt_instances_ignore.append(data_sample.ignored_instances)
+            else:
+                batch_gt_instances_ignore.append(None)
+        if self.with_semantic:
+            semantic_results = self._semantic_forward_train(
+                feats_dict, voxels_dict, batch_gt_instances_3d)
+            losses.update(semantic_results.pop('loss_semantic'))
+
+        sample_results = self._assign_and_sample(rpn_results_list,
+                                                 batch_gt_instances_3d,
+                                                 batch_gt_instances_ignore)
+        if self.with_bbox:
+            feats_dict.update(semantic_results)
+            bbox_results = self._bbox_forward_train(feats_dict, voxels_dict,
+                                                    sample_results)
+            losses.update(bbox_results['loss_bbox'])
 
-        if batch_data_samples[0].get('proposals', None) is None:
-            rpn_results_list = self.img_rpn_head.predict(
-                x, batch_data_samples, rescale=False)
-        else:
-            rpn_results_list = [
-                data_sample.proposals for data_sample in batch_data_samples
-            ]
-        results_list = self.img_roi_head.predict(
-            x, rpn_results_list, batch_data_samples, rescale=rescale, **kwargs)
-        return results_list
+        return losses
 
-    def predict(self, batch_inputs_dict: Dict[str, Optional[Tensor]],
-                batch_data_samples: List[Det3DDataSample],
-                **kwargs) -> List[Det3DDataSample]:
-        """Forward of testing.
+    def _forward(self, feats_dict: dict,
+                 rpn_results_list: InstanceList) -> Tuple:
+        """Network forward process. Usually includes backbone, neck and head
+        forward without any post-processing.
 
         Args:
-            batch_inputs_dict (dict): The model input dict which include
-                'points' keys.
-
-                - points (list[torch.Tensor]): Point cloud of each sample.
-            batch_data_samples (List[:obj:`Det3DDataSample`]): The Data
-                Samples. It usually includes information such as
-                `gt_instance_3d`.
+            feats_dict (dict): Contains features from the first stage.
+            rpn_results_list (List[:obj:`InstanceData`]): Detection results
+                of rpn head.
 
         Returns:
-            list[:obj:`Det3DDataSample`]: Detection results of the
-            input sample. Each Det3DDataSample usually contain
-            'pred_instances_3d'. And the ``pred_instances_3d`` usually
-            contains following keys.
-
-            - scores_3d (Tensor): Classification scores, has a shape
-                (num_instances, )
-            - labels_3d (Tensor): Labels of bboxes, has a shape
-                (num_instances, ).
-            - bbox_3d (:obj:`BaseInstance3DBoxes`): Prediction of bboxes,
-                contains a tensor with shape (num_instances, 7).
+            tuple: A tuple of results from roi head.
         """
-        batch_input_metas = [item.metainfo for item in batch_data_samples]
-        img_feats, pts_feats = self.extract_feat(batch_inputs_dict,
-                                                 batch_input_metas)
-        if pts_feats and self.with_pts_bbox:
-            results_list_3d = self.pts_bbox_head.predict(
-                pts_feats, batch_data_samples, **kwargs)
-        else:
-            results_list_3d = None
-
-        if img_feats and self.with_img_bbox:
-            # TODO check this for camera modality
-            results_list_2d = self.predict_imgs(img_feats, batch_data_samples,
-                                                **kwargs)
-        else:
-            results_list_2d = None
-
-        detsamples = self.add_pred_to_datasample(batch_data_samples,
-                                                 results_list_3d,
-                                                 results_list_2d)
-        return detsamples
+        voxel_dict = feats_dict.pop('voxel_dict')
+        semantic_results = self.semantic_head(feats_dict['seg_features'])
+        feats_dict.update(semantic_results)
+        rois = bbox3d2roi([res['bbox_3d'].tensor for res in rpn_results_list])
+        bbox_results = self._bbox_forward(feats_dict, voxel_dict, rois)
+        cls_score = bbox_results['cls_score']
+        bbox_pred = bbox_results['bbox_pred']
+        return cls_score, bbox_pred
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/parta2.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/parta2.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/point_rcnn.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/point_rcnn.py`

 * *Files 2% similar despite different names*

```diff
@@ -28,15 +28,15 @@
                  backbone: dict,
                  neck: Optional[dict] = None,
                  rpn_head: Optional[dict] = None,
                  roi_head: Optional[dict] = None,
                  train_cfg: Optional[dict] = None,
                  test_cfg: Optional[dict] = None,
                  init_cfg: Optional[dict] = None,
-                 data_preprocessor: Optional[dict] = None) -> Optional:
+                 data_preprocessor: Optional[dict] = None) -> None:
         super(PointRCNN, self).__init__(
             backbone=backbone,
             neck=neck,
             rpn_head=rpn_head,
             roi_head=roi_head,
             train_cfg=train_cfg,
             test_cfg=test_cfg,
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/pv_rcnn.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/pv_rcnn.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/sassd.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/sassd.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/single_stage.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/single_stage.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/single_stage_mono3d.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/single_stage_mono3d.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/smoke_mono3d.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/smoke_mono3d.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/ssd3dnet.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/ssd3dnet.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/two_stage.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/two_stage.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/votenet.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/votenet.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/detectors/voxelnet.py` & `mmdet3d-1.1.1/mmdet3d/models/detectors/voxelnet.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/layers/__init__.py` & `mmdet3d-1.1.1/mmdet3d/models/layers/__init__.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,30 +1,36 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from .box3d_nms import (aligned_3d_nms, box3d_multiclass_nms, circle_nms,
                         nms_bev, nms_normal_bev)
 from .dgcnn_modules import DGCNNFAModule, DGCNNFPModule, DGCNNGFModule
 from .edge_fusion_module import EdgeFusionModule
 from .fusion_layers import (PointFusion, VoteFusion, apply_3d_transformation,
                             bbox_2d_transform, coord_2d_transform)
+from .minkowski_engine_block import (MinkowskiBasicBlock, MinkowskiBottleneck,
+                                     MinkowskiConvModule)
 from .mlp import MLP
 from .norm import NaiveSyncBatchNorm1d, NaiveSyncBatchNorm2d
 from .paconv import PAConv, PAConvCUDA
 from .pointnet_modules import (PAConvCUDASAModule, PAConvCUDASAModuleMSG,
                                PAConvSAModule, PAConvSAModuleMSG,
                                PointFPModule, PointSAModule, PointSAModuleMSG,
                                build_sa_module)
 from .sparse_block import (SparseBasicBlock, SparseBottleneck,
                            make_sparse_convmodule)
+from .torchsparse_block import (TorchSparseBasicBlock, TorchSparseBottleneck,
+                                TorchSparseConvModule)
 from .transformer import GroupFree3DMHA
 from .vote_module import VoteModule
 
 __all__ = [
     'VoteModule', 'GroupFree3DMHA', 'EdgeFusionModule', 'DGCNNFAModule',
     'DGCNNFPModule', 'DGCNNGFModule', 'NaiveSyncBatchNorm1d',
     'NaiveSyncBatchNorm2d', 'PAConv', 'PAConvCUDA', 'SparseBasicBlock',
     'SparseBottleneck', 'make_sparse_convmodule', 'PointFusion', 'VoteFusion',
     'apply_3d_transformation', 'bbox_2d_transform', 'coord_2d_transform',
     'MLP', 'box3d_multiclass_nms', 'aligned_3d_nms', 'circle_nms', 'nms_bev',
     'nms_normal_bev', 'build_sa_module', 'PointSAModuleMSG', 'PointSAModule',
     'PointFPModule', 'PAConvSAModule', 'PAConvSAModuleMSG',
-    'PAConvCUDASAModule', 'PAConvCUDASAModuleMSG'
+    'PAConvCUDASAModule', 'PAConvCUDASAModuleMSG', 'TorchSparseConvModule',
+    'TorchSparseBasicBlock', 'TorchSparseBottleneck', 'MinkowskiConvModule',
+    'MinkowskiBasicBlock', 'MinkowskiBottleneck'
 ]
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/layers/box3d_nms.py` & `mmdet3d-1.1.1/mmdet3d/models/layers/box3d_nms.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/layers/dgcnn_modules/dgcnn_fa_module.py` & `mmdet3d-1.1.1/mmdet3d/models/layers/dgcnn_modules/dgcnn_fa_module.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/layers/dgcnn_modules/dgcnn_fp_module.py` & `mmdet3d-1.1.1/mmdet3d/models/layers/dgcnn_modules/dgcnn_fp_module.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/layers/dgcnn_modules/dgcnn_gf_module.py` & `mmdet3d-1.1.1/mmdet3d/models/layers/dgcnn_modules/dgcnn_gf_module.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/layers/edge_fusion_module.py` & `mmdet3d-1.1.1/mmdet3d/models/layers/edge_fusion_module.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/layers/fusion_layers/coord_transform.py` & `mmdet3d-1.1.1/mmdet3d/models/layers/fusion_layers/coord_transform.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/layers/fusion_layers/point_fusion.py` & `mmdet3d-1.1.1/mmdet3d/models/layers/fusion_layers/point_fusion.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/layers/fusion_layers/vote_fusion.py` & `mmdet3d-1.1.1/mmdet3d/models/layers/fusion_layers/vote_fusion.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/layers/mlp.py` & `mmdet3d-1.1.1/mmdet3d/models/layers/mlp.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/layers/norm.py` & `mmdet3d-1.1.1/mmdet3d/models/layers/norm.py`

 * *Files 2% similar despite different names*

```diff
@@ -53,16 +53,14 @@
             input (Tensor): Has shape (N, C) or (N, C, L), where N is
                 the batch size, C is the number of features or
                 channels, and L is the sequence length
 
         Returns:
             Tensor: Has shape (N, C) or (N, C, L), same shape as input.
         """
-        assert input.dtype == torch.float32, \
-            f'input should be in float32 type, got {input.dtype}'
         using_dist = dist.is_available() and dist.is_initialized()
         if (not using_dist) or dist.get_world_size() == 1 \
                 or not self.training:
             return super().forward(input)
         assert input.shape[0] > 0, 'SyncBN does not support empty inputs'
         is_two_dim = input.dim() == 2
         if is_two_dim:
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/layers/paconv/paconv.py` & `mmdet3d-1.1.1/mmdet3d/models/layers/paconv/paconv.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/layers/paconv/utils.py` & `mmdet3d-1.1.1/mmdet3d/models/layers/paconv/utils.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/layers/pointnet_modules/__init__.py` & `mmdet3d-1.1.1/mmdet3d/models/layers/pointnet_modules/__init__.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/layers/pointnet_modules/builder.py` & `mmdet3d-1.1.1/mmdet3d/models/layers/pointnet_modules/builder.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from typing import Union
 
 from mmengine.registry import Registry
 from torch import nn as nn
 
-SA_MODULES = Registry('point_sa_module')
+SA_MODULES = Registry(
+    name='point_sa_module',
+    locations=['mmdet3d.models.layers.pointnet_modules'])
 
 
 def build_sa_module(cfg: Union[dict, None], *args, **kwargs) -> nn.Module:
     """Build PointNet2 set abstraction (SA) module.
 
     Args:
         cfg (dict or None): The SA module config, which should contain:
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/layers/pointnet_modules/paconv_sa_module.py` & `mmdet3d-1.1.1/mmdet3d/models/layers/pointnet_modules/paconv_sa_module.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/layers/pointnet_modules/point_fp_module.py` & `mmdet3d-1.1.1/mmdet3d/models/layers/pointnet_modules/point_fp_module.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/layers/pointnet_modules/point_sa_module.py` & `mmdet3d-1.1.1/mmdet3d/models/layers/pointnet_modules/point_sa_module.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/layers/pointnet_modules/stack_point_sa_module.py` & `mmdet3d-1.1.1/mmdet3d/models/layers/pointnet_modules/stack_point_sa_module.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/layers/sparse_block.py` & `mmdet3d-1.1.1/mmdet3d/models/layers/sparse_block.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-from typing import Tuple, Union
+from typing import Optional, Tuple, Union
 
 from mmcv.cnn import build_conv_layer, build_norm_layer
 from mmdet.models.backbones.resnet import BasicBlock, Bottleneck
 from torch import nn
 
 from mmdet3d.utils import OptConfigType
 from .spconv import IS_SPCONV2_AVAILABLE
@@ -31,31 +31,38 @@
 
     Args:
         inplanes (int): Inplanes of block.
         planes (int): Planes of block.
         stride (int or Tuple[int]): Stride of the first block. Defaults to 1.
         downsample (Module, optional): Down sample module for block.
             Defaults to None.
+        indice_key (str): Indice key for spconv. Default to None.
         conv_cfg (:obj:`ConfigDict` or dict, optional): Config dict for
             convolution layer. Defaults to None.
         norm_cfg (:obj:`ConfigDict` or dict, optional): Config dict for
             normalization layer. Defaults to None.
     """
 
     expansion = 4
 
     def __init__(self,
                  inplanes: int,
                  planes: int,
                  stride: Union[int, Tuple[int]] = 1,
                  downsample: nn.Module = None,
+                 indice_key=None,
                  conv_cfg: OptConfigType = None,
                  norm_cfg: OptConfigType = None) -> None:
 
         SparseModule.__init__(self)
+        if conv_cfg is None:
+            conv_cfg = dict(type='SubMConv3d')
+        conv_cfg.setdefault('indice_key', indice_key)
+        if norm_cfg is None:
+            norm_cfg = dict(type='BN1d')
         Bottleneck.__init__(
             self,
             inplanes,
             planes,
             stride=stride,
             downsample=downsample,
             conv_cfg=conv_cfg,
@@ -72,15 +79,15 @@
         out = replace_feature(out, self.bn2(out.features))
         out = replace_feature(out, self.relu(out.features))
 
         out = self.conv3(out)
         out = replace_feature(out, self.bn3(out.features))
 
         if self.downsample is not None:
-            identity = self.downsample(x)
+            identity = self.downsample(x).features
 
         out = replace_feature(out, out.features + identity)
         out = replace_feature(out, self.relu(out.features))
 
         return out
 
 
@@ -91,30 +98,37 @@
 
     Args:
         inplanes (int): Inplanes of block.
         planes (int): Planes of block.
         stride (int or Tuple[int]): Stride of the first block. Defaults to 1.
         downsample (Module, optional): Down sample module for block.
             Defaults to None.
+        indice_key (str): Indice key for spconv. Default to None.
         conv_cfg (:obj:`ConfigDict` or dict, optional): Config dict for
             convolution layer. Defaults to None.
         norm_cfg (:obj:`ConfigDict` or dict, optional): Config dict for
             normalization layer. Defaults to None.
     """
 
     expansion = 1
 
     def __init__(self,
                  inplanes: int,
                  planes: int,
                  stride: Union[int, Tuple[int]] = 1,
                  downsample: nn.Module = None,
+                 indice_key: Optional[str] = None,
                  conv_cfg: OptConfigType = None,
                  norm_cfg: OptConfigType = None) -> None:
         SparseModule.__init__(self)
+        if conv_cfg is None:
+            conv_cfg = dict(type='SubMConv3d')
+        conv_cfg.setdefault('indice_key', indice_key)
+        if norm_cfg is None:
+            norm_cfg = dict(type='BN1d')
         BasicBlock.__init__(
             self,
             inplanes,
             planes,
             stride=stride,
             downsample=downsample,
             conv_cfg=conv_cfg,
@@ -128,33 +142,32 @@
         out = replace_feature(out, self.norm1(out.features))
         out = replace_feature(out, self.relu(out.features))
 
         out = self.conv2(out)
         out = replace_feature(out, self.norm2(out.features))
 
         if self.downsample is not None:
-            identity = self.downsample(x)
+            identity = self.downsample(x).features
 
         out = replace_feature(out, out.features + identity)
         out = replace_feature(out, self.relu(out.features))
 
         return out
 
 
-def make_sparse_convmodule(
-    in_channels: int,
-    out_channels: int,
-    kernel_size: Union[int, Tuple[int]],
-    indice_key: str,
-    stride: Union[int, Tuple[int]] = 1,
-    padding: Union[int, Tuple[int]] = 0,
-    conv_type: str = 'SubMConv3d',
-    norm_cfg: OptConfigType = None,
-    order: Tuple[str] = ('conv', 'norm', 'act')
-) -> SparseSequential:
+def make_sparse_convmodule(in_channels: int,
+                           out_channels: int,
+                           kernel_size: Union[int, Tuple[int]],
+                           indice_key: Optional[str] = None,
+                           stride: Union[int, Tuple[int]] = 1,
+                           padding: Union[int, Tuple[int]] = 0,
+                           conv_type: str = 'SubMConv3d',
+                           norm_cfg: OptConfigType = None,
+                           order: Tuple[str] = ('conv', 'norm', 'act'),
+                           **kwargs) -> SparseSequential:
     """Make sparse convolution module.
 
     Args:
         in_channels (int): The number of input channels.
         out_channels (int): The number of out channels.
         kernel_size (int | Tuple[int]): Kernel size of convolution.
         indice_key (str): The indice key used for sparse tensor.
@@ -171,14 +184,16 @@
     Returns:
         spconv.SparseSequential: sparse convolution module.
     """
     assert isinstance(order, tuple) and len(order) <= 3
     assert set(order) | {'conv', 'norm', 'act'} == {'conv', 'norm', 'act'}
 
     conv_cfg = dict(type=conv_type, indice_key=indice_key)
+    if norm_cfg is None:
+        norm_cfg = dict(type='BN1d')
 
     layers = list()
     for layer in order:
         if layer == 'conv':
             if conv_type not in [
                     'SparseInverseConv3d', 'SparseInverseConv2d',
                     'SparseInverseConv1d'
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/layers/spconv/overwrite_spconv/write_spconv2.py` & `mmdet3d-1.1.1/mmdet3d/models/layers/spconv/overwrite_spconv/write_spconv2.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/layers/transformer.py` & `mmdet3d-1.1.1/mmdet3d/models/layers/transformer.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/layers/vote_module.py` & `mmdet3d-1.1.1/mmdet3d/models/layers/vote_module.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/losses/__init__.py` & `mmdet3d-1.1.1/mmdet3d/models/losses/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,16 +1,17 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from mmdet.models.losses import FocalLoss, SmoothL1Loss, binary_cross_entropy
 
 from .axis_aligned_iou_loss import AxisAlignedIoULoss, axis_aligned_iou_loss
 from .chamfer_distance import ChamferDistance, chamfer_distance
+from .lovasz_loss import LovaszLoss
 from .multibin_loss import MultiBinLoss
 from .paconv_regularization_loss import PAConvRegularizationLoss
 from .rotated_iou_loss import RotatedIoU3DLoss, rotated_iou_3d_loss
 from .uncertain_smooth_l1_loss import UncertainL1Loss, UncertainSmoothL1Loss
 
 __all__ = [
     'FocalLoss', 'SmoothL1Loss', 'binary_cross_entropy', 'ChamferDistance',
     'chamfer_distance', 'axis_aligned_iou_loss', 'AxisAlignedIoULoss',
     'PAConvRegularizationLoss', 'UncertainL1Loss', 'UncertainSmoothL1Loss',
-    'MultiBinLoss', 'RotatedIoU3DLoss', 'rotated_iou_3d_loss'
+    'MultiBinLoss', 'RotatedIoU3DLoss', 'rotated_iou_3d_loss', 'LovaszLoss'
 ]
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/losses/axis_aligned_iou_loss.py` & `mmdet3d-1.1.1/mmdet3d/models/losses/axis_aligned_iou_loss.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,73 +1,79 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Optional
+
 import torch
 from mmdet.models.losses.utils import weighted_loss
+from torch import Tensor
 from torch import nn as nn
 
 from mmdet3d.registry import MODELS
 from mmdet3d.structures import AxisAlignedBboxOverlaps3D
 
 
 @weighted_loss
-def axis_aligned_iou_loss(pred, target):
+def axis_aligned_iou_loss(pred: Tensor, target: Tensor) -> Tensor:
     """Calculate the IoU loss (1-IoU) of two set of axis aligned bounding
     boxes. Note that predictions and targets are one-to-one corresponded.
 
     Args:
-        pred (torch.Tensor): Bbox predictions with shape [..., 3].
-        target (torch.Tensor): Bbox targets (gt) with shape [..., 3].
+        pred (Tensor): Bbox predictions with shape [..., 3].
+        target (Tensor): Bbox targets (gt) with shape [..., 3].
 
     Returns:
-        torch.Tensor: IoU loss between predictions and targets.
+        Tensor: IoU loss between predictions and targets.
     """
 
     axis_aligned_iou = AxisAlignedBboxOverlaps3D()(
         pred, target, is_aligned=True)
     iou_loss = 1 - axis_aligned_iou
     return iou_loss
 
 
 @MODELS.register_module()
 class AxisAlignedIoULoss(nn.Module):
     """Calculate the IoU loss (1-IoU) of axis aligned bounding boxes.
 
     Args:
         reduction (str): Method to reduce losses.
-            The valid reduction method are none, sum or mean.
-        loss_weight (float, optional): Weight of loss. Defaults to 1.0.
+            The valid reduction method are 'none', 'sum' or 'mean'.
+            Defaults to 'mean'.
+        loss_weight (float): Weight of loss. Defaults to 1.0.
     """
 
-    def __init__(self, reduction='mean', loss_weight=1.0):
+    def __init__(self,
+                 reduction: str = 'mean',
+                 loss_weight: float = 1.0) -> None:
         super(AxisAlignedIoULoss, self).__init__()
         assert reduction in ['none', 'sum', 'mean']
         self.reduction = reduction
         self.loss_weight = loss_weight
 
     def forward(self,
-                pred,
-                target,
-                weight=None,
-                avg_factor=None,
-                reduction_override=None,
-                **kwargs):
+                pred: Tensor,
+                target: Tensor,
+                weight: Optional[Tensor] = None,
+                avg_factor: Optional[float] = None,
+                reduction_override: Optional[str] = None,
+                **kwargs) -> Tensor:
         """Forward function of loss calculation.
 
         Args:
-            pred (torch.Tensor): Bbox predictions with shape [..., 3].
-            target (torch.Tensor): Bbox targets (gt) with shape [..., 3].
-            weight (torch.Tensor | float, optional): Weight of loss.
+            pred (Tensor): Bbox predictions with shape [..., 3].
+            target (Tensor): Bbox targets (gt) with shape [..., 3].
+            weight (Tensor, optional): Weight of loss.
                 Defaults to None.
-            avg_factor (int, optional): Average factor that is used to average
-                the loss. Defaults to None.
+            avg_factor (float, optional): Average factor that is used to
+                average the loss. Defaults to None.
             reduction_override (str, optional): Method to reduce losses.
                 The valid reduction method are 'none', 'sum' or 'mean'.
                 Defaults to None.
 
         Returns:
-            torch.Tensor: IoU loss between predictions and targets.
+            Tensor: IoU loss between predictions and targets.
         """
         assert reduction_override in (None, 'none', 'mean', 'sum')
         reduction = (
             reduction_override if reduction_override else self.reduction)
         if (weight is not None) and (not torch.any(weight > 0)) and (
                 reduction != 'none'):
             return (pred * weight).sum()
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/losses/multibin_loss.py` & `mmdet3d-1.1.1/mmdet3d/models/losses/multibin_loss.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,32 +1,37 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Optional
+
 import torch
 from mmdet.models.losses.utils import weighted_loss
+from torch import Tensor
 from torch import nn as nn
 from torch.nn import functional as F
 
 from mmdet3d.registry import MODELS
 
 
 @weighted_loss
-def multibin_loss(pred_orientations, gt_orientations, num_dir_bins=4):
+def multibin_loss(pred_orientations: Tensor,
+                  gt_orientations: Tensor,
+                  num_dir_bins: int = 4) -> Tensor:
     """Multi-Bin Loss.
 
     Args:
-        pred_orientations(torch.Tensor): Predicted local vector
+        pred_orientations(Tensor): Predicted local vector
             orientation in [axis_cls, head_cls, sin, cos] format.
             shape (N, num_dir_bins * 4)
-        gt_orientations(torch.Tensor): Corresponding gt bboxes,
+        gt_orientations(Tensor): Corresponding gt bboxes,
             shape (N, num_dir_bins * 2).
-        num_dir_bins(int, optional): Number of bins to encode
+        num_dir_bins(int): Number of bins to encode
             direction angle.
-            Defaults: 4.
+            Defaults to 4.
 
-    Return:
-        torch.Tensor: Loss tensor.
+    Returns:
+        Tensor: Loss tensor.
     """
     cls_losses = 0
     reg_losses = 0
     reg_cnt = 0
     for i in range(num_dir_bins):
         # bin cls loss
         cls_ce_loss = F.cross_entropy(
@@ -58,36 +63,45 @@
 
 
 @MODELS.register_module()
 class MultiBinLoss(nn.Module):
     """Multi-Bin Loss for orientation.
 
     Args:
-        reduction (str, optional): The method to reduce the loss.
+        reduction (str): The method to reduce the loss.
             Options are 'none', 'mean' and 'sum'. Defaults to 'none'.
-        loss_weight (float, optional): The weight of loss. Defaults
+        loss_weight (float): The weight of loss. Defaults
             to 1.0.
     """
 
-    def __init__(self, reduction='none', loss_weight=1.0):
+    def __init__(self,
+                 reduction: str = 'none',
+                 loss_weight: float = 1.0) -> None:
         super(MultiBinLoss, self).__init__()
         assert reduction in ['none', 'sum', 'mean']
         self.reduction = reduction
         self.loss_weight = loss_weight
 
-    def forward(self, pred, target, num_dir_bins, reduction_override=None):
+    def forward(self,
+                pred: Tensor,
+                target: Tensor,
+                num_dir_bins: int,
+                reduction_override: Optional[str] = None) -> Tensor:
         """Forward function.
 
         Args:
-            pred (torch.Tensor): The prediction.
-            target (torch.Tensor): The learning target of the prediction.
+            pred (Tensor): The prediction.
+            target (Tensor): The learning target of the prediction.
             num_dir_bins (int): Number of bins to encode direction angle.
             reduction_override (str, optional): The reduction method used to
                 override the original reduction method of the loss.
                 Defaults to None.
+
+        Returns:
+            Tensor: Loss tensor.
         """
         assert reduction_override in (None, 'none', 'mean', 'sum')
         reduction = (
             reduction_override if reduction_override else self.reduction)
         loss = self.loss_weight * multibin_loss(
             pred, target, num_dir_bins=num_dir_bins, reduction=reduction)
         return loss
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/losses/paconv_regularization_loss.py` & `mmdet3d-1.1.1/mmdet3d/models/losses/paconv_regularization_loss.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,26 +1,29 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import List, Optional
+
 import torch
 from mmdet.models.losses.utils import weight_reduce_loss
+from torch import Tensor
 from torch import nn as nn
 
 from mmdet3d.registry import MODELS
 from ..layers import PAConv, PAConvCUDA
 
 
-def weight_correlation(conv):
+def weight_correlation(conv: nn.Module) -> Tensor:
     """Calculate correlations between kernel weights in Conv's weight bank as
     regularization loss. The cosine similarity is used as metrics.
 
     Args:
         conv (nn.Module): A Conv modules to be regularized.
             Currently we only support `PAConv` and `PAConvCUDA`.
 
     Returns:
-        torch.Tensor: Correlations between each kernel weights in weight bank.
+        Tensor: Correlations between each kernel weights in weight bank.
     """
     assert isinstance(conv, (PAConv, PAConvCUDA)), \
         f'unsupported module type {type(conv)}'
     kernels = conv.weight_bank  # [C_in, num_kernels * C_out]
     in_channels = conv.in_channels
     out_channels = conv.out_channels
     num_kernels = conv.num_kernels
@@ -40,25 +43,26 @@
     # the square is to ensure positive loss, refer to:
     # https://github.com/CVMI-Lab/PAConv/blob/main/scene_seg/tool/train.py#L208
     corr = torch.sum(torch.triu(cosine_sims, diagonal=1)**2)
 
     return corr
 
 
-def paconv_regularization_loss(modules, reduction):
+def paconv_regularization_loss(modules: List[nn.Module],
+                               reduction: str) -> Tensor:
     """Computes correlation loss of PAConv weight kernels as regularization.
 
     Args:
         modules (List[nn.Module] | :obj:`generator`):
             A list or a python generator of torch.nn.Modules.
         reduction (str): Method to reduce losses among PAConv modules.
-            The valid reduction method are none, sum or mean.
+            The valid reduction method are 'none', 'sum' or 'mean'.
 
     Returns:
-        torch.Tensor: Correlation loss of kernel weights.
+        Tensor: Correlation loss of kernel weights.
     """
     corr_loss = []
     for module in modules:
         if isinstance(module, (PAConv, PAConvCUDA)):
             corr_loss.append(weight_correlation(module))
     corr_loss = torch.stack(corr_loss)
 
@@ -73,36 +77,42 @@
     """Calculate correlation loss of kernel weights in PAConv's weight bank.
 
     This is used as a regularization term in PAConv model training.
 
     Args:
         reduction (str): Method to reduce losses. The reduction is performed
             among all PAConv modules instead of prediction tensors.
-            The valid reduction method are none, sum or mean.
-        loss_weight (float, optional): Weight of loss. Defaults to 1.0.
+            The valid reduction method are 'none', 'sum' or 'mean'.
+            Defaults to 'mean'.
+        loss_weight (float): Weight of loss. Defaults to 1.0.
     """
 
-    def __init__(self, reduction='mean', loss_weight=1.0):
+    def __init__(self,
+                 reduction: str = 'mean',
+                 loss_weight: float = 1.0) -> None:
         super(PAConvRegularizationLoss, self).__init__()
         assert reduction in ['none', 'sum', 'mean']
         self.reduction = reduction
         self.loss_weight = loss_weight
 
-    def forward(self, modules, reduction_override=None, **kwargs):
+    def forward(self,
+                modules: List[nn.Module],
+                reduction_override: Optional[str] = None,
+                **kwargs) -> Tensor:
         """Forward function of loss calculation.
 
         Args:
             modules (List[nn.Module] | :obj:`generator`):
                 A list or a python generator of torch.nn.Modules.
             reduction_override (str, optional): Method to reduce losses.
                 The valid reduction method are 'none', 'sum' or 'mean'.
                 Defaults to None.
 
         Returns:
-            torch.Tensor: Correlation loss of kernel weights.
+            Tensor: Correlation loss of kernel weights.
         """
         assert reduction_override in (None, 'none', 'mean', 'sum')
         reduction = (
             reduction_override if reduction_override else self.reduction)
 
         return self.loss_weight * paconv_regularization_loss(
             modules, reduction=reduction)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/losses/rotated_iou_loss.py` & `mmdet3d-1.1.1/mmdet3d/models/losses/rotated_iou_loss.py`

 * *Files 12% similar despite different names*

```diff
@@ -7,72 +7,75 @@
 from torch import Tensor
 from torch import nn as nn
 
 from mmdet3d.registry import MODELS
 
 
 @weighted_loss
-def rotated_iou_3d_loss(pred, target: Tensor) -> Tensor:
+def rotated_iou_3d_loss(pred: Tensor, target: Tensor) -> Tensor:
     """Calculate the IoU loss (1-IoU) of two sets of rotated bounding boxes.
 
     Note that predictions and targets are one-to-one corresponded.
+
     Args:
-        pred (torch.Tensor): Bbox predictions with shape [N, 7]
+        pred (Tensor): Bbox predictions with shape [N, 7]
             (x, y, z, w, l, h, alpha).
-        target (torch.Tensor): Bbox targets (gt) with shape [N, 7]
+        target (Tensor): Bbox targets (gt) with shape [N, 7]
             (x, y, z, w, l, h, alpha).
+
     Returns:
-        torch.Tensor: IoU loss between predictions and targets.
+        Tensor: IoU loss between predictions and targets.
     """
     iou_loss = 1 - diff_iou_rotated_3d(pred.unsqueeze(0),
                                        target.unsqueeze(0))[0]
     return iou_loss
 
 
 @MODELS.register_module()
 class RotatedIoU3DLoss(nn.Module):
     """Calculate the IoU loss (1-IoU) of rotated bounding boxes.
 
     Args:
         reduction (str): Method to reduce losses.
-            The valid reduction method are none, sum or mean.
-        loss_weight (float, optional): Weight of loss. Defaults to 1.0.
+            The valid reduction method are 'none', 'sum' or 'mean'.
+            Defaults to 'mean'.
+        loss_weight (float): Weight of loss. Defaults to 1.0.
     """
 
     def __init__(self,
                  reduction: str = 'mean',
-                 loss_weight: Optional[float] = 1.0):
+                 loss_weight: float = 1.0) -> None:
         super().__init__()
         self.reduction = reduction
         self.loss_weight = loss_weight
 
     def forward(self,
                 pred: Tensor,
                 target: Tensor,
                 weight: Optional[Tensor] = None,
-                avg_factor: Optional[int] = None,
+                avg_factor: Optional[float] = None,
                 reduction_override: Optional[str] = None,
                 **kwargs) -> Tensor:
         """Forward function of loss calculation.
 
         Args:
-            pred (torch.Tensor): Bbox predictions with shape [..., 7]
+            pred (Tensor): Bbox predictions with shape [..., 7]
                 (x, y, z, w, l, h, alpha).
-            target (torch.Tensor): Bbox targets (gt) with shape [..., 7]
+            target (Tensor): Bbox targets (gt) with shape [..., 7]
                 (x, y, z, w, l, h, alpha).
-            weight (torch.Tensor | float, optional): Weight of loss.
+            weight (Tensor, optional): Weight of loss.
                 Defaults to None.
-            avg_factor (int, optional): Average factor that is used to average
-                the loss. Defaults to None.
+            avg_factor (float, optional): Average factor that is used to
+                average the loss. Defaults to None.
             reduction_override (str, optional): Method to reduce losses.
                 The valid reduction method are 'none', 'sum' or 'mean'.
                 Defaults to None.
 
         Returns:
-            torch.Tensor: IoU loss between predictions and targets.
+            Tensor: IoU loss between predictions and targets.
         """
         if weight is not None and not torch.any(weight > 0):
             return pred.sum() * weight.sum()  # 0
         assert reduction_override in (None, 'none', 'mean', 'sum')
         reduction = (
             reduction_override if reduction_override else self.reduction)
         if weight is not None and weight.dim() > 1:
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/losses/uncertain_smooth_l1_loss.py` & `mmdet3d-1.1.1/mmdet3d/models/losses/uncertain_smooth_l1_loss.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,30 +1,37 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Optional
+
 import torch
 from mmdet.models.losses.utils import weighted_loss
+from torch import Tensor
 from torch import nn as nn
 
 from mmdet3d.registry import MODELS
 
 
 @weighted_loss
-def uncertain_smooth_l1_loss(pred, target, sigma, alpha=1.0, beta=1.0):
+def uncertain_smooth_l1_loss(pred: Tensor,
+                             target: Tensor,
+                             sigma: Tensor,
+                             alpha: float = 1.0,
+                             beta: float = 1.0) -> Tensor:
     """Smooth L1 loss with uncertainty.
 
     Args:
-        pred (torch.Tensor): The prediction.
-        target (torch.Tensor): The learning target of the prediction.
-        sigma (torch.Tensor): The sigma for uncertainty.
-        alpha (float, optional): The coefficient of log(sigma).
+        pred (Tensor): The prediction.
+        target (Tensor): The learning target of the prediction.
+        sigma (Tensor): The sigma for uncertainty.
+        alpha (float): The coefficient of log(sigma).
             Defaults to 1.0.
-        beta (float, optional): The threshold in the piecewise function.
+        beta (float): The threshold in the piecewise function.
             Defaults to 1.0.
 
     Returns:
-        torch.Tensor: Calculated loss
+        Tensor: Calculated loss
     """
     assert beta > 0
     assert target.numel() > 0
     assert pred.size() == target.size() == sigma.size(), 'The size of pred ' \
         f'{pred.size()}, target {target.size()}, and sigma {sigma.size()} ' \
         'are inconsistent.'
     diff = torch.abs(pred - target)
@@ -32,26 +39,29 @@
                        diff - 0.5 * beta)
     loss = torch.exp(-sigma) * loss + alpha * sigma
 
     return loss
 
 
 @weighted_loss
-def uncertain_l1_loss(pred, target, sigma, alpha=1.0):
+def uncertain_l1_loss(pred: Tensor,
+                      target: Tensor,
+                      sigma: Tensor,
+                      alpha: float = 1.0) -> Tensor:
     """L1 loss with uncertainty.
 
     Args:
-        pred (torch.Tensor): The prediction.
-        target (torch.Tensor): The learning target of the prediction.
-        sigma (torch.Tensor): The sigma for uncertainty.
-        alpha (float, optional): The coefficient of log(sigma).
+        pred (Tensor): The prediction.
+        target (Tensor): The learning target of the prediction.
+        sigma (Tensor): The sigma for uncertainty.
+        alpha (float): The coefficient of log(sigma).
             Defaults to 1.0.
 
     Returns:
-        torch.Tensor: Calculated loss
+        Tensor: Calculated loss
     """
     assert target.numel() > 0
     assert pred.size() == target.size() == sigma.size(), 'The size of pred ' \
         f'{pred.size()}, target {target.size()}, and sigma {sigma.size()} ' \
         'are inconsistent.'
     loss = torch.abs(pred - target)
     loss = torch.exp(-sigma) * loss + alpha * sigma
@@ -63,52 +73,59 @@
     r"""Smooth L1 loss with uncertainty.
 
     Please refer to `PGD <https://arxiv.org/abs/2107.14160>`_ and
     `Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry
     and Semantics <https://arxiv.org/abs/1705.07115>`_ for more details.
 
     Args:
-        alpha (float, optional): The coefficient of log(sigma).
+        alpha (float): The coefficient of log(sigma).
             Defaults to 1.0.
-        beta (float, optional): The threshold in the piecewise function.
+        beta (float): The threshold in the piecewise function.
             Defaults to 1.0.
-        reduction (str, optional): The method to reduce the loss.
+        reduction (str): The method to reduce the loss.
             Options are 'none', 'mean' and 'sum'. Defaults to 'mean'.
-        loss_weight (float, optional): The weight of loss. Defaults to 1.0
+        loss_weight (float): The weight of loss. Defaults to 1.0
     """
 
-    def __init__(self, alpha=1.0, beta=1.0, reduction='mean', loss_weight=1.0):
+    def __init__(self,
+                 alpha: float = 1.0,
+                 beta: float = 1.0,
+                 reduction: str = 'mean',
+                 loss_weight: float = 1.0) -> None:
         super(UncertainSmoothL1Loss, self).__init__()
         assert reduction in ['none', 'sum', 'mean']
         self.alpha = alpha
         self.beta = beta
         self.reduction = reduction
         self.loss_weight = loss_weight
 
     def forward(self,
-                pred,
-                target,
-                sigma,
-                weight=None,
-                avg_factor=None,
-                reduction_override=None,
-                **kwargs):
+                pred: Tensor,
+                target: Tensor,
+                sigma: Tensor,
+                weight: Optional[Tensor] = None,
+                avg_factor: Optional[float] = None,
+                reduction_override: Optional[str] = None,
+                **kwargs) -> Tensor:
         """Forward function.
 
         Args:
-            pred (torch.Tensor): The prediction.
-            target (torch.Tensor): The learning target of the prediction.
-            sigma (torch.Tensor): The sigma for uncertainty.
-            weight (torch.Tensor, optional): The weight of loss for each
+            pred (Tensor): The prediction.
+            target (Tensor): The learning target of the prediction.
+            sigma (Tensor): The sigma for uncertainty.
+            weight (Tensor, optional): The weight of loss for each
                 prediction. Defaults to None.
-            avg_factor (int, optional): Average factor that is used to average
-                the loss. Defaults to None.
+            avg_factor (float, optional): Average factor that is used to
+                average the loss. Defaults to None.
             reduction_override (str, optional): The reduction method used to
                 override the original reduction method of the loss.
                 Defaults to None.
+
+        Returns:
+            Tensor: Calculated loss
         """
         assert reduction_override in (None, 'none', 'mean', 'sum')
         reduction = (
             reduction_override if reduction_override else self.reduction)
         loss_bbox = self.loss_weight * uncertain_smooth_l1_loss(
             pred,
             target,
@@ -123,48 +140,54 @@
 
 
 @MODELS.register_module()
 class UncertainL1Loss(nn.Module):
     """L1 loss with uncertainty.
 
     Args:
-        alpha (float, optional): The coefficient of log(sigma).
+        alpha (float): The coefficient of log(sigma).
             Defaults to 1.0.
-        reduction (str, optional): The method to reduce the loss.
+        reduction (str): The method to reduce the loss.
             Options are 'none', 'mean' and 'sum'. Defaults to 'mean'.
-        loss_weight (float, optional): The weight of loss. Defaults to 1.0.
+        loss_weight (float): The weight of loss. Defaults to 1.0.
     """
 
-    def __init__(self, alpha=1.0, reduction='mean', loss_weight=1.0):
+    def __init__(self,
+                 alpha: float = 1.0,
+                 reduction: str = 'mean',
+                 loss_weight: float = 1.0) -> None:
         super(UncertainL1Loss, self).__init__()
         assert reduction in ['none', 'sum', 'mean']
         self.alpha = alpha
         self.reduction = reduction
         self.loss_weight = loss_weight
 
     def forward(self,
-                pred,
-                target,
-                sigma,
-                weight=None,
-                avg_factor=None,
-                reduction_override=None):
+                pred: Tensor,
+                target: Tensor,
+                sigma: Tensor,
+                weight: Optional[Tensor] = None,
+                avg_factor: Optional[float] = None,
+                reduction_override: Optional[str] = None) -> Tensor:
         """Forward function.
 
         Args:
-            pred (torch.Tensor): The prediction.
-            target (torch.Tensor): The learning target of the prediction.
-            sigma (torch.Tensor): The sigma for uncertainty.
-            weight (torch.Tensor, optional): The weight of loss for each
+            pred (Tensor): The prediction.
+            target (Tensor): The learning target of the prediction.
+            sigma (Tensor): The sigma for uncertainty.
+            weight (Tensor, optional): The weight of loss for each
                 prediction. Defaults to None.
-            avg_factor (int, optional): Average factor that is used to average
-                the loss. Defaults to None.
+            avg_factor (float, optional): Average factor that is used to
+                average the loss. Defaults to None.
             reduction_override (str, optional): The reduction method used to
                 override the original reduction method of the loss.
                 Defaults to None.
+
+        Returns:
+            Tensor: Calculated loss
         """
         assert reduction_override in (None, 'none', 'mean', 'sum')
         reduction = (
             reduction_override if reduction_override else self.reduction)
         loss_bbox = self.loss_weight * uncertain_l1_loss(
             pred,
             target,
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/middle_encoders/pillar_scatter.py` & `mmdet3d-1.1.1/mmdet3d/models/middle_encoders/pillar_scatter.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,12 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import List
+
 import torch
-from torch import nn
+from torch import Tensor, nn
 
 from mmdet3d.registry import MODELS
 
 
 @MODELS.register_module()
 class PointPillarsScatter(nn.Module):
     """Point Pillar's Scatter.
@@ -12,32 +14,34 @@
     Converts learned features from dense tensor to sparse pseudo image.
 
     Args:
         in_channels (int): Channels of input features.
         output_shape (list[int]): Required output shape of features.
     """
 
-    def __init__(self, in_channels, output_shape):
+    def __init__(self, in_channels: int, output_shape: List[int]):
         super().__init__()
         self.output_shape = output_shape
         self.ny = output_shape[0]
         self.nx = output_shape[1]
         self.in_channels = in_channels
-        self.fp16_enabled = False
 
-    def forward(self, voxel_features, coors, batch_size=None):
+    def forward(self,
+                voxel_features: Tensor,
+                coors: Tensor,
+                batch_size: int = None) -> Tensor:
         """Foraward function to scatter features."""
         # TODO: rewrite the function in a batch manner
         # no need to deal with different batch cases
         if batch_size is not None:
             return self.forward_batch(voxel_features, coors, batch_size)
         else:
             return self.forward_single(voxel_features, coors)
 
-    def forward_single(self, voxel_features, coors):
+    def forward_single(self, voxel_features: Tensor, coors: Tensor) -> Tensor:
         """Scatter features of single sample.
 
         Args:
             voxel_features (torch.Tensor): Voxel features in shape (N, M, C).
             coors (torch.Tensor): Coordinates of each voxel.
                 The first column indicates the sample ID.
         """
@@ -53,15 +57,16 @@
         voxels = voxel_features.t()
         # Now scatter the blob back to the canvas.
         canvas[:, indices] = voxels
         # Undo the column stacking to final 4-dim tensor
         canvas = canvas.view(1, self.in_channels, self.ny, self.nx)
         return canvas
 
-    def forward_batch(self, voxel_features, coors, batch_size):
+    def forward_batch(self, voxel_features: Tensor, coors: Tensor,
+                      batch_size: int) -> Tensor:
         """Scatter features of single sample.
 
         Args:
             voxel_features (torch.Tensor): Voxel features in shape (N, M, C).
             coors (torch.Tensor): Coordinates of each voxel in shape (N, 4).
                 The first column indicates the sample ID.
             batch_size (int): Number of samples in the current batch.
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/middle_encoders/sparse_encoder.py` & `mmdet3d-1.1.1/mmdet3d/models/middle_encoders/sparse_encoder.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,35 +1,38 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-from typing import List, Tuple
+from typing import Dict, List, Optional, Tuple, Union
 
 import torch
 from mmcv.ops import points_in_boxes_all, three_interpolate, three_nn
 from mmdet.models.losses import sigmoid_focal_loss, smooth_l1_loss
+from mmengine.runner import amp
 from torch import Tensor
 from torch import nn as nn
 
 from mmdet3d.models.layers import SparseBasicBlock, make_sparse_convmodule
 from mmdet3d.models.layers.spconv import IS_SPCONV2_AVAILABLE
 from mmdet3d.registry import MODELS
 from mmdet3d.structures import BaseInstance3DBoxes
 
 if IS_SPCONV2_AVAILABLE:
     from spconv.pytorch import SparseConvTensor, SparseSequential
 else:
     from mmcv.ops import SparseConvTensor, SparseSequential
 
+TwoTupleIntType = Tuple[Tuple[int]]
+
 
 @MODELS.register_module()
 class SparseEncoder(nn.Module):
     r"""Sparse encoder for SECOND and Part-A2.
 
     Args:
         in_channels (int): The number of input channels.
         sparse_shape (list[int]): The sparse shape of input tensor.
-        order (list[str], optional): Order of conv module.
+        order (tuple[str], optional): Order of conv module.
             Defaults to ('conv', 'norm', 'act').
         norm_cfg (dict, optional): Config of normalization layer. Defaults to
             dict(type='BN1d', eps=1e-3, momentum=0.01).
         base_channels (int, optional): Out channels for conv_input layer.
             Defaults to 16.
         output_channels (int, optional): Out channels for conv_out layer.
             Defaults to 128.
@@ -41,38 +44,42 @@
             Defaults to ((1, ), (1, 1, 1), (1, 1, 1), ((0, 1, 1), 1, 1)).
         block_type (str, optional): Type of the block to use.
             Defaults to 'conv_module'.
         return_middle_feats (bool): Whether output middle features.
             Default to False.
     """
 
-    def __init__(self,
-                 in_channels,
-                 sparse_shape,
-                 order=('conv', 'norm', 'act'),
-                 norm_cfg=dict(type='BN1d', eps=1e-3, momentum=0.01),
-                 base_channels=16,
-                 output_channels=128,
-                 encoder_channels=((16, ), (32, 32, 32), (64, 64, 64), (64, 64,
-                                                                        64)),
-                 encoder_paddings=((1, ), (1, 1, 1), (1, 1, 1), ((0, 1, 1), 1,
-                                                                 1)),
-                 block_type='conv_module',
-                 return_middle_feats=False):
+    def __init__(
+            self,
+            in_channels: int,
+            sparse_shape: List[int],
+            order: Optional[Tuple[str]] = ('conv', 'norm', 'act'),
+            norm_cfg: Optional[dict] = dict(
+                type='BN1d', eps=1e-3, momentum=0.01),
+            base_channels: Optional[int] = 16,
+            output_channels: Optional[int] = 128,
+            encoder_channels: Optional[TwoTupleIntType] = ((16, ), (32, 32,
+                                                                    32),
+                                                           (64, 64,
+                                                            64), (64, 64, 64)),
+            encoder_paddings: Optional[TwoTupleIntType] = ((1, ), (1, 1, 1),
+                                                           (1, 1, 1),
+                                                           ((0, 1, 1), 1, 1)),
+            block_type: Optional[str] = 'conv_module',
+            return_middle_feats: Optional[bool] = False):
         super().__init__()
         assert block_type in ['conv_module', 'basicblock']
         self.sparse_shape = sparse_shape
         self.in_channels = in_channels
         self.order = order
         self.base_channels = base_channels
         self.output_channels = output_channels
         self.encoder_channels = encoder_channels
         self.encoder_paddings = encoder_paddings
         self.stage_num = len(self.encoder_channels)
-        self.fp16_enabled = False
         self.return_middle_feats = return_middle_feats
         # Spconv init all weight on its own
 
         assert isinstance(order, tuple) and len(order) == 3
         assert set(order) == {'conv', 'norm', 'act'}
 
         if self.order[0] != 'conv':  # pre activate
@@ -107,15 +114,17 @@
             kernel_size=(3, 1, 1),
             stride=(2, 1, 1),
             norm_cfg=norm_cfg,
             padding=0,
             indice_key='spconv_down2',
             conv_type='SparseConv3d')
 
-    def forward(self, voxel_features, coors, batch_size):
+    @amp.autocast(enabled=False)
+    def forward(self, voxel_features: Tensor, coors: Tensor,
+                batch_size: int) -> Union[Tensor, Tuple[Tensor, list]]:
         """Forward of SparseEncoder.
 
         Args:
             voxel_features (torch.Tensor): Voxel features in shape (N, C).
             coors (torch.Tensor): Coordinates in shape (N, 4),
                 the columns in the order of (batch_idx, z_idx, y_idx, x_idx).
             batch_size (int): Batch size.
@@ -149,20 +158,22 @@
         spatial_features = spatial_features.view(N, C * D, H, W)
 
         if self.return_middle_feats:
             return spatial_features, encode_features
         else:
             return spatial_features
 
-    def make_encoder_layers(self,
-                            make_block,
-                            norm_cfg,
-                            in_channels,
-                            block_type='conv_module',
-                            conv_cfg=dict(type='SubMConv3d')):
+    def make_encoder_layers(
+        self,
+        make_block: nn.Module,
+        norm_cfg: Dict,
+        in_channels: int,
+        block_type: Optional[str] = 'conv_module',
+        conv_cfg: Optional[dict] = dict(type='SubMConv3d')
+    ) -> int:
         """make encoder layers using sparse convs.
 
         Args:
             make_block (method): A bounded function to build blocks.
             norm_cfg (dict[str]): Config of normalization layer.
             in_channels (int): The number of encoder input channels.
             block_type (str, optional): Type of the block to use.
@@ -251,26 +262,30 @@
         encoder_paddings (tuple[tuple[int]], optional):
             Paddings of each encode block.
             Defaults to ((1, ), (1, 1, 1), (1, 1, 1), ((0, 1, 1), 1, 1)).
         block_type (str, optional): Type of the block to use.
             Defaults to 'conv_module'.
     """
 
-    def __init__(self,
-                 in_channels: int,
-                 sparse_shape: List[int],
-                 order: Tuple[str] = ('conv', 'norm', 'act'),
-                 norm_cfg: dict = dict(type='BN1d', eps=1e-3, momentum=0.01),
-                 base_channels: int = 16,
-                 output_channels: int = 128,
-                 encoder_channels: Tuple[tuple] = ((16, ), (32, 32, 32),
-                                                   (64, 64, 64), (64, 64, 64)),
-                 encoder_paddings: Tuple[tuple] = ((1, ), (1, 1, 1), (1, 1, 1),
-                                                   ((0, 1, 1), 1, 1)),
-                 block_type: str = 'conv_module'):
+    def __init__(
+            self,
+            in_channels: int,
+            sparse_shape: List[int],
+            order: Tuple[str] = ('conv', 'norm', 'act'),
+            norm_cfg: dict = dict(type='BN1d', eps=1e-3, momentum=0.01),
+            base_channels: int = 16,
+            output_channels: int = 128,
+            encoder_channels: Optional[TwoTupleIntType] = ((16, ), (32, 32,
+                                                                    32),
+                                                           (64, 64,
+                                                            64), (64, 64, 64)),
+            encoder_paddings: Optional[TwoTupleIntType] = ((1, ), (1, 1, 1),
+                                                           (1, 1, 1),
+                                                           ((0, 1, 1), 1, 1)),
+            block_type: str = 'conv_module'):
         super(SparseEncoderSASSD, self).__init__(
             in_channels=in_channels,
             sparse_shape=sparse_shape,
             order=order,
             norm_cfg=norm_cfg,
             base_channels=base_channels,
             output_channels=output_channels,
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/middle_encoders/sparse_unet.py` & `mmdet3d-1.1.1/mmdet3d/models/middle_encoders/sparse_unet.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,23 +1,28 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Dict, List, Optional, Tuple
+
 import torch
+from torch import Tensor, nn
 
 from mmdet3d.models.layers.spconv import IS_SPCONV2_AVAILABLE
 
 if IS_SPCONV2_AVAILABLE:
     from spconv.pytorch import SparseConvTensor, SparseSequential
 else:
     from mmcv.ops import SparseConvTensor, SparseSequential
 
 from mmengine.model import BaseModule
 
 from mmdet3d.models.layers import SparseBasicBlock, make_sparse_convmodule
 from mmdet3d.models.layers.sparse_block import replace_feature
 from mmdet3d.registry import MODELS
 
+TwoTupleIntType = Tuple[Tuple[int]]
+
 
 @MODELS.register_module()
 class SparseUNet(BaseModule):
     r"""SparseUNet for PartA^2.
 
     See the `paper <https://arxiv.org/abs/1907.03670>`_ for more details.
 
@@ -31,41 +36,47 @@
             Convolutional channels of each encode block.
         encoder_paddings (tuple[tuple[int]]): Paddings of each encode block.
         decoder_channels (tuple[tuple[int]]):
             Convolutional channels of each decode block.
         decoder_paddings (tuple[tuple[int]]): Paddings of each decode block.
     """
 
-    def __init__(self,
-                 in_channels,
-                 sparse_shape,
-                 order=('conv', 'norm', 'act'),
-                 norm_cfg=dict(type='BN1d', eps=1e-3, momentum=0.01),
-                 base_channels=16,
-                 output_channels=128,
-                 encoder_channels=((16, ), (32, 32, 32), (64, 64, 64), (64, 64,
-                                                                        64)),
-                 encoder_paddings=((1, ), (1, 1, 1), (1, 1, 1), ((0, 1, 1), 1,
-                                                                 1)),
-                 decoder_channels=((64, 64, 64), (64, 64, 32), (32, 32, 16),
-                                   (16, 16, 16)),
-                 decoder_paddings=((1, 0), (1, 0), (0, 0), (0, 1)),
-                 init_cfg=None):
+    def __init__(
+            self,
+            in_channels: int,
+            sparse_shape: List[int],
+            order: Tuple[str] = ('conv', 'norm', 'act'),
+            norm_cfg: dict = dict(type='BN1d', eps=1e-3, momentum=0.01),
+            base_channels: int = 16,
+            output_channels: int = 128,
+            encoder_channels: Optional[TwoTupleIntType] = ((16, ), (32, 32,
+                                                                    32),
+                                                           (64, 64,
+                                                            64), (64, 64, 64)),
+            encoder_paddings: Optional[TwoTupleIntType] = ((1, ), (1, 1, 1),
+                                                           (1, 1, 1),
+                                                           ((0, 1, 1), 1, 1)),
+            decoder_channels: Optional[TwoTupleIntType] = ((64, 64,
+                                                            64), (64, 64, 32),
+                                                           (32, 32,
+                                                            16), (16, 16, 16)),
+            decoder_paddings: Optional[TwoTupleIntType] = ((1, 0), (1, 0),
+                                                           (0, 0), (0, 1)),
+            init_cfg: bool = None):
         super().__init__(init_cfg=init_cfg)
         self.sparse_shape = sparse_shape
         self.in_channels = in_channels
         self.order = order
         self.base_channels = base_channels
         self.output_channels = output_channels
         self.encoder_channels = encoder_channels
         self.encoder_paddings = encoder_paddings
         self.decoder_channels = decoder_channels
         self.decoder_paddings = decoder_paddings
         self.stage_num = len(self.encoder_channels)
-        self.fp16_enabled = False
         # Spconv init all weight on its own
 
         assert isinstance(order, tuple) and len(order) == 3
         assert set(order) == {'conv', 'norm', 'act'}
 
         if self.order[0] != 'conv':  # pre activate
             self.conv_input = make_sparse_convmodule(
@@ -98,15 +109,16 @@
             kernel_size=(3, 1, 1),
             stride=(2, 1, 1),
             norm_cfg=norm_cfg,
             padding=0,
             indice_key='spconv_down2',
             conv_type='SparseConv3d')
 
-    def forward(self, voxel_features, coors, batch_size):
+    def forward(self, voxel_features: Tensor, coors: Tensor,
+                batch_size: int) -> Dict[str, Tensor]:
         """Forward of SparseUNet.
 
         Args:
             voxel_features (torch.float32): Voxel features in shape [N, C].
             coors (torch.int32): Coordinates in shape [N, 4],
                 the columns in the order of (batch_idx, z_idx, y_idx, x_idx).
             batch_size (int): Batch size.
@@ -149,16 +161,18 @@
         seg_features = decode_features[-1].features
 
         ret = dict(
             spatial_features=spatial_features, seg_features=seg_features)
 
         return ret
 
-    def decoder_layer_forward(self, x_lateral, x_bottom, lateral_layer,
-                              merge_layer, upsample_layer):
+    def decoder_layer_forward(
+            self, x_lateral: SparseConvTensor, x_bottom: SparseConvTensor,
+            lateral_layer: SparseBasicBlock, merge_layer: SparseSequential,
+            upsample_layer: SparseSequential) -> SparseConvTensor:
         """Forward of upsample and residual block.
 
         Args:
             x_lateral (:obj:`SparseConvTensor`): Lateral tensor.
             x_bottom (:obj:`SparseConvTensor`): Feature from bottom layer.
             lateral_layer (SparseBasicBlock): Convolution for lateral tensor.
             merge_layer (SparseSequential): Convolution for merging features.
@@ -173,15 +187,16 @@
         x_merge = merge_layer(x)
         x = self.reduce_channel(x, x_merge.features.shape[1])
         x = replace_feature(x, x_merge.features + x.features)
         x = upsample_layer(x)
         return x
 
     @staticmethod
-    def reduce_channel(x, out_channels):
+    def reduce_channel(x: SparseConvTensor,
+                       out_channels: int) -> SparseConvTensor:
         """reduce channel for element-wise addition.
 
         Args:
             x (:obj:`SparseConvTensor`): Sparse tensor, ``x.features``
                 are in shape (N, C1).
             out_channels (int): The number of channel after reduction.
 
@@ -191,15 +206,16 @@
         features = x.features
         n, in_channels = features.shape
         assert (in_channels % out_channels
                 == 0) and (in_channels >= out_channels)
         x = replace_feature(x, features.view(n, out_channels, -1).sum(dim=2))
         return x
 
-    def make_encoder_layers(self, make_block, norm_cfg, in_channels):
+    def make_encoder_layers(self, make_block: nn.Module, norm_cfg: dict,
+                            in_channels: int) -> int:
         """make encoder layers using sparse convs.
 
         Args:
             make_block (method): A bounded function to build blocks.
             norm_cfg (dict[str]): Config of normalization layer.
             in_channels (int): The number of encoder input channels.
 
@@ -237,15 +253,16 @@
                             conv_type='SubMConv3d'))
                 in_channels = out_channels
             stage_name = f'encoder_layer{i + 1}'
             stage_layers = SparseSequential(*blocks_list)
             self.encoder_layers.add_module(stage_name, stage_layers)
         return out_channels
 
-    def make_decoder_layers(self, make_block, norm_cfg, in_channels):
+    def make_decoder_layers(self, make_block: nn.Module, norm_cfg: dict,
+                            in_channels: int) -> int:
         """make decoder layers using sparse convs.
 
         Args:
             make_block (method): A bounded function to build blocks.
             norm_cfg (dict[str]): Config of normalization layer.
             in_channels (int): The number of encoder input channels.
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/middle_encoders/voxel_set_abstraction.py` & `mmdet3d-1.1.1/mmdet3d/models/middle_encoders/voxel_set_abstraction.py`

 * *Files 1% similar despite different names*

```diff
@@ -3,20 +3,21 @@
 
 import mmengine
 import torch
 import torch.nn as nn
 from mmcv.cnn import ConvModule
 from mmcv.ops.furthest_point_sample import furthest_point_sample
 from mmengine.model import BaseModule
+from torch import Tensor
 
 from mmdet3d.registry import MODELS
 from mmdet3d.utils import InstanceList
 
 
-def bilinear_interpolate_torch(inputs, x, y):
+def bilinear_interpolate_torch(inputs: Tensor, x: Tensor, y: Tensor) -> Tensor:
     """Bilinear interpolate for inputs."""
     x0 = torch.floor(x).long()
     x1 = x0 + 1
 
     y0 = torch.floor(y).long()
     y1 = y0 + 1
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/necks/dla_neck.py` & `mmdet3d-1.1.1/mmdet3d/models/necks/dla_neck.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/necks/imvoxel_neck.py` & `mmdet3d-1.1.1/mmdet3d/models/necks/imvoxel_neck.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/necks/pointnet2_fp_neck.py` & `mmdet3d-1.1.1/mmdet3d/models/necks/pointnet2_fp_neck.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/necks/second_fpn.py` & `mmdet3d-1.1.1/mmdet3d/models/necks/second_fpn.py`

 * *Files 2% similar despite different names*

```diff
@@ -34,15 +34,14 @@
                  init_cfg=None):
         # if for GroupNorm,
         # cfg is dict(type='GN', num_groups=num_groups, eps=1e-3, affine=True)
         super(SECONDFPN, self).__init__(init_cfg=init_cfg)
         assert len(out_channels) == len(upsample_strides) == len(in_channels)
         self.in_channels = in_channels
         self.out_channels = out_channels
-        self.fp16_enabled = False
 
         deblocks = []
         for i, out_channel in enumerate(out_channels):
             stride = upsample_strides[i]
             if stride > 1 or (stride == 1 and not use_conv_for_no_stride):
                 upsample_layer = build_upsample_layer(
                     upsample_cfg,
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/__init__.py` & `mmdet3d-1.1.1/mmdet3d/models/roi_heads/__init__.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/base_3droi_head.py` & `mmdet3d-1.1.1/mmdet3d/models/roi_heads/base_3droi_head.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/bbox_heads/__init__.py` & `mmdet3d-1.1.1/mmdet3d/models/roi_heads/bbox_heads/__init__.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/bbox_heads/h3d_bbox_head.py` & `mmdet3d-1.1.1/mmdet3d/models/roi_heads/bbox_heads/h3d_bbox_head.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/bbox_heads/parta2_bbox_head.py` & `mmdet3d-1.1.1/mmdet3d/models/roi_heads/bbox_heads/parta2_bbox_head.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/bbox_heads/point_rcnn_bbox_head.py` & `mmdet3d-1.1.1/mmdet3d/models/roi_heads/bbox_heads/point_rcnn_bbox_head.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/bbox_heads/pv_rcnn_bbox_head.py` & `mmdet3d-1.1.1/mmdet3d/models/roi_heads/bbox_heads/pv_rcnn_bbox_head.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/h3d_roi_head.py` & `mmdet3d-1.1.1/mmdet3d/models/roi_heads/h3d_roi_head.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/mask_heads/foreground_segmentation_head.py` & `mmdet3d-1.1.1/mmdet3d/models/roi_heads/mask_heads/foreground_segmentation_head.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/mask_heads/pointwise_semantic_head.py` & `mmdet3d-1.1.1/mmdet3d/models/roi_heads/mask_heads/pointwise_semantic_head.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/mask_heads/primitive_head.py` & `mmdet3d-1.1.1/mmdet3d/models/roi_heads/mask_heads/primitive_head.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/part_aggregation_roi_head.py` & `mmdet3d-1.1.1/mmdet3d/models/roi_heads/pv_rcnn_roi_head.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,128 +1,246 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-from typing import Dict, List, Tuple
+from typing import List, Optional
 
-from mmdet.models.task_modules import AssignResult, SamplingResult
-from mmengine import ConfigDict
-from torch import Tensor
+import torch
+from mmdet.models.task_modules import AssignResult
+from mmdet.models.task_modules.samplers import SamplingResult
 from torch.nn import functional as F
 
+from mmdet3d.models.roi_heads.base_3droi_head import Base3DRoIHead
 from mmdet3d.registry import MODELS
 from mmdet3d.structures import bbox3d2roi
+from mmdet3d.structures.det3d_data_sample import SampleList
 from mmdet3d.utils import InstanceList
-from ...structures.det3d_data_sample import SampleList
-from .base_3droi_head import Base3DRoIHead
 
 
 @MODELS.register_module()
-class PartAggregationROIHead(Base3DRoIHead):
-    """Part aggregation roi head for PartA2.
+class PVRCNNRoiHead(Base3DRoIHead):
+    """RoI head for PV-RCNN.
 
     Args:
-        semantic_head (ConfigDict): Config of semantic head.
-        num_classes (int): The number of classes.
-        seg_roi_extractor (ConfigDict): Config of seg_roi_extractor.
-        bbox_roi_extractor (ConfigDict): Config of part_roi_extractor.
-        bbox_head (ConfigDict): Config of bbox_head.
-        train_cfg (ConfigDict): Training config.
-        test_cfg (ConfigDict): Testing config.
+        num_classes (int): The number of classes. Defaults to 3.
+        semantic_head (dict, optional): Config of semantic head.
+            Defaults to None.
+        bbox_roi_extractor (dict, optional): Config of roi_extractor.
+            Defaults to None.
+        bbox_head (dict, optional): Config of bbox_head. Defaults to None.
+        train_cfg (dict, optional): Train config of model.
+            Defaults to None.
+        test_cfg (dict, optional): Train config of model.
+            Defaults to None.
+        init_cfg (dict, optional): Initialize config of
+            model. Defaults to None.
     """
 
     def __init__(self,
-                 semantic_head: dict,
                  num_classes: int = 3,
-                 seg_roi_extractor: dict = None,
-                 bbox_head: dict = None,
-                 bbox_roi_extractor: dict = None,
-                 train_cfg: dict = None,
-                 test_cfg: dict = None,
-                 init_cfg: dict = None) -> None:
-        super(PartAggregationROIHead, self).__init__(
+                 semantic_head: Optional[dict] = None,
+                 bbox_roi_extractor: Optional[dict] = None,
+                 bbox_head: Optional[dict] = None,
+                 train_cfg: Optional[dict] = None,
+                 test_cfg: Optional[dict] = None,
+                 init_cfg: Optional[dict] = None):
+        super(PVRCNNRoiHead, self).__init__(
             bbox_head=bbox_head,
             bbox_roi_extractor=bbox_roi_extractor,
             train_cfg=train_cfg,
             test_cfg=test_cfg,
             init_cfg=init_cfg)
         self.num_classes = num_classes
-        assert semantic_head is not None
-        self.init_seg_head(seg_roi_extractor, semantic_head)
-
-    def init_seg_head(self, seg_roi_extractor: dict,
-                      semantic_head: dict) -> None:
-        """Initialize semantic head and seg roi extractor.
-
-        Args:
-            seg_roi_extractor (dict): Config of seg
-                roi extractor.
-            semantic_head (dict): Config of semantic head.
-        """
         self.semantic_head = MODELS.build(semantic_head)
-        self.seg_roi_extractor = MODELS.build(seg_roi_extractor)
+
+        self.init_assigner_sampler()
 
     @property
     def with_semantic(self):
         """bool: whether the head has semantic branch"""
         return hasattr(self,
                        'semantic_head') and self.semantic_head is not None
 
-    def _bbox_forward_train(self, feats_dict: Dict, voxels_dict: Dict,
-                            sampling_results: List[SamplingResult]) -> Dict:
+    def loss(self, feats_dict: dict, rpn_results_list: InstanceList,
+             batch_data_samples: SampleList, **kwargs) -> dict:
+        """Training forward function of PVRCNNROIHead.
+
+        Args:
+            feats_dict (dict): Contains point-wise features.
+            rpn_results_list (List[:obj:`InstanceData`]): Detection results
+                of rpn head.
+            batch_data_samples (List[:obj:`Det3DDataSample`]): The Data
+                samples. It usually includes information such as
+                `gt_instance_3d`, `gt_panoptic_seg_3d` and `gt_sem_seg_3d`.
+
+        Returns:
+            dict: losses from each head.
+
+            - loss_semantic (torch.Tensor): loss of semantic head.
+            - loss_bbox (torch.Tensor): loss of bboxes.
+            - loss_cls (torch.Tensor): loss of object classification.
+            - loss_corner (torch.Tensor): loss of bboxes corners.
+        """
+        losses = dict()
+        batch_gt_instances_3d = []
+        batch_gt_instances_ignore = []
+        for data_sample in batch_data_samples:
+            batch_gt_instances_3d.append(data_sample.gt_instances_3d)
+            if 'ignored_instances' in data_sample:
+                batch_gt_instances_ignore.append(data_sample.ignored_instances)
+            else:
+                batch_gt_instances_ignore.append(None)
+        if self.with_semantic:
+            semantic_results = self._semantic_forward_train(
+                feats_dict['keypoint_features'], feats_dict['keypoints'],
+                batch_gt_instances_3d)
+            losses['loss_semantic'] = semantic_results['loss_semantic']
+
+        sample_results = self._assign_and_sample(rpn_results_list,
+                                                 batch_gt_instances_3d)
+        if self.with_bbox:
+            bbox_results = self._bbox_forward_train(
+                semantic_results['seg_preds'],
+                feats_dict['fusion_keypoint_features'],
+                feats_dict['keypoints'], sample_results)
+            losses.update(bbox_results['loss_bbox'])
+
+        return losses
+
+    def predict(self, feats_dict: dict, rpn_results_list: InstanceList,
+                batch_data_samples: SampleList, **kwargs) -> SampleList:
+        """Perform forward propagation of the roi head and predict detection
+        results on the features of the upstream network.
+
+        Args:
+            feats_dict (dict): Contains point-wise features.
+            rpn_results_list (List[:obj:`InstanceData`]): Detection results
+                of rpn head.
+            batch_data_samples (List[:obj:`Det3DDataSample`]): The Data
+                samples. It usually includes information such as
+                `gt_instance_3d`, `gt_panoptic_seg_3d` and `gt_sem_seg_3d`.
+
+        Returns:
+            list[:obj:`InstanceData`]: Detection results of each sample
+            after the post process.
+            Each item usually contains following keys.
+
+            - scores_3d (Tensor): Classification scores, has a shape
+              (num_instances, )
+            - labels_3d (Tensor): Labels of bboxes, has a shape
+              (num_instances, ).
+            - bboxes_3d (BaseInstance3DBoxes): Prediction of bboxes,
+              contains a tensor with shape (num_instances, C), where
+              C >= 7.
+        """
+        assert self.with_bbox, 'Bbox head must be implemented.'
+        assert self.with_semantic, 'Semantic head must be implemented.'
+
+        batch_input_metas = [
+            data_samples.metainfo for data_samples in batch_data_samples
+        ]
+
+        semantic_results = self.semantic_head(feats_dict['keypoint_features'])
+        point_features = feats_dict[
+            'fusion_keypoint_features'] * semantic_results[
+                'seg_preds'].sigmoid().max(
+                    dim=-1, keepdim=True).values
+        rois = bbox3d2roi(
+            [res['bboxes_3d'].tensor for res in rpn_results_list])
+        labels_3d = [res['labels_3d'] for res in rpn_results_list]
+        bbox_results = self._bbox_forward(point_features,
+                                          feats_dict['keypoints'], rois)
+
+        results_list = self.bbox_head.get_results(rois,
+                                                  bbox_results['bbox_scores'],
+                                                  bbox_results['bbox_reg'],
+                                                  labels_3d, batch_input_metas,
+                                                  self.test_cfg)
+        return results_list
+
+    def _bbox_forward_train(self, seg_preds: torch.Tensor,
+                            keypoint_features: torch.Tensor,
+                            keypoints: torch.Tensor,
+                            sampling_results: SamplingResult) -> dict:
         """Forward training function of roi_extractor and bbox_head.
 
         Args:
-            feats_dict (dict): Contains features from the first stage.
-            voxels_dict (dict): Contains information of voxels.
+            seg_preds (torch.Tensor): Point-wise semantic features.
+            keypoint_features (torch.Tensor): key points features
+                from points encoder.
+            keypoints (torch.Tensor): Coordinate of key points.
             sampling_results (:obj:`SamplingResult`): Sampled results used
                 for training.
 
         Returns:
             dict: Forward results including losses and predictions.
         """
         rois = bbox3d2roi([res.bboxes for res in sampling_results])
-        bbox_results = self._bbox_forward(feats_dict, voxels_dict, rois)
+        keypoint_features = keypoint_features * seg_preds.sigmoid().max(
+            dim=-1, keepdim=True).values
+        bbox_results = self._bbox_forward(keypoint_features, keypoints, rois)
 
         bbox_targets = self.bbox_head.get_targets(sampling_results,
                                                   self.train_cfg)
-        loss_bbox = self.bbox_head.loss(bbox_results['cls_score'],
-                                        bbox_results['bbox_pred'], rois,
+        loss_bbox = self.bbox_head.loss(bbox_results['bbox_scores'],
+                                        bbox_results['bbox_reg'], rois,
                                         *bbox_targets)
 
         bbox_results.update(loss_bbox=loss_bbox)
         return bbox_results
 
+    def _bbox_forward(self, keypoint_features: torch.Tensor,
+                      keypoints: torch.Tensor, rois: torch.Tensor) -> dict:
+        """Forward function of roi_extractor and bbox_head used in both
+        training and testing.
+
+        Args:
+            rois (Tensor): Roi boxes.
+            keypoint_features (torch.Tensor): key points features
+                from points encoder.
+            keypoints (torch.Tensor): Coordinate of key points.
+            rois (Tensor): Roi boxes.
+
+        Returns:
+            dict: Contains predictions of bbox_head and
+                features of roi_extractor.
+        """
+        pooled_keypoint_features = self.bbox_roi_extractor(
+            keypoint_features, keypoints[..., 1:], keypoints[..., 0].int(),
+            rois)
+        bbox_score, bbox_reg = self.bbox_head(pooled_keypoint_features)
+
+        bbox_results = dict(bbox_scores=bbox_score, bbox_reg=bbox_reg)
+        return bbox_results
+
     def _assign_and_sample(
-            self, rpn_results_list: InstanceList,
-            batch_gt_instances_3d: InstanceList,
-            batch_gt_instances_ignore: InstanceList) -> List[SamplingResult]:
+            self, proposal_list: InstanceList,
+            batch_gt_instances_3d: InstanceList) -> List[SamplingResult]:
         """Assign and sample proposals for training.
 
         Args:
-            rpn_results_list (List[:obj:`InstanceData`]): Detection results
-                of rpn head.
+            proposal_list (list[:obj:`InstancesData`]): Proposals produced by
+                rpn head.
             batch_gt_instances_3d (list[:obj:`InstanceData`]): Batch of
                 gt_instances. It usually includes ``bboxes_3d`` and
                 ``labels_3d`` attributes.
-            batch_gt_instances_ignore (list): Ignore instances of gt bboxes.
 
         Returns:
             list[:obj:`SamplingResult`]: Sampled results of each training
                 sample.
         """
         sampling_results = []
         # bbox assign
-        for batch_idx in range(len(rpn_results_list)):
-            cur_proposal_list = rpn_results_list[batch_idx]
+        for batch_idx in range(len(proposal_list)):
+            cur_proposal_list = proposal_list[batch_idx]
             cur_boxes = cur_proposal_list['bboxes_3d']
             cur_labels_3d = cur_proposal_list['labels_3d']
             cur_gt_instances_3d = batch_gt_instances_3d[batch_idx]
-            cur_gt_instances_ignore = batch_gt_instances_ignore[batch_idx]
             cur_gt_instances_3d.bboxes_3d = cur_gt_instances_3d.\
                 bboxes_3d.tensor
-            cur_gt_bboxes = cur_gt_instances_3d.bboxes_3d.to(cur_boxes.device)
-            cur_gt_labels = cur_gt_instances_3d.labels_3d
+            cur_gt_bboxes = batch_gt_instances_3d[batch_idx].bboxes_3d.to(
+                cur_boxes.device)
+            cur_gt_labels = batch_gt_instances_3d[batch_idx].labels_3d
 
             batch_num_gts = 0
             # 0 is bg
             batch_gt_indis = cur_gt_labels.new_full((len(cur_boxes), ), 0)
             batch_max_overlaps = cur_boxes.tensor.new_zeros(len(cur_boxes))
             # -1 is bg
             batch_gt_labels = cur_gt_labels.new_full((len(cur_boxes), ), -1)
@@ -130,16 +248,15 @@
             # each class may have its own assigner
             if isinstance(self.bbox_assigner, list):
                 for i, assigner in enumerate(self.bbox_assigner):
                     gt_per_cls = (cur_gt_labels == i)
                     pred_per_cls = (cur_labels_3d == i)
                     cur_assign_res = assigner.assign(
                         cur_proposal_list[pred_per_cls],
-                        cur_gt_instances_3d[gt_per_cls],
-                        cur_gt_instances_ignore)
+                        cur_gt_instances_3d[gt_per_cls])
                     # gather assign_results in different class into one result
                     batch_num_gts += cur_assign_res.num_gts
                     # gt inds (1-based)
                     gt_inds_arange_pad = gt_per_cls.nonzero(
                         as_tuple=False).view(-1) + 1
                     # pad 0 for indice unassigned
                     gt_inds_arange_pad = F.pad(
@@ -157,223 +274,39 @@
                     batch_gt_labels[pred_per_cls] = cur_assign_res.labels
 
                 assign_result = AssignResult(batch_num_gts, batch_gt_indis,
                                              batch_max_overlaps,
                                              batch_gt_labels)
             else:  # for single class
                 assign_result = self.bbox_assigner.assign(
-                    cur_proposal_list, cur_gt_instances_3d,
-                    cur_gt_instances_ignore)
+                    cur_proposal_list, cur_gt_instances_3d)
             # sample boxes
             sampling_result = self.bbox_sampler.sample(assign_result,
                                                        cur_boxes.tensor,
                                                        cur_gt_bboxes,
                                                        cur_gt_labels)
             sampling_results.append(sampling_result)
         return sampling_results
 
-    def _semantic_forward_train(self, feats_dict: dict, voxel_dict: dict,
-                                batch_gt_instances_3d: InstanceList) -> Dict:
+    def _semantic_forward_train(self, keypoint_features: torch.Tensor,
+                                keypoints: torch.Tensor,
+                                batch_gt_instances_3d: InstanceList) -> dict:
         """Train semantic head.
 
         Args:
-            feats_dict (dict): Contains features from the first stage.
-            voxel_dict (dict): Contains information of voxels.
+            keypoint_features (torch.Tensor): key points features
+                from points encoder.
+            keypoints (torch.Tensor): Coordinate of key points.
             batch_gt_instances_3d (list[:obj:`InstanceData`]): Batch of
                 gt_instances. It usually includes ``bboxes_3d`` and
                 ``labels_3d`` attributes.
 
         Returns:
             dict: Segmentation results including losses
         """
-        semantic_results = self.semantic_head(feats_dict['seg_features'])
+        semantic_results = self.semantic_head(keypoint_features)
         semantic_targets = self.semantic_head.get_targets(
-            voxel_dict, batch_gt_instances_3d)
+            keypoints, batch_gt_instances_3d)
         loss_semantic = self.semantic_head.loss(semantic_results,
                                                 semantic_targets)
-        semantic_results.update(loss_semantic=loss_semantic)
+        semantic_results.update(loss_semantic)
         return semantic_results
-
-    def predict(self,
-                feats_dict: Dict,
-                rpn_results_list: InstanceList,
-                batch_data_samples: SampleList,
-                rescale: bool = False,
-                **kwargs) -> InstanceList:
-        """Perform forward propagation of the roi head and predict detection
-        results on the features of the upstream network.
-
-        Args:
-            feats_dict (dict): Contains features from the first stage.
-            rpn_results_list (List[:obj:`InstanceData`]): Detection results
-                of rpn head.
-            batch_data_samples (List[:obj:`Det3DDataSample`]): The Data
-                samples. It usually includes information such as
-                `gt_instance_3d`, `gt_panoptic_seg_3d` and `gt_sem_seg_3d`.
-            rescale (bool): If True, return boxes in original image space.
-                Defaults to False.
-
-        Returns:
-            list[:obj:`InstanceData`]: Detection results of each sample
-            after the post process.
-            Each item usually contains following keys.
-
-            - scores_3d (Tensor): Classification scores, has a shape
-              (num_instances, )
-            - labels_3d (Tensor): Labels of bboxes, has a shape
-              (num_instances, ).
-            - bboxes_3d (BaseInstance3DBoxes): Prediction of bboxes,
-              contains a tensor with shape (num_instances, C), where
-              C >= 7.
-        """
-        assert self.with_bbox, 'Bbox head must be implemented in PartA2.'
-        assert self.with_semantic, 'Semantic head must be implemented' \
-                                   ' in PartA2.'
-
-        batch_input_metas = [
-            data_samples.metainfo for data_samples in batch_data_samples
-        ]
-        voxels_dict = feats_dict.pop('voxels_dict')
-        # TODO: Split predict semantic and bbox
-        results_list = self.predict_bbox(feats_dict, voxels_dict,
-                                         batch_input_metas, rpn_results_list,
-                                         self.test_cfg)
-        return results_list
-
-    def predict_bbox(self, feats_dict: Dict, voxel_dict: Dict,
-                     batch_input_metas: List[dict],
-                     rpn_results_list: InstanceList,
-                     test_cfg: ConfigDict) -> InstanceList:
-        """Perform forward propagation of the bbox head and predict detection
-        results on the features of the upstream network.
-
-        Args:
-            feats_dict (dict): Contains features from the first stage.
-            voxel_dict (dict): Contains information of voxels.
-            batch_input_metas (list[dict], Optional): Batch image meta info.
-                Defaults to None.
-            rpn_results_list (List[:obj:`InstanceData`]): Detection results
-                of rpn head.
-            test_cfg (Config): Test config.
-
-        Returns:
-            list[:obj:`InstanceData`]: Detection results of each sample
-            after the post process.
-            Each item usually contains following keys.
-
-            - scores_3d (Tensor): Classification scores, has a shape
-              (num_instances, )
-            - labels_3d (Tensor): Labels of bboxes, has a shape
-              (num_instances, ).
-            - bboxes_3d (BaseInstance3DBoxes): Prediction of bboxes,
-              contains a tensor with shape (num_instances, C), where
-              C >= 7.
-        """
-        semantic_results = self.semantic_head(feats_dict['seg_features'])
-        feats_dict.update(semantic_results)
-        rois = bbox3d2roi(
-            [res['bboxes_3d'].tensor for res in rpn_results_list])
-        labels_3d = [res['labels_3d'] for res in rpn_results_list]
-        cls_preds = [res['cls_preds'] for res in rpn_results_list]
-        bbox_results = self._bbox_forward(feats_dict, voxel_dict, rois)
-
-        bbox_list = self.bbox_head.get_results(rois, bbox_results['cls_score'],
-                                               bbox_results['bbox_pred'],
-                                               labels_3d, cls_preds,
-                                               batch_input_metas, test_cfg)
-        return bbox_list
-
-    def _bbox_forward(self, feats_dict: Dict, voxel_dict: Dict,
-                      rois: Tensor) -> Dict:
-        """Forward function of roi_extractor and bbox_head used in both
-        training and testing.
-
-        Args:
-            feats_dict (dict): Contains features from the first stage.
-            voxel_dict (dict): Contains information of voxels.
-            rois (Tensor): Roi boxes.
-
-        Returns:
-            dict: Contains predictions of bbox_head and
-                features of roi_extractor.
-        """
-        pooled_seg_feats = self.seg_roi_extractor(feats_dict['seg_features'],
-                                                  voxel_dict['voxel_centers'],
-                                                  voxel_dict['coors'][...,
-                                                                      0], rois)
-        pooled_part_feats = self.bbox_roi_extractor(
-            feats_dict['part_feats'], voxel_dict['voxel_centers'],
-            voxel_dict['coors'][..., 0], rois)
-        cls_score, bbox_pred = self.bbox_head(pooled_seg_feats,
-                                              pooled_part_feats)
-
-        bbox_results = dict(
-            cls_score=cls_score,
-            bbox_pred=bbox_pred,
-            pooled_seg_feats=pooled_seg_feats,
-            pooled_part_feats=pooled_part_feats)
-        return bbox_results
-
-    def loss(self, feats_dict: Dict, rpn_results_list: InstanceList,
-             batch_data_samples: SampleList, **kwargs) -> dict:
-        """Perform forward propagation and loss calculation of the detection
-        roi on the features of the upstream network.
-
-        Args:
-            feats_dict (dict): Contains features from the first stage.
-            rpn_results_list (List[:obj:`InstanceData`]): Detection results
-                of rpn head.
-            batch_data_samples (List[:obj:`Det3DDataSample`]): The Data
-                samples. It usually includes information such as
-                `gt_instance_3d`, `gt_panoptic_seg_3d` and `gt_sem_seg_3d`.
-
-        Returns:
-            dict[str, Tensor]: A dictionary of loss components
-        """
-        assert len(rpn_results_list) == len(batch_data_samples)
-        losses = dict()
-        batch_gt_instances_3d = []
-        batch_gt_instances_ignore = []
-        voxels_dict = feats_dict.pop('voxels_dict')
-        for data_sample in batch_data_samples:
-            batch_gt_instances_3d.append(data_sample.gt_instances_3d)
-            if 'ignored_instances' in data_sample:
-                batch_gt_instances_ignore.append(data_sample.ignored_instances)
-            else:
-                batch_gt_instances_ignore.append(None)
-        if self.with_semantic:
-            semantic_results = self._semantic_forward_train(
-                feats_dict, voxels_dict, batch_gt_instances_3d)
-            losses.update(semantic_results.pop('loss_semantic'))
-
-        sample_results = self._assign_and_sample(rpn_results_list,
-                                                 batch_gt_instances_3d,
-                                                 batch_gt_instances_ignore)
-        if self.with_bbox:
-            feats_dict.update(semantic_results)
-            bbox_results = self._bbox_forward_train(feats_dict, voxels_dict,
-                                                    sample_results)
-            losses.update(bbox_results['loss_bbox'])
-
-        return losses
-
-    def _forward(self, feats_dict: dict,
-                 rpn_results_list: InstanceList) -> Tuple:
-        """Network forward process. Usually includes backbone, neck and head
-        forward without any post-processing.
-
-        Args:
-            feats_dict (dict): Contains features from the first stage.
-            rpn_results_list (List[:obj:`InstanceData`]): Detection results
-                of rpn head.
-
-        Returns:
-            tuple: A tuple of results from roi head.
-        """
-        voxel_dict = feats_dict.pop('voxel_dict')
-        semantic_results = self.semantic_head(feats_dict['seg_features'])
-        feats_dict.update(semantic_results)
-        rois = bbox3d2roi([res['bbox_3d'].tensor for res in rpn_results_list])
-        bbox_results = self._bbox_forward(feats_dict, voxel_dict, rois)
-        cls_score = bbox_results['cls_score']
-        bbox_pred = bbox_results['bbox_pred']
-        return cls_score, bbox_pred
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/point_rcnn_roi_head.py` & `mmdet3d-1.1.1/mmdet3d/models/roi_heads/point_rcnn_roi_head.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/roi_extractors/batch_roigridpoint_extractor.py` & `mmdet3d-1.1.1/mmdet3d/models/roi_heads/roi_extractors/batch_roigridpoint_extractor.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/roi_extractors/single_roiaware_extractor.py` & `mmdet3d-1.1.1/mmdet3d/models/roi_heads/roi_extractors/single_roiaware_extractor.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/roi_heads/roi_extractors/single_roipoint_extractor.py` & `mmdet3d-1.1.1/mmdet3d/models/roi_heads/roi_extractors/single_roipoint_extractor.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/segmentors/base.py` & `mmdet3d-1.1.1/mmdet3d/models/segmentors/base.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,97 +1,97 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from abc import ABCMeta, abstractmethod
-from typing import List, Tuple, Union
+from typing import Dict, List, Union
 
 from mmengine.model import BaseModel
 from torch import Tensor
 
-from mmdet3d.structures import Det3DDataSample, PointData
+from mmdet3d.structures import PointData
 from mmdet3d.structures.det3d_data_sample import (ForwardResults,
                                                   OptSampleList, SampleList)
 from mmdet3d.utils import OptConfigType, OptMultiConfig
 
 
 class Base3DSegmentor(BaseModel, metaclass=ABCMeta):
     """Base class for 3D segmentors.
 
     Args:
-        data_preprocessor (dict, optional): Model preprocessing config
-            for processing the input data. it usually includes
-            ``to_rgb``, ``pad_size_divisor``, ``pad_val``,
-            ``mean`` and ``std``. Default to None.
-       init_cfg (dict, optional): the config to control the
-           initialization. Default to None.
+        data_preprocessor (dict or ConfigDict, optional): Model preprocessing
+            config for processing the input data. it usually includes
+            ``to_rgb``, ``pad_size_divisor``, ``pad_val``, ``mean`` and
+            ``std``. Defaults to None.
+       init_cfg (dict or ConfigDict, optional): The config to control the
+           initialization. Defaults to None.
     """
 
     def __init__(self,
                  data_preprocessor: OptConfigType = None,
                  init_cfg: OptMultiConfig = None):
         super(Base3DSegmentor, self).__init__(
             data_preprocessor=data_preprocessor, init_cfg=init_cfg)
 
     @property
     def with_neck(self) -> bool:
-        """bool: whether the segmentor has neck"""
+        """bool: Whether the segmentor has neck."""
         return hasattr(self, 'neck') and self.neck is not None
 
     @property
     def with_auxiliary_head(self) -> bool:
-        """bool: whether the segmentor has auxiliary head"""
+        """bool: Whether the segmentor has auxiliary head."""
         return hasattr(self,
                        'auxiliary_head') and self.auxiliary_head is not None
 
     @property
     def with_decode_head(self) -> bool:
-        """bool: whether the segmentor has decode head"""
+        """bool: Whether the segmentor has decode head."""
         return hasattr(self, 'decode_head') and self.decode_head is not None
 
     @property
     def with_regularization_loss(self) -> bool:
-        """bool: whether the segmentor has regularization loss for weight"""
+        """bool: Whether the segmentor has regularization loss for weight."""
         return hasattr(self, 'loss_regularization') and \
             self.loss_regularization is not None
 
     @abstractmethod
-    def extract_feat(self, batch_inputs: Tensor) -> bool:
+    def extract_feat(self, batch_inputs: Tensor) -> dict:
         """Placeholder for extract features from images."""
         pass
 
     @abstractmethod
     def encode_decode(self, batch_inputs: Tensor,
-                      batch_data_samples: SampleList):
+                      batch_data_samples: SampleList) -> Tensor:
         """Placeholder for encode images with backbone and decode into a
         semantic segmentation map of the same size as input."""
         pass
 
     def forward(self,
                 inputs: Union[dict, List[dict]],
                 data_samples: OptSampleList = None,
                 mode: str = 'tensor') -> ForwardResults:
         """The unified entry for a forward process in both training and test.
 
         The method should accept three modes: "tensor", "predict" and "loss":
 
         - "tensor": Forward the whole network and return tensor or tuple of
-        tensor without any post-processing, same as a common nn.Module.
+          tensor without any post-processing, same as a common nn.Module.
         - "predict": Forward and return the predictions, which are fully
-        processed to a list of :obj:`SegDataSample`.
+          processed to a list of :obj:`SegDataSample`.
         - "loss": Forward and return a dict of losses according to the given
-        inputs and data samples.
+          inputs and data samples.
 
         Note that this method doesn't handle neither back propagation nor
         optimizer updating, which are done in the :meth:`train_step`.
 
         Args:
-            inputs (dict | List[dict]): Input sample dict which
-                includes 'points' and 'imgs' keys.
+            inputs (dict or List[dict]): Input sample dict which includes
+                'points' and 'imgs' keys.
 
-                - points (list[torch.Tensor]): Point cloud of each sample.
-                - imgs (torch.Tensor): Image tensor has shape (B, C, H, W).
-            data_samples (list[:obj:`Det3DDataSample`], optional):
+                - points (List[Tensor]): Point cloud of each sample.
+                - imgs (Tensor): Image tensor has shape (B, C, H, W).
+            data_samples (List[:obj:`Det3DDataSample`], optional):
                 The annotation data of every samples. Defaults to None.
             mode (str): Return what kind of value. Defaults to 'tensor'.
 
         Returns:
             The return type depends on ``mode``.
 
             - If ``mode="tensor"``, return a tensor or a tuple of tensor.
@@ -105,62 +105,61 @@
         elif mode == 'tensor':
             return self._forward(inputs, data_samples)
         else:
             raise RuntimeError(f'Invalid mode "{mode}". '
                                'Only supports loss, predict and tensor mode')
 
     @abstractmethod
-    def loss(self, batch_inputs: Tensor,
-             batch_data_samples: SampleList) -> dict:
+    def loss(self, batch_inputs: dict,
+             batch_data_samples: SampleList) -> Dict[str, Tensor]:
         """Calculate losses from a batch of inputs and data samples."""
         pass
 
     @abstractmethod
-    def predict(self, batch_inputs: Tensor,
+    def predict(self, batch_inputs: dict,
                 batch_data_samples: SampleList) -> SampleList:
         """Predict results from a batch of inputs and data samples with post-
         processing."""
         pass
 
     @abstractmethod
-    def _forward(
-            self,
-            batch_inputs: Tensor,
-            batch_data_samples: OptSampleList = None) -> Tuple[List[Tensor]]:
+    def _forward(self,
+                 batch_inputs: dict,
+                 batch_data_samples: OptSampleList = None) -> Tensor:
         """Network forward process.
 
         Usually includes backbone, neck and head forward without any post-
         processing.
         """
         pass
 
-    @abstractmethod
-    def aug_test(self, batch_inputs, batch_img_metas):
-        """Placeholder for augmentation test."""
-        pass
-
-    def postprocess_result(self, seg_pred_list: List[dict],
-                           batch_img_metas: List[dict]) -> list:
+    def postprocess_result(self, seg_logits_list: List[Tensor],
+                           batch_data_samples: SampleList) -> SampleList:
         """Convert results list to `Det3DDataSample`.
 
         Args:
-            seg_logits_list (List[dict]): List of segmentation results,
+            seg_logits_list (List[Tensor]): List of segmentation results,
                 seg_logits from model of each input point clouds sample.
+            batch_data_samples (List[:obj:`Det3DDataSample`]): The det3d data
+                samples. It usually includes information such as `metainfo` and
+                `gt_pts_seg`.
 
         Returns:
-            list[:obj:`Det3DDataSample`]: Segmentation results of the
-            input images. Each Det3DDataSample usually contain:
+            List[:obj:`Det3DDataSample`]: Segmentation results of the input
+            points. Each Det3DDataSample usually contains:
 
-            - ``pred_pts_seg``(PixelData): Prediction of 3D
-                semantic segmentation.
+            - ``pred_pts_seg`` (PointData): Prediction of 3D semantic
+              segmentation.
+            - ``pts_seg_logits`` (PointData): Predicted logits of 3D semantic
+              segmentation before normalization.
         """
-        predictions = []
 
-        for i in range(len(seg_pred_list)):
-            img_meta = batch_img_metas[i]
-            seg_pred = seg_pred_list[i]
-            prediction = Det3DDataSample(**{'metainfo': img_meta.metainfo})
-            prediction.set_data({'eval_ann_info': img_meta.eval_ann_info})
-            prediction.set_data(
-                {'pred_pts_seg': PointData(**{'pts_semantic_mask': seg_pred})})
-            predictions.append(prediction)
-        return predictions
+        for i in range(len(seg_logits_list)):
+            seg_logits = seg_logits_list[i]
+            seg_pred = seg_logits.argmax(dim=0)
+            batch_data_samples[i].set_data({
+                'pts_seg_logits':
+                PointData(**{'pts_seg_logits': seg_logits}),
+                'pred_pts_seg':
+                PointData(**{'pts_semantic_mask': seg_pred})
+            })
+        return batch_data_samples
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/segmentors/encoder_decoder.py` & `mmdet3d-1.1.1/mmdet3d/models/segmentors/encoder_decoder.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-from typing import List, Tuple
+from typing import Dict, List, Tuple
 
 import numpy as np
 import torch
 from torch import Tensor
 from torch import nn as nn
-from torch.nn import functional as F
 
 from mmdet3d.registry import MODELS
 from mmdet3d.utils import ConfigType, OptConfigType, OptMultiConfig
 from ...structures.det3d_data_sample import OptSampleList, SampleList
 from ..utils import add_prefix
 from .base import Base3DSegmentor
 
@@ -25,62 +24,69 @@
     1. The ``loss`` method is used to calculate the loss of model,
     which includes two steps: (1) Extracts features to obtain the feature maps
     (2) Call the decode head loss function to forward decode head model and
     calculate losses.
 
     .. code:: text
 
-     loss(): extract_feat() -> _decode_head_forward_train() -> _auxiliary_head_forward_train (optional)
-     _decode_head_forward_train(): decode_head.loss()
-     _auxiliary_head_forward_train(): auxiliary_head.loss (optional)
+    loss(): extract_feat() -> _decode_head_forward_train() -> _auxiliary_head_forward_train (optional)
+    _decode_head_forward_train(): decode_head.loss()
+    _auxiliary_head_forward_train(): auxiliary_head.loss (optional)
 
     2. The ``predict`` method is used to predict segmentation results,
     which includes two steps: (1) Run inference function to obtain the list of
     seg_logits (2) Call post-processing function to obtain list of
-    ``SegDataSampel`` including ``pred_sem_seg`` and ``seg_logits``.
+    ``Det3DDataSample`` including ``pred_pts_seg``.
 
     .. code:: text
 
     predict(): inference() -> postprocess_result()
     inference(): whole_inference()/slide_inference()
     whole_inference()/slide_inference(): encoder_decoder()
     encoder_decoder(): extract_feat() -> decode_head.predict()
 
     4 The ``_forward`` method is used to output the tensor by running the model,
     which includes two steps: (1) Extracts features to obtain the feature maps
-    (2)Call the decode head forward function to forward decode head model.
+    (2) Call the decode head forward function to forward decode head model.
 
     .. code:: text
 
     _forward(): extract_feat() -> _decode_head.forward()
 
     Args:
-
-        backbone (ConfigType): The config for the backnone of segmentor.
-        decode_head (ConfigType): The config for the decode head of segmentor.
-        neck (OptConfigType): The config for the neck of segmentor.
-            Defaults to None.
-        auxiliary_head (OptConfigType): The config for the auxiliary head of
+        backbone (dict or :obj:`ConfigDict`): The config for the backnone of
+            segmentor.
+        decode_head (dict or :obj:`ConfigDict`): The config for the decode
+            head of segmentor.
+        neck (dict or :obj:`ConfigDict`, optional): The config for the neck of
+            segmentor. Defaults to None.
+        auxiliary_head (dict or :obj:`ConfigDict` or List[dict or
+            :obj:`ConfigDict`], optional): The config for the auxiliary head of
             segmentor. Defaults to None.
-        loss_regularization (OptiConfigType): The config for the regularization
+        loss_regularization (dict or :obj:`ConfigDict` or List[dict or
+            :obj:`ConfigDict`], optional): The config for the regularization
             loass. Defaults to None.
-        train_cfg (OptConfigType): The config for training. Defaults to None.
-        test_cfg (OptConfigType): The config for testing. Defaults to None.
-        data_preprocessor (OptConfigType): The pre-process config of
-            :class:`BaseDataPreprocessor`. Defaults to None.
-        init_cfg (OptMultiConfig): The weight initialized config for
-            :class:`BaseModule`. Defaults to None.
+        train_cfg (dict or :obj:`ConfigDict`, optional): The config for
+            training. Defaults to None.
+        test_cfg (dict or :obj:`ConfigDict`, optional): The config for testing.
+            Defaults to None.
+        data_preprocessor (dict or :obj:`ConfigDict`, optional): The
+            pre-process config of :class:`BaseDataPreprocessor`.
+            Defaults to None.
+        init_cfg (dict or :obj:`ConfigDict` or List[dict or :obj:`ConfigDict`],
+            optional): The weight initialized config for :class:`BaseModule`.
+            Defaults to None.
     """  # noqa: E501
 
     def __init__(self,
                  backbone: ConfigType,
                  decode_head: ConfigType,
                  neck: OptConfigType = None,
-                 auxiliary_head: OptConfigType = None,
-                 loss_regularization: OptConfigType = None,
+                 auxiliary_head: OptMultiConfig = None,
+                 loss_regularization: OptMultiConfig = None,
                  train_cfg: OptConfigType = None,
                  test_cfg: OptConfigType = None,
                  data_preprocessor: OptConfigType = None,
                  init_cfg: OptMultiConfig = None) -> None:
         super(EncoderDecoder3D, self).__init__(
             data_preprocessor=data_preprocessor, init_cfg=init_cfg)
         self.backbone = MODELS.build(backbone)
@@ -93,96 +99,120 @@
         self.train_cfg = train_cfg
         self.test_cfg = test_cfg
 
         assert self.with_decode_head, \
             '3D EncoderDecoder Segmentor should have a decode_head'
 
     def _init_decode_head(self, decode_head: ConfigType) -> None:
-        """Initialize ``decode_head``"""
+        """Initialize ``decode_head``."""
         self.decode_head = MODELS.build(decode_head)
         self.num_classes = self.decode_head.num_classes
 
-    def _init_auxiliary_head(self, auxiliary_head: ConfigType) -> None:
-        """Initialize ``auxiliary_head``"""
+    def _init_auxiliary_head(self,
+                             auxiliary_head: OptMultiConfig = None) -> None:
+        """Initialize ``auxiliary_head``."""
         if auxiliary_head is not None:
             if isinstance(auxiliary_head, list):
                 self.auxiliary_head = nn.ModuleList()
                 for head_cfg in auxiliary_head:
                     self.auxiliary_head.append(MODELS.build(head_cfg))
             else:
                 self.auxiliary_head = MODELS.build(auxiliary_head)
 
     def _init_loss_regularization(self,
-                                  loss_regularization: ConfigType) -> None:
-        """Initialize ``loss_regularization``"""
+                                  loss_regularization: OptMultiConfig = None
+                                  ) -> None:
+        """Initialize ``loss_regularization``."""
         if loss_regularization is not None:
             if isinstance(loss_regularization, list):
                 self.loss_regularization = nn.ModuleList()
                 for loss_cfg in loss_regularization:
                     self.loss_regularization.append(MODELS.build(loss_cfg))
             else:
                 self.loss_regularization = MODELS.build(loss_regularization)
 
-    def extract_feat(self, batch_inputs: Tensor) -> Tensor:
+    def extract_feat(self, batch_inputs: Tensor) -> dict:
         """Extract features from points."""
         x = self.backbone(batch_inputs)
         if self.with_neck:
             x = self.neck(x)
         return x
 
     def encode_decode(self, batch_inputs: Tensor,
                       batch_input_metas: List[dict]) -> Tensor:
         """Encode points with backbone and decode into a semantic segmentation
         map of the same size as input.
 
         Args:
-            batch_input (torch.Tensor): Input point cloud sample
-            batch_input_metas (list[dict]): Meta information of each sample.
+            batch_input (Tensor): Input point cloud sample
+            batch_input_metas (List[dict]): Meta information of a batch of
+                samples.
 
         Returns:
-            torch.Tensor: Segmentation logits of shape [B, num_classes, N].
+            Tensor: Segmentation logits of shape [B, num_classes, N].
         """
         x = self.extract_feat(batch_inputs)
         seg_logits = self.decode_head.predict(x, batch_input_metas,
                                               self.test_cfg)
         return seg_logits
 
-    def _decode_head_forward_train(self, batch_inputs_dict: dict,
-                                   batch_data_samples: SampleList) -> dict:
-        """Run forward function and calculate loss for decode head in
-        training."""
+    def _decode_head_forward_train(
+            self, batch_inputs_dict: dict,
+            batch_data_samples: SampleList) -> Dict[str, Tensor]:
+        """Run forward function and calculate loss for decode head in training.
+
+        Args:
+            batch_input (Tensor): Input point cloud sample
+            batch_data_samples (List[:obj:`Det3DDataSample`]): The det3d data
+                samples. It usually includes information such as `metainfo` and
+                `gt_pts_seg`.
+
+        Returns:
+            Dict[str, Tensor]: A dictionary of loss components for decode head.
+        """
         losses = dict()
         loss_decode = self.decode_head.loss(batch_inputs_dict,
                                             batch_data_samples, self.train_cfg)
 
         losses.update(add_prefix(loss_decode, 'decode'))
         return losses
 
     def _auxiliary_head_forward_train(
         self,
         batch_inputs_dict: dict,
         batch_data_samples: SampleList,
-    ) -> dict:
+    ) -> Dict[str, Tensor]:
         """Run forward function and calculate loss for auxiliary head in
-        training."""
+        training.
+
+        Args:
+            batch_input (Tensor): Input point cloud sample
+            batch_data_samples (List[:obj:`Det3DDataSample`]): The det3d data
+                samples. It usually includes information such as `metainfo` and
+                `gt_pts_seg`.
+
+        Returns:
+            Dict[str, Tensor]: A dictionary of loss components for auxiliary
+            head.
+        """
         losses = dict()
         if isinstance(self.auxiliary_head, nn.ModuleList):
             for idx, aux_head in enumerate(self.auxiliary_head):
                 loss_aux = aux_head.loss(batch_inputs_dict, batch_data_samples,
                                          self.train_cfg)
                 losses.update(add_prefix(loss_aux, f'aux_{idx}'))
         else:
             loss_aux = self.auxiliary_head.loss(batch_inputs_dict,
                                                 batch_data_samples,
                                                 self.train_cfg)
             losses.update(add_prefix(loss_aux, 'aux'))
 
         return losses
 
-    def _loss_regularization_forward_train(self) -> dict:
+    def _loss_regularization_forward_train(self) -> Dict[str, Tensor]:
         """Calculate regularization loss for model weight in training."""
         losses = dict()
         if isinstance(self.loss_regularization, nn.ModuleList):
             for idx, regularize_loss in enumerate(self.loss_regularization):
                 loss_regularize = dict(
                     loss_regularize=regularize_loss(self.modules()))
                 losses.update(add_prefix(loss_regularize, f'regularize_{idx}'))
@@ -190,30 +220,29 @@
             loss_regularize = dict(
                 loss_regularize=self.loss_regularization(self.modules()))
             losses.update(add_prefix(loss_regularize, 'regularize'))
 
         return losses
 
     def loss(self, batch_inputs_dict: dict,
-             batch_data_samples: SampleList) -> dict:
+             batch_data_samples: SampleList) -> Dict[str, Tensor]:
         """Calculate losses from a batch of inputs and data samples.
 
         Args:
             batch_inputs_dict (dict): Input sample dict which
                 includes 'points' and 'imgs' keys.
 
-                - points (list[torch.Tensor]): Point cloud of each sample.
-                - imgs (torch.Tensor, optional): Image tensor has shape
-                  (B, C, H, W).
-            batch_data_samples (list[:obj:`Det3DDataSample`]): The det3d
-                data samples. It usually includes information such
-                as `metainfo` and `gt_pts_sem_seg`.
+                - points (List[Tensor]): Point cloud of each sample.
+                - imgs (Tensor, optional): Image tensor has shape (B, C, H, W).
+            batch_data_samples (List[:obj:`Det3DDataSample`]): The det3d data
+                samples. It usually includes information such as `metainfo` and
+                `gt_pts_seg`.
 
         Returns:
-            dict[str, Tensor]: a dictionary of loss components.
+            Dict[str, Tensor]: A dictionary of loss components.
         """
 
         # extract features using backbone
         points = torch.stack(batch_inputs_dict['points'])
         x = self.extract_feat(points)
 
         losses = dict()
@@ -237,26 +266,26 @@
                           patch_center: Tensor,
                           coord_max: Tensor,
                           feats: Tensor,
                           use_normalized_coord: bool = False) -> Tensor:
         """Generating model input.
 
         Generate input by subtracting patch center and adding additional
-            features. Currently support colors and normalized xyz as features.
+        features. Currently support colors and normalized xyz as features.
 
         Args:
-            coords (torch.Tensor): Sampled 3D point coordinate of shape [S, 3].
-            patch_center (torch.Tensor): Center coordinate of the patch.
-            coord_max (torch.Tensor): Max coordinate of all 3D points.
-            feats (torch.Tensor): Features of sampled points of shape [S, C].
-            use_normalized_coord (bool, optional): Whether to use normalized
-                xyz as additional features. Defaults to False.
+            coords (Tensor): Sampled 3D point coordinate of shape [S, 3].
+            patch_center (Tensor): Center coordinate of the patch.
+            coord_max (Tensor): Max coordinate of all 3D points.
+            feats (Tensor): Features of sampled points of shape [S, C].
+            use_normalized_coord (bool): Whether to use normalized xyz as
+                additional features. Defaults to False.
 
         Returns:
-            torch.Tensor: The generated input data of shape [S, 3+C'].
+            Tensor: The generated input data of shape [S, 3+C'].
         """
         # subtract patch center, the z dimension is not centered
         centered_coords = coords.clone()
         centered_coords[:, 0] -= patch_center[0]
         centered_coords[:, 1] -= patch_center[1]
 
         # normalized coordinates as extra features
@@ -277,31 +306,30 @@
                                   eps: float = 1e-3) -> Tuple[Tensor, Tensor]:
         """Sampling points in a sliding window fashion.
 
         First sample patches to cover all the input points.
         Then sample points in each patch to batch points of a certain number.
 
         Args:
-            points (torch.Tensor): Input points of shape [N, 3+C].
+            points (Tensor): Input points of shape [N, 3+C].
             num_points (int): Number of points to be sampled in each patch.
-            block_size (float, optional): Size of a patch to sample.
-            sample_rate (float, optional): Stride used in sliding patch.
-                Defaults to 0.5.
-            use_normalized_coord (bool, optional): Whether to use normalized
-                xyz as additional features. Defaults to False.
-            eps (float, optional): A value added to patch boundary to guarantee
-                points coverage. Defaults to 1e-3.
+            block_size (float): Size of a patch to sample.
+            sample_rate (float): Stride used in sliding patch. Defaults to 0.5.
+            use_normalized_coord (bool): Whether to use normalized xyz as
+                additional features. Defaults to False.
+            eps (float): A value added to patch boundary to guarantee points
+                coverage. Defaults to 1e-3.
 
         Returns:
-            tuple:
+            Tuple[Tensor, Tensor]:
 
-                - patch_points (torch.Tensor): Points of different patches of
-                  shape [K, N, 3+C].
-                - patch_idxs (torch.Tensor): Index of each point in
-                  `patch_points`, of shape [K, N].
+            - patch_points (Tensor): Points of different patches of shape
+              [K, N, 3+C].
+            - patch_idxs (Tensor): Index of each point in `patch_points` of
+              shape [K, N].
         """
         device = points.device
         # we assume the first three dims are points' 3D coordinates
         # and the rest dims are their per-point features
         coords = points[:, :3]
         feats = points[:, 3:]
 
@@ -368,21 +396,21 @@
 
         # make sure all points are sampled at least once
         assert torch.unique(patch_idxs).shape[0] == points.shape[0], \
             'some points are not sampled in sliding inference'
 
         return patch_points, patch_idxs
 
-    def slide_inference(self, point: Tensor, img_meta: List[dict],
+    def slide_inference(self, point: Tensor, input_meta: dict,
                         rescale: bool) -> Tensor:
         """Inference by sliding-window with overlap.
 
         Args:
-            point (torch.Tensor): Input points of shape [N, 3+C].
-            img_meta (dict): Meta information of input sample.
+            point (Tensor): Input points of shape [N, 3+C].
+            input_meta (dict): Meta information of input sample.
             rescale (bool): Whether transform to original number of points.
                 Will be used for voxelization based segmentors.
 
         Returns:
             Tensor: The output segmentation map of shape [num_classes, N].
         """
         num_points = self.test_cfg.num_points
@@ -397,15 +425,16 @@
         feats_dim = patch_points.shape[1]
         seg_logits = []  # save patch predictions
 
         for batch_idx in range(0, patch_points.shape[0], batch_size):
             batch_points = patch_points[batch_idx:batch_idx + batch_size]
             batch_points = batch_points.view(-1, num_points, feats_dim)
             # batch_seg_logit is of shape [B, num_classes, N]
-            batch_seg_logit = self.encode_decode(batch_points, img_meta)
+            batch_seg_logit = self.encode_decode(batch_points,
+                                                 [input_meta] * batch_size)
             batch_seg_logit = batch_seg_logit.transpose(1, 2).contiguous()
             seg_logits.append(batch_seg_logit.view(-1, self.num_classes))
 
         # aggregate per-point logits by indexing sum and dividing count
         seg_logits = torch.cat(seg_logits, dim=0)  # [K*N, num_classes]
         expand_patch_idxs = patch_idxs.unsqueeze(1).repeat(1, self.num_classes)
         preds = point.new_zeros((point.shape[0], self.num_classes)).\
@@ -413,108 +442,104 @@
         count_mat = torch.bincount(patch_idxs)
         preds = preds / count_mat[:, None]
 
         # TODO: if rescale and voxelization segmentor
 
         return preds.transpose(0, 1)  # to [num_classes, K*N]
 
-    def whole_inference(self, points: Tensor, input_metas: List[dict],
+    def whole_inference(self, points: Tensor, batch_input_metas: List[dict],
                         rescale: bool) -> Tensor:
         """Inference with full scene (one forward pass without sliding)."""
-        seg_logit = self.encode_decode(points, input_metas)
+        seg_logit = self.encode_decode(points, batch_input_metas)
         # TODO: if rescale and voxelization segmentor
         return seg_logit
 
-    def inference(self, points: Tensor, input_metas: List[dict],
+    def inference(self, points: Tensor, batch_input_metas: List[dict],
                   rescale: bool) -> Tensor:
         """Inference with slide/whole style.
 
         Args:
-            points (torch.Tensor): Input points of shape [B, N, 3+C].
-            input_metas (list[dict]): Meta information of each sample.
+            points (Tensor): Input points of shape [B, N, 3+C].
+            batch_input_metas (List[dict]): Meta information of a batch of
+                samples.
             rescale (bool): Whether transform to original number of points.
                 Will be used for voxelization based segmentors.
 
         Returns:
             Tensor: The output segmentation map.
         """
         assert self.test_cfg.mode in ['slide', 'whole']
         if self.test_cfg.mode == 'slide':
             seg_logit = torch.stack([
-                self.slide_inference(point, img_meta, rescale)
-                for point, img_meta in zip(points, input_metas)
+                self.slide_inference(point, input_meta, rescale)
+                for point, input_meta in zip(points, batch_input_metas)
             ], 0)
         else:
-            seg_logit = self.whole_inference(points, input_metas, rescale)
-        output = F.softmax(seg_logit, dim=1)
-        return output
+            seg_logit = self.whole_inference(points, batch_input_metas,
+                                             rescale)
+        return seg_logit
 
     def predict(self,
                 batch_inputs_dict: dict,
                 batch_data_samples: SampleList,
                 rescale: bool = True) -> SampleList:
         """Simple test with single scene.
 
         Args:
-            batch_inputs_dict (dict): Input sample dict which
-                includes 'points' and 'imgs' keys.
+            batch_inputs_dict (dict): Input sample dict which includes 'points'
+                and 'imgs' keys.
 
-                - points (list[torch.Tensor]): Point cloud of each sample.
-                - imgs (torch.Tensor, optional): Image tensor has shape
-                    (B, C, H, W).
-            batch_data_samples (list[:obj:`Det3DDataSample`]): The det3d
-                data samples. It usually includes information such
-                as `metainfo` and `gt_pts_sem_seg`.
+                - points (List[Tensor]): Point cloud of each sample.
+                - imgs (Tensor, optional): Image tensor has shape (B, C, H, W).
+            batch_data_samples (List[:obj:`Det3DDataSample`]): The det3d data
+                samples. It usually includes information such as `metainfo` and
+                `gt_pts_seg`.
             rescale (bool): Whether transform to original number of points.
                 Will be used for voxelization based segmentors.
                 Defaults to True.
 
         Returns:
-            list[dict]: The output prediction result with following keys:
+            List[:obj:`Det3DDataSample`]: Segmentation results of the input
+            points. Each Det3DDataSample usually contains:
 
-                - semantic_mask (Tensor): Segmentation mask of shape [N].
+            - ``pred_pts_seg`` (PointData): Prediction of 3D semantic
+              segmentation.
+            - ``pts_seg_logits`` (PointData): Predicted logits of 3D semantic
+              segmentation before normalization.
         """
         # 3D segmentation requires per-point prediction, so it's impossible
         # to use down-sampling to get a batch of scenes with same num_points
         # therefore, we only support testing one scene every time
-        seg_pred_list = []
+        seg_logits_list = []
         batch_input_metas = []
         for data_sample in batch_data_samples:
             batch_input_metas.append(data_sample.metainfo)
 
         points = batch_inputs_dict['points']
         for point, input_meta in zip(points, batch_input_metas):
-            seg_prob = self.inference(
+            seg_logits = self.inference(
                 point.unsqueeze(0), [input_meta], rescale)[0]
-            seg_map = seg_prob.argmax(0)  # [N]
-            # to cpu tensor for consistency with det3d
-            seg_map = seg_map.cpu()
-            seg_pred_list.append(seg_map)
+            seg_logits_list.append(seg_logits)
 
-        return self.postprocess_result(seg_pred_list, batch_data_samples)
+        return self.postprocess_result(seg_logits_list, batch_data_samples)
 
     def _forward(self,
                  batch_inputs_dict: dict,
                  batch_data_samples: OptSampleList = None) -> Tensor:
         """Network forward process.
 
         Args:
-            batch_inputs_dict (dict): Input sample dict which
-                includes 'points' and 'imgs' keys.
+            batch_inputs_dict (dict): Input sample dict which includes 'points'
+                and 'imgs' keys.
 
-                - points (list[torch.Tensor]): Point cloud of each sample.
-                - imgs (torch.Tensor, optional): Image tensor has shape
-                  (B, C, H, W).
-            batch_data_samples (List[:obj:`Det3DDataSample`]): The seg
-                data samples. It usually includes information such
-                as `metainfo` and `gt_pts_sem_seg`.
+                - points (List[Tensor]): Point cloud of each sample.
+                - imgs (Tensor, optional): Image tensor has shape (B, C, H, W).
+            batch_data_samples (List[:obj:`Det3DDataSample`]): The det3d data
+                samples. It usually includes information such as `metainfo` and
+                `gt_pts_seg`.
 
         Returns:
             Tensor: Forward output of model without any post-processes.
         """
         points = torch.stack(batch_inputs_dict['points'])
         x = self.extract_feat(points)
         return self.decode_head.forward(x)
-
-    def aug_test(self, batch_inputs, batch_img_metas):
-        """Placeholder for augmentation test."""
-        pass
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/__init__.py` & `mmdet3d-1.1.1/mmdet3d/models/task_modules/__init__.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/anchor/__init__.py` & `mmdet3d-1.1.1/mmdet3d/models/task_modules/anchor/__init__.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/anchor/anchor_3d_generator.py` & `mmdet3d-1.1.1/mmdet3d/models/task_modules/anchor/anchor_3d_generator.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,13 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import List, Tuple, Union
+
 import mmengine
 import torch
+from torch import Tensor
 
 from mmdet3d.registry import TASK_UTILS
 
 
 @TASK_UTILS.register_module()
 class Anchor3DRangeGenerator(object):
     """3D Anchor Generator by range.
@@ -33,21 +36,21 @@
         size_per_range (bool, optional): Whether to use separate ranges for
             different sizes. If size_per_range is True, the ranges should have
             the same length as the sizes, if not, it will be duplicated.
             Defaults to True.
     """
 
     def __init__(self,
-                 ranges,
-                 sizes=[[3.9, 1.6, 1.56]],
-                 scales=[1],
-                 rotations=[0, 1.5707963],
-                 custom_values=(),
-                 reshape_out=True,
-                 size_per_range=True):
+                 ranges: List[List[float]],
+                 sizes: List[List[float]] = [[3.9, 1.6, 1.56]],
+                 scales: List[int] = [1],
+                 rotations: List[float] = [0, 1.5707963],
+                 custom_values: Tuple[float] = (),
+                 reshape_out: bool = True,
+                 size_per_range: bool = True) -> None:
         assert mmengine.is_list_of(ranges, list)
         if size_per_range:
             if len(sizes) != len(ranges):
                 assert len(ranges) == 1
                 ranges = ranges * len(sizes)
             assert len(ranges) == len(sizes)
         else:
@@ -60,37 +63,40 @@
         self.ranges = ranges
         self.rotations = rotations
         self.custom_values = custom_values
         self.cached_anchors = None
         self.reshape_out = reshape_out
         self.size_per_range = size_per_range
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         s = self.__class__.__name__ + '('
         s += f'anchor_range={self.ranges},\n'
         s += f'scales={self.scales},\n'
         s += f'sizes={self.sizes},\n'
         s += f'rotations={self.rotations},\n'
         s += f'reshape_out={self.reshape_out},\n'
         s += f'size_per_range={self.size_per_range})'
         return s
 
     @property
-    def num_base_anchors(self):
-        """list[int]: Total number of base anchors in a feature grid."""
+    def num_base_anchors(self) -> int:
+        """int: Total number of base anchors in a feature grid."""
         num_rot = len(self.rotations)
         num_size = torch.tensor(self.sizes).reshape(-1, 3).size(0)
         return num_rot * num_size
 
     @property
-    def num_levels(self):
+    def num_levels(self) -> int:
         """int: Number of feature levels that the generator is applied to."""
         return len(self.scales)
 
-    def grid_anchors(self, featmap_sizes, device='cuda'):
+    def grid_anchors(
+            self,
+            featmap_sizes: List[Tuple[int]],
+            device: Union[str, torch.device] = 'cuda') -> List[Tensor]:
         """Generate grid anchors in multiple feature levels.
 
         Args:
             featmap_sizes (list[tuple]): List of feature map sizes in
                 multiple feature levels.
             device (str, optional): Device where the anchors will be put on.
                 Defaults to 'cuda'.
@@ -108,15 +114,19 @@
             anchors = self.single_level_grid_anchors(
                 featmap_sizes[i], self.scales[i], device=device)
             if self.reshape_out:
                 anchors = anchors.reshape(-1, anchors.size(-1))
             multi_level_anchors.append(anchors)
         return multi_level_anchors
 
-    def single_level_grid_anchors(self, featmap_size, scale, device='cuda'):
+    def single_level_grid_anchors(
+            self,
+            featmap_size: Tuple[int],
+            scale: int,
+            device: Union[str, torch.device] = 'cuda') -> Tensor:
         """Generate grid anchors of a single level feature map.
 
         This function is usually called by method ``self.grid_anchors``.
 
         Args:
             featmap_size (tuple[int]): Size of the feature map.
             scale (float): Scale factor of the anchors in the current level.
@@ -148,21 +158,22 @@
                     scale,
                     anchor_size,
                     self.rotations,
                     device=device))
         mr_anchors = torch.cat(mr_anchors, dim=-3)
         return mr_anchors
 
-    def anchors_single_range(self,
-                             feature_size,
-                             anchor_range,
-                             scale=1,
-                             sizes=[[3.9, 1.6, 1.56]],
-                             rotations=[0, 1.5707963],
-                             device='cuda'):
+    def anchors_single_range(
+            self,
+            feature_size: Tuple[int],
+            anchor_range: Union[Tensor, List[float]],
+            scale: int = 1,
+            sizes: Union[List[List[float]], List[float]] = [[3.9, 1.6, 1.56]],
+            rotations: List[float] = [0, 1.5707963],
+            device: Union[str, torch.device] = 'cuda') -> Tensor:
         """Generate anchors in a single range.
 
         Args:
             feature_size (list[float] | tuple[float]): Feature map size. It is
                 either a list of a tuple of [D, H, W](in order of z, y, and x).
             anchor_range (torch.Tensor | list[float]): Range of anchors with
                 shape [6]. The order is consistent with that of anchors, i.e.,
@@ -244,25 +255,26 @@
     Args:
         anchor_corner (bool, optional): Whether to align with the corner of the
             voxel grid. By default it is False and the anchor's center will be
             the same as the corresponding voxel's center, which is also the
             center of the corresponding greature grid. Defaults to False.
     """
 
-    def __init__(self, align_corner=False, **kwargs):
+    def __init__(self, align_corner: bool = False, **kwargs) -> None:
         super(AlignedAnchor3DRangeGenerator, self).__init__(**kwargs)
         self.align_corner = align_corner
 
-    def anchors_single_range(self,
-                             feature_size,
-                             anchor_range,
-                             scale,
-                             sizes=[[3.9, 1.6, 1.56]],
-                             rotations=[0, 1.5707963],
-                             device='cuda'):
+    def anchors_single_range(
+            self,
+            feature_size: List[int],
+            anchor_range: List[float],
+            scale: int,
+            sizes: Union[List[List[float]], List[float]] = [[3.9, 1.6, 1.56]],
+            rotations: List[float] = [0, 1.5707963],
+            device: Union[str, torch.device] = 'cuda') -> Tensor:
         """Generate anchors in a single range.
 
         Args:
             feature_size (list[float] | tuple[float]): Feature map size. It is
                 either a list of a tuple of [D, H, W](in order of z, y, and x).
             anchor_range (torch.Tensor | list[float]): Range of anchors with
                 shape [6]. The order is consistent with that of anchors, i.e.,
@@ -348,20 +360,23 @@
     Note that feature maps of different classes may be different.
 
     Args:
         kwargs (dict): Arguments are the same as those in
             :class:`AlignedAnchor3DRangeGenerator`.
     """
 
-    def __init__(self, **kwargs):
+    def __init__(self, **kwargs) -> None:
         super(AlignedAnchor3DRangeGeneratorPerCls, self).__init__(**kwargs)
         assert len(self.scales) == 1, 'Multi-scale feature map levels are' + \
             ' not supported currently in this kind of anchor generator.'
 
-    def grid_anchors(self, featmap_sizes, device='cuda'):
+    def grid_anchors(
+            self,
+            featmap_sizes: List[Tuple[int]],
+            device: Union[str, torch.device] = 'cuda') -> List[List[Tensor]]:
         """Generate grid anchors in multiple feature levels.
 
         Args:
             featmap_sizes (list[tuple]): List of feature map sizes for
                 different classes in a single feature level.
             device (str, optional): Device where the anchors will be put on.
                 Defaults to 'cuda'.
@@ -375,15 +390,19 @@
         """
         multi_level_anchors = []
         anchors = self.multi_cls_grid_anchors(
             featmap_sizes, self.scales[0], device=device)
         multi_level_anchors.append(anchors)
         return multi_level_anchors
 
-    def multi_cls_grid_anchors(self, featmap_sizes, scale, device='cuda'):
+    def multi_cls_grid_anchors(
+            self,
+            featmap_sizes: List[Tuple[int]],
+            scale: int,
+            device: Union[str, torch.device] = 'cuda') -> List[Tensor]:
         """Generate grid anchors of a single level feature map for multi-class
         with different feature map sizes.
 
         This function is usually called by method ``self.grid_anchors``.
 
         Args:
             featmap_sizes (list[tuple]): List of feature map sizes for
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/anchor/builder.py` & `mmdet3d-1.1.1/mmdet3d/models/task_modules/anchor/builder.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,22 +1,24 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import warnings
+from typing import Any
 
 from mmdet3d.registry import TASK_UTILS
+from mmdet3d.utils import ConfigType
 
 PRIOR_GENERATORS = TASK_UTILS
 
 ANCHOR_GENERATORS = TASK_UTILS
 
 
-def build_prior_generator(cfg, default_args=None):
+def build_prior_generator(cfg: ConfigType, default_args=None) -> Any:
     warnings.warn(
         '``build_prior_generator`` would be deprecated soon, please use '
         '``mmdet3d.registry.TASK_UTILS.build()`` ')
     return TASK_UTILS.build(cfg, default_args=default_args)
 
 
-def build_anchor_generator(cfg, default_args=None):
+def build_anchor_generator(cfg: ConfigType, default_args=None) -> Any:
     warnings.warn(
         '``build_anchor_generator`` would be deprecated soon, please use '
         '``mmdet3d.registry.TASK_UTILS.build()`` ')
     return TASK_UTILS.build(cfg, default_args=default_args)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/assigners/max_3d_iou_assigner.py` & `mmdet3d-1.1.1/mmdet3d/models/task_modules/assigners/max_3d_iou_assigner.py`

 * *Files 0% similar despite different names*

```diff
@@ -38,24 +38,26 @@
             in the second stage. Details are demonstrated in Step 4.
         gpu_assign_thr (int): The upper bound of the number of GT for GPU
             assign. When the number of gt is above this threshold, will assign
             on CPU device. Negative values mean not assign on CPU.
         iou_calculator (dict): Config of overlaps Calculator.
     """
 
-    def __init__(self,
-                 pos_iou_thr: float,
-                 neg_iou_thr: Union[float, tuple],
-                 min_pos_iou: float = .0,
-                 gt_max_assign_all: bool = True,
-                 ignore_iof_thr: float = -1,
-                 ignore_wrt_candidates: bool = True,
-                 match_low_quality: bool = True,
-                 gpu_assign_thr: float = -1,
-                 iou_calculator: dict = dict(type='BboxOverlaps2D')):
+    def __init__(
+        self,
+        pos_iou_thr: float,
+        neg_iou_thr: Union[float, tuple],
+        min_pos_iou: float = .0,
+        gt_max_assign_all: bool = True,
+        ignore_iof_thr: float = -1,
+        ignore_wrt_candidates: bool = True,
+        match_low_quality: bool = True,
+        gpu_assign_thr: float = -1,
+        iou_calculator: dict = dict(type='BboxOverlaps2D')
+    ) -> None:
         self.pos_iou_thr = pos_iou_thr
         self.neg_iou_thr = neg_iou_thr
         self.min_pos_iou = min_pos_iou
         self.gt_max_assign_all = gt_max_assign_all
         self.ignore_iof_thr = ignore_iof_thr
         self.ignore_wrt_candidates = ignore_wrt_candidates
         self.gpu_assign_thr = gpu_assign_thr
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/builder.py` & `mmdet3d-1.1.1/mmdet3d/models/task_modules/builder.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,29 +1,31 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import warnings
+from typing import Any
 
 from mmdet3d.registry import TASK_UTILS
+from mmdet3d.utils.typing_utils import ConfigType
 
 BBOX_ASSIGNERS = TASK_UTILS
 BBOX_SAMPLERS = TASK_UTILS
 BBOX_CODERS = TASK_UTILS
 
 
-def build_assigner(cfg, **default_args):
+def build_assigner(cfg: ConfigType, **default_args) -> Any:
     """Builder of box assigner."""
     warnings.warn('``build_assigner`` would be deprecated soon, please use '
                   '``mmdet3d.registry.TASK_UTILS.build()`` ')
     return TASK_UTILS.build(cfg, default_args=default_args)
 
 
-def build_sampler(cfg, **default_args):
+def build_sampler(cfg: ConfigType, **default_args) -> Any:
     """Builder of box sampler."""
     warnings.warn('``build_sampler`` would be deprecated soon, please use '
                   '``mmdet3d.registry.TASK_UTILS.build()`` ')
     return TASK_UTILS.build(cfg, default_args=default_args)
 
 
-def build_bbox_coder(cfg, **default_args):
+def build_bbox_coder(cfg: ConfigType, **default_args) -> Any:
     """Builder of box coder."""
     warnings.warn('``build_bbox_coder`` would be deprecated soon, please use '
                   '``mmdet3d.registry.TASK_UTILS.build()`` ')
     return TASK_UTILS.build(cfg, default_args=default_args)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/coders/__init__.py` & `mmdet3d-1.1.1/mmdet3d/models/task_modules/coders/__init__.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/coders/anchor_free_bbox_coder.py` & `mmdet3d-1.1.1/mmdet3d/models/task_modules/coders/anchor_free_bbox_coder.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,31 +1,36 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Dict
+
 import numpy as np
 import torch
+from torch import Tensor
 
 from mmdet3d.registry import TASK_UTILS
+from mmdet3d.structures import BaseInstance3DBoxes
 from .partial_bin_based_bbox_coder import PartialBinBasedBBoxCoder
 
 
 @TASK_UTILS.register_module()
 class AnchorFreeBBoxCoder(PartialBinBasedBBoxCoder):
     """Anchor free bbox coder for 3D boxes.
 
     Args:
         num_dir_bins (int): Number of bins to encode direction angle.
         with_rot (bool): Whether the bbox is with rotation.
     """
 
-    def __init__(self, num_dir_bins, with_rot=True):
+    def __init__(self, num_dir_bins: int, with_rot: bool = True) -> None:
         super(AnchorFreeBBoxCoder, self).__init__(
             num_dir_bins, 0, [], with_rot=with_rot)
         self.num_dir_bins = num_dir_bins
         self.with_rot = with_rot
 
-    def encode(self, gt_bboxes_3d, gt_labels_3d):
+    def encode(self, gt_bboxes_3d: BaseInstance3DBoxes,
+               gt_labels_3d: Tensor) -> tuple:
         """Encode ground truth to prediction targets.
 
         Args:
             gt_bboxes_3d (BaseInstance3DBoxes): Ground truth bboxes
                 with shape (n, 7).
             gt_labels_3d (torch.Tensor): Ground truth classes.
 
@@ -47,15 +52,15 @@
         else:
             dir_class_target = gt_labels_3d.new_zeros(box_num)
             dir_res_target = gt_bboxes_3d.tensor.new_zeros(box_num)
 
         return (center_target, size_res_target, dir_class_target,
                 dir_res_target)
 
-    def decode(self, bbox_out):
+    def decode(self, bbox_out: dict) -> Tensor:
         """Decode predicted parts to bbox3d.
 
         Args:
             bbox_out (dict): Predictions from model, should contain keys below.
 
                 - center: predicted bottom center of bboxes.
                 - dir_class: predicted bbox direction class.
@@ -81,15 +86,16 @@
 
         # decode bbox size
         bbox_size = torch.clamp(bbox_out['size'] * 2, min=0.1)
 
         bbox3d = torch.cat([center, bbox_size, dir_angle], dim=-1)
         return bbox3d
 
-    def split_pred(self, cls_preds, reg_preds, base_xyz):
+    def split_pred(self, cls_preds: Tensor, reg_preds: Tensor,
+                   base_xyz: Tensor) -> Dict[str, Tensor]:
         """Split predicted features to specific parts.
 
         Args:
             cls_preds (torch.Tensor): Class predicted features to split.
             reg_preds (torch.Tensor): Regression predicted features to split.
             base_xyz (torch.Tensor): Coordinates of points.
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/coders/centerpoint_bbox_coders.py` & `mmdet3d-1.1.1/mmdet3d/models/task_modules/coders/centerpoint_bbox_coders.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,13 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Dict, List, Optional, Tuple
+
 import torch
 from mmdet.models.task_modules import BaseBBoxCoder
+from torch import Tensor
 
 from mmdet3d.registry import TASK_UTILS
 
 
 @TASK_UTILS.register_module()
 class CenterPointBBoxCoder(BaseBBoxCoder):
     """Bbox coder for CenterPoint.
@@ -18,31 +21,34 @@
         max_num (int, optional): Max number to be kept. Default: 100.
         score_threshold (float, optional): Threshold to filter boxes
             based on score. Default: None.
         code_size (int, optional): Code size of bboxes. Default: 9
     """
 
     def __init__(self,
-                 pc_range,
-                 out_size_factor,
-                 voxel_size,
-                 post_center_range=None,
-                 max_num=100,
-                 score_threshold=None,
-                 code_size=9):
+                 pc_range: List[float],
+                 out_size_factor: int,
+                 voxel_size: List[float],
+                 post_center_range: Optional[List[float]] = None,
+                 max_num: int = 100,
+                 score_threshold: Optional[float] = None,
+                 code_size: int = 9) -> None:
 
         self.pc_range = pc_range
         self.out_size_factor = out_size_factor
         self.voxel_size = voxel_size
         self.post_center_range = post_center_range
         self.max_num = max_num
         self.score_threshold = score_threshold
         self.code_size = code_size
 
-    def _gather_feat(self, feats, inds, feat_masks=None):
+    def _gather_feat(self,
+                     feats: Tensor,
+                     inds: Tensor,
+                     feat_masks: Optional[Tensor] = None) -> Tensor:
         """Given feats and indexes, returns the gathered feats.
 
         Args:
             feats (torch.Tensor): Features to be transposed and gathered
                 with the shape of [B, 2, W, H].
             inds (torch.Tensor): Indexes with the shape of [B, N].
             feat_masks (torch.Tensor, optional): Mask of the feats.
@@ -56,15 +62,15 @@
         feats = feats.gather(1, inds)
         if feat_masks is not None:
             feat_masks = feat_masks.unsqueeze(2).expand_as(feats)
             feats = feats[feat_masks]
             feats = feats.view(-1, dim)
         return feats
 
-    def _topk(self, scores, K=80):
+    def _topk(self, scores: Tensor, K: int = 80) -> Tuple[Tensor]:
         """Get indexes based on scores.
 
         Args:
             scores (torch.Tensor): scores with the shape of [B, N, W, H].
             K (int, optional): Number to be kept. Defaults to 80.
 
         Returns:
@@ -91,15 +97,15 @@
         topk_ys = self._gather_feat(topk_ys.view(batch, -1, 1),
                                     topk_ind).view(batch, K)
         topk_xs = self._gather_feat(topk_xs.view(batch, -1, 1),
                                     topk_ind).view(batch, K)
 
         return topk_score, topk_inds, topk_clses, topk_ys, topk_xs
 
-    def _transpose_and_gather_feat(self, feat, ind):
+    def _transpose_and_gather_feat(self, feat: Tensor, ind: Tensor) -> Tensor:
         """Given feats and indexes, returns the transposed and gathered feats.
 
         Args:
             feat (torch.Tensor): Features to be transposed and gathered
                 with the shape of [B, 2, W, H].
             ind (torch.Tensor): Indexes with the shape of [B, N].
 
@@ -111,22 +117,22 @@
         feat = self._gather_feat(feat, ind)
         return feat
 
     def encode(self):
         pass
 
     def decode(self,
-               heat,
-               rot_sine,
-               rot_cosine,
-               hei,
-               dim,
-               vel,
-               reg=None,
-               task_id=-1):
+               heat: Tensor,
+               rot_sine: Tensor,
+               rot_cosine: Tensor,
+               hei: Tensor,
+               dim: Tensor,
+               vel: Tensor,
+               reg: Optional[Tensor] = None,
+               task_id: int = -1) -> List[Dict[str, Tensor]]:
         """Decode bboxes.
 
         Args:
             heat (torch.Tensor): Heatmap with the shape of [B, N, W, H].
             rot_sine (torch.Tensor): Sine of rotation with the shape of
                 [B, 1, W, H].
             rot_cosine (torch.Tensor): Cosine of rotation with the shape of
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/coders/delta_xyzwhlr_bbox_coder.py` & `mmdet3d-1.1.1/mmdet3d/models/task_modules/coders/delta_xyzwhlr_bbox_coder.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,28 +1,29 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import torch
 from mmdet.models.task_modules import BaseBBoxCoder
+from torch import Tensor
 
 from mmdet3d.registry import TASK_UTILS
 
 
 @TASK_UTILS.register_module()
 class DeltaXYZWLHRBBoxCoder(BaseBBoxCoder):
     """Bbox Coder for 3D boxes.
 
     Args:
         code_size (int): The dimension of boxes to be encoded.
     """
 
-    def __init__(self, code_size=7):
+    def __init__(self, code_size: int = 7) -> None:
         super(DeltaXYZWLHRBBoxCoder, self).__init__()
         self.code_size = code_size
 
     @staticmethod
-    def encode(src_boxes, dst_boxes):
+    def encode(src_boxes: Tensor, dst_boxes: Tensor) -> Tensor:
         """Get box regression transformation deltas (dx, dy, dz, dx_size,
         dy_size, dz_size, dr, dv*) that can be used to transform the
         `src_boxes` into the `target_boxes`.
 
         Args:
             src_boxes (torch.Tensor): source boxes, e.g., object proposals.
             dst_boxes (torch.Tensor): target of the transformation, e.g.,
@@ -51,15 +52,15 @@
         lt = torch.log(lg / la)
         wt = torch.log(wg / wa)
         ht = torch.log(hg / ha)
         rt = rg - ra
         return torch.cat([xt, yt, zt, wt, lt, ht, rt, *cts], dim=-1)
 
     @staticmethod
-    def decode(anchors, deltas):
+    def decode(anchors: Tensor, deltas: Tensor) -> Tensor:
         """Apply transformation `deltas` (dx, dy, dz, dx_size, dy_size,
         dz_size, dr, dv*) to `boxes`.
 
         Args:
             anchors (torch.Tensor): Parameters of anchors with shape (N, 7).
             deltas (torch.Tensor): Encoded boxes with shape
                 (N, 7+n) [x, y, z, x_size, y_size, z_size, r, velo*].
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/coders/fcos3d_bbox_coder.py` & `mmdet3d-1.1.1/mmdet3d/models/task_modules/coders/fcos3d_bbox_coder.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,11 +1,14 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Optional, Tuple
+
 import numpy as np
 import torch
 from mmdet.models.task_modules import BaseBBoxCoder
+from torch import Tensor
 
 from mmdet3d.registry import TASK_UTILS
 from mmdet3d.structures.bbox_3d import limit_period
 
 
 @TASK_UTILS.register_module()
 class FCOS3DBBoxCoder(BaseBBoxCoder):
@@ -18,29 +21,34 @@
             dimension. Defaults to None.
         code_size (int): The dimension of boxes to be encoded. Defaults to 7.
         norm_on_bbox (bool): Whether to apply normalization on the bounding
             box 2D attributes. Defaults to True.
     """
 
     def __init__(self,
-                 base_depths=None,
-                 base_dims=None,
-                 code_size=7,
-                 norm_on_bbox=True):
+                 base_depths: Optional[Tuple[Tuple[float]]] = None,
+                 base_dims: Optional[Tuple[Tuple[float]]] = None,
+                 code_size: int = 7,
+                 norm_on_bbox: bool = True) -> None:
         super(FCOS3DBBoxCoder, self).__init__()
         self.base_depths = base_depths
         self.base_dims = base_dims
         self.bbox_code_size = code_size
         self.norm_on_bbox = norm_on_bbox
 
     def encode(self, gt_bboxes_3d, gt_labels_3d, gt_bboxes, gt_labels):
         # TODO: refactor the encoder in the FCOS3D and PGD head
         pass
 
-    def decode(self, bbox, scale, stride, training, cls_score=None):
+    def decode(self,
+               bbox: Tensor,
+               scale: tuple,
+               stride: int,
+               training: bool,
+               cls_score: Optional[Tensor] = None) -> Tensor:
         """Decode regressed results into 3D predictions.
 
         Note that offsets are not transformed to the projected 3D centers.
 
         Args:
             bbox (torch.Tensor): Raw bounding box predictions in shape
                 [N, C, H, W].
@@ -96,15 +104,16 @@
             if not training:
                 # Note that this line is conducted only when testing
                 bbox[:, :2] *= stride
 
         return bbox
 
     @staticmethod
-    def decode_yaw(bbox, centers2d, dir_cls, dir_offset, cam2img):
+    def decode_yaw(bbox: Tensor, centers2d: Tensor, dir_cls: Tensor,
+                   dir_offset: float, cam2img: Tensor) -> Tensor:
         """Decode yaw angle and change it from local to global.i.
 
         Args:
             bbox (torch.Tensor): Bounding box predictions in shape
                 [N, C] with yaws to be decoded.
             centers2d (torch.Tensor): Projected 3D-center on the image planes
                 corresponding to the box predictions.
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/coders/groupfree3d_bbox_coder.py` & `mmdet3d-1.1.1/mmdet3d/models/task_modules/coders/groupfree3d_bbox_coder.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,12 +1,16 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Dict, List
+
 import numpy as np
 import torch
+from torch import Tensor
 
 from mmdet3d.registry import TASK_UTILS
+from mmdet3d.structures.bbox_3d import BaseInstance3DBoxes
 from .partial_bin_based_bbox_coder import PartialBinBasedBBoxCoder
 
 
 @TASK_UTILS.register_module()
 class GroupFree3DBBoxCoder(PartialBinBasedBBoxCoder):
     """Modified partial bin based bbox coder for GroupFree3D.
 
@@ -17,27 +21,28 @@
         with_rot (bool, optional): Whether the bbox is with rotation.
             Defaults to True.
         size_cls_agnostic (bool, optional): Whether the predicted size is
             class-agnostic. Defaults to True.
     """
 
     def __init__(self,
-                 num_dir_bins,
-                 num_sizes,
-                 mean_sizes,
-                 with_rot=True,
-                 size_cls_agnostic=True):
+                 num_dir_bins: int,
+                 num_sizes: int,
+                 mean_sizes: List[List[int]],
+                 with_rot: bool = True,
+                 size_cls_agnostic: bool = True) -> None:
         super(GroupFree3DBBoxCoder, self).__init__(
             num_dir_bins=num_dir_bins,
             num_sizes=num_sizes,
             mean_sizes=mean_sizes,
             with_rot=with_rot)
         self.size_cls_agnostic = size_cls_agnostic
 
-    def encode(self, gt_bboxes_3d, gt_labels_3d):
+    def encode(self, gt_bboxes_3d: BaseInstance3DBoxes,
+               gt_labels_3d: Tensor) -> tuple:
         """Encode ground truth to prediction targets.
 
         Args:
             gt_bboxes_3d (BaseInstance3DBoxes): Ground truth bboxes
                 with shape (n, 7).
             gt_labels_3d (torch.Tensor): Ground truth classes.
 
@@ -61,15 +66,15 @@
         else:
             dir_class_target = gt_labels_3d.new_zeros(box_num)
             dir_res_target = gt_bboxes_3d.tensor.new_zeros(box_num)
 
         return (center_target, size_target, size_class_target, size_res_target,
                 dir_class_target, dir_res_target)
 
-    def decode(self, bbox_out, prefix=''):
+    def decode(self, bbox_out: dict, prefix: str = '') -> Tensor:
         """Decode predicted parts to bbox3d.
 
         Args:
             bbox_out (dict): Predictions from model, should contain keys below.
 
                 - center: predicted bottom center of bboxes.
                 - dir_class: predicted bbox direction class.
@@ -112,15 +117,19 @@
                                            size_class.reshape(-1))
             bbox_size = size_base.reshape(batch_size, num_proposal,
                                           -1) + size_res.squeeze(2)
 
         bbox3d = torch.cat([center, bbox_size, dir_angle], dim=-1)
         return bbox3d
 
-    def split_pred(self, cls_preds, reg_preds, base_xyz, prefix=''):
+    def split_pred(self,
+                   cls_preds: Tensor,
+                   reg_preds: Tensor,
+                   base_xyz: Tensor,
+                   prefix: str = '') -> Dict[str, Tensor]:
         """Split predicted features to specific parts.
 
         Args:
             cls_preds (torch.Tensor): Class predicted features to split.
             reg_preds (torch.Tensor): Regression predicted features to split.
             base_xyz (torch.Tensor): Coordinates of points.
             prefix (str, optional): Decode predictions with specific prefix.
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/coders/monoflex_bbox_coder.py` & `mmdet3d-1.1.1/mmdet3d/models/task_modules/coders/monoflex_bbox_coder.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,18 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Dict, List, Tuple
+
 import numpy as np
 import torch
 from mmdet.models.task_modules import BaseBBoxCoder
+from torch import Tensor
 from torch.nn import functional as F
 
 from mmdet3d.registry import TASK_UTILS
+from mmdet3d.structures.bbox_3d import BaseInstance3DBoxes
 
 
 @TASK_UTILS.register_module()
 class MonoFlexCoder(BaseBBoxCoder):
     """Bbox Coder for MonoFlex.
 
     Args:
@@ -31,27 +35,27 @@
         bin_margin (float): Margin of multibin representations.
         code_size (int): The dimension of boxes to be encoded.
         eps (float, optional): A value added to the denominator for numerical
             stability. Default 1e-3.
     """
 
     def __init__(self,
-                 depth_mode,
-                 base_depth,
-                 depth_range,
-                 combine_depth,
-                 uncertainty_range,
-                 base_dims,
-                 dims_mode,
-                 multibin,
-                 num_dir_bins,
-                 bin_centers,
-                 bin_margin,
-                 code_size,
-                 eps=1e-3):
+                 depth_mode: str,
+                 base_depth: Tuple[float],
+                 depth_range: list,
+                 combine_depth: bool,
+                 uncertainty_range: list,
+                 base_dims: Tuple[Tuple[float]],
+                 dims_mode: str,
+                 multibin: bool,
+                 num_dir_bins: int,
+                 bin_centers: List[float],
+                 bin_margin: float,
+                 code_size: int,
+                 eps: float = 1e-3) -> None:
         super(MonoFlexCoder, self).__init__()
 
         # depth related
         self.depth_mode = depth_mode
         self.base_depth = base_depth
         self.depth_range = depth_range
         self.combine_depth = combine_depth
@@ -67,15 +71,15 @@
         self.bin_centers = bin_centers
         self.bin_margin = bin_margin
 
         # output related
         self.bbox_code_size = code_size
         self.eps = eps
 
-    def encode(self, gt_bboxes_3d):
+    def encode(self, gt_bboxes_3d: BaseInstance3DBoxes) -> Tensor:
         """Encode ground truth to prediction targets.
 
         Args:
             gt_bboxes_3d (`BaseInstance3DBoxes`): Ground truth 3D bboxes.
                 shape: (N, 7).
 
         Returns:
@@ -101,15 +105,16 @@
             encode_local_yaw[inds, i] = 1
             encode_local_yaw[inds, i + self.num_dir_bins] = offset[inds]
 
         orientation_target = encode_local_yaw
 
         return orientation_target
 
-    def decode(self, bbox, base_centers2d, labels, downsample_ratio, cam2imgs):
+    def decode(self, bbox: Tensor, base_centers2d: Tensor, labels: Tensor,
+               downsample_ratio: int, cam2imgs: Tensor) -> Dict[str, Tensor]:
         """Decode bounding box regression into 3D predictions.
 
         Args:
             bbox (Tensor): Raw bounding box predictions for each
                 predict center2d point.
                 shape: (N, C)
             base_centers2d (torch.Tensor): Base centers2d for 3D bboxes.
@@ -207,15 +212,15 @@
             combined_depth=pred_combined_depth,
             direct_depth_uncertainty=pred_direct_depth_uncertainty,
             keypoints_depth_uncertainty=pred_keypoints_depth_uncertainty,
         )
 
         return preds
 
-    def decode_direct_depth(self, depth_offsets):
+    def decode_direct_depth(self, depth_offsets: Tensor) -> Tensor:
         """Transform depth offset to directly regressed depth.
 
         Args:
             depth_offsets (torch.Tensor): Predicted depth offsets.
                 shape: (N, )
 
         Return:
@@ -235,20 +240,20 @@
         if self.depth_range is not None:
             direct_depth = torch.clamp(
                 direct_depth, min=self.depth_range[0], max=self.depth_range[1])
 
         return direct_depth
 
     def decode_location(self,
-                        base_centers2d,
-                        offsets2d,
-                        depths,
-                        cam2imgs,
-                        downsample_ratio,
-                        pad_mode='default'):
+                        base_centers2d: Tensor,
+                        offsets2d: Tensor,
+                        depths: Tensor,
+                        cam2imgs: Tensor,
+                        downsample_ratio: Tensor,
+                        pad_mode: str = 'default') -> Tuple[Tensor]:
         """Retrieve object location.
 
         Args:
             base_centers2d (torch.Tensor): predicted base centers2d.
                 shape: (N, 2)
             offsets2d (torch.Tensor): The offsets between real centers2d
                 and base centers2d.
@@ -279,21 +284,23 @@
         centers2d_extend = \
             torch.cat((centers2d_img, centers2d_img.new_ones(N, 1)),
                       dim=1).unsqueeze(-1)
         locations = torch.matmul(cam2imgs_inv, centers2d_extend).squeeze(-1)
 
         return locations[:, :3]
 
-    def keypoints2depth(self,
-                        keypoints2d,
-                        dimensions,
-                        cam2imgs,
-                        downsample_ratio=4,
-                        group0_index=[(7, 3), (0, 4)],
-                        group1_index=[(2, 6), (1, 5)]):
+    def keypoints2depth(
+            self,
+            keypoints2d: Tensor,
+            dimensions: Tensor,
+            cam2imgs: Tensor,
+            downsample_ratio: int = 4,
+            group0_index: List[Tuple[int]] = [(7, 3), (0, 4)],
+            group1_index: List[Tuple[int]] = [(2, 6),
+                                              (1, 5)]) -> Tuple[Tensor]:
         """Decode depth form three groups of keypoints and geometry projection
         model. 2D keypoints inlucding 8 coreners and top/bottom centers will be
         divided into three groups which will be used to calculate three depths
         of object.
 
         .. code-block:: none
 
@@ -379,15 +386,15 @@
         keypoints_depth = torch.stack(
             (center_depth, corner_group0_depth, corner_group1_depth), dim=1)
         keypoints_depth = torch.clamp(
             keypoints_depth, min=self.depth_range[0], max=self.depth_range[1])
 
         return keypoints_depth
 
-    def decode_dims(self, labels, dims_offset):
+    def decode_dims(self, labels: Tensor, dims_offset: Tensor) -> Tensor:
         """Retrieve object dimensions.
 
         Args:
             labels (torch.Tensor): Each points' category id.
                 shape: (N, K)
             dims_offset (torch.Tensor): Dimension offsets.
                 shape: (N, 3)
@@ -407,15 +414,16 @@
             cls_dimension_std = dims_std[labels, :]
             dimensions = dims_offset * cls_dimension_mean + cls_dimension_std
         else:
             raise ValueError
 
         return dimensions
 
-    def decode_orientation(self, ori_vector, locations):
+    def decode_orientation(self, ori_vector: Tensor,
+                           locations: Tensor) -> Tuple[Tensor]:
         """Retrieve object orientation.
 
         Args:
             ori_vector (torch.Tensor): Local orientation vector
                 in [axis_cls, head_cls, sin, cos] format.
                 shape: (N, num_dir_bins * 4)
             locations (torch.Tensor): Object location.
@@ -463,15 +471,16 @@
         if len(larger_idx) != 0:
             local_yaws[larger_idx] -= 2 * np.pi
         if len(small_idx) != 0:
             local_yaws[small_idx] += 2 * np.pi
 
         return yaws, local_yaws
 
-    def decode_bboxes2d(self, reg_bboxes2d, base_centers2d):
+    def decode_bboxes2d(self, reg_bboxes2d: Tensor,
+                        base_centers2d: Tensor) -> Tensor:
         """Retrieve [x1, y1, x2, y2] format 2D bboxes.
 
         Args:
             reg_bboxes2d (torch.Tensor): Predicted FCOS style
                 2D bboxes.
                 shape: (N, 4)
             base_centers2d (torch.Tensor): predicted base centers2d.
@@ -488,15 +497,16 @@
         xs_max = centers_x + reg_bboxes2d[..., 2]
         ys_max = centers_y + reg_bboxes2d[..., 3]
 
         bboxes2d = torch.stack([xs_min, ys_min, xs_max, ys_max], dim=-1)
 
         return bboxes2d
 
-    def combine_depths(self, depth, depth_uncertainty):
+    def combine_depths(self, depth: Tensor,
+                       depth_uncertainty: Tensor) -> Tensor:
         """Combine all the prediced depths with depth uncertainty.
 
         Args:
             depth (torch.Tensor): Predicted depths of each object.
                 2D bboxes.
                 shape: (N, 4)
             depth_uncertainty (torch.Tensor): Depth uncertainty for
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/coders/partial_bin_based_bbox_coder.py` & `mmdet3d-1.1.1/mmdet3d/models/task_modules/coders/partial_bin_based_bbox_coder.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,35 +1,44 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Dict, List
+
 import numpy as np
 import torch
 from mmdet.models.task_modules import BaseBBoxCoder
+from torch import Tensor
 
 from mmdet3d.registry import TASK_UTILS
+from mmdet3d.structures.bbox_3d import BaseInstance3DBoxes
 
 
 @TASK_UTILS.register_module()
 class PartialBinBasedBBoxCoder(BaseBBoxCoder):
     """Partial bin based bbox coder.
 
     Args:
         num_dir_bins (int): Number of bins to encode direction angle.
         num_sizes (int): Number of size clusters.
         mean_sizes (list[list[int]]): Mean size of bboxes in each class.
         with_rot (bool): Whether the bbox is with rotation.
     """
 
-    def __init__(self, num_dir_bins, num_sizes, mean_sizes, with_rot=True):
+    def __init__(self,
+                 num_dir_bins: int,
+                 num_sizes: int,
+                 mean_sizes: List[List[int]],
+                 with_rot: bool = True):
         super(PartialBinBasedBBoxCoder, self).__init__()
         assert len(mean_sizes) == num_sizes
         self.num_dir_bins = num_dir_bins
         self.num_sizes = num_sizes
         self.mean_sizes = mean_sizes
         self.with_rot = with_rot
 
-    def encode(self, gt_bboxes_3d, gt_labels_3d):
+    def encode(self, gt_bboxes_3d: BaseInstance3DBoxes,
+               gt_labels_3d: Tensor) -> tuple:
         """Encode ground truth to prediction targets.
 
         Args:
             gt_bboxes_3d (BaseInstance3DBoxes): Ground truth bboxes
                 with shape (n, 7).
             gt_labels_3d (torch.Tensor): Ground truth classes.
 
@@ -52,15 +61,15 @@
         else:
             dir_class_target = gt_labels_3d.new_zeros(box_num)
             dir_res_target = gt_bboxes_3d.tensor.new_zeros(box_num)
 
         return (center_target, size_class_target, size_res_target,
                 dir_class_target, dir_res_target)
 
-    def decode(self, bbox_out, suffix=''):
+    def decode(self, bbox_out: dict, suffix: str = '') -> Tensor:
         """Decode predicted parts to bbox3d.
 
         Args:
             bbox_out (dict): Predictions from model, should contain keys below.
 
                 - center: predicted bottom center of bboxes.
                 - dir_class: predicted bbox direction class.
@@ -95,15 +104,16 @@
         size_base = torch.index_select(mean_sizes, 0, size_class.reshape(-1))
         bbox_size = size_base.reshape(batch_size, num_proposal,
                                       -1) + size_res.squeeze(2)
 
         bbox3d = torch.cat([center, bbox_size, dir_angle], dim=-1)
         return bbox3d
 
-    def decode_corners(self, center, size_res, size_class):
+    def decode_corners(self, center: Tensor, size_res: Tensor,
+                       size_class: Tensor) -> Tensor:
         """Decode center, size residuals and class to corners. Only useful for
         axis-aligned bounding boxes, so angle isn't considered.
 
         Args:
             center (torch.Tensor): Shape [B, N, 3]
             size_res (torch.Tensor): Shape [B, N, 3] or [B, N, C, 3]
             size_class (torch.Tensor): Shape: [B, N] or [B, N, 1]
@@ -133,15 +143,16 @@
         size_full = torch.clamp(size_full, 0)
         half_size_full = size_full / 2
         corner1 = center - half_size_full
         corner2 = center + half_size_full
         corners = torch.cat([corner1, corner2], dim=-1)
         return corners
 
-    def split_pred(self, cls_preds, reg_preds, base_xyz):
+    def split_pred(self, cls_preds: Tensor, reg_preds: Tensor,
+                   base_xyz: Tensor) -> Dict[str, Tensor]:
         """Split predicted features to specific parts.
 
         Args:
             cls_preds (torch.Tensor): Class predicted features to split.
             reg_preds (torch.Tensor): Regression predicted features to split.
             base_xyz (torch.Tensor): Coordinates of points.
 
@@ -197,15 +208,15 @@
         start = end
 
         # decode semantic score
         results['sem_scores'] = cls_preds_trans[..., start:].contiguous()
 
         return results
 
-    def angle2class(self, angle):
+    def angle2class(self, angle: Tensor) -> tuple:
         """Convert continuous angle to a discrete class and a residual.
 
         Convert continuous angle to a discrete class and a small
         regression number from class center angle to current angle.
 
         Args:
             angle (torch.Tensor): Angle is from 0-2pi (or -pi~pi),
@@ -218,15 +229,18 @@
         angle_per_class = 2 * np.pi / float(self.num_dir_bins)
         shifted_angle = (angle + angle_per_class / 2) % (2 * np.pi)
         angle_cls = shifted_angle // angle_per_class
         angle_res = shifted_angle - (
             angle_cls * angle_per_class + angle_per_class / 2)
         return angle_cls.long(), angle_res
 
-    def class2angle(self, angle_cls, angle_res, limit_period=True):
+    def class2angle(self,
+                    angle_cls: Tensor,
+                    angle_res: Tensor,
+                    limit_period: bool = True) -> Tensor:
         """Inverse function to angle2class.
 
         Args:
             angle_cls (torch.Tensor): Angle class to decode.
             angle_res (torch.Tensor): Angle residual to decode.
             limit_period (bool): Whether to limit angle to [-pi, pi].
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/coders/pgd_bbox_coder.py` & `mmdet3d-1.1.1/mmdet3d/models/task_modules/coders/pgd_bbox_coder.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,13 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Tuple
+
 import numpy as np
 import torch
+from torch import Tensor
 from torch.nn import functional as F
 
 from mmdet3d.registry import TASK_UTILS
 from .fcos3d_bbox_coder import FCOS3DBBoxCoder
 
 
 @TASK_UTILS.register_module()
@@ -12,21 +15,21 @@
     """Bounding box coder for PGD."""
 
     def encode(self, gt_bboxes_3d, gt_labels_3d, gt_bboxes, gt_labels):
         # TODO: refactor the encoder codes in the FCOS3D and PGD head
         pass
 
     def decode_2d(self,
-                  bbox,
-                  scale,
-                  stride,
-                  max_regress_range,
-                  training,
-                  pred_keypoints=False,
-                  pred_bbox2d=True):
+                  bbox: Tensor,
+                  scale: tuple,
+                  stride: int,
+                  max_regress_range: int,
+                  training: bool,
+                  pred_keypoints: bool = False,
+                  pred_bbox2d: bool = True) -> Tensor:
         """Decode regressed 2D attributes.
 
         Args:
             bbox (torch.Tensor): Raw bounding box predictions in shape
                 [N, C, H, W].
             scale (tuple[`Scale`]): Learnable scale parameters.
             stride (int): Stride for a specific feature level.
@@ -66,16 +69,17 @@
                 if pred_bbox2d:
                     bbox[:, -4:] *= stride
         else:
             if pred_bbox2d:
                 bbox[:, -4:] = bbox.clone()[:, -4:].exp()
         return bbox
 
-    def decode_prob_depth(self, depth_cls_preds, depth_range, depth_unit,
-                          division, num_depth_cls):
+    def decode_prob_depth(self, depth_cls_preds: Tensor,
+                          depth_range: Tuple[float], depth_unit: int,
+                          division: str, num_depth_cls: int) -> Tensor:
         """Decode probabilistic depth map.
 
         Args:
             depth_cls_preds (torch.Tensor): Depth probabilistic map in shape
                 [..., self.num_depth_cls] (raw output before softmax).
             depth_range (tuple[float]): Range of depth estimation.
             depth_unit (int): Unit of depth range division.
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/coders/point_xyzwhlr_bbox_coder.py` & `mmdet3d-1.1.1/mmdet3d/models/task_modules/coders/point_xyzwhlr_bbox_coder.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,38 +1,48 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import List, Optional
+
 import numpy as np
 import torch
 from mmdet.models.task_modules import BaseBBoxCoder
+from torch import Tensor
 
 from mmdet3d.registry import TASK_UTILS
+from mmdet3d.structures import BaseInstance3DBoxes
 
 
 @TASK_UTILS.register_module()
 class PointXYZWHLRBBoxCoder(BaseBBoxCoder):
     """Point based bbox coder for 3D boxes.
 
     Args:
         code_size (int): The dimension of boxes to be encoded.
         use_mean_size (bool, optional): Whether using anchors based on class.
             Defaults to True.
         mean_size (list[list[float]], optional): Mean size of bboxes in
             each class. Defaults to None.
     """
 
-    def __init__(self, code_size=7, use_mean_size=True, mean_size=None):
+    def __init__(self,
+                 code_size: int = 7,
+                 use_mean_size: bool = True,
+                 mean_size: List[List[float]] = None):
         super(PointXYZWHLRBBoxCoder, self).__init__()
         self.code_size = code_size
         self.use_mean_size = use_mean_size
         if self.use_mean_size:
             self.mean_size = torch.from_numpy(np.array(mean_size)).float()
             assert self.mean_size.min() > 0, \
                 f'The min of mean_size should > 0, however currently it is '\
                 f'{self.mean_size.min()}, please check it in your config.'
 
-    def encode(self, gt_bboxes_3d, points, gt_labels_3d=None):
+    def encode(self,
+               gt_bboxes_3d: BaseInstance3DBoxes,
+               points: Tensor,
+               gt_labels_3d: Optional[Tensor] = None) -> Tensor:
         """Encode ground truth to prediction targets.
 
         Args:
             gt_bboxes_3d (:obj:`BaseInstance3DBoxes`): Ground truth bboxes
                 with shape (N, 7 + C).
             points (torch.Tensor): Point cloud with shape (N, 3).
             gt_labels_3d (torch.Tensor, optional): Ground truth classes.
@@ -71,15 +81,18 @@
 
         return torch.cat(
             [xt, yt, zt, dxt, dyt, dzt,
              torch.cos(rg),
              torch.sin(rg), *cgs],
             dim=-1)
 
-    def decode(self, box_encodings, points, pred_labels_3d=None):
+    def decode(self,
+               box_encodings: Tensor,
+               points: Tensor,
+               pred_labels_3d: Optional[Tensor] = None) -> Tensor:
         """Decode predicted parts and points to bbox3d.
 
         Args:
             box_encodings (torch.Tensor): Encoded boxes with shape (N, 8 + C).
             points (torch.Tensor): Point cloud with shape (N, 3).
             pred_labels_3d (torch.Tensor): Bbox predicted labels (N, M).
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/coders/smoke_bbox_coder.py` & `mmdet3d-1.1.1/mmdet3d/models/task_modules/coders/smoke_bbox_coder.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,33 +1,40 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import List, Optional, Tuple
+
 import numpy as np
 import torch
 from mmdet.models.task_modules import BaseBBoxCoder
+from torch import Tensor
 
 from mmdet3d.registry import TASK_UTILS
+from mmdet3d.structures import CameraInstance3DBoxes
 
 
 @TASK_UTILS.register_module()
 class SMOKECoder(BaseBBoxCoder):
     """Bbox Coder for SMOKE.
 
     Args:
         base_depth (tuple[float]): Depth references for decode box depth.
         base_dims (tuple[tuple[float]]): Dimension references [l, h, w]
             for decode box dimension for each category.
         code_size (int): The dimension of boxes to be encoded.
     """
 
-    def __init__(self, base_depth, base_dims, code_size):
+    def __init__(self, base_depth: Tuple[float], base_dims: Tuple[float],
+                 code_size: int):
         super(SMOKECoder, self).__init__()
         self.base_depth = base_depth
         self.base_dims = base_dims
         self.bbox_code_size = code_size
 
-    def encode(self, locations, dimensions, orientations, input_metas):
+    def encode(self, locations: Optional[Tensor], dimensions: Tensor,
+               orientations: Tensor,
+               input_metas: List[dict]) -> CameraInstance3DBoxes:
         """Encode CameraInstance3DBoxes by locations, dimensions, orientations.
 
         Args:
             locations (Tensor): Center location for 3D boxes.
                 (N, 3)
             dimensions (Tensor): Dimensions for 3D boxes.
                 shape (N, 3)
@@ -46,20 +53,20 @@
             'match the bbox_code_size.'
         batch_bboxes = input_metas[0]['box_type_3d'](
             bboxes, box_dim=self.bbox_code_size)
 
         return batch_bboxes
 
     def decode(self,
-               reg,
-               points,
-               labels,
-               cam2imgs,
-               trans_mats,
-               locations=None):
+               reg: Tensor,
+               points: Tensor,
+               labels: Tensor,
+               cam2imgs: Tensor,
+               trans_mats: Tensor,
+               locations: Optional[Tensor] = None) -> Tuple[Tensor]:
         """Decode regression into locations, dimensions, orientations.
 
         Args:
             reg (Tensor): Batch regression for each predict center2d point.
                 shape: (batch * K (max_objs), C)
             points(Tensor): Batch projected bbox centers on image plane.
                 shape: (batch * K (max_objs) , 2)
@@ -100,23 +107,24 @@
                 orientations, pred_locations)
         else:
             pred_orientations = self._decode_orientation(
                 orientations, locations)
 
         return pred_locations, pred_dimensions, pred_orientations
 
-    def _decode_depth(self, depth_offsets):
+    def _decode_depth(self, depth_offsets: Tensor) -> Tensor:
         """Transform depth offset to depth."""
         base_depth = depth_offsets.new_tensor(self.base_depth)
         depths = depth_offsets * base_depth[1] + base_depth[0]
 
         return depths
 
-    def _decode_location(self, points, centers2d_offsets, depths, cam2imgs,
-                         trans_mats):
+    def _decode_location(self, points: Tensor, centers2d_offsets: Tensor,
+                         depths: Tensor, cam2imgs: Tensor,
+                         trans_mats: Tensor) -> Tensor:
         """Retrieve objects location in camera coordinate based on projected
         points.
 
         Args:
             points (Tensor): Projected points on feature map in (x, y)
                 shape: (batch * K, 2)
             centers2d_offset (Tensor): Project points offset in
@@ -148,15 +156,15 @@
         if cam2imgs.shape[1] == 4:
             centers2d_img = torch.cat(
                 (centers2d_img, centers2d.new_ones(N, 1, 1)), dim=1)
         locations = torch.matmul(cam2imgs_inv, centers2d_img).squeeze(2)
 
         return locations[:, :3]
 
-    def _decode_dimension(self, labels, dims_offset):
+    def _decode_dimension(self, labels: Tensor, dims_offset: Tensor) -> Tensor:
         """Transform dimension offsets to dimension according to its category.
 
         Args:
             labels (Tensor): Each points' category id.
                 shape: (N, K)
             dims_offset (Tensor): Dimension offsets.
                 shape: (N, 3)
@@ -164,15 +172,16 @@
         labels = labels.flatten().long()
         base_dims = dims_offset.new_tensor(self.base_dims)
         dims_select = base_dims[labels, :]
         dimensions = dims_offset.exp() * dims_select
 
         return dimensions
 
-    def _decode_orientation(self, ori_vector, locations):
+    def _decode_orientation(self, ori_vector: Tensor,
+                            locations: Optional[Tensor]) -> Tensor:
         """Retrieve object orientation.
 
         Args:
             ori_vector (Tensor): Local orientation in [sin, cos] format.
                 shape: (N, 2)
             locations (Tensor): Object location.
                 shape: (N, 3)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/samplers/__init__.py` & `mmdet3d-1.1.1/mmdet3d/models/task_modules/samplers/__init__.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/samplers/iou_neg_piecewise_sampler.py` & `mmdet3d-1.1.1/mmdet3d/models/task_modules/samplers/iou_neg_piecewise_sampler.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,11 +1,15 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import math
+from typing import Optional, Union
 
 import torch
+from mmdet.models.task_modules import AssignResult
+from numpy import ndarray
+from torch import Tensor
 
 from mmdet3d.registry import TASK_UTILS
 from . import RandomSampler, SamplingResult
 
 
 @TASK_UTILS.register_module()
 class IoUNegPiecewiseSampler(RandomSampler):
@@ -25,42 +29,44 @@
             indicate the upper bound of this piece.
         neg_pos_ub (float): The total ratio to limit the upper bound
             number of negative samples.
         add_gt_as_proposals (bool): Whether to add gt as proposals.
     """
 
     def __init__(self,
-                 num,
-                 pos_fraction=None,
-                 neg_piece_fractions=None,
-                 neg_iou_piece_thrs=None,
-                 neg_pos_ub=-1,
-                 add_gt_as_proposals=False,
-                 return_iou=False):
+                 num: int,
+                 pos_fraction: Optional[float] = None,
+                 neg_piece_fractions: Optional[list] = None,
+                 neg_iou_piece_thrs: Optional[list] = None,
+                 neg_pos_ub: float = -1,
+                 add_gt_as_proposals: bool = False,
+                 return_iou: bool = False) -> None:
         super(IoUNegPiecewiseSampler,
               self).__init__(num, pos_fraction, neg_pos_ub,
                              add_gt_as_proposals)
         assert isinstance(neg_piece_fractions, list)
         assert len(neg_piece_fractions) == len(neg_iou_piece_thrs)
         self.neg_piece_fractions = neg_piece_fractions
         self.neg_iou_thr = neg_iou_piece_thrs
         self.return_iou = return_iou
         self.neg_piece_num = len(self.neg_piece_fractions)
 
-    def _sample_pos(self, assign_result, num_expected, **kwargs):
+    def _sample_pos(self, assign_result: AssignResult, num_expected: int,
+                    **kwargs) -> Union[Tensor, ndarray]:
         """Randomly sample some positive samples."""
         pos_inds = torch.nonzero(assign_result.gt_inds > 0, as_tuple=False)
         if pos_inds.numel() != 0:
             pos_inds = pos_inds.squeeze(1)
         if pos_inds.numel() <= num_expected:
             return pos_inds
         else:
             return self.random_choice(pos_inds, num_expected)
 
-    def _sample_neg(self, assign_result, num_expected, **kwargs):
+    def _sample_neg(self, assign_result: AssignResult, num_expected: int,
+                    **kwargs) -> Tensor:
         """Randomly sample some negative samples."""
         neg_inds = torch.nonzero(assign_result.gt_inds == 0, as_tuple=False)
         if neg_inds.numel() != 0:
             neg_inds = neg_inds.squeeze(1)
         if len(neg_inds) <= 0:
             return neg_inds.squeeze(1)
         else:
@@ -123,19 +129,19 @@
                     neg_inds_choice = torch.cat(
                         [neg_inds_choice, neg_inds[piece_choice]], dim=0)
                     extend_num = 0
             assert len(neg_inds_choice) == num_expected
             return neg_inds_choice
 
     def sample(self,
-               assign_result,
-               bboxes,
-               gt_bboxes,
-               gt_labels=None,
-               **kwargs):
+               assign_result: AssignResult,
+               bboxes: Tensor,
+               gt_bboxes: Tensor,
+               gt_labels: Optional[Tensor] = None,
+               **kwargs) -> SamplingResult:
         """Sample positive and negative bboxes.
 
         This is a simple implementation of bbox sampling given candidates,
         assigning results and ground truth bboxes.
 
         Args:
             assign_result (:obj:`AssignResult`): Bbox assigning results.
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/samplers/pseudosample.py` & `mmdet3d-1.1.1/mmdet3d/models/task_modules/samplers/pseudosample.py`

 * *Files 6% similar despite different names*

```diff
@@ -21,15 +21,15 @@
         raise NotImplementedError
 
     def _sample_neg(self, **kwargs):
         """Sample negative samples."""
         raise NotImplementedError
 
     def sample(self, assign_result: AssignResult, pred_instances: InstanceData,
-               gt_instances: InstanceData, *args, **kwargs):
+               gt_instances: InstanceData, *args, **kwargs) -> SamplingResult:
         """Directly returns the positive and negative indices  of samples.
 
         Args:
             assign_result (:obj:`AssignResult`): Bbox assigning results.
             pred_instances (:obj:`InstaceData`): Instances of model
                 predictions. It includes ``priors``, and the priors can
                 be anchors, points, or bboxes predicted by the model,
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/task_modules/voxel/voxel_generator.py` & `mmdet3d-1.1.1/mmdet3d/models/task_modules/voxel/voxel_generator.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,8 +1,10 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import List, Tuple, Union
+
 import numba
 import numpy as np
 
 from mmdet3d.registry import MODELS
 
 
 @MODELS.register_module()
@@ -14,78 +16,79 @@
         point_cloud_range (list[float]): Range of points
         max_num_points (int): Maximum number of points in a single voxel
         max_voxels (int, optional): Maximum number of voxels.
             Defaults to 20000.
     """
 
     def __init__(self,
-                 voxel_size,
-                 point_cloud_range,
-                 max_num_points,
-                 max_voxels=20000):
+                 voxel_size: List[float],
+                 point_cloud_range: List[float],
+                 max_num_points: int,
+                 max_voxels: int = 20000):
 
         point_cloud_range = np.array(point_cloud_range, dtype=np.float32)
         # [0, -40, -3, 70.4, 40, 1]
         voxel_size = np.array(voxel_size, dtype=np.float32)
         grid_size = (point_cloud_range[3:] -
                      point_cloud_range[:3]) / voxel_size
         grid_size = np.round(grid_size).astype(np.int64)
 
         self._voxel_size = voxel_size
         self._point_cloud_range = point_cloud_range
         self._max_num_points = max_num_points
         self._max_voxels = max_voxels
         self._grid_size = grid_size
 
-    def generate(self, points):
+    def generate(self, points: np.ndarray) -> Tuple[np.ndarray]:
         """Generate voxels given points."""
         return points_to_voxel(points, self._voxel_size,
                                self._point_cloud_range, self._max_num_points,
                                True, self._max_voxels)
 
     @property
-    def voxel_size(self):
+    def voxel_size(self) -> List[float]:
         """list[float]: Size of a single voxel."""
         return self._voxel_size
 
     @property
-    def max_num_points_per_voxel(self):
+    def max_num_points_per_voxel(self) -> int:
         """int: Maximum number of points per voxel."""
         return self._max_num_points
 
     @property
-    def point_cloud_range(self):
+    def point_cloud_range(self) -> List[float]:
         """list[float]: Range of point cloud."""
         return self._point_cloud_range
 
     @property
-    def grid_size(self):
+    def grid_size(self) -> np.ndarray:
         """np.ndarray: The size of grids."""
         return self._grid_size
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         """str: Return a string that describes the module."""
         repr_str = self.__class__.__name__
         indent = ' ' * (len(repr_str) + 1)
         repr_str += f'(voxel_size={self._voxel_size},\n'
         repr_str += indent + 'point_cloud_range='
         repr_str += f'{self._point_cloud_range.tolist()},\n'
         repr_str += indent + f'max_num_points={self._max_num_points},\n'
         repr_str += indent + f'max_voxels={self._max_voxels},\n'
         repr_str += indent + f'grid_size={self._grid_size.tolist()}'
         repr_str += ')'
         return repr_str
 
 
-def points_to_voxel(points,
-                    voxel_size,
-                    coors_range,
-                    max_points=35,
-                    reverse_index=True,
-                    max_voxels=20000):
+def points_to_voxel(points: np.ndarray,
+                    voxel_size: Union[list, tuple, np.ndarray],
+                    coors_range: Union[List[float], List[Tuple[float]],
+                                       List[np.ndarray]],
+                    max_points: int = 35,
+                    reverse_index: bool = True,
+                    max_voxels: int = 20000) -> Tuple[np.ndarray]:
     """convert kitti points(N, >=3) to voxels.
 
     Args:
         points (np.ndarray): [N, ndim]. points[:, :3] contain xyz points and
             points[:, 3:] contain other information such as reflectivity.
         voxel_size (list, tuple, np.ndarray): [3] xyz, indicate voxel size
         coors_range (list[float | tuple[float] | ndarray]): Voxel range.
@@ -134,23 +137,25 @@
     voxels = voxels[:voxel_num]
     num_points_per_voxel = num_points_per_voxel[:voxel_num]
 
     return voxels, coors, num_points_per_voxel
 
 
 @numba.jit(nopython=True)
-def _points_to_voxel_reverse_kernel(points,
-                                    voxel_size,
-                                    coors_range,
-                                    num_points_per_voxel,
-                                    coor_to_voxelidx,
-                                    voxels,
-                                    coors,
-                                    max_points=35,
-                                    max_voxels=20000):
+def _points_to_voxel_reverse_kernel(points: np.ndarray,
+                                    voxel_size: Union[list, tuple, np.ndarray],
+                                    coors_range: Union[List[float],
+                                                       List[Tuple[float]],
+                                                       List[np.ndarray]],
+                                    num_points_per_voxel: int,
+                                    coor_to_voxelidx: np.ndarray,
+                                    voxels: np.ndarray,
+                                    coors: np.ndarray,
+                                    max_points: int = 35,
+                                    max_voxels: int = 20000):
     """convert kitti points(N, >=3) to voxels.
 
     Args:
         points (np.ndarray): [N, ndim]. points[:, :3] contain xyz points and
             points[:, 3:] contain other information such as reflectivity.
         voxel_size (list, tuple, np.ndarray): [3] xyz, indicate voxel size
         coors_range (list[float | tuple[float] | ndarray]): Range of voxels.
@@ -208,33 +213,34 @@
         if num < max_points:
             voxels[voxelidx, num] = points[i]
             num_points_per_voxel[voxelidx] += 1
     return voxel_num
 
 
 @numba.jit(nopython=True)
-def _points_to_voxel_kernel(points,
-                            voxel_size,
-                            coors_range,
-                            num_points_per_voxel,
-                            coor_to_voxelidx,
-                            voxels,
-                            coors,
-                            max_points=35,
-                            max_voxels=20000):
+def _points_to_voxel_kernel(points: np.ndarray,
+                            voxel_size: Union[list, tuple, np.ndarray],
+                            coors_range: Union[List[float], List[Tuple[float]],
+                                               List[np.ndarray]],
+                            num_points_per_voxel: int,
+                            coor_to_voxelidx: np.ndarray,
+                            voxels: np.ndarray,
+                            coors: np.ndarray,
+                            max_points: int = 35,
+                            max_voxels: int = 200000):
     """convert kitti points(N, >=3) to voxels.
 
     Args:
         points (np.ndarray): [N, ndim]. points[:, :3] contain xyz points and
             points[:, 3:] contain other information such as reflectivity.
         voxel_size (list, tuple, np.ndarray): [3] xyz, indicate voxel size.
         coors_range (list[float | tuple[float] | ndarray]): Range of voxels.
             format: xyzxyz, minmax
         num_points_per_voxel (int): Number of points per voxel.
-        coor_to_voxel_idx (np.ndarray): A voxel grid of shape (D, H, W),
+        coor_to_voxelidx (np.ndarray): A voxel grid of shape (D, H, W),
             which has the same shape as the complete voxel map. It indicates
             the index of each corresponding voxel.
         voxels (np.ndarray): Created empty voxels.
         coors (np.ndarray): Created coordinates of each voxel.
         max_points (int): Indicate maximum points contained in a voxel.
         max_voxels (int): Maximum number of voxels this function create.
             for second, 20000 is a good choice. Points should be shuffled for
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/test_time_augs/merge_augs.py` & `mmdet3d-1.1.1/mmdet3d/models/test_time_augs/merge_augs.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,39 +1,44 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import List
+
 import torch
 
 from mmdet3d.structures import bbox3d2result, bbox3d_mapping_back, xywhr2xyxyr
+from mmdet3d.utils import ConfigType
 from ..layers import nms_bev, nms_normal_bev
 
 
-def merge_aug_bboxes_3d(aug_results, aug_batch_input_metas, test_cfg):
+def merge_aug_bboxes_3d(aug_results: List[dict],
+                        aug_batch_input_metas: List[dict],
+                        test_cfg: ConfigType) -> dict:
     """Merge augmented detection 3D bboxes and scores.
 
     Args:
-        aug_results (list[dict]): The dict of detection results.
+        aug_results (List[dict]): The dict of detection results.
             The dict contains the following keys
 
             - bbox_3d (:obj:`BaseInstance3DBoxes`): Detection bbox.
-            - scores_3d (torch.Tensor): Detection scores.
-            - labels_3d (torch.Tensor): Predicted box labels.
-        img_metas (list[dict]): Meta information of each sample.
-        test_cfg (dict): Test config.
+            - scores_3d (Tensor): Detection scores.
+            - labels_3d (Tensor): Predicted box labels.
+        aug_batch_input_metas (List[dict]): Meta information of each sample.
+        test_cfg (dict or :obj:`ConfigDict`): Test config.
 
     Returns:
         dict: Bounding boxes results in cpu mode, containing merged results.
 
             - bbox_3d (:obj:`BaseInstance3DBoxes`): Merged detection bbox.
             - scores_3d (torch.Tensor): Merged detection scores.
             - labels_3d (torch.Tensor): Merged predicted box labels.
     """
 
     assert len(aug_results) == len(aug_batch_input_metas), \
-        '"aug_results" should have the same length as "img_metas", got len(' \
-        f'aug_results)={len(aug_results)} and ' \
-        f'len(img_metas)={len(aug_batch_input_metas)}'
+        '"aug_results" should have the same length as ' \
+        f'"aug_batch_input_metas", got len(aug_results)={len(aug_results)} ' \
+        f'and len(aug_batch_input_metas)={len(aug_batch_input_metas)}'
 
     recovered_bboxes = []
     recovered_scores = []
     recovered_labels = []
 
     for bboxes, input_info in zip(aug_results, aug_batch_input_metas):
         scale_factor = input_info['pcd_scale_factor']
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/utils/__init__.py` & `mmdet3d-1.1.1/mmdet3d/models/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/utils/edge_indices.py` & `mmdet3d-1.1.1/mmdet3d/models/utils/edge_indices.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,39 +1,42 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import List
+
 import numpy as np
 import torch
+from torch import Tensor
 
 
-def get_edge_indices(img_metas,
-                     downsample_ratio,
-                     step=1,
-                     pad_mode='default',
-                     dtype=np.float32,
-                     device='cpu'):
+def get_edge_indices(img_metas: List[dict],
+                     downsample_ratio: int,
+                     step: int = 1,
+                     pad_mode: str = 'default',
+                     dtype: type = np.float32,
+                     device: str = 'cpu') -> List[Tensor]:
     """Function to filter the objects label outside the image.
     The edge_indices are generated using numpy on cpu rather
     than on CUDA due to the latency issue. When batch size = 8,
     this function with numpy array is ~8 times faster than that
     with CUDA tensor (0.09s and 0.72s in 100 runs).
 
     Args:
-        img_metas (list[dict]): Meta information of each image, e.g.,
+        img_metas (List[dict]): Meta information of each image, e.g.,
             image size, scaling factor, etc.
         downsample_ratio (int): Downsample ratio of output feature,
-        step (int, optional): Step size used for generateing
-            edge indices. Default: 1.
-        pad_mode (str, optional): Padding mode during data pipeline.
-            Default: 'default'.
-        dtype (torch.dtype, optional): Dtype of edge indices tensor.
-            Default: np.float32.
-        device (str, optional): Device of edge indices tensor.
-            Default: 'cpu'.
+        step (int): Step size used for generateing
+            edge indices. Defaults to 1.
+        pad_mode (str): Padding mode during data pipeline.
+            Defaults to 'default'.
+        dtype (type): Dtype of edge indices tensor.
+            Defaults to np.float32.
+        device (str): Device of edge indices tensor.
+            Defaults to 'cpu'.
 
     Returns:
-        list[Tensor]: Edge indices for each image in batch data.
+        List[Tensor]: Edge indices for each image in batch data.
     """
     edge_indices_list = []
     for i in range(len(img_metas)):
         img_shape = img_metas[i]['img_shape']
         pad_shape = img_metas[i]['pad_shape']
         h, w = img_shape[:2]
         pad_h, pad_w = pad_shape
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/utils/gaussian.py` & `mmdet3d-1.1.1/mmdet3d/models/utils/gaussian.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,42 +1,48 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import List, Tuple
+
 import numpy as np
 import torch
+from torch import Tensor
 
 
-def gaussian_2d(shape, sigma=1):
+def gaussian_2d(shape: Tuple[int, int], sigma: float = 1) -> np.ndarray:
     """Generate gaussian map.
 
     Args:
-        shape (list[int]): Shape of the map.
-        sigma (float, optional): Sigma to generate gaussian map.
+        shape (Tuple[int]): Shape of the map.
+        sigma (float): Sigma to generate gaussian map.
             Defaults to 1.
 
     Returns:
         np.ndarray: Generated gaussian map.
     """
     m, n = [(ss - 1.) / 2. for ss in shape]
     y, x = np.ogrid[-m:m + 1, -n:n + 1]
 
     h = np.exp(-(x * x + y * y) / (2 * sigma * sigma))
     h[h < np.finfo(h.dtype).eps * h.max()] = 0
     return h
 
 
-def draw_heatmap_gaussian(heatmap, center, radius, k=1):
+def draw_heatmap_gaussian(heatmap: Tensor,
+                          center: Tensor,
+                          radius: int,
+                          k: int = 1) -> Tensor:
     """Get gaussian masked heatmap.
 
     Args:
-        heatmap (torch.Tensor): Heatmap to be masked.
-        center (torch.Tensor): Center coord of the heatmap.
+        heatmap (Tensor): Heatmap to be masked.
+        center (Tensor): Center coord of the heatmap.
         radius (int): Radius of gaussian.
-        K (int, optional): Multiple of masked_gaussian. Defaults to 1.
+        k (int): Multiple of masked_gaussian. Defaults to 1.
 
     Returns:
-        torch.Tensor: Masked heatmap.
+        Tensor: Masked heatmap.
     """
     diameter = 2 * radius + 1
     gaussian = gaussian_2d((diameter, diameter), sigma=diameter / 6)
 
     x, y = int(center[0]), int(center[1])
 
     height, width = heatmap.shape[0:2]
@@ -50,23 +56,24 @@
                  radius - left:radius + right]).to(heatmap.device,
                                                    torch.float32)
     if min(masked_gaussian.shape) > 0 and min(masked_heatmap.shape) > 0:
         torch.max(masked_heatmap, masked_gaussian * k, out=masked_heatmap)
     return heatmap
 
 
-def gaussian_radius(det_size, min_overlap=0.5):
+def gaussian_radius(det_size: Tuple[Tensor, Tensor],
+                    min_overlap: float = 0.5) -> Tensor:
     """Get radius of gaussian.
 
     Args:
-        det_size (tuple[torch.Tensor]): Size of the detection result.
-        min_overlap (float, optional): Gaussian_overlap. Defaults to 0.5.
+        det_size (Tuple[Tensor]): Size of the detection result.
+        min_overlap (float): Gaussian_overlap. Defaults to 0.5.
 
     Returns:
-        torch.Tensor: Computed radius.
+        Tensor: Computed radius.
     """
     height, width = det_size
 
     a1 = 1
     b1 = (height + width)
     c1 = width * height * (1 - min_overlap) / (1 + min_overlap)
     sq1 = torch.sqrt(b1**2 - 4 * a1 * c1)
@@ -82,32 +89,36 @@
     b3 = -2 * min_overlap * (height + width)
     c3 = (min_overlap - 1) * width * height
     sq3 = torch.sqrt(b3**2 - 4 * a3 * c3)
     r3 = (b3 + sq3) / 2
     return min(r1, r2, r3)
 
 
-def get_ellip_gaussian_2D(heatmap, center, radius_x, radius_y, k=1):
+def get_ellip_gaussian_2D(heatmap: Tensor,
+                          center: List[int],
+                          radius_x: int,
+                          radius_y: int,
+                          k: int = 1) -> Tensor:
     """Generate 2D ellipse gaussian heatmap.
 
     Args:
         heatmap (Tensor): Input heatmap, the gaussian kernel will cover on
             it and maintain the max value.
-        center (list[int]): Coord of gaussian kernel's center.
+        center (List[int]): Coord of gaussian kernel's center.
         radius_x (int): X-axis radius of gaussian kernel.
         radius_y (int): Y-axis radius of gaussian kernel.
-        k (int, optional): Coefficient of gaussian kernel. Default: 1.
+        k (int): Coefficient of gaussian kernel. Defaults to 1.
 
     Returns:
         out_heatmap (Tensor): Updated heatmap covered by gaussian kernel.
     """
     diameter_x, diameter_y = 2 * radius_x + 1, 2 * radius_y + 1
     gaussian_kernel = ellip_gaussian2D((radius_x, radius_y),
-                                       sigma_x=diameter_x / 6,
-                                       sigma_y=diameter_y / 6,
+                                       sigma_x=diameter_x // 6,
+                                       sigma_y=diameter_y // 6,
                                        dtype=heatmap.dtype,
                                        device=heatmap.device)
 
     x, y = int(center[0]), int(center[1])
     height, width = heatmap.shape[0:2]
 
     left, right = min(x, radius_x), min(width - x, radius_x + 1)
@@ -121,30 +132,30 @@
         masked_heatmap,
         masked_gaussian * k,
         out=out_heatmap[y - top:y + bottom, x - left:x + right])
 
     return out_heatmap
 
 
-def ellip_gaussian2D(radius,
-                     sigma_x,
-                     sigma_y,
-                     dtype=torch.float32,
-                     device='cpu'):
+def ellip_gaussian2D(radius: Tuple[int, int],
+                     sigma_x: int,
+                     sigma_y: int,
+                     dtype: torch.dtype = torch.float32,
+                     device: str = 'cpu') -> Tensor:
     """Generate 2D ellipse gaussian kernel.
 
     Args:
-        radius (tuple(int)): Ellipse radius (radius_x, radius_y) of gaussian
+        radius (Tuple[int]): Ellipse radius (radius_x, radius_y) of gaussian
             kernel.
         sigma_x (int): X-axis sigma of gaussian function.
         sigma_y (int): Y-axis sigma of gaussian function.
-        dtype (torch.dtype, optional): Dtype of gaussian tensor.
-            Default: torch.float32.
-        device (str, optional): Device of gaussian tensor.
-            Default: 'cpu'.
+        dtype (torch.dtype): Dtype of gaussian tensor.
+            Defaults to torch.float32.
+        device (str): Device of gaussian tensor.
+            Defaults to 'cpu'.
 
     Returns:
         h (Tensor): Gaussian kernel with a
             ``(2 * radius_y + 1) * (2 * radius_x + 1)`` shape.
     """
     x = torch.arange(
         -radius[0], radius[0] + 1, dtype=dtype, device=device).view(1, -1)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/utils/gen_keypoints.py` & `mmdet3d-1.1.1/mmdet3d/models/utils/gen_keypoints.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,33 +1,37 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import List, Tuple
+
 import torch
+from torch import Tensor
 
-from mmdet3d.structures import points_cam2img
+from mmdet3d.structures import CameraInstance3DBoxes, points_cam2img
 
 
-def get_keypoints(gt_bboxes_3d_list,
-                  centers2d_list,
-                  img_metas,
-                  use_local_coords=True):
+def get_keypoints(
+        gt_bboxes_3d_list: List[CameraInstance3DBoxes],
+        centers2d_list: List[Tensor],
+        img_metas: List[dict],
+        use_local_coords: bool = True) -> Tuple[List[Tensor], List[Tensor]]:
     """Function to filter the objects label outside the image.
 
     Args:
-        gt_bboxes_list (list[Tensor]): Ground truth bboxes of each image,
-            shape (num_gt, 4).
-        centers2d_list (list[Tensor]): Projected 3D centers onto 2D image,
+        gt_bboxes_3d_list (List[:obj:`CameraInstance3DBoxes`]): Ground truth
+            bboxes of each image.
+        centers2d_list (List[Tensor]): Projected 3D centers onto 2D image,
             shape (num_gt, 2).
-        img_metas (list[dict]): Meta information of each image, e.g.,
+        img_metas (List[dict]): Meta information of each image, e.g.,
             image size, scaling factor, etc.
-        use_local_coords (bool, optional): Wheher to use local coordinates
-            for keypoints. Default: True.
+        use_local_coords (bool): Whether to use local coordinates
+            for keypoints. Defaults to True.
 
     Returns:
-        tuple[list[Tensor]]: It contains two elements, the first is the
-        keypoints for each projected 2D bbox in batch data. The second is
-        the visible mask of depth calculated by keypoints.
+        Tuple[List[Tensor], List[Tensor]]: It contains two elements,
+        the first is the keypoints for each projected 2D bbox in batch data.
+        The second is the visible mask of depth calculated by keypoints.
     """
 
     assert len(gt_bboxes_3d_list) == len(centers2d_list)
     bs = len(gt_bboxes_3d_list)
     keypoints2d_list = []
     keypoints_depth_mask_list = []
 
@@ -52,16 +56,16 @@
         keypoints_x_visible = (keypoints2d[..., 0] >= 0) & (
             keypoints2d[..., 0] <= w - 1)
         keypoints_y_visible = (keypoints2d[..., 1] >= 0) & (
             keypoints2d[..., 1] <= h - 1)
         keypoints_z_visible = (keypoints3d[..., -1] > 0)
 
         # (N, 1O)
-        keypoints_visible = keypoints_x_visible & \
-            keypoints_y_visible & keypoints_z_visible
+        keypoints_visible = \
+            keypoints_x_visible & keypoints_y_visible & keypoints_z_visible
         # center, diag-02, diag-13
         keypoints_depth_valid = torch.stack(
             (keypoints_visible[:, [8, 9]].all(dim=1),
              keypoints_visible[:, [0, 3, 5, 6]].all(dim=1),
              keypoints_visible[:, [1, 2, 4, 7]].all(dim=1)),
             dim=1)
         keypoints_visible = keypoints_visible.float()
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/utils/handle_objs.py` & `mmdet3d-1.1.1/mmdet3d/models/utils/handle_objs.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,46 +1,57 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import List, Tuple
+
 import torch
+from torch import Tensor
+
+from mmdet3d.structures import CameraInstance3DBoxes
 
 
-def filter_outside_objs(gt_bboxes_list, gt_labels_list, gt_bboxes_3d_list,
-                        gt_labels_3d_list, centers2d_list, img_metas):
+def filter_outside_objs(gt_bboxes_list: List[Tensor],
+                        gt_labels_list: List[Tensor],
+                        gt_bboxes_3d_list: List[CameraInstance3DBoxes],
+                        gt_labels_3d_list: List[Tensor],
+                        centers2d_list: List[Tensor],
+                        img_metas: List[dict]) -> None:
     """Function to filter the objects label outside the image.
 
     Args:
-        gt_bboxes_list (list[Tensor]): Ground truth bboxes of each image,
+        gt_bboxes_list (List[Tensor]): Ground truth bboxes of each image,
             each has shape (num_gt, 4).
-        gt_labels_list (list[Tensor]): Ground truth labels of each box,
+        gt_labels_list (List[Tensor]): Ground truth labels of each box,
             each has shape (num_gt,).
-        gt_bboxes_3d_list (list[Tensor]): 3D Ground truth bboxes of each
-            image, each has shape (num_gt, bbox_code_size).
-        gt_labels_3d_list (list[Tensor]): 3D Ground truth labels of each
+        gt_bboxes_3d_list (List[:obj:`CameraInstance3DBoxes`]): 3D Ground
+            truth bboxes of each image, each has shape
+            (num_gt, bbox_code_size).
+        gt_labels_3d_list (List[Tensor]): 3D Ground truth labels of each
             box, each has shape (num_gt,).
-        centers2d_list (list[Tensor]): Projected 3D centers onto 2D image,
+        centers2d_list (List[Tensor]): Projected 3D centers onto 2D image,
             each has shape (num_gt, 2).
         img_metas (list[dict]): Meta information of each image, e.g.,
             image size, scaling factor, etc.
     """
     bs = len(centers2d_list)
 
     for i in range(bs):
         centers2d = centers2d_list[i].clone()
         img_shape = img_metas[i]['img_shape']
         keep_inds = (centers2d[:, 0] > 0) & \
-            (centers2d[:, 0] < img_shape[1]) & \
-            (centers2d[:, 1] > 0) & \
-            (centers2d[:, 1] < img_shape[0])
+                    (centers2d[:, 0] < img_shape[1]) & \
+                    (centers2d[:, 1] > 0) & \
+                    (centers2d[:, 1] < img_shape[0])
         centers2d_list[i] = centers2d[keep_inds]
         gt_labels_list[i] = gt_labels_list[i][keep_inds]
         gt_bboxes_list[i] = gt_bboxes_list[i][keep_inds]
         gt_bboxes_3d_list[i].tensor = gt_bboxes_3d_list[i].tensor[keep_inds]
         gt_labels_3d_list[i] = gt_labels_3d_list[i][keep_inds]
 
 
-def get_centers2d_target(centers2d, centers, img_shape):
+def get_centers2d_target(centers2d: Tensor, centers: Tensor,
+                         img_shape: tuple) -> Tensor:
     """Function to get target centers2d.
 
     Args:
         centers2d (Tensor): Projected 3D centers onto 2D images.
         centers (Tensor): Centers of 2d gt bboxes.
         img_shape (tuple): Resized image shape.
 
@@ -76,48 +87,51 @@
 
     min_idx = min_idx.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, 2)
     centers2d_target = valid_intersects.gather(dim=1, index=min_idx).squeeze(1)
 
     return centers2d_target
 
 
-def handle_proj_objs(centers2d_list, gt_bboxes_list, img_metas):
+def handle_proj_objs(
+        centers2d_list: List[Tensor], gt_bboxes_list: List[Tensor],
+        img_metas: List[dict]
+) -> Tuple[List[Tensor], List[Tensor], List[Tensor]]:
     """Function to handle projected object centers2d, generate target
     centers2d.
 
     Args:
-        gt_bboxes_list (list[Tensor]): Ground truth bboxes of each image,
+        gt_bboxes_list (List[Tensor]): Ground truth bboxes of each image,
             shape (num_gt, 4).
-        centers2d_list (list[Tensor]): Projected 3D centers onto 2D image,
+        centers2d_list (List[Tensor]): Projected 3D centers onto 2D image,
             shape (num_gt, 2).
-        img_metas (list[dict]): Meta information of each image, e.g.,
+        img_metas (List[dict]): Meta information of each image, e.g.,
             image size, scaling factor, etc.
 
     Returns:
-        tuple[list[Tensor]]: It contains three elements. The first is the
-        target centers2d after handling the truncated objects. The second
-        is the offsets between target centers2d and round int dtype
-        centers2d,and the last is the truncation mask for each object in
-        batch data.
+        Tuple[List[Tensor], List[Tensor], List[Tensor]]: It contains three
+        elements. The first is the target centers2d after handling the
+        truncated objects. The second is the offsets between target centers2d
+        and round int dtype centers2d,and the last is the truncation mask
+        for each object in batch data.
     """
     bs = len(centers2d_list)
     centers2d_target_list = []
     trunc_mask_list = []
     offsets2d_list = []
     # for now, only pad mode that img is padded by right and
     # bottom side is supported.
     for i in range(bs):
         centers2d = centers2d_list[i]
         gt_bbox = gt_bboxes_list[i]
         img_shape = img_metas[i]['img_shape']
         centers2d_target = centers2d.clone()
         inside_inds = (centers2d[:, 0] > 0) & \
-            (centers2d[:, 0] < img_shape[1]) & \
-            (centers2d[:, 1] > 0) & \
-            (centers2d[:, 1] < img_shape[0])
+                      (centers2d[:, 0] < img_shape[1]) & \
+                      (centers2d[:, 1] > 0) & \
+                      (centers2d[:, 1] < img_shape[0])
         outside_inds = ~inside_inds
 
         # if there are outside objects
         if outside_inds.any():
             centers = (gt_bbox[:, :2] + gt_bbox[:, 2:]) / 2
             outside_centers2d = centers2d[outside_inds]
             match_centers = centers[outside_inds]
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/voxel_encoders/pillar_encoder.py` & `mmdet3d-1.1.1/mmdet3d/models/voxel_encoders/pillar_encoder.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Optional, Tuple
+
 import torch
 from mmcv.cnn import build_norm_layer
 from mmcv.ops import DynamicScatter
-from torch import nn
+from torch import Tensor, nn
 
 from mmdet3d.registry import MODELS
 from .utils import PFNLayer, get_paddings_indicator
 
 
 @MODELS.register_module()
 class PillarFeatureNet(nn.Module):
@@ -33,37 +35,38 @@
         mode (str, optional): The mode to gather point features. Options are
             'max' or 'avg'. Defaults to 'max'.
         legacy (bool, optional): Whether to use the new behavior or
             the original behavior. Defaults to True.
     """
 
     def __init__(self,
-                 in_channels=4,
-                 feat_channels=(64, ),
-                 with_distance=False,
-                 with_cluster_center=True,
-                 with_voxel_center=True,
-                 voxel_size=(0.2, 0.2, 4),
-                 point_cloud_range=(0, -40, -3, 70.4, 40, 1),
-                 norm_cfg=dict(type='BN1d', eps=1e-3, momentum=0.01),
-                 mode='max',
-                 legacy=True):
+                 in_channels: Optional[int] = 4,
+                 feat_channels: Optional[tuple] = (64, ),
+                 with_distance: Optional[bool] = False,
+                 with_cluster_center: Optional[bool] = True,
+                 with_voxel_center: Optional[bool] = True,
+                 voxel_size: Optional[Tuple[float]] = (0.2, 0.2, 4),
+                 point_cloud_range: Optional[Tuple[float]] = (0, -40, -3, 70.4,
+                                                              40, 1),
+                 norm_cfg: Optional[dict] = dict(
+                     type='BN1d', eps=1e-3, momentum=0.01),
+                 mode: Optional[str] = 'max',
+                 legacy: Optional[bool] = True):
         super(PillarFeatureNet, self).__init__()
         assert len(feat_channels) > 0
         self.legacy = legacy
         if with_cluster_center:
             in_channels += 3
         if with_voxel_center:
             in_channels += 3
         if with_distance:
             in_channels += 1
         self._with_distance = with_distance
         self._with_cluster_center = with_cluster_center
         self._with_voxel_center = with_voxel_center
-        self.fp16_enabled = False
         # Create PillarFeatureNet layers
         self.in_channels = in_channels
         feat_channels = [in_channels] + list(feat_channels)
         pfn_layers = []
         for i in range(len(feat_channels) - 1):
             in_filters = feat_channels[i]
             out_filters = feat_channels[i + 1]
@@ -85,15 +88,16 @@
         self.vy = voxel_size[1]
         self.vz = voxel_size[2]
         self.x_offset = self.vx / 2 + point_cloud_range[0]
         self.y_offset = self.vy / 2 + point_cloud_range[1]
         self.z_offset = self.vz / 2 + point_cloud_range[2]
         self.point_cloud_range = point_cloud_range
 
-    def forward(self, features, num_points, coors, *args, **kwargs):
+    def forward(self, features: Tensor, num_points: Tensor, coors: Tensor,
+                *args, **kwargs) -> Tensor:
         """Forward function.
 
         Args:
             features (torch.Tensor): Point features or raw points in shape
                 (N, M, C).
             num_points (torch.Tensor): Number of points in each pillar.
             coors (torch.Tensor): Coordinates of each voxel.
@@ -184,36 +188,37 @@
         mode (str, optional): The mode to gather point features. Options are
             'max' or 'avg'. Defaults to 'max'.
         legacy (bool, optional): Whether to use the new behavior or
             the original behavior. Defaults to True.
     """
 
     def __init__(self,
-                 in_channels=4,
-                 feat_channels=(64, ),
-                 with_distance=False,
-                 with_cluster_center=True,
-                 with_voxel_center=True,
-                 voxel_size=(0.2, 0.2, 4),
-                 point_cloud_range=(0, -40, -3, 70.4, 40, 1),
-                 norm_cfg=dict(type='BN1d', eps=1e-3, momentum=0.01),
-                 mode='max',
-                 legacy=True):
+                 in_channels: Optional[int] = 4,
+                 feat_channels: Optional[tuple] = (64, ),
+                 with_distance: Optional[bool] = False,
+                 with_cluster_center: Optional[bool] = True,
+                 with_voxel_center: Optional[bool] = True,
+                 voxel_size: Optional[Tuple[float]] = (0.2, 0.2, 4),
+                 point_cloud_range: Optional[Tuple[float]] = (0, -40, -3, 70.4,
+                                                              40, 1),
+                 norm_cfg: Optional[dict] = dict(
+                     type='BN1d', eps=1e-3, momentum=0.01),
+                 mode: Optional[str] = 'max',
+                 legacy: Optional[bool] = True):
         super(DynamicPillarFeatureNet, self).__init__(
             in_channels,
             feat_channels,
             with_distance,
             with_cluster_center=with_cluster_center,
             with_voxel_center=with_voxel_center,
             voxel_size=voxel_size,
             point_cloud_range=point_cloud_range,
             norm_cfg=norm_cfg,
             mode=mode,
             legacy=legacy)
-        self.fp16_enabled = False
         feat_channels = [self.in_channels] + list(feat_channels)
         pfn_layers = []
         # TODO: currently only support one PFNLayer
 
         for i in range(len(feat_channels) - 1):
             in_filters = feat_channels[i]
             out_filters = feat_channels[i + 1]
@@ -227,15 +232,16 @@
         self.num_pfn = len(pfn_layers)
         self.pfn_layers = nn.ModuleList(pfn_layers)
         self.pfn_scatter = DynamicScatter(voxel_size, point_cloud_range,
                                           (mode != 'max'))
         self.cluster_scatter = DynamicScatter(
             voxel_size, point_cloud_range, average_points=True)
 
-    def map_voxel_center_to_point(self, pts_coors, voxel_mean, voxel_coors):
+    def map_voxel_center_to_point(self, pts_coors: Tensor, voxel_mean: Tensor,
+                                  voxel_coors: Tensor) -> Tensor:
         """Map the centers of voxels to its corresponding points.
 
         Args:
             pts_coors (torch.Tensor): The coordinates of each points, shape
                 (M, 3), where M is the number of points.
             voxel_mean (torch.Tensor): The mean or aggregated features of a
                 voxel, shape (N, C), where N is the number of voxels.
@@ -266,15 +272,15 @@
         # Step 2: get voxel mean for each point
         voxel_index = (
             pts_coors[:, 0] * canvas_y * canvas_x +
             pts_coors[:, 2] * canvas_x + pts_coors[:, 3])
         center_per_point = canvas[:, voxel_index.long()].t()
         return center_per_point
 
-    def forward(self, features, coors):
+    def forward(self, features: Tensor, coors: Tensor) -> Tensor:
         """Forward function.
 
         Args:
             features (torch.Tensor): Point features or raw points in shape
                 (N, M, C).
             coors (torch.Tensor): Coordinates of each voxel
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/voxel_encoders/utils.py` & `mmdet3d-1.1.1/mmdet3d/models/voxel_encoders/utils.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,15 +1,19 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Optional
+
 import torch
 from mmcv.cnn import build_norm_layer
-from torch import nn
+from torch import Tensor, nn
 from torch.nn import functional as F
 
 
-def get_paddings_indicator(actual_num, max_num, axis=0):
+def get_paddings_indicator(actual_num: Tensor,
+                           max_num: Tensor,
+                           axis: int = 0) -> Tensor:
     """Create boolean mask by actually number of a padded tensor.
 
     Args:
         actual_num (torch.Tensor): Actual number of points in each voxel.
         max_num (int): Max number of points in each voxel
 
     Returns:
@@ -42,29 +46,29 @@
         max_out (bool): Whether aggregate the features of points inside
             each voxel and only return voxel features.
         cat_max (bool): Whether concatenate the aggregated features
             and pointwise features.
     """
 
     def __init__(self,
-                 in_channels,
-                 out_channels,
-                 norm_cfg=dict(type='BN1d', eps=1e-3, momentum=0.01),
-                 max_out=True,
-                 cat_max=True):
+                 in_channels: int,
+                 out_channels: int,
+                 norm_cfg: Optional[dict] = dict(
+                     type='BN1d', eps=1e-3, momentum=0.01),
+                 max_out: Optional[bool] = True,
+                 cat_max: Optional[bool] = True):
         super(VFELayer, self).__init__()
-        self.fp16_enabled = False
         self.cat_max = cat_max
         self.max_out = max_out
         # self.units = int(out_channels / 2)
 
         self.norm = build_norm_layer(norm_cfg, out_channels)[1]
         self.linear = nn.Linear(in_channels, out_channels, bias=False)
 
-    def forward(self, inputs):
+    def forward(self, inputs: Tensor) -> Tensor:
         """Forward function.
 
         Args:
             inputs (torch.Tensor): Voxels features of shape (N, M, C).
                 N is the number of voxels, M is the number of points in
                 voxels, C is the number of channels of point features.
 
@@ -116,35 +120,38 @@
         last_layer (bool, optional): If last_layer, there is no
             concatenation of features. Defaults to False.
         mode (str, optional): Pooling model to gather features inside voxels.
             Defaults to 'max'.
     """
 
     def __init__(self,
-                 in_channels,
-                 out_channels,
-                 norm_cfg=dict(type='BN1d', eps=1e-3, momentum=0.01),
-                 last_layer=False,
-                 mode='max'):
+                 in_channels: int,
+                 out_channels: int,
+                 norm_cfg: Optional[dict] = dict(
+                     type='BN1d', eps=1e-3, momentum=0.01),
+                 last_layer: Optional[bool] = False,
+                 mode: Optional[str] = 'max'):
 
         super().__init__()
-        self.fp16_enabled = False
         self.name = 'PFNLayer'
         self.last_vfe = last_layer
         if not self.last_vfe:
             out_channels = out_channels // 2
         self.units = out_channels
 
         self.norm = build_norm_layer(norm_cfg, self.units)[1]
         self.linear = nn.Linear(in_channels, self.units, bias=False)
 
         assert mode in ['max', 'avg']
         self.mode = mode
 
-    def forward(self, inputs, num_voxels=None, aligned_distance=None):
+    def forward(self,
+                inputs: Tensor,
+                num_voxels: Optional[Tensor] = None,
+                aligned_distance: Optional[Tensor] = None) -> Tensor:
         """Forward function.
 
         Args:
             inputs (torch.Tensor): Pillar/Voxel inputs with shape (N, M, C).
                 N is the number of voxels, M is the number of points in
                 voxels, C is the number of channels of point features.
             num_voxels (torch.Tensor, optional): Number of points in each
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/models/voxel_encoders/voxel_encoder.py` & `mmdet3d-1.1.1/mmdet3d/models/voxel_encoders/voxel_encoder.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,8 +1,10 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Optional, Sequence, Tuple
+
 import torch
 from mmcv.cnn import build_norm_layer
 from mmcv.ops import DynamicScatter
 from torch import Tensor, nn
 
 from mmdet3d.registry import MODELS
 from .utils import VFELayer, get_paddings_indicator
@@ -17,15 +19,14 @@
     Args:
         num_features (int, optional): Number of features to use. Default: 4.
     """
 
     def __init__(self, num_features: int = 4) -> None:
         super(HardSimpleVFE, self).__init__()
         self.num_features = num_features
-        self.fp16_enabled = False
 
     def forward(self, features: Tensor, num_points: Tensor, coors: Tensor,
                 *args, **kwargs) -> Tensor:
         """Forward function.
 
         Args:
             features (torch.Tensor): Point features in shape
@@ -52,22 +53,22 @@
 
     Args:
         voxel_size (tupe[float]): Size of a single voxel
         point_cloud_range (tuple[float]): Range of the point cloud and voxels
     """
 
     def __init__(self,
-                 voxel_size=(0.2, 0.2, 4),
-                 point_cloud_range=(0, -40, -3, 70.4, 40, 1)):
+                 voxel_size: Tuple[float] = (0.2, 0.2, 4),
+                 point_cloud_range: Tuple[float] = (0, -40, -3, 70.4, 40, 1)):
         super(DynamicSimpleVFE, self).__init__()
         self.scatter = DynamicScatter(voxel_size, point_cloud_range, True)
-        self.fp16_enabled = False
 
     @torch.no_grad()
-    def forward(self, features, coors, *args, **kwargs):
+    def forward(self, features: Tensor, coors: Tensor, *args,
+                **kwargs) -> Tensor:
         """Forward function.
 
         Args:
             features (torch.Tensor): Point features in shape
                 (N, 3(4)). N is the number of points.
             coors (torch.Tensor): Coordinates of voxels.
 
@@ -110,50 +111,48 @@
         fusion_layer (dict, optional): The config dict of fusion
             layer used in multi-modal detectors. Defaults to None.
         return_point_feats (bool, optional): Whether to return the features
             of each points. Defaults to False.
     """
 
     def __init__(self,
-                 in_channels=4,
-                 feat_channels=[],
-                 with_distance=False,
-                 with_cluster_center=False,
-                 with_voxel_center=False,
-                 voxel_size=(0.2, 0.2, 4),
-                 point_cloud_range=(0, -40, -3, 70.4, 40, 1),
-                 norm_cfg=dict(type='BN1d', eps=1e-3, momentum=0.01),
-                 mode='max',
-                 fusion_layer=None,
-                 return_point_feats=False):
+                 in_channels: int = 4,
+                 feat_channels: list = [],
+                 with_distance: bool = False,
+                 with_cluster_center: bool = False,
+                 with_voxel_center: bool = False,
+                 voxel_size: Tuple[float] = (0.2, 0.2, 4),
+                 point_cloud_range: Tuple[float] = (0, -40, -3, 70.4, 40, 1),
+                 norm_cfg: dict = dict(type='BN1d', eps=1e-3, momentum=0.01),
+                 mode: str = 'max',
+                 fusion_layer: dict = None,
+                 return_point_feats: bool = False):
         super(DynamicVFE, self).__init__()
         assert mode in ['avg', 'max']
         assert len(feat_channels) > 0
         if with_cluster_center:
             in_channels += 3
         if with_voxel_center:
             in_channels += 3
         if with_distance:
             in_channels += 1
         self.in_channels = in_channels
         self._with_distance = with_distance
         self._with_cluster_center = with_cluster_center
         self._with_voxel_center = with_voxel_center
         self.return_point_feats = return_point_feats
-        self.fp16_enabled = False
 
         # Need pillar (voxel) size and x/y offset in order to calculate offset
         self.vx = voxel_size[0]
         self.vy = voxel_size[1]
         self.vz = voxel_size[2]
         self.x_offset = self.vx / 2 + point_cloud_range[0]
         self.y_offset = self.vy / 2 + point_cloud_range[1]
         self.z_offset = self.vz / 2 + point_cloud_range[2]
         self.point_cloud_range = point_cloud_range
-        self.scatter = DynamicScatter(voxel_size, point_cloud_range, True)
 
         feat_channels = [self.in_channels] + list(feat_channels)
         vfe_layers = []
         for i in range(len(feat_channels) - 1):
             in_filters = feat_channels[i]
             out_filters = feat_channels[i + 1]
             if i > 0:
@@ -169,15 +168,16 @@
                                           (mode != 'max'))
         self.cluster_scatter = DynamicScatter(
             voxel_size, point_cloud_range, average_points=True)
         self.fusion_layer = None
         if fusion_layer is not None:
             self.fusion_layer = MODELS.build(fusion_layer)
 
-    def map_voxel_center_to_point(self, pts_coors, voxel_mean, voxel_coors):
+    def map_voxel_center_to_point(self, pts_coors: Tensor, voxel_mean: Tensor,
+                                  voxel_coors: Tensor) -> Tensor:
         """Map voxel features to its corresponding points.
 
         Args:
             pts_coors (torch.Tensor): Voxel coordinate of each point.
             voxel_mean (torch.Tensor): Voxel features to be mapped.
             voxel_coors (torch.Tensor): Coordinates of valid voxels
 
@@ -212,21 +212,21 @@
             pts_coors[:, 1] * canvas_y * canvas_x +
             pts_coors[:, 2] * canvas_x + pts_coors[:, 3])
         voxel_inds = canvas[voxel_index.long()]
         center_per_point = voxel_mean[voxel_inds, ...]
         return center_per_point
 
     def forward(self,
-                features,
-                coors,
-                points=None,
-                img_feats=None,
-                img_metas=None,
+                features: Tensor,
+                coors: Tensor,
+                points: Optional[Sequence[Tensor]] = None,
+                img_feats: Optional[Sequence[Tensor]] = None,
+                img_metas: Optional[dict] = None,
                 *args,
-                **kwargs):
+                **kwargs) -> tuple:
         """Forward functions.
 
         Args:
             features (torch.Tensor): Features of voxels, shape is NxC.
             coors (torch.Tensor): Coordinates of voxels, shape is  Nx(1+NDim).
             points (list[torch.Tensor], optional): Raw points used to guide the
                 multi-modality fusion. Defaults to None.
@@ -311,49 +311,47 @@
         fusion_layer (dict, optional): The config dict of fusion layer
             used in multi-modal detectors. Defaults to None.
         return_point_feats (bool, optional): Whether to return the
             features of each points. Defaults to False.
     """
 
     def __init__(self,
-                 in_channels=4,
-                 feat_channels=[],
-                 with_distance=False,
-                 with_cluster_center=False,
-                 with_voxel_center=False,
-                 voxel_size=(0.2, 0.2, 4),
-                 point_cloud_range=(0, -40, -3, 70.4, 40, 1),
-                 norm_cfg=dict(type='BN1d', eps=1e-3, momentum=0.01),
-                 mode='max',
-                 fusion_layer=None,
-                 return_point_feats=False):
+                 in_channels: int = 4,
+                 feat_channels: list = [],
+                 with_distance: bool = False,
+                 with_cluster_center: bool = False,
+                 with_voxel_center: bool = False,
+                 voxel_size: Tuple[float] = (0.2, 0.2, 4),
+                 point_cloud_range: Tuple[float] = (0, -40, -3, 70.4, 40, 1),
+                 norm_cfg: dict = dict(type='BN1d', eps=1e-3, momentum=0.01),
+                 mode: str = 'max',
+                 fusion_layer: dict = None,
+                 return_point_feats: bool = False):
         super(HardVFE, self).__init__()
         assert len(feat_channels) > 0
         if with_cluster_center:
             in_channels += 3
         if with_voxel_center:
             in_channels += 3
         if with_distance:
             in_channels += 1
         self.in_channels = in_channels
         self._with_distance = with_distance
         self._with_cluster_center = with_cluster_center
         self._with_voxel_center = with_voxel_center
         self.return_point_feats = return_point_feats
-        self.fp16_enabled = False
 
         # Need pillar (voxel) size and x/y offset to calculate pillar offset
         self.vx = voxel_size[0]
         self.vy = voxel_size[1]
         self.vz = voxel_size[2]
         self.x_offset = self.vx / 2 + point_cloud_range[0]
         self.y_offset = self.vy / 2 + point_cloud_range[1]
         self.z_offset = self.vz / 2 + point_cloud_range[2]
         self.point_cloud_range = point_cloud_range
-        self.scatter = DynamicScatter(voxel_size, point_cloud_range, True)
 
         feat_channels = [self.in_channels] + list(feat_channels)
         vfe_layers = []
         for i in range(len(feat_channels) - 1):
             in_filters = feat_channels[i]
             out_filters = feat_channels[i + 1]
             if i > 0:
@@ -379,21 +377,21 @@
         self.num_vfe = len(vfe_layers)
 
         self.fusion_layer = None
         if fusion_layer is not None:
             self.fusion_layer = MODELS.build(fusion_layer)
 
     def forward(self,
-                features,
-                num_points,
-                coors,
-                img_feats=None,
-                img_metas=None,
+                features: Tensor,
+                num_points: Tensor,
+                coors: Tensor,
+                img_feats: Optional[Sequence[Tensor]] = None,
+                img_metas: Optional[dict] = None,
                 *args,
-                **kwargs):
+                **kwargs) -> tuple:
         """Forward functions.
 
         Args:
             features (torch.Tensor): Features of voxels, shape is MxNxC.
             num_points (torch.Tensor): Number of points in each voxel.
             coors (torch.Tensor): Coordinates of voxels, shape is Mx(1+NDim).
             img_feats (list[torch.Tensor], optional): Image features used for
@@ -448,16 +446,18 @@
 
         if (self.fusion_layer is not None and img_feats is not None):
             voxel_feats = self.fusion_with_mask(features, mask, voxel_feats,
                                                 coors, img_feats, img_metas)
 
         return voxel_feats
 
-    def fusion_with_mask(self, features, mask, voxel_feats, coors, img_feats,
-                         img_metas):
+    def fusion_with_mask(self, features: Tensor, mask: Tensor,
+                         voxel_feats: Tensor, coors: Tensor,
+                         img_feats: Sequence[Tensor],
+                         img_metas: Sequence[dict]) -> Tensor:
         """Fuse image and point features with mask.
 
         Args:
             features (torch.Tensor): Features of voxel, usually it is the
                 values of points in voxels.
             mask (torch.Tensor): Mask indicates valid features in each voxel.
             voxel_feats (torch.Tensor): Features of voxels.
@@ -482,7 +482,159 @@
         voxel_canvas = voxel_feats.new_zeros(
             size=(voxel_feats.size(0), voxel_feats.size(1),
                   point_feats.size(-1)))
         voxel_canvas[mask] = point_feats
         out = torch.max(voxel_canvas, dim=1)[0]
 
         return out
+
+
+@MODELS.register_module()
+class SegVFE(nn.Module):
+    """Voxel feature encoder used in segmentation task.
+
+    It encodes features of voxels and their points. It could also fuse
+    image feature into voxel features in a point-wise manner.
+    The number of points inside the voxel varies.
+
+    Args:
+        in_channels (int): Input channels of VFE. Defaults to 6.
+        feat_channels (list(int)): Channels of features in VFE.
+        with_voxel_center (bool): Whether to use the distance
+            to center of voxel for each points inside a voxel.
+            Defaults to False.
+        voxel_size (tuple[float]): Size of a single voxel (rho, phi, z).
+            Defaults to None.
+        grid_shape (tuple[float]): The grid shape of voxelization.
+            Defaults to (480, 360, 32).
+        point_cloud_range (tuple[float]): The range of points or voxels.
+            Defaults to (0, -3.14159265359, -4, 50, 3.14159265359, 2).
+        norm_cfg (dict): Config dict of normalization layers.
+        mode (str): The mode when pooling features of points
+            inside a voxel. Available options include 'max' and 'avg'.
+            Defaults to 'max'.
+        with_pre_norm (bool): Whether to use the norm layer before
+            input vfe layer.
+        feat_compression (int, optional): The voxel feature compression
+            channels, Defaults to None
+        return_point_feats (bool): Whether to return the features
+            of each points. Defaults to False.
+    """
+
+    def __init__(self,
+                 in_channels: int = 6,
+                 feat_channels: Sequence[int] = [],
+                 with_voxel_center: bool = False,
+                 voxel_size: Optional[Sequence[float]] = None,
+                 grid_shape: Sequence[float] = (480, 360, 32),
+                 point_cloud_range: Sequence[float] = (0, -3.14159265359, -4,
+                                                       50, 3.14159265359, 2),
+                 norm_cfg: dict = dict(type='BN1d', eps=1e-5, momentum=0.1),
+                 mode: bool = 'max',
+                 with_pre_norm: bool = True,
+                 feat_compression: Optional[int] = None,
+                 return_point_feats: bool = False) -> None:
+        super(SegVFE, self).__init__()
+        assert mode in ['avg', 'max']
+        assert len(feat_channels) > 0
+        assert not (voxel_size and grid_shape), \
+            'voxel_size and grid_shape cannot be setting at the same time'
+        if with_voxel_center:
+            in_channels += 3
+        self.in_channels = in_channels
+        self._with_voxel_center = with_voxel_center
+        self.return_point_feats = return_point_feats
+
+        self.point_cloud_range = point_cloud_range
+        point_cloud_range = torch.tensor(
+            point_cloud_range, dtype=torch.float32)
+        if voxel_size:
+            self.voxel_size = voxel_size
+            voxel_size = torch.tensor(voxel_size, dtype=torch.float32)
+            grid_shape = (point_cloud_range[3:] -
+                          point_cloud_range[:3]) / voxel_size
+            grid_shape = torch.round(grid_shape).long().tolist()
+            self.grid_shape = grid_shape
+        elif grid_shape:
+            grid_shape = torch.tensor(grid_shape, dtype=torch.float32)
+            voxel_size = (point_cloud_range[3:] - point_cloud_range[:3]) / (
+                grid_shape - 1)
+            voxel_size = voxel_size.tolist()
+            self.voxel_size = voxel_size
+        else:
+            raise ValueError('must assign a value to voxel_size or grid_shape')
+
+        # Need pillar (voxel) size and x/y offset in order to calculate offset
+        self.vx = self.voxel_size[0]
+        self.vy = self.voxel_size[1]
+        self.vz = self.voxel_size[2]
+        self.x_offset = self.vx / 2 + point_cloud_range[0]
+        self.y_offset = self.vy / 2 + point_cloud_range[1]
+        self.z_offset = self.vz / 2 + point_cloud_range[2]
+
+        feat_channels = [self.in_channels] + list(feat_channels)
+        if with_pre_norm:
+            self.pre_norm = build_norm_layer(norm_cfg, self.in_channels)[1]
+        vfe_layers = []
+        for i in range(len(feat_channels) - 1):
+            in_filters = feat_channels[i]
+            out_filters = feat_channels[i + 1]
+            norm_layer = build_norm_layer(norm_cfg, out_filters)[1]
+            if i == len(feat_channels) - 2:
+                vfe_layers.append(nn.Linear(in_filters, out_filters))
+            else:
+                vfe_layers.append(
+                    nn.Sequential(
+                        nn.Linear(in_filters, out_filters), norm_layer,
+                        nn.ReLU(inplace=True)))
+        self.vfe_layers = nn.ModuleList(vfe_layers)
+        self.vfe_scatter = DynamicScatter(self.voxel_size,
+                                          self.point_cloud_range,
+                                          (mode != 'max'))
+        self.compression_layers = None
+        if feat_compression is not None:
+            self.compression_layers = nn.Sequential(
+                nn.Linear(feat_channels[-1], feat_compression), nn.ReLU())
+
+    def forward(self, features: Tensor, coors: Tensor, *args,
+                **kwargs) -> Tuple[Tensor]:
+        """Forward functions.
+
+        Args:
+            features (Tensor): Features of voxels, shape is NxC.
+            coors (Tensor): Coordinates of voxels, shape is  Nx(1+NDim).
+
+        Returns:
+            tuple: If `return_point_feats` is False, returns voxel features and
+                its coordinates. If `return_point_feats` is True, returns
+                feature of each points inside voxels additionally.
+        """
+        features_ls = [features]
+
+        # Find distance of x, y, and z from voxel center
+        if self._with_voxel_center:
+            f_center = features.new_zeros(size=(features.size(0), 3))
+            f_center[:, 0] = features[:, 0] - (
+                coors[:, 1].type_as(features) * self.vx + self.x_offset)
+            f_center[:, 1] = features[:, 1] - (
+                coors[:, 2].type_as(features) * self.vy + self.y_offset)
+            f_center[:, 2] = features[:, 2] - (
+                coors[:, 3].type_as(features) * self.vz + self.z_offset)
+            features_ls.append(f_center)
+
+        # Combine together feature decorations
+        features = torch.cat(features_ls[::-1], dim=-1)
+        if self.pre_norm is not None:
+            features = self.pre_norm(features)
+
+        point_feats = []
+        for vfe in self.vfe_layers:
+            features = vfe(features)
+            point_feats.append(features)
+        voxel_feats, voxel_coors = self.vfe_scatter(features, coors)
+
+        if self.compression_layers is not None:
+            voxel_feats = self.compression_layers(voxel_feats)
+
+        if self.return_point_feats:
+            return voxel_feats, voxel_coors, point_feats
+        return voxel_feats, voxel_coors
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/structures/__init__.py` & `mmdet3d-1.1.1/mmdet3d/structures/__init__.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/structures/bbox_3d/__init__.py` & `mmdet3d-1.1.1/mmdet3d/structures/bbox_3d/__init__.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/structures/bbox_3d/base_box3d.py` & `mmdet3d-1.1.1/mmdet3d/structures/bbox_3d/base_box3d.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,61 +1,73 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import warnings
 from abc import abstractmethod
+from typing import Iterator, Optional, Sequence, Tuple, Union
 
 import numpy as np
 import torch
 from mmcv.ops import box_iou_rotated, points_in_boxes_all, points_in_boxes_part
+from torch import Tensor
 
+from mmdet3d.structures.points import BasePoints
 from .utils import limit_period
 
 
-class BaseInstance3DBoxes(object):
+class BaseInstance3DBoxes:
     """Base class for 3D Boxes.
 
     Note:
-        The box is bottom centered, i.e. the relative position of origin in
-        the box is (0.5, 0.5, 0).
+        The box is bottom centered, i.e. the relative position of origin in the
+        box is (0.5, 0.5, 0).
 
     Args:
-        tensor (torch.Tensor | np.ndarray | list): a N x box_dim matrix.
-        box_dim (int): Number of the dimension of a box.
-            Each row is (x, y, z, x_size, y_size, z_size, yaw).
-            Defaults to 7.
-        with_yaw (bool): Whether the box is with yaw rotation.
-            If False, the value of yaw will be set to 0 as minmax boxes.
-            Defaults to True.
-        origin (tuple[float], optional): Relative position of the box origin.
+        tensor (Tensor or np.ndarray or Sequence[Sequence[float]]): The boxes
+            data with shape (N, box_dim).
+        box_dim (int): Number of the dimension of a box. Each row is
+            (x, y, z, x_size, y_size, z_size, yaw). Defaults to 7.
+        with_yaw (bool): Whether the box is with yaw rotation. If False, the
+            value of yaw will be set to 0 as minmax boxes. Defaults to True.
+        origin (Tuple[float]): Relative position of the box origin.
             Defaults to (0.5, 0.5, 0). This will guide the box be converted to
             (0.5, 0.5, 0) mode.
 
     Attributes:
-        tensor (torch.Tensor): Float matrix of N x box_dim.
-        box_dim (int): Integer indicating the dimension of a box.
-            Each row is (x, y, z, x_size, y_size, z_size, yaw, ...).
+        tensor (Tensor): Float matrix with shape (N, box_dim).
+        box_dim (int): Integer indicating the dimension of a box. Each row is
+            (x, y, z, x_size, y_size, z_size, yaw, ...).
         with_yaw (bool): If True, the value of yaw will be set to 0 as minmax
             boxes.
     """
 
-    def __init__(self, tensor, box_dim=7, with_yaw=True, origin=(0.5, 0.5, 0)):
-        if isinstance(tensor, torch.Tensor):
+    YAW_AXIS: int = 0
+
+    def __init__(
+        self,
+        tensor: Union[Tensor, np.ndarray, Sequence[Sequence[float]]],
+        box_dim: int = 7,
+        with_yaw: bool = True,
+        origin: Tuple[float, float, float] = (0.5, 0.5, 0)
+    ) -> None:
+        if isinstance(tensor, Tensor):
             device = tensor.device
         else:
             device = torch.device('cpu')
         tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)
         if tensor.numel() == 0:
-            # Use reshape, so we don't end up creating a new tensor that
-            # does not depend on the inputs (and consequently confuses jit)
-            tensor = tensor.reshape((0, box_dim)).to(
-                dtype=torch.float32, device=device)
-        assert tensor.dim() == 2 and tensor.size(-1) == box_dim, tensor.size()
+            # Use reshape, so we don't end up creating a new tensor that does
+            # not depend on the inputs (and consequently confuses jit)
+            tensor = tensor.reshape((-1, box_dim))
+        assert tensor.dim() == 2 and tensor.size(-1) == box_dim, \
+            ('The box dimension must be 2 and the length of the last '
+             f'dimension must be {box_dim}, but got boxes with shape '
+             f'{tensor.shape}.')
 
         if tensor.shape[-1] == 6:
-            # If the dimension of boxes is 6, we expand box_dim by padding
-            # 0 as a fake yaw and set with_yaw to False.
+            # If the dimension of boxes is 6, we expand box_dim by padding 0 as
+            # a fake yaw and set with_yaw to False
             assert box_dim == 6
             fake_rot = tensor.new_zeros(tensor.shape[0], 1)
             tensor = torch.cat((tensor, fake_rot), dim=-1)
             self.box_dim = box_dim + 1
             self.with_yaw = False
         else:
             self.box_dim = box_dim
@@ -64,90 +76,95 @@
 
         if origin != (0.5, 0.5, 0):
             dst = self.tensor.new_tensor((0.5, 0.5, 0))
             src = self.tensor.new_tensor(origin)
             self.tensor[:, :3] += self.tensor[:, 3:6] * (dst - src)
 
     @property
-    def volume(self):
-        """torch.Tensor: A vector with volume of each box."""
+    def shape(self) -> torch.Size:
+        """torch.Size: Shape of boxes."""
+        return self.tensor.shape
+
+    @property
+    def volume(self) -> Tensor:
+        """Tensor: A vector with volume of each box in shape (N, )."""
         return self.tensor[:, 3] * self.tensor[:, 4] * self.tensor[:, 5]
 
     @property
-    def dims(self):
-        """torch.Tensor: Size dimensions of each box in shape (N, 3)."""
+    def dims(self) -> Tensor:
+        """Tensor: Size dimensions of each box in shape (N, 3)."""
         return self.tensor[:, 3:6]
 
     @property
-    def yaw(self):
-        """torch.Tensor: A vector with yaw of each box in shape (N, )."""
+    def yaw(self) -> Tensor:
+        """Tensor: A vector with yaw of each box in shape (N, )."""
         return self.tensor[:, 6]
 
     @property
-    def height(self):
-        """torch.Tensor: A vector with height of each box in shape (N, )."""
+    def height(self) -> Tensor:
+        """Tensor: A vector with height of each box in shape (N, )."""
         return self.tensor[:, 5]
 
     @property
-    def top_height(self):
-        """torch.Tensor:
-            A vector with the top height of each box in shape (N, )."""
+    def top_height(self) -> Tensor:
+        """Tensor: A vector with top height of each box in shape (N, )."""
         return self.bottom_height + self.height
 
     @property
-    def bottom_height(self):
-        """torch.Tensor:
-            A vector with bottom's height of each box in shape (N, )."""
+    def bottom_height(self) -> Tensor:
+        """Tensor: A vector with bottom height of each box in shape (N, )."""
         return self.tensor[:, 2]
 
     @property
-    def center(self):
+    def center(self) -> Tensor:
         """Calculate the center of all the boxes.
 
         Note:
-            In MMDetection3D's convention, the bottom center is
-            usually taken as the default center.
+            In MMDetection3D's convention, the bottom center is usually taken
+            as the default center.
 
-            The relative position of the centers in different kinds of
-            boxes are different, e.g., the relative center of a boxes is
-            (0.5, 1.0, 0.5) in camera and (0.5, 0.5, 0) in lidar.
-            It is recommended to use ``bottom_center`` or ``gravity_center``
-            for clearer usage.
+            The relative position of the centers in different kinds of boxes
+            are different, e.g., the relative center of a boxes is
+            (0.5, 1.0, 0.5) in camera and (0.5, 0.5, 0) in lidar. It is
+            recommended to use ``bottom_center`` or ``gravity_center`` for
+            clearer usage.
 
         Returns:
-            torch.Tensor: A tensor with center of each box in shape (N, 3).
+            Tensor: A tensor with center of each box in shape (N, 3).
         """
         return self.bottom_center
 
     @property
-    def bottom_center(self):
-        """torch.Tensor: A tensor with center of each box in shape (N, 3)."""
+    def bottom_center(self) -> Tensor:
+        """Tensor: A tensor with center of each box in shape (N, 3)."""
         return self.tensor[:, :3]
 
     @property
-    def gravity_center(self):
-        """torch.Tensor: A tensor with center of each box in shape (N, 3)."""
-        pass
+    def gravity_center(self) -> Tensor:
+        """Tensor: A tensor with center of each box in shape (N, 3)."""
+        bottom_center = self.bottom_center
+        gravity_center = torch.zeros_like(bottom_center)
+        gravity_center[:, :2] = bottom_center[:, :2]
+        gravity_center[:, 2] = bottom_center[:, 2] + self.tensor[:, 5] * 0.5
+        return gravity_center
 
     @property
-    def corners(self):
-        """torch.Tensor:
-            a tensor with 8 corners of each box in shape (N, 8, 3)."""
+    def corners(self) -> Tensor:
+        """Tensor: A tensor with 8 corners of each box in shape (N, 8, 3)."""
         pass
 
     @property
-    def bev(self):
-        """torch.Tensor: 2D BEV box of each box with rotation
-            in XYWHR format, in shape (N, 5)."""
+    def bev(self) -> Tensor:
+        """Tensor: 2D BEV box of each box with rotation in XYWHR format, in
+        shape (N, 5)."""
         return self.tensor[:, [0, 1, 3, 4, 6]]
 
     @property
-    def nearest_bev(self):
-        """torch.Tensor: A tensor of 2D BEV box of each box
-            without rotation."""
+    def nearest_bev(self) -> Tensor:
+        """Tensor: A tensor of 2D BEV box of each box without rotation."""
         # Obtain BEV boxes with rotation in XYWHR format
         bev_rotated_boxes = self.bev
         # convert the rotation to a valid range
         rotations = bev_rotated_boxes[:, -1]
         normed_rotations = torch.abs(limit_period(rotations, 0.5, np.pi))
 
         # find the center of boxes
@@ -157,305 +174,384 @@
                                   bev_rotated_boxes[:, :4])
 
         centers = bboxes_xywh[:, :2]
         dims = bboxes_xywh[:, 2:]
         bev_boxes = torch.cat([centers - dims / 2, centers + dims / 2], dim=-1)
         return bev_boxes
 
-    def in_range_bev(self, box_range):
+    def in_range_bev(
+            self, box_range: Union[Tensor, np.ndarray,
+                                   Sequence[float]]) -> Tensor:
         """Check whether the boxes are in the given range.
 
         Args:
-            box_range (list | torch.Tensor): the range of box
-                (x_min, y_min, x_max, y_max)
+            box_range (Tensor or np.ndarray or Sequence[float]): The range of
+                box in order of (x_min, y_min, x_max, y_max).
 
         Note:
-            The original implementation of SECOND checks whether boxes in
-            a range by checking whether the points are in a convex
-            polygon, we reduce the burden for simpler cases.
+            The original implementation of SECOND checks whether boxes in a
+            range by checking whether the points are in a convex polygon, we
+            reduce the burden for simpler cases.
 
         Returns:
-            torch.Tensor: Whether each box is inside the reference range.
+            Tensor: A binary vector indicating whether each box is inside the
+            reference range.
         """
         in_range_flags = ((self.bev[:, 0] > box_range[0])
                           & (self.bev[:, 1] > box_range[1])
                           & (self.bev[:, 0] < box_range[2])
                           & (self.bev[:, 1] < box_range[3]))
         return in_range_flags
 
     @abstractmethod
-    def rotate(self, angle, points=None):
+    def rotate(
+        self,
+        angle: Union[Tensor, np.ndarray, float],
+        points: Optional[Union[Tensor, np.ndarray, BasePoints]] = None
+    ) -> Union[Tuple[Tensor, Tensor], Tuple[np.ndarray, np.ndarray], Tuple[
+            BasePoints, Tensor], None]:
         """Rotate boxes with points (optional) with the given angle or rotation
         matrix.
 
         Args:
-            angle (float | torch.Tensor | np.ndarray):
-                Rotation angle or rotation matrix.
-            points (torch.Tensor | numpy.ndarray |
-                :obj:`BasePoints`, optional):
+            angle (Tensor or np.ndarray or float): Rotation angle or rotation
+                matrix.
+            points (Tensor or np.ndarray or :obj:`BasePoints`, optional):
                 Points to rotate. Defaults to None.
+
+        Returns:
+            tuple or None: When ``points`` is None, the function returns None,
+            otherwise it returns the rotated points and the rotation matrix
+            ``rot_mat_T``.
         """
         pass
 
     @abstractmethod
-    def flip(self, bev_direction='horizontal'):
+    def flip(
+        self,
+        bev_direction: str = 'horizontal',
+        points: Optional[Union[Tensor, np.ndarray, BasePoints]] = None
+    ) -> Union[Tensor, np.ndarray, BasePoints, None]:
         """Flip the boxes in BEV along given BEV direction.
 
         Args:
-            bev_direction (str, optional): Direction by which to flip.
-                Can be chosen from 'horizontal' and 'vertical'.
-                Defaults to 'horizontal'.
+            bev_direction (str): Direction by which to flip. Can be chosen from
+                'horizontal' and 'vertical'. Defaults to 'horizontal'.
+            points (Tensor or np.ndarray or :obj:`BasePoints`, optional):
+                Points to flip. Defaults to None.
+
+        Returns:
+            Tensor or np.ndarray or :obj:`BasePoints` or None: When ``points``
+            is None, the function returns None, otherwise it returns the
+            flipped points.
         """
         pass
 
-    def translate(self, trans_vector):
+    def translate(self, trans_vector: Union[Tensor, np.ndarray]) -> None:
         """Translate boxes with the given translation vector.
 
         Args:
-            trans_vector (torch.Tensor): Translation vector of size (1, 3).
+            trans_vector (Tensor or np.ndarray): Translation vector of size
+                1x3.
         """
-        if not isinstance(trans_vector, torch.Tensor):
+        if not isinstance(trans_vector, Tensor):
             trans_vector = self.tensor.new_tensor(trans_vector)
         self.tensor[:, :3] += trans_vector
 
-    def in_range_3d(self, box_range):
+    def in_range_3d(
+            self, box_range: Union[Tensor, np.ndarray,
+                                   Sequence[float]]) -> Tensor:
         """Check whether the boxes are in the given range.
 
         Args:
-            box_range (list | torch.Tensor): The range of box
-                (x_min, y_min, z_min, x_max, y_max, z_max)
+            box_range (Tensor or np.ndarray or Sequence[float]): The range of
+                box (x_min, y_min, z_min, x_max, y_max, z_max).
 
         Note:
-            In the original implementation of SECOND, checking whether
-            a box in the range checks whether the points are in a convex
-            polygon, we try to reduce the burden for simpler cases.
+            In the original implementation of SECOND, checking whether a box in
+            the range checks whether the points are in a convex polygon, we try
+            to reduce the burden for simpler cases.
 
         Returns:
-            torch.Tensor: A binary vector indicating whether each box is
-                inside the reference range.
+            Tensor: A binary vector indicating whether each point is inside the
+            reference range.
         """
         in_range_flags = ((self.tensor[:, 0] > box_range[0])
                           & (self.tensor[:, 1] > box_range[1])
                           & (self.tensor[:, 2] > box_range[2])
                           & (self.tensor[:, 0] < box_range[3])
                           & (self.tensor[:, 1] < box_range[4])
                           & (self.tensor[:, 2] < box_range[5]))
         return in_range_flags
 
     @abstractmethod
-    def convert_to(self, dst, rt_mat=None):
+    def convert_to(self,
+                   dst: int,
+                   rt_mat: Optional[Union[Tensor, np.ndarray]] = None,
+                   correct_yaw: bool = False) -> 'BaseInstance3DBoxes':
         """Convert self to ``dst`` mode.
 
         Args:
-            dst (:obj:`Box3DMode`): The target Box mode.
-            rt_mat (np.ndarray | torch.Tensor, optional): The rotation and
+            dst (int): The target Box mode.
+            rt_mat (Tensor or np.ndarray, optional): The rotation and
                 translation matrix between different coordinates.
-                Defaults to None.
-                The conversion from `src` coordinates to `dst` coordinates
-                usually comes along the change of sensors, e.g., from camera
-                to LiDAR. This requires a transformation matrix.
+                Defaults to None. The conversion from ``src`` coordinates to
+                ``dst`` coordinates usually comes along the change of sensors,
+                e.g., from camera to LiDAR. This requires a transformation
+                matrix.
+            correct_yaw (bool): Whether to convert the yaw angle to the target
+                coordinate. Defaults to False.
 
         Returns:
-            :obj:`BaseInstance3DBoxes`: The converted box of the same type
-                in the `dst` mode.
+            :obj:`BaseInstance3DBoxes`: The converted box of the same type in
+            the ``dst`` mode.
         """
         pass
 
-    def scale(self, scale_factor):
+    def scale(self, scale_factor: float) -> None:
         """Scale the box with horizontal and vertical scaling factors.
 
         Args:
             scale_factors (float): Scale factors to scale the boxes.
         """
         self.tensor[:, :6] *= scale_factor
         self.tensor[:, 7:] *= scale_factor  # velocity
 
-    def limit_yaw(self, offset=0.5, period=np.pi):
+    def limit_yaw(self, offset: float = 0.5, period: float = np.pi) -> None:
         """Limit the yaw to a given period and offset.
 
         Args:
-            offset (float, optional): The offset of the yaw. Defaults to 0.5.
-            period (float, optional): The expected period. Defaults to np.pi.
+            offset (float): The offset of the yaw. Defaults to 0.5.
+            period (float): The expected period. Defaults to np.pi.
         """
         self.tensor[:, 6] = limit_period(self.tensor[:, 6], offset, period)
 
-    def nonempty(self, threshold=0.0):
+    def nonempty(self, threshold: float = 0.0) -> Tensor:
         """Find boxes that are non-empty.
 
-        A box is considered empty,
-        if either of its side is no larger than threshold.
+        A box is considered empty if either of its side is no larger than
+        threshold.
 
         Args:
-            threshold (float, optional): The threshold of minimal sizes.
-                Defaults to 0.0.
+            threshold (float): The threshold of minimal sizes. Defaults to 0.0.
 
         Returns:
-            torch.Tensor: A binary vector which represents whether each
-                box is empty (False) or non-empty (True).
+            Tensor: A binary vector which represents whether each box is empty
+            (False) or non-empty (True).
         """
         box = self.tensor
         size_x = box[..., 3]
         size_y = box[..., 4]
         size_z = box[..., 5]
         keep = ((size_x > threshold)
                 & (size_y > threshold) & (size_z > threshold))
         return keep
 
-    def __getitem__(self, item):
+    def __getitem__(
+            self, item: Union[int, slice, np.ndarray,
+                              Tensor]) -> 'BaseInstance3DBoxes':
         """
+        Args:
+            item (int or slice or np.ndarray or Tensor): Index of boxes.
+
         Note:
             The following usage are allowed:
-            1. `new_boxes = boxes[3]`:
-                return a `Boxes` that contains only one box.
-            2. `new_boxes = boxes[2:10]`:
-                return a slice of boxes.
-            3. `new_boxes = boxes[vector]`:
-                where vector is a torch.BoolTensor with `length = len(boxes)`.
-                Nonzero elements in the vector will be selected.
+
+            1. `new_boxes = boxes[3]`: Return a `Boxes` that contains only one
+               box.
+            2. `new_boxes = boxes[2:10]`: Return a slice of boxes.
+            3. `new_boxes = boxes[vector]`: Where vector is a
+               torch.BoolTensor with `length = len(boxes)`. Nonzero elements in
+               the vector will be selected.
+
             Note that the returned Boxes might share storage with this Boxes,
-            subject to Pytorch's indexing semantics.
+            subject to PyTorch's indexing semantics.
 
         Returns:
             :obj:`BaseInstance3DBoxes`: A new object of
-                :class:`BaseInstance3DBoxes` after indexing.
+            :class:`BaseInstance3DBoxes` after indexing.
         """
         original_type = type(self)
         if isinstance(item, int):
             return original_type(
                 self.tensor[item].view(1, -1),
                 box_dim=self.box_dim,
                 with_yaw=self.with_yaw)
         b = self.tensor[item]
         assert b.dim() == 2, \
             f'Indexing on Boxes with {item} failed to return a matrix!'
         return original_type(b, box_dim=self.box_dim, with_yaw=self.with_yaw)
 
-    def __len__(self):
+    def __len__(self) -> int:
         """int: Number of boxes in the current object."""
         return self.tensor.shape[0]
 
-    def __repr__(self):
-        """str: Return a strings that describes the object."""
+    def __repr__(self) -> str:
+        """str: Return a string that describes the object."""
         return self.__class__.__name__ + '(\n    ' + str(self.tensor) + ')'
 
     @classmethod
-    def cat(cls, boxes_list):
+    def cat(cls, boxes_list: Sequence['BaseInstance3DBoxes']
+            ) -> 'BaseInstance3DBoxes':
         """Concatenate a list of Boxes into a single Boxes.
 
         Args:
-            boxes_list (list[:obj:`BaseInstance3DBoxes`]): List of boxes.
+            boxes_list (Sequence[:obj:`BaseInstance3DBoxes`]): List of boxes.
 
         Returns:
-            :obj:`BaseInstance3DBoxes`: The concatenated Boxes.
+            :obj:`BaseInstance3DBoxes`: The concatenated boxes.
         """
         assert isinstance(boxes_list, (list, tuple))
         if len(boxes_list) == 0:
             return cls(torch.empty(0))
         assert all(isinstance(box, cls) for box in boxes_list)
 
         # use torch.cat (v.s. layers.cat)
         # so the returned boxes never share storage with input
         cat_boxes = cls(
             torch.cat([b.tensor for b in boxes_list], dim=0),
-            box_dim=boxes_list[0].tensor.shape[1],
+            box_dim=boxes_list[0].box_dim,
             with_yaw=boxes_list[0].with_yaw)
         return cat_boxes
 
-    def to(self, device, *args, **kwargs):
+    def numpy(self) -> np.ndarray:
+        """Reload ``numpy`` from self.tensor."""
+        return self.tensor.numpy()
+
+    def to(self, device: Union[str, torch.device], *args,
+           **kwargs) -> 'BaseInstance3DBoxes':
         """Convert current boxes to a specific device.
 
         Args:
-            device (str | :obj:`torch.device`): The name of the device.
+            device (str or :obj:`torch.device`): The name of the device.
 
         Returns:
-            :obj:`BaseInstance3DBoxes`: A new boxes object on the
-                specific device.
+            :obj:`BaseInstance3DBoxes`: A new boxes object on the specific
+            device.
         """
         original_type = type(self)
         return original_type(
             self.tensor.to(device, *args, **kwargs),
             box_dim=self.box_dim,
             with_yaw=self.with_yaw)
 
-    def clone(self):
-        """Clone the Boxes.
+    def cpu(self) -> 'BaseInstance3DBoxes':
+        """Convert current boxes to cpu device.
+
+        Returns:
+            :obj:`BaseInstance3DBoxes`: A new boxes object on the cpu device.
+        """
+        original_type = type(self)
+        return original_type(
+            self.tensor.cpu(), box_dim=self.box_dim, with_yaw=self.with_yaw)
+
+    def cuda(self, *args, **kwargs) -> 'BaseInstance3DBoxes':
+        """Convert current boxes to cuda device.
 
         Returns:
-            :obj:`BaseInstance3DBoxes`: Box object with the same properties
-                as self.
+            :obj:`BaseInstance3DBoxes`: A new boxes object on the cuda device.
+        """
+        original_type = type(self)
+        return original_type(
+            self.tensor.cuda(*args, **kwargs),
+            box_dim=self.box_dim,
+            with_yaw=self.with_yaw)
+
+    def clone(self) -> 'BaseInstance3DBoxes':
+        """Clone the boxes.
+
+        Returns:
+            :obj:`BaseInstance3DBoxes`: Box object with the same properties as
+            self.
         """
         original_type = type(self)
         return original_type(
             self.tensor.clone(), box_dim=self.box_dim, with_yaw=self.with_yaw)
 
+    def detach(self) -> 'BaseInstance3DBoxes':
+        """Detach the boxes.
+
+        Returns:
+            :obj:`BaseInstance3DBoxes`: Box object with the same properties as
+            self.
+        """
+        original_type = type(self)
+        return original_type(
+            self.tensor.detach(), box_dim=self.box_dim, with_yaw=self.with_yaw)
+
     @property
-    def device(self):
-        """str: The device of the boxes are on."""
+    def device(self) -> torch.device:
+        """torch.device: The device of the boxes are on."""
         return self.tensor.device
 
-    def __iter__(self):
-        """Yield a box as a Tensor of shape (4,) at a time.
+    def __iter__(self) -> Iterator[Tensor]:
+        """Yield a box as a Tensor at a time.
 
         Returns:
-            torch.Tensor: A box of shape (4,).
+            Iterator[Tensor]: A box of shape (box_dim, ).
         """
         yield from self.tensor
 
     @classmethod
-    def height_overlaps(cls, boxes1, boxes2, mode='iou'):
+    def height_overlaps(cls, boxes1: 'BaseInstance3DBoxes',
+                        boxes2: 'BaseInstance3DBoxes') -> Tensor:
         """Calculate height overlaps of two boxes.
 
         Note:
-            This function calculates the height overlaps between boxes1 and
-            boxes2,  boxes1 and boxes2 should be in the same type.
+            This function calculates the height overlaps between ``boxes1`` and
+            ``boxes2``, ``boxes1`` and ``boxes2`` should be in the same type.
 
         Args:
             boxes1 (:obj:`BaseInstance3DBoxes`): Boxes 1 contain N boxes.
             boxes2 (:obj:`BaseInstance3DBoxes`): Boxes 2 contain M boxes.
-            mode (str, optional): Mode of IoU calculation. Defaults to 'iou'.
 
         Returns:
-            torch.Tensor: Calculated iou of boxes.
+            Tensor: Calculated height overlap of the boxes.
         """
         assert isinstance(boxes1, BaseInstance3DBoxes)
         assert isinstance(boxes2, BaseInstance3DBoxes)
-        assert type(boxes1) == type(boxes2), '"boxes1" and "boxes2" should' \
-            f'be in the same type, got {type(boxes1)} and {type(boxes2)}.'
+        assert type(boxes1) == type(boxes2), \
+            '"boxes1" and "boxes2" should be in the same type, ' \
+            f'but got {type(boxes1)} and {type(boxes2)}.'
 
         boxes1_top_height = boxes1.top_height.view(-1, 1)
         boxes1_bottom_height = boxes1.bottom_height.view(-1, 1)
         boxes2_top_height = boxes2.top_height.view(1, -1)
         boxes2_bottom_height = boxes2.bottom_height.view(1, -1)
 
         heighest_of_bottom = torch.max(boxes1_bottom_height,
                                        boxes2_bottom_height)
         lowest_of_top = torch.min(boxes1_top_height, boxes2_top_height)
         overlaps_h = torch.clamp(lowest_of_top - heighest_of_bottom, min=0)
         return overlaps_h
 
     @classmethod
-    def overlaps(cls, boxes1, boxes2, mode='iou'):
+    def overlaps(cls,
+                 boxes1: 'BaseInstance3DBoxes',
+                 boxes2: 'BaseInstance3DBoxes',
+                 mode: str = 'iou') -> Tensor:
         """Calculate 3D overlaps of two boxes.
 
         Note:
             This function calculates the overlaps between ``boxes1`` and
             ``boxes2``, ``boxes1`` and ``boxes2`` should be in the same type.
 
         Args:
             boxes1 (:obj:`BaseInstance3DBoxes`): Boxes 1 contain N boxes.
             boxes2 (:obj:`BaseInstance3DBoxes`): Boxes 2 contain M boxes.
-            mode (str, optional): Mode of iou calculation. Defaults to 'iou'.
+            mode (str): Mode of iou calculation. Defaults to 'iou'.
 
         Returns:
-            torch.Tensor: Calculated 3D overlaps of the boxes.
+            Tensor: Calculated 3D overlap of the boxes.
         """
         assert isinstance(boxes1, BaseInstance3DBoxes)
         assert isinstance(boxes2, BaseInstance3DBoxes)
-        assert type(boxes1) == type(boxes2), '"boxes1" and "boxes2" should' \
-            f'be in the same type, got {type(boxes1)} and {type(boxes2)}.'
+        assert type(boxes1) == type(boxes2), \
+            '"boxes1" and "boxes2" should be in the same type, ' \
+            f'but got {type(boxes1)} and {type(boxes2)}.'
 
         assert mode in ['iou', 'iof']
 
         rows = len(boxes1)
         cols = len(boxes2)
         if rows * cols == 0:
             return boxes1.tensor.new(rows, cols)
@@ -463,15 +559,15 @@
         # height overlap
         overlaps_h = cls.height_overlaps(boxes1, boxes2)
 
         # Restrict the min values of W and H to avoid memory overflow in
         # ``box_iou_rotated``.
         boxes1_bev, boxes2_bev = boxes1.bev, boxes2.bev
         boxes1_bev[:, 2:4] = boxes1_bev[:, 2:4].clamp(min=1e-4)
-        boxes2_bev[:, 2:4] = boxes2.bev[:, 2:4].clamp(min=1e-4)
+        boxes2_bev[:, 2:4] = boxes2_bev[:, 2:4].clamp(min=1e-4)
 
         # bev overlap
         iou2d = box_iou_rotated(boxes1_bev, boxes2_bev)
         areas1 = (boxes1_bev[:, 2] * boxes1_bev[:, 3]).unsqueeze(1).expand(
             rows, cols)
         areas2 = (boxes2_bev[:, 2] * boxes2_bev[:, 3]).unsqueeze(0).expand(
             rows, cols)
@@ -488,76 +584,89 @@
             iou3d = overlaps_3d / torch.clamp(
                 volume1 + volume2 - overlaps_3d, min=1e-8)
         else:
             iou3d = overlaps_3d / torch.clamp(volume1, min=1e-8)
 
         return iou3d
 
-    def new_box(self, data):
+    def new_box(
+        self, data: Union[Tensor, np.ndarray, Sequence[Sequence[float]]]
+    ) -> 'BaseInstance3DBoxes':
         """Create a new box object with data.
 
-        The new box and its tensor has the similar properties
-            as self and self.tensor, respectively.
+        The new box and its tensor has the similar properties as self and
+        self.tensor, respectively.
 
         Args:
-            data (torch.Tensor | numpy.array | list): Data to be copied.
+            data (Tensor or np.ndarray or Sequence[Sequence[float]]): Data to
+                be copied.
 
         Returns:
-            :obj:`BaseInstance3DBoxes`: A new bbox object with ``data``,
-                the object's other properties are similar to ``self``.
+            :obj:`BaseInstance3DBoxes`: A new bbox object with ``data``, the
+            object's other properties are similar to ``self``.
         """
         new_tensor = self.tensor.new_tensor(data) \
-            if not isinstance(data, torch.Tensor) else data.to(self.device)
+            if not isinstance(data, Tensor) else data.to(self.device)
         original_type = type(self)
         return original_type(
             new_tensor, box_dim=self.box_dim, with_yaw=self.with_yaw)
 
-    def points_in_boxes_part(self, points, boxes_override=None):
+    def points_in_boxes_part(
+            self,
+            points: Tensor,
+            boxes_override: Optional[Tensor] = None) -> Tensor:
         """Find the box in which each point is.
 
         Args:
-            points (torch.Tensor): Points in shape (1, M, 3) or (M, 3),
-                3 dimensions are (x, y, z) in LiDAR or depth coordinate.
-            boxes_override (torch.Tensor, optional): Boxes to override
-                `self.tensor`. Defaults to None.
-
-        Returns:
-            torch.Tensor: The index of the first box that each point
-                is in, in shape (M, ). Default value is -1
-                (if the point is not enclosed by any box).
+            points (Tensor): Points in shape (1, M, 3) or (M, 3), 3 dimensions
+                are (x, y, z) in LiDAR or depth coordinate.
+            boxes_override (Tensor, optional): Boxes to override `self.tensor`.
+                Defaults to None.
 
         Note:
-            If a point is enclosed by multiple boxes, the index of the
-            first box will be returned.
+            If a point is enclosed by multiple boxes, the index of the first
+            box will be returned.
+
+        Returns:
+            Tensor: The index of the first box that each point is in with shape
+            (M, ). Default value is -1 (if the point is not enclosed by any
+            box).
         """
         if boxes_override is not None:
             boxes = boxes_override
         else:
             boxes = self.tensor
-        if points.dim() == 2:
-            points = points.unsqueeze(0)
-        box_idx = points_in_boxes_part(points,
-                                       boxes.unsqueeze(0).to(
-                                           points.device)).squeeze(0)
-        return box_idx
 
-    def points_in_boxes_all(self, points, boxes_override=None):
+        points_clone = points.clone()[..., :3]
+        if points_clone.dim() == 2:
+            points_clone = points_clone.unsqueeze(0)
+        else:
+            assert points_clone.dim() == 3 and points_clone.shape[0] == 1
+
+        boxes = boxes.to(points_clone.device).unsqueeze(0)
+        box_idx = points_in_boxes_part(points_clone, boxes)
+
+        return box_idx.squeeze(0)
+
+    def points_in_boxes_all(self,
+                            points: Tensor,
+                            boxes_override: Optional[Tensor] = None) -> Tensor:
         """Find all boxes in which each point is.
 
         Args:
-            points (torch.Tensor): Points in shape (1, M, 3) or (M, 3),
-                3 dimensions are (x, y, z) in LiDAR or depth coordinate.
-            boxes_override (torch.Tensor, optional): Boxes to override
-                `self.tensor`. Defaults to None.
+            points (Tensor): Points in shape (1, M, 3) or (M, 3), 3 dimensions
+                are (x, y, z) in LiDAR or depth coordinate.
+            boxes_override (Tensor, optional): Boxes to override `self.tensor`.
+                Defaults to None.
 
         Returns:
-            torch.Tensor: A tensor indicating whether a point is in a box,
-                in shape (M, T). T is the number of boxes. Denote this
-                tensor as A, if the m^th point is in the t^th box, then
-                `A[m, t] == 1`, elsewise `A[m, t] == 0`.
+            Tensor: A tensor indicating whether a point is in a box with shape
+            (M, T). T is the number of boxes. Denote this tensor as A, it the
+            m^th point is in the t^th box, then `A[m, t] == 1`, otherwise
+            `A[m, t] == 0`.
         """
         if boxes_override is not None:
             boxes = boxes_override
         else:
             boxes = self.tensor
 
         points_clone = points.clone()[..., :3]
@@ -567,18 +676,22 @@
             assert points_clone.dim() == 3 and points_clone.shape[0] == 1
 
         boxes = boxes.to(points_clone.device).unsqueeze(0)
         box_idxs_of_pts = points_in_boxes_all(points_clone, boxes)
 
         return box_idxs_of_pts.squeeze(0)
 
-    def points_in_boxes(self, points, boxes_override=None):
-        warnings.warn('DeprecationWarning: points_in_boxes is a '
-                      'deprecated method, please consider using '
-                      'points_in_boxes_part.')
+    def points_in_boxes(self,
+                        points: Tensor,
+                        boxes_override: Optional[Tensor] = None) -> Tensor:
+        warnings.warn('DeprecationWarning: points_in_boxes is a deprecated '
+                      'method, please consider using points_in_boxes_part.')
         return self.points_in_boxes_part(points, boxes_override)
 
-    def points_in_boxes_batch(self, points, boxes_override=None):
+    def points_in_boxes_batch(
+            self,
+            points: Tensor,
+            boxes_override: Optional[Tensor] = None) -> Tensor:
         warnings.warn('DeprecationWarning: points_in_boxes_batch is a '
                       'deprecated method, please consider using '
                       'points_in_boxes_all.')
         return self.points_in_boxes_all(points, boxes_override)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/structures/bbox_3d/box_3d_mode.py` & `mmdet3d-1.1.1/mmdet3d/structures/bbox_3d/box_3d_mode.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,38 +1,40 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from enum import IntEnum, unique
+from typing import Optional, Sequence, Union
 
 import numpy as np
 import torch
+from torch import Tensor
 
 from .base_box3d import BaseInstance3DBoxes
 from .cam_box3d import CameraInstance3DBoxes
 from .depth_box3d import DepthInstance3DBoxes
 from .lidar_box3d import LiDARInstance3DBoxes
 from .utils import limit_period
 
 
 @unique
 class Box3DMode(IntEnum):
-    r"""Enum of different ways to represent a box.
+    """Enum of different ways to represent a box.
 
     Coordinates in LiDAR:
 
     .. code-block:: none
 
                     up z
                        ^   x front
                        |  /
                        | /
         left y <------ 0
 
     The relative coordinate of bottom center in a LiDAR box is (0.5, 0.5, 0),
     and the yaw is around the z axis, thus the rotation axis=2.
 
-    Coordinates in camera:
+    Coordinates in Camera:
 
     .. code-block:: none
 
                 z front
                /
               /
              0 ------> x right
@@ -40,15 +42,15 @@
              |
              v
         down y
 
     The relative coordinate of bottom center in a CAM box is (0.5, 1.0, 0.5),
     and the yaw is around the y axis, thus the rotation axis=1.
 
-    Coordinates in Depth mode:
+    Coordinates in Depth:
 
     .. code-block:: none
 
         up z
            ^   y front
            |  /
            | /
@@ -59,38 +61,45 @@
     """
 
     LIDAR = 0
     CAM = 1
     DEPTH = 2
 
     @staticmethod
-    def convert(box, src, dst, rt_mat=None, with_yaw=True, correct_yaw=False):
-        """Convert boxes from `src` mode to `dst` mode.
+    def convert(
+        box: Union[Sequence[float], np.ndarray, Tensor, BaseInstance3DBoxes],
+        src: 'Box3DMode',
+        dst: 'Box3DMode',
+        rt_mat: Optional[Union[np.ndarray, Tensor]] = None,
+        with_yaw: bool = True,
+        correct_yaw: bool = False
+    ) -> Union[Sequence[float], np.ndarray, Tensor, BaseInstance3DBoxes]:
+        """Convert boxes from ``src`` mode to ``dst`` mode.
 
         Args:
-            box (tuple | list | np.ndarray |
-                torch.Tensor | :obj:`BaseInstance3DBoxes`):
-                Can be a k-tuple, k-list or an Nxk array/tensor, where k = 7.
-            src (:obj:`Box3DMode`): The src Box mode.
-            dst (:obj:`Box3DMode`): The target Box mode.
-            rt_mat (np.ndarray | torch.Tensor, optional): The rotation and
+            box (Sequence[float] or np.ndarray or Tensor or
+                :obj:`BaseInstance3DBoxes`): Can be a k-tuple, k-list or an Nxk
+                array/tensor.
+            src (:obj:`Box3DMode`): The source box mode.
+            dst (:obj:`Box3DMode`): The target box mode.
+            rt_mat (np.ndarray or Tensor, optional): The rotation and
                 translation matrix between different coordinates.
-                Defaults to None.
-                The conversion from `src` coordinates to `dst` coordinates
-                usually comes along the change of sensors, e.g., from camera
-                to LiDAR. This requires a transformation matrix.
-            with_yaw (bool, optional): If `box` is an instance of
+                Defaults to None. The conversion from ``src`` coordinates to
+                ``dst`` coordinates usually comes along the change of sensors,
+                e.g., from camera to LiDAR. This requires a transformation
+                matrix.
+            with_yaw (bool): If ``box`` is an instance of
                 :obj:`BaseInstance3DBoxes`, whether or not it has a yaw angle.
                 Defaults to True.
             correct_yaw (bool): If the yaw is rotated by rt_mat.
+                Defaults to False.
 
         Returns:
-            (tuple | list | np.ndarray | torch.Tensor |
-                :obj:`BaseInstance3DBoxes`):
-                The converted box of the same type.
+            Sequence[float] or np.ndarray or Tensor or
+            :obj:`BaseInstance3DBoxes`: The converted box of the same type.
         """
         if src == dst:
             return box
 
         is_numpy = isinstance(box, np.ndarray)
         is_Instance3DBoxes = isinstance(box, BaseInstance3DBoxes)
         single_box = isinstance(box, (list, tuple))
@@ -204,15 +213,15 @@
                     yaw = yaw - np.pi / 2
                     yaw = limit_period(yaw, period=np.pi * 2)
         else:
             raise NotImplementedError(
                 f'Conversion from Box3DMode {src} to {dst} '
                 'is not supported yet')
 
-        if not isinstance(rt_mat, torch.Tensor):
+        if not isinstance(rt_mat, Tensor):
             rt_mat = arr.new_tensor(rt_mat)
         if rt_mat.size(1) == 4:
             extended_xyz = torch.cat(
                 [arr[..., :3], arr.new_ones(arr.size(0), 1)], dim=-1)
             xyz = extended_xyz @ rt_mat.t()
         else:
             xyz = arr[..., :3] @ rt_mat.t()
@@ -247,12 +256,12 @@
                 target_type = CameraInstance3DBoxes
             elif dst == Box3DMode.LIDAR:
                 target_type = LiDARInstance3DBoxes
             elif dst == Box3DMode.DEPTH:
                 target_type = DepthInstance3DBoxes
             else:
                 raise NotImplementedError(
-                    f'Conversion to {dst} through {original_type}'
-                    ' is not supported yet')
+                    f'Conversion to {dst} through {original_type} '
+                    'is not supported yet')
             return target_type(arr, box_dim=arr.size(-1), with_yaw=with_yaw)
         else:
             return arr
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/structures/bbox_3d/cam_box3d.py` & `mmdet3d-1.1.1/mmdet3d/structures/bbox_3d/cam_box3d.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,66 +1,84 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Optional, Sequence, Tuple, Union
+
 import numpy as np
 import torch
+from torch import Tensor
 
 from mmdet3d.structures.points import BasePoints
 from .base_box3d import BaseInstance3DBoxes
 from .utils import rotation_3d_in_axis, yaw2local
 
 
 class CameraInstance3DBoxes(BaseInstance3DBoxes):
     """3D boxes of instances in CAM coordinates.
 
-    Coordinates in camera:
+    Coordinates in Camera:
 
     .. code-block:: none
 
                 z front (yaw=-0.5*pi)
                /
               /
              0 ------> x right (yaw=0)
              |
              |
              v
         down y
 
     The relative coordinate of bottom center in a CAM box is (0.5, 1.0, 0.5),
-    and the yaw is around the y axis, thus the rotation axis=1.
-    The yaw is 0 at the positive direction of x axis, and decreases from
-    the positive direction of x to the positive direction of z.
+    and the yaw is around the y axis, thus the rotation axis=1. The yaw is 0 at
+    the positive direction of x axis, and decreases from the positive direction
+    of x to the positive direction of z.
+
+    Args:
+        tensor (Tensor or np.ndarray or Sequence[Sequence[float]]): The boxes
+            data with shape (N, box_dim).
+        box_dim (int): Number of the dimension of a box. Each row is
+            (x, y, z, x_size, y_size, z_size, yaw). Defaults to 7.
+        with_yaw (bool): Whether the box is with yaw rotation. If False, the
+            value of yaw will be set to 0 as minmax boxes. Defaults to True.
+        origin (Tuple[float]): Relative position of the box origin.
+            Defaults to (0.5, 1.0, 0.5). This will guide the box be converted
+            to (0.5, 1.0, 0.5) mode.
 
     Attributes:
-        tensor (torch.Tensor): Float matrix in shape (N, box_dim).
-        box_dim (int): Integer indicating the dimension of a box
-            Each row is (x, y, z, x_size, y_size, z_size, yaw, ...).
-        with_yaw (bool): If True, the value of yaw will be set to 0 as
-            axis-aligned boxes tightly enclosing the original boxes.
+        tensor (Tensor): Float matrix with shape (N, box_dim).
+        box_dim (int): Integer indicating the dimension of a box. Each row is
+            (x, y, z, x_size, y_size, z_size, yaw, ...).
+        with_yaw (bool): If True, the value of yaw will be set to 0 as minmax
+            boxes.
     """
     YAW_AXIS = 1
 
-    def __init__(self,
-                 tensor,
-                 box_dim=7,
-                 with_yaw=True,
-                 origin=(0.5, 1.0, 0.5)):
-        if isinstance(tensor, torch.Tensor):
+    def __init__(
+        self,
+        tensor: Union[Tensor, np.ndarray, Sequence[Sequence[float]]],
+        box_dim: int = 7,
+        with_yaw: bool = True,
+        origin: Tuple[float, float, float] = (0.5, 1.0, 0.5)
+    ) -> None:
+        if isinstance(tensor, Tensor):
             device = tensor.device
         else:
             device = torch.device('cpu')
         tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)
         if tensor.numel() == 0:
-            # Use reshape, so we don't end up creating a new tensor that
-            # does not depend on the inputs (and consequently confuses jit)
-            tensor = tensor.reshape((0, box_dim)).to(
-                dtype=torch.float32, device=device)
-        assert tensor.dim() == 2 and tensor.size(-1) == box_dim, tensor.size()
+            # Use reshape, so we don't end up creating a new tensor that does
+            # not depend on the inputs (and consequently confuses jit)
+            tensor = tensor.reshape((-1, box_dim))
+        assert tensor.dim() == 2 and tensor.size(-1) == box_dim, \
+            ('The box dimension must be 2 and the length of the last '
+             f'dimension must be {box_dim}, but got boxes with shape '
+             f'{tensor.shape}.')
 
         if tensor.shape[-1] == 6:
-            # If the dimension of boxes is 6, we expand box_dim by padding
-            # 0 as a fake yaw and set with_yaw to False.
+            # If the dimension of boxes is 6, we expand box_dim by padding 0 as
+            # a fake yaw and set with_yaw to False
             assert box_dim == 6
             fake_rot = tensor.new_zeros(tensor.shape[0], 1)
             tensor = torch.cat((tensor, fake_rot), dim=-1)
             self.box_dim = box_dim + 1
             self.with_yaw = False
         else:
             self.box_dim = box_dim
@@ -69,124 +87,125 @@
 
         if origin != (0.5, 1.0, 0.5):
             dst = self.tensor.new_tensor((0.5, 1.0, 0.5))
             src = self.tensor.new_tensor(origin)
             self.tensor[:, :3] += self.tensor[:, 3:6] * (dst - src)
 
     @property
-    def height(self):
-        """torch.Tensor: A vector with height of each box in shape (N, )."""
+    def height(self) -> Tensor:
+        """Tensor: A vector with height of each box in shape (N, )."""
         return self.tensor[:, 4]
 
     @property
-    def top_height(self):
-        """torch.Tensor:
-            A vector with the top height of each box in shape (N, )."""
+    def top_height(self) -> Tensor:
+        """Tensor: A vector with top height of each box in shape (N, )."""
         # the positive direction is down rather than up
         return self.bottom_height - self.height
 
     @property
-    def bottom_height(self):
-        """torch.Tensor:
-            A vector with bottom's height of each box in shape (N, )."""
+    def bottom_height(self) -> Tensor:
+        """Tensor: A vector with bottom height of each box in shape (N, )."""
         return self.tensor[:, 1]
 
     @property
-    def local_yaw(self):
-        """torch.Tensor:
-            A vector with local yaw of each box in shape (N, ).
-            local_yaw equals to alpha in kitti, which is commonly
-            used in monocular 3D object detection task, so only
-            :obj:`CameraInstance3DBoxes` has the property.
-        """
+    def local_yaw(self) -> Tensor:
+        """Tensor: A vector with local yaw of each box in shape (N, ).
+        local_yaw equals to alpha in kitti, which is commonly used in monocular
+        3D object detection task, so only :obj:`CameraInstance3DBoxes` has the
+        property."""
         yaw = self.yaw
         loc = self.gravity_center
         local_yaw = yaw2local(yaw, loc)
 
         return local_yaw
 
     @property
-    def gravity_center(self):
-        """torch.Tensor: A tensor with center of each box in shape (N, 3)."""
+    def gravity_center(self) -> Tensor:
+        """Tensor: A tensor with center of each box in shape (N, 3)."""
         bottom_center = self.bottom_center
         gravity_center = torch.zeros_like(bottom_center)
         gravity_center[:, [0, 2]] = bottom_center[:, [0, 2]]
         gravity_center[:, 1] = bottom_center[:, 1] - self.tensor[:, 4] * 0.5
         return gravity_center
 
     @property
-    def corners(self):
-        """torch.Tensor: Coordinates of corners of all the boxes in
-                         shape (N, 8, 3).
-
-        Convert the boxes to  in clockwise order, in the form of
-        (x0y0z0, x0y0z1, x0y1z1, x0y1z0, x1y0z0, x1y0z1, x1y1z1, x1y1z0)
+    def corners(self) -> Tensor:
+        """Convert boxes to corners in clockwise order, in the form of (x0y0z0,
+        x0y0z1, x0y1z1, x0y1z0, x1y0z0, x1y0z1, x1y1z1, x1y1z0).
 
         .. code-block:: none
 
                          front z
                               /
                              /
                (x0, y0, z1) + -----------  + (x1, y0, z1)
                            /|            / |
                           / |           /  |
             (x0, y0, z0) + ----------- +   + (x1, y1, z1)
                          |  /      .   |  /
                          | / origin    | /
-            (x0, y1, z0) + ----------- + -------> x right
+            (x0, y1, z0) + ----------- + -------> right x
                          |             (x1, y1, z0)
                          |
                          v
                     down y
+
+        Returns:
+            Tensor: A tensor with 8 corners of each box in shape (N, 8, 3).
         """
         if self.tensor.numel() == 0:
             return torch.empty([0, 8, 3], device=self.tensor.device)
 
         dims = self.dims
         corners_norm = torch.from_numpy(
             np.stack(np.unravel_index(np.arange(8), [2] * 3), axis=1)).to(
                 device=dims.device, dtype=dims.dtype)
 
         corners_norm = corners_norm[[0, 1, 3, 2, 4, 5, 7, 6]]
-        # use relative origin [0.5, 1, 0.5]
+        # use relative origin (0.5, 1, 0.5)
         corners_norm = corners_norm - dims.new_tensor([0.5, 1, 0.5])
         corners = dims.view([-1, 1, 3]) * corners_norm.reshape([1, 8, 3])
 
         corners = rotation_3d_in_axis(
             corners, self.tensor[:, 6], axis=self.YAW_AXIS)
         corners += self.tensor[:, :3].view(-1, 1, 3)
         return corners
 
     @property
-    def bev(self):
-        """torch.Tensor: 2D BEV box of each box with rotation
-            in XYWHR format, in shape (N, 5)."""
+    def bev(self) -> Tensor:
+        """Tensor: 2D BEV box of each box with rotation in XYWHR format, in
+        shape (N, 5)."""
         bev = self.tensor[:, [0, 2, 3, 5, 6]].clone()
         # positive direction of the gravity axis
         # in cam coord system points to the earth
         # so the bev yaw angle needs to be reversed
         bev[:, -1] = -bev[:, -1]
         return bev
 
-    def rotate(self, angle, points=None):
+    def rotate(
+        self,
+        angle: Union[Tensor, np.ndarray, float],
+        points: Optional[Union[Tensor, np.ndarray, BasePoints]] = None
+    ) -> Union[Tuple[Tensor, Tensor], Tuple[np.ndarray, np.ndarray], Tuple[
+            BasePoints, Tensor], None]:
         """Rotate boxes with points (optional) with the given angle or rotation
         matrix.
 
         Args:
-            angle (float | torch.Tensor | np.ndarray):
-                Rotation angle or rotation matrix.
-            points (torch.Tensor | np.ndarray | :obj:`BasePoints`, optional):
+            angle (Tensor or np.ndarray or float): Rotation angle or rotation
+                matrix.
+            points (Tensor or np.ndarray or :obj:`BasePoints`, optional):
                 Points to rotate. Defaults to None.
 
         Returns:
-            tuple or None: When ``points`` is None, the function returns
-                None, otherwise it returns the rotated points and the
-                rotation matrix ``rot_mat_T``.
+            tuple or None: When ``points`` is None, the function returns None,
+            otherwise it returns the rotated points and the rotation matrix
+            ``rot_mat_T``.
         """
-        if not isinstance(angle, torch.Tensor):
+        if not isinstance(angle, Tensor):
             angle = self.tensor.new_tensor(angle)
 
         assert angle.shape == torch.Size([3, 3]) or angle.numel() == 1, \
             f'invalid rotation angle shape {angle.shape}'
 
         if angle.numel() == 1:
             self.tensor[:, 0:3], rot_mat_T = rotation_3d_in_axis(
@@ -200,73 +219,81 @@
             rot_cos = rot_mat_T[0, 0]
             angle = np.arctan2(rot_sin, rot_cos)
             self.tensor[:, 0:3] = self.tensor[:, 0:3] @ rot_mat_T
 
         self.tensor[:, 6] += angle
 
         if points is not None:
-            if isinstance(points, torch.Tensor):
+            if isinstance(points, Tensor):
                 points[:, :3] = points[:, :3] @ rot_mat_T
             elif isinstance(points, np.ndarray):
                 rot_mat_T = rot_mat_T.cpu().numpy()
                 points[:, :3] = np.dot(points[:, :3], rot_mat_T)
             elif isinstance(points, BasePoints):
                 points.rotate(rot_mat_T)
             else:
                 raise ValueError
             return points, rot_mat_T
 
-    def flip(self, bev_direction='horizontal', points=None):
+    def flip(
+        self,
+        bev_direction: str = 'horizontal',
+        points: Optional[Union[Tensor, np.ndarray, BasePoints]] = None
+    ) -> Union[Tensor, np.ndarray, BasePoints, None]:
         """Flip the boxes in BEV along given BEV direction.
 
         In CAM coordinates, it flips the x (horizontal) or z (vertical) axis.
 
         Args:
-            bev_direction (str): Flip direction (horizontal or vertical).
-            points (torch.Tensor | np.ndarray | :obj:`BasePoints`, optional):
+            bev_direction (str): Direction by which to flip. Can be chosen from
+                'horizontal' and 'vertical'. Defaults to 'horizontal'.
+            points (Tensor or np.ndarray or :obj:`BasePoints`, optional):
                 Points to flip. Defaults to None.
 
         Returns:
-            torch.Tensor, numpy.ndarray or None: Flipped points.
+            Tensor or np.ndarray or :obj:`BasePoints` or None: When ``points``
+            is None, the function returns None, otherwise it returns the
+            flipped points.
         """
         assert bev_direction in ('horizontal', 'vertical')
         if bev_direction == 'horizontal':
             self.tensor[:, 0::7] = -self.tensor[:, 0::7]
             if self.with_yaw:
                 self.tensor[:, 6] = -self.tensor[:, 6] + np.pi
         elif bev_direction == 'vertical':
             self.tensor[:, 2::7] = -self.tensor[:, 2::7]
             if self.with_yaw:
                 self.tensor[:, 6] = -self.tensor[:, 6]
 
         if points is not None:
-            assert isinstance(points, (torch.Tensor, np.ndarray, BasePoints))
-            if isinstance(points, (torch.Tensor, np.ndarray)):
+            assert isinstance(points, (Tensor, np.ndarray, BasePoints))
+            if isinstance(points, (Tensor, np.ndarray)):
                 if bev_direction == 'horizontal':
                     points[:, 0] = -points[:, 0]
                 elif bev_direction == 'vertical':
                     points[:, 2] = -points[:, 2]
             elif isinstance(points, BasePoints):
                 points.flip(bev_direction)
             return points
 
     @classmethod
-    def height_overlaps(cls, boxes1, boxes2, mode='iou'):
+    def height_overlaps(cls, boxes1: 'CameraInstance3DBoxes',
+                        boxes2: 'CameraInstance3DBoxes') -> Tensor:
         """Calculate height overlaps of two boxes.
 
-        This function calculates the height overlaps between ``boxes1`` and
-        ``boxes2``, where ``boxes1`` and ``boxes2`` should be in the same type.
+        Note:
+            This function calculates the height overlaps between ``boxes1`` and
+            ``boxes2``, ``boxes1`` and ``boxes2`` should be in the same type.
 
         Args:
             boxes1 (:obj:`CameraInstance3DBoxes`): Boxes 1 contain N boxes.
             boxes2 (:obj:`CameraInstance3DBoxes`): Boxes 2 contain M boxes.
-            mode (str, optional): Mode of iou calculation. Defaults to 'iou'.
 
         Returns:
-            torch.Tensor: Calculated iou of boxes' heights.
+            Tensor: Calculated height overlap of the boxes.
         """
         assert isinstance(boxes1, CameraInstance3DBoxes)
         assert isinstance(boxes2, CameraInstance3DBoxes)
 
         boxes1_top_height = boxes1.top_height.view(-1, 1)
         boxes1_bottom_height = boxes1.bottom_height.view(-1, 1)
         boxes2_top_height = boxes2.top_height.view(1, -1)
@@ -276,86 +303,101 @@
         # in cam coord system points to the earth
         heighest_of_bottom = torch.min(boxes1_bottom_height,
                                        boxes2_bottom_height)
         lowest_of_top = torch.max(boxes1_top_height, boxes2_top_height)
         overlaps_h = torch.clamp(heighest_of_bottom - lowest_of_top, min=0)
         return overlaps_h
 
-    def convert_to(self, dst, rt_mat=None, correct_yaw=False):
+    def convert_to(self,
+                   dst: int,
+                   rt_mat: Optional[Union[Tensor, np.ndarray]] = None,
+                   correct_yaw: bool = False) -> 'BaseInstance3DBoxes':
         """Convert self to ``dst`` mode.
 
         Args:
-            dst (:obj:`Box3DMode`): The target Box mode.
-            rt_mat (np.ndarray | torch.Tensor, optional): The rotation and
+            dst (int): The target Box mode.
+            rt_mat (Tensor or np.ndarray, optional): The rotation and
                 translation matrix between different coordinates.
-                Defaults to None.
-                The conversion from ``src`` coordinates to ``dst`` coordinates
-                usually comes along the change of sensors, e.g., from camera
-                to LiDAR. This requires a transformation matrix.
+                Defaults to None. The conversion from ``src`` coordinates to
+                ``dst`` coordinates usually comes along the change of sensors,
+                e.g., from camera to LiDAR. This requires a transformation
+                matrix.
             correct_yaw (bool): Whether to convert the yaw angle to the target
                 coordinate. Defaults to False.
+
         Returns:
-            :obj:`BaseInstance3DBoxes`:
-                The converted box of the same type in the ``dst`` mode.
+            :obj:`BaseInstance3DBoxes`: The converted box of the same type in
+            the ``dst`` mode.
         """
         from .box_3d_mode import Box3DMode
 
         # TODO: always set correct_yaw=True
         return Box3DMode.convert(
             box=self,
             src=Box3DMode.CAM,
             dst=dst,
             rt_mat=rt_mat,
             correct_yaw=correct_yaw)
 
-    def points_in_boxes_part(self, points, boxes_override=None):
+    def points_in_boxes_part(
+            self,
+            points: Tensor,
+            boxes_override: Optional[Tensor] = None) -> Tensor:
         """Find the box in which each point is.
 
         Args:
-            points (torch.Tensor): Points in shape (1, M, 3) or (M, 3),
-                3 dimensions are (x, y, z) in LiDAR or depth coordinate.
-            boxes_override (torch.Tensor, optional): Boxes to override
-                `self.tensor `. Defaults to None.
+            points (Tensor): Points in shape (1, M, 3) or (M, 3), 3 dimensions
+                are (x, y, z) in LiDAR or depth coordinate.
+            boxes_override (Tensor, optional): Boxes to override `self.tensor`.
+                Defaults to None.
 
         Returns:
-            torch.Tensor: The index of the box in which
-                each point is, in shape (M, ). Default value is -1
-                (if the point is not enclosed by any box).
+            Tensor: The index of the first box that each point is in with shape
+            (M, ). Default value is -1 (if the point is not enclosed by any
+            box).
         """
         from .coord_3d_mode import Coord3DMode
 
         points_lidar = Coord3DMode.convert(points, Coord3DMode.CAM,
                                            Coord3DMode.LIDAR)
         if boxes_override is not None:
             boxes_lidar = boxes_override
         else:
-            boxes_lidar = Coord3DMode.convert(self.tensor, Coord3DMode.CAM,
-                                              Coord3DMode.LIDAR)
+            boxes_lidar = Coord3DMode.convert(
+                self.tensor,
+                Coord3DMode.CAM,
+                Coord3DMode.LIDAR,
+                is_point=False)
 
         box_idx = super().points_in_boxes_part(points_lidar, boxes_lidar)
         return box_idx
 
-    def points_in_boxes_all(self, points, boxes_override=None):
+    def points_in_boxes_all(self,
+                            points: Tensor,
+                            boxes_override: Optional[Tensor] = None) -> Tensor:
         """Find all boxes in which each point is.
 
         Args:
-            points (torch.Tensor): Points in shape (1, M, 3) or (M, 3),
-                3 dimensions are (x, y, z) in LiDAR or depth coordinate.
-            boxes_override (torch.Tensor, optional): Boxes to override
-                `self.tensor `. Defaults to None.
+            points (Tensor): Points in shape (1, M, 3) or (M, 3), 3 dimensions
+                are (x, y, z) in LiDAR or depth coordinate.
+            boxes_override (Tensor, optional): Boxes to override `self.tensor`.
+                Defaults to None.
 
         Returns:
-            torch.Tensor: The index of all boxes in which each point is,
-                in shape (B, M, T).
+            Tensor: The index of all boxes in which each point is with shape
+            (M, T).
         """
         from .coord_3d_mode import Coord3DMode
 
         points_lidar = Coord3DMode.convert(points, Coord3DMode.CAM,
                                            Coord3DMode.LIDAR)
         if boxes_override is not None:
             boxes_lidar = boxes_override
         else:
-            boxes_lidar = Coord3DMode.convert(self.tensor, Coord3DMode.CAM,
-                                              Coord3DMode.LIDAR)
+            boxes_lidar = Coord3DMode.convert(
+                self.tensor,
+                Coord3DMode.CAM,
+                Coord3DMode.LIDAR,
+                is_point=False)
 
         box_idx = super().points_in_boxes_all(points_lidar, boxes_lidar)
         return box_idx
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/structures/bbox_3d/coord_3d_mode.py` & `mmdet3d-1.1.1/mmdet3d/structures/bbox_3d/coord_3d_mode.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,38 +1,39 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from enum import IntEnum, unique
+from typing import Optional, Sequence, Union
 
 import numpy as np
 import torch
+from torch import Tensor
 
 from mmdet3d.structures.points import (BasePoints, CameraPoints, DepthPoints,
                                        LiDARPoints)
 from .base_box3d import BaseInstance3DBoxes
 from .box_3d_mode import Box3DMode
 
 
 @unique
 class Coord3DMode(IntEnum):
-    r"""Enum of different ways to represent a box
-        and point cloud.
+    """Enum of different ways to represent a box and point cloud.
 
     Coordinates in LiDAR:
 
     .. code-block:: none
 
                     up z
                        ^   x front
                        |  /
                        | /
         left y <------ 0
 
     The relative coordinate of bottom center in a LiDAR box is (0.5, 0.5, 0),
     and the yaw is around the z axis, thus the rotation axis=2.
 
-    Coordinates in camera:
+    Coordinates in Camera:
 
     .. code-block:: none
 
                 z front
                /
               /
              0 ------> x right
@@ -40,15 +41,15 @@
              |
              v
         down y
 
     The relative coordinate of bottom center in a CAM box is (0.5, 1.0, 0.5),
     and the yaw is around the y axis, thus the rotation axis=1.
 
-    Coordinates in Depth mode:
+    Coordinates in Depth:
 
     .. code-block:: none
 
         up z
            ^   y front
            |  /
            | /
@@ -59,114 +60,151 @@
     """
 
     LIDAR = 0
     CAM = 1
     DEPTH = 2
 
     @staticmethod
-    def convert(input, src, dst, rt_mat=None, with_yaw=True, is_point=True):
-        """Convert boxes or points from `src` mode to `dst` mode.
+    def convert(input: Union[Sequence[float], np.ndarray, Tensor,
+                             BaseInstance3DBoxes, BasePoints],
+                src: Union[Box3DMode, 'Coord3DMode'],
+                dst: Union[Box3DMode, 'Coord3DMode'],
+                rt_mat: Optional[Union[np.ndarray, Tensor]] = None,
+                with_yaw: bool = True,
+                correct_yaw: bool = False,
+                is_point: bool = True):
+        """Convert boxes or points from ``src`` mode to ``dst`` mode.
 
         Args:
-            input (tuple | list | np.ndarray | torch.Tensor |
-                :obj:`BaseInstance3DBoxes` | :obj:`BasePoints`):
-                Can be a k-tuple, k-list or an Nxk array/tensor, where k = 7.
-            src (:obj:`Box3DMode` | :obj:`Coord3DMode`): The source mode.
-            dst (:obj:`Box3DMode` | :obj:`Coord3DMode`): The target mode.
-            rt_mat (np.ndarray | torch.Tensor, optional): The rotation and
+            input (Sequence[float] or np.ndarray or Tensor or
+                :obj:`BaseInstance3DBoxes` or :obj:`BasePoints`): Can be a
+                k-tuple, k-list or an Nxk array/tensor.
+            src (:obj:`Box3DMode` or :obj:`Coord3DMode`): The source mode.
+            dst (:obj:`Box3DMode` or :obj:`Coord3DMode`): The target mode.
+            rt_mat (np.ndarray or Tensor, optional): The rotation and
                 translation matrix between different coordinates.
-                Defaults to None.
-                The conversion from `src` coordinates to `dst` coordinates
-                usually comes along the change of sensors, e.g., from camera
-                to LiDAR. This requires a transformation matrix.
-            with_yaw (bool): If `box` is an instance of
+                Defaults to None. The conversion from ``src`` coordinates to
+                ``dst`` coordinates usually comes along the change of sensors,
+                e.g., from camera to LiDAR. This requires a transformation
+                matrix.
+            with_yaw (bool): If ``box`` is an instance of
                 :obj:`BaseInstance3DBoxes`, whether or not it has a yaw angle.
                 Defaults to True.
-            is_point (bool): If `input` is neither an instance of
+            correct_yaw (bool): If the yaw is rotated by rt_mat.
+                Defaults to False.
+            is_point (bool): If ``input`` is neither an instance of
                 :obj:`BaseInstance3DBoxes` nor an instance of
                 :obj:`BasePoints`, whether or not it is point data.
                 Defaults to True.
 
         Returns:
-            (tuple | list | np.ndarray | torch.Tensor |
-                :obj:`BaseInstance3DBoxes` | :obj:`BasePoints`):
-                The converted box of the same type.
+            Sequence[float] or np.ndarray or Tensor or
+            :obj:`BaseInstance3DBoxes` or :obj:`BasePoints`: The converted box
+            or points of the same type.
         """
         if isinstance(input, BaseInstance3DBoxes):
             return Coord3DMode.convert_box(
-                input, src, dst, rt_mat=rt_mat, with_yaw=with_yaw)
+                input,
+                src,
+                dst,
+                rt_mat=rt_mat,
+                with_yaw=with_yaw,
+                correct_yaw=correct_yaw)
         elif isinstance(input, BasePoints):
             return Coord3DMode.convert_point(input, src, dst, rt_mat=rt_mat)
-        elif isinstance(input, (tuple, list, np.ndarray, torch.Tensor)):
+        elif isinstance(input, (tuple, list, np.ndarray, Tensor)):
             if is_point:
                 return Coord3DMode.convert_point(
                     input, src, dst, rt_mat=rt_mat)
             else:
                 return Coord3DMode.convert_box(
-                    input, src, dst, rt_mat=rt_mat, with_yaw=with_yaw)
+                    input,
+                    src,
+                    dst,
+                    rt_mat=rt_mat,
+                    with_yaw=with_yaw,
+                    correct_yaw=correct_yaw)
         else:
             raise NotImplementedError
 
     @staticmethod
-    def convert_box(box, src, dst, rt_mat=None, with_yaw=True):
-        """Convert boxes from `src` mode to `dst` mode.
+    def convert_box(
+        box: Union[Sequence[float], np.ndarray, Tensor, BaseInstance3DBoxes],
+        src: Box3DMode,
+        dst: Box3DMode,
+        rt_mat: Optional[Union[np.ndarray, Tensor]] = None,
+        with_yaw: bool = True,
+        correct_yaw: bool = False
+    ) -> Union[Sequence[float], np.ndarray, Tensor, BaseInstance3DBoxes]:
+        """Convert boxes from ``src`` mode to ``dst`` mode.
 
         Args:
-            box (tuple | list | np.ndarray |
-                torch.Tensor | :obj:`BaseInstance3DBoxes`):
-                Can be a k-tuple, k-list or an Nxk array/tensor, where k = 7.
-            src (:obj:`Box3DMode`): The src Box mode.
-            dst (:obj:`Box3DMode`): The target Box mode.
-            rt_mat (np.ndarray | torch.Tensor, optional): The rotation and
+            box (Sequence[float] or np.ndarray or Tensor or
+                :obj:`BaseInstance3DBoxes`): Can be a k-tuple, k-list or an Nxk
+                array/tensor.
+            src (:obj:`Box3DMode`): The source box mode.
+            dst (:obj:`Box3DMode`): The target box mode.
+            rt_mat (np.ndarray or Tensor, optional): The rotation and
                 translation matrix between different coordinates.
-                Defaults to None.
-                The conversion from `src` coordinates to `dst` coordinates
-                usually comes along the change of sensors, e.g., from camera
-                to LiDAR. This requires a transformation matrix.
-            with_yaw (bool): If `box` is an instance of
+                Defaults to None. The conversion from ``src`` coordinates to
+                ``dst`` coordinates usually comes along the change of sensors,
+                e.g., from camera to LiDAR. This requires a transformation
+                matrix.
+            with_yaw (bool): If ``box`` is an instance of
                 :obj:`BaseInstance3DBoxes`, whether or not it has a yaw angle.
                 Defaults to True.
+            correct_yaw (bool): If the yaw is rotated by rt_mat.
+                Defaults to False.
 
         Returns:
-            (tuple | list | np.ndarray | torch.Tensor |
-                :obj:`BaseInstance3DBoxes`):
-                The converted box of the same type.
+            Sequence[float] or np.ndarray or Tensor or
+            :obj:`BaseInstance3DBoxes`: The converted box of the same type.
         """
-        return Box3DMode.convert(box, src, dst, rt_mat=rt_mat)
+        return Box3DMode.convert(
+            box,
+            src,
+            dst,
+            rt_mat=rt_mat,
+            with_yaw=with_yaw,
+            correct_yaw=correct_yaw)
 
     @staticmethod
-    def convert_point(point, src, dst, rt_mat=None):
-        """Convert points from `src` mode to `dst` mode.
+    def convert_point(
+        point: Union[Sequence[float], np.ndarray, Tensor, BasePoints],
+        src: 'Coord3DMode',
+        dst: 'Coord3DMode',
+        rt_mat: Optional[Union[np.ndarray, Tensor]] = None,
+    ) -> Union[Sequence[float], np.ndarray, Tensor, BasePoints]:
+        """Convert points from ``src`` mode to ``dst`` mode.
 
         Args:
-            point (tuple | list | np.ndarray |
-                torch.Tensor | :obj:`BasePoints`):
+            box (Sequence[float] or np.ndarray or Tensor or :obj:`BasePoints`):
                 Can be a k-tuple, k-list or an Nxk array/tensor.
-            src (:obj:`CoordMode`): The src Point mode.
-            dst (:obj:`CoordMode`): The target Point mode.
-            rt_mat (np.ndarray | torch.Tensor, optional): The rotation and
+            src (:obj:`Coord3DMode`): The source point mode.
+            dst (:obj:`Coord3DMode`): The target point mode.
+            rt_mat (np.ndarray or Tensor, optional): The rotation and
                 translation matrix between different coordinates.
-                Defaults to None.
-                The conversion from `src` coordinates to `dst` coordinates
-                usually comes along the change of sensors, e.g., from camera
-                to LiDAR. This requires a transformation matrix.
+                Defaults to None. The conversion from ``src`` coordinates to
+                ``dst`` coordinates usually comes along the change of sensors,
+                e.g., from camera to LiDAR. This requires a transformation
+                matrix.
 
         Returns:
-            (tuple | list | np.ndarray | torch.Tensor | :obj:`BasePoints`):
-                The converted point of the same type.
+            Sequence[float] or np.ndarray or Tensor or :obj:`BasePoints`: The
+            converted point of the same type.
         """
         if src == dst:
             return point
 
         is_numpy = isinstance(point, np.ndarray)
         is_InstancePoints = isinstance(point, BasePoints)
         single_point = isinstance(point, (list, tuple))
         if single_point:
             assert len(point) >= 3, (
-                'CoordMode.convert takes either a k-tuple/list or '
+                'Coord3DMode.convert takes either a k-tuple/list or '
                 'an Nxk array/tensor, where k >= 3')
             arr = torch.tensor(point)[None, :]
         else:
             # avoid modifying the input point
             if is_numpy:
                 arr = torch.from_numpy(np.asarray(point)).clone()
             elif is_InstancePoints:
@@ -194,15 +232,15 @@
             if rt_mat is None:
                 rt_mat = arr.new_tensor([[0, 1, 0], [-1, 0, 0], [0, 0, 1]])
         else:
             raise NotImplementedError(
                 f'Conversion from Coord3DMode {src} to {dst} '
                 'is not supported yet')
 
-        if not isinstance(rt_mat, torch.Tensor):
+        if not isinstance(rt_mat, Tensor):
             rt_mat = arr.new_tensor(rt_mat)
         if rt_mat.size(1) == 4:
             extended_xyz = torch.cat(
                 [arr[..., :3], arr.new_ones(arr.size(0), 1)], dim=-1)
             xyz = extended_xyz @ rt_mat.t()
         else:
             xyz = arr[..., :3] @ rt_mat.t()
@@ -221,15 +259,15 @@
                 target_type = CameraPoints
             elif dst == Coord3DMode.LIDAR:
                 target_type = LiDARPoints
             elif dst == Coord3DMode.DEPTH:
                 target_type = DepthPoints
             else:
                 raise NotImplementedError(
-                    f'Conversion to {dst} through {original_type}'
-                    ' is not supported yet')
+                    f'Conversion to {dst} through {original_type} '
+                    'is not supported yet')
             return target_type(
                 arr,
                 points_dim=arr.size(-1),
                 attribute_dims=point.attribute_dims)
         else:
             return arr
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/structures/bbox_3d/depth_box3d.py` & `mmdet3d-1.1.1/mmdet3d/structures/bbox_3d/depth_box3d.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,79 +1,68 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Optional, Tuple, Union
+
 import numpy as np
 import torch
+from torch import Tensor
 
 from mmdet3d.structures.points import BasePoints
 from .base_box3d import BaseInstance3DBoxes
 from .utils import rotation_3d_in_axis
 
 
 class DepthInstance3DBoxes(BaseInstance3DBoxes):
-    """3D boxes of instances in Depth coordinates.
+    """3D boxes of instances in DEPTH coordinates.
 
     Coordinates in Depth:
 
     .. code-block:: none
 
-                    up z    y front (yaw=0.5*pi)
-                       ^   ^
-                       |  /
-                       | /
-                       0 ------> x right (yaw=0)
+        up z    y front (yaw=0.5*pi)
+           ^   ^
+           |  /
+           | /
+           0 ------> x right (yaw=0)
 
     The relative coordinate of bottom center in a Depth box is (0.5, 0.5, 0),
-    and the yaw is around the z axis, thus the rotation axis=2.
-    The yaw is 0 at the positive direction of x axis, and decreases from
-    the positive direction of x to the positive direction of y.
-    Also note that rotation of DepthInstance3DBoxes is counterclockwise,
-    which is reverse to the definition of the yaw angle (clockwise).
-
-    A refactor is ongoing to make the three coordinate systems
-    easier to understand and convert between each other.
+    and the yaw is around the z axis, thus the rotation axis=2. The yaw is 0 at
+    the positive direction of x axis, and increases from the positive direction
+    of x to the positive direction of y.
 
     Attributes:
-        tensor (torch.Tensor): Float matrix of N x box_dim.
-        box_dim (int): Integer indicates the dimension of a box
-            Each row is (x, y, z, x_size, y_size, z_size, yaw, ...).
+        tensor (Tensor): Float matrix with shape (N, box_dim).
+        box_dim (int): Integer indicating the dimension of a box. Each row is
+            (x, y, z, x_size, y_size, z_size, yaw, ...).
         with_yaw (bool): If True, the value of yaw will be set to 0 as minmax
             boxes.
     """
     YAW_AXIS = 2
 
     @property
-    def gravity_center(self):
-        """torch.Tensor: A tensor with center of each box in shape (N, 3)."""
-        bottom_center = self.bottom_center
-        gravity_center = torch.zeros_like(bottom_center)
-        gravity_center[:, :2] = bottom_center[:, :2]
-        gravity_center[:, 2] = bottom_center[:, 2] + self.tensor[:, 5] * 0.5
-        return gravity_center
-
-    @property
-    def corners(self):
-        """torch.Tensor: Coordinates of corners of all the boxes
-        in shape (N, 8, 3).
-
-        Convert the boxes to corners in clockwise order, in form of
-        ``(x0y0z0, x0y0z1, x0y1z1, x0y1z0, x1y0z0, x1y0z1, x1y1z1, x1y1z0)``
+    def corners(self) -> Tensor:
+        """Convert boxes to corners in clockwise order, in the form of (x0y0z0,
+        x0y0z1, x0y1z1, x0y1z0, x1y0z0, x1y0z1, x1y1z1, x1y1z0).
 
         .. code-block:: none
 
-                                           up z
-                            front y           ^
-                                 /            |
-                                /             |
-                  (x0, y1, z1) + -----------  + (x1, y1, z1)
-                              /|            / |
-                             / |           /  |
-               (x0, y0, z1) + ----------- +   + (x1, y1, z0)
-                            |  /      .   |  /
-                            | / origin    | /
-               (x0, y0, z0) + ----------- + --------> right x
-                                          (x1, y0, z0)
+                                        up z
+                         front y           ^
+                              /            |
+                             /             |
+               (x0, y1, z1) + -----------  + (x1, y1, z1)
+                           /|            / |
+                          / |           /  |
+            (x0, y0, z1) + ----------- +   + (x1, y1, z0)
+                         |  /      .   |  /
+                         | / origin    | /
+            (x0, y0, z0) + ----------- + --------> right x
+                                       (x1, y0, z0)
+
+        Returns:
+            Tensor: A tensor with 8 corners of each box in shape (N, 8, 3).
         """
         if self.tensor.numel() == 0:
             return torch.empty([0, 8, 3], device=self.tensor.device)
 
         dims = self.dims
         corners_norm = torch.from_numpy(
             np.stack(np.unravel_index(np.arange(8), [2] * 3), axis=1)).to(
@@ -86,30 +75,35 @@
 
         # rotate around z axis
         corners = rotation_3d_in_axis(
             corners, self.tensor[:, 6], axis=self.YAW_AXIS)
         corners += self.tensor[:, :3].view(-1, 1, 3)
         return corners
 
-    def rotate(self, angle, points=None):
+    def rotate(
+        self,
+        angle: Union[Tensor, np.ndarray, float],
+        points: Optional[Union[Tensor, np.ndarray, BasePoints]] = None
+    ) -> Union[Tuple[Tensor, Tensor], Tuple[np.ndarray, np.ndarray], Tuple[
+            BasePoints, Tensor], None]:
         """Rotate boxes with points (optional) with the given angle or rotation
         matrix.
 
         Args:
-            angle (float | torch.Tensor | np.ndarray):
-                Rotation angle or rotation matrix.
-            points (torch.Tensor | np.ndarray | :obj:`BasePoints`, optional):
+            angle (Tensor or np.ndarray or float): Rotation angle or rotation
+                matrix.
+            points (Tensor or np.ndarray or :obj:`BasePoints`, optional):
                 Points to rotate. Defaults to None.
 
         Returns:
-            tuple or None: When ``points`` is None, the function returns
-                None, otherwise it returns the rotated points and the
-                rotation matrix ``rot_mat_T``.
+            tuple or None: When ``points`` is None, the function returns None,
+            otherwise it returns the rotated points and the rotation matrix
+            ``rot_mat_T``.
         """
-        if not isinstance(angle, torch.Tensor):
+        if not isinstance(angle, Tensor):
             angle = self.tensor.new_tensor(angle)
 
         assert angle.shape == torch.Size([3, 3]) or angle.numel() == 1, \
             f'invalid rotation angle shape {angle.shape}'
 
         if angle.numel() == 1:
             self.tensor[:, 0:3], rot_mat_T = rotation_3d_in_axis(
@@ -135,100 +129,116 @@
                     dim=1, keepdim=True)[0]
             new_y_size = corners_rot[..., 1].max(
                 dim=1, keepdim=True)[0] - corners_rot[..., 1].min(
                     dim=1, keepdim=True)[0]
             self.tensor[:, 3:5] = torch.cat((new_x_size, new_y_size), dim=-1)
 
         if points is not None:
-            if isinstance(points, torch.Tensor):
+            if isinstance(points, Tensor):
                 points[:, :3] = points[:, :3] @ rot_mat_T
             elif isinstance(points, np.ndarray):
                 rot_mat_T = rot_mat_T.cpu().numpy()
                 points[:, :3] = np.dot(points[:, :3], rot_mat_T)
             elif isinstance(points, BasePoints):
                 points.rotate(rot_mat_T)
             else:
                 raise ValueError
             return points, rot_mat_T
 
-    def flip(self, bev_direction='horizontal', points=None):
+    def flip(
+        self,
+        bev_direction: str = 'horizontal',
+        points: Optional[Union[Tensor, np.ndarray, BasePoints]] = None
+    ) -> Union[Tensor, np.ndarray, BasePoints, None]:
         """Flip the boxes in BEV along given BEV direction.
 
-        In Depth coordinates, it flips x (horizontal) or y (vertical) axis.
+        In Depth coordinates, it flips the x (horizontal) or y (vertical) axis.
 
         Args:
-            bev_direction (str, optional): Flip direction
-                (horizontal or vertical). Defaults to 'horizontal'.
-            points (torch.Tensor | np.ndarray | :obj:`BasePoints`, optional):
+            bev_direction (str): Direction by which to flip. Can be chosen from
+                'horizontal' and 'vertical'. Defaults to 'horizontal'.
+            points (Tensor or np.ndarray or :obj:`BasePoints`, optional):
                 Points to flip. Defaults to None.
 
         Returns:
-            torch.Tensor, numpy.ndarray or None: Flipped points.
+            Tensor or np.ndarray or :obj:`BasePoints` or None: When ``points``
+            is None, the function returns None, otherwise it returns the
+            flipped points.
         """
         assert bev_direction in ('horizontal', 'vertical')
         if bev_direction == 'horizontal':
             self.tensor[:, 0::7] = -self.tensor[:, 0::7]
             if self.with_yaw:
                 self.tensor[:, 6] = -self.tensor[:, 6] + np.pi
         elif bev_direction == 'vertical':
             self.tensor[:, 1::7] = -self.tensor[:, 1::7]
             if self.with_yaw:
                 self.tensor[:, 6] = -self.tensor[:, 6]
 
         if points is not None:
-            assert isinstance(points, (torch.Tensor, np.ndarray, BasePoints))
-            if isinstance(points, (torch.Tensor, np.ndarray)):
+            assert isinstance(points, (Tensor, np.ndarray, BasePoints))
+            if isinstance(points, (Tensor, np.ndarray)):
                 if bev_direction == 'horizontal':
                     points[:, 0] = -points[:, 0]
                 elif bev_direction == 'vertical':
                     points[:, 1] = -points[:, 1]
             elif isinstance(points, BasePoints):
                 points.flip(bev_direction)
             return points
 
-    def convert_to(self, dst, rt_mat=None):
+    def convert_to(self,
+                   dst: int,
+                   rt_mat: Optional[Union[Tensor, np.ndarray]] = None,
+                   correct_yaw: bool = False) -> 'BaseInstance3DBoxes':
         """Convert self to ``dst`` mode.
 
         Args:
-            dst (:obj:`Box3DMode`): The target Box mode.
-            rt_mat (np.ndarray | torch.Tensor, optional): The rotation and
+            dst (int): The target Box mode.
+            rt_mat (Tensor or np.ndarray, optional): The rotation and
                 translation matrix between different coordinates.
-                Defaults to None.
-                The conversion from ``src`` coordinates to ``dst`` coordinates
-                usually comes along the change of sensors, e.g., from camera
-                to LiDAR. This requires a transformation matrix.
+                Defaults to None. The conversion from ``src`` coordinates to
+                ``dst`` coordinates usually comes along the change of sensors,
+                e.g., from camera to LiDAR. This requires a transformation
+                matrix.
+            correct_yaw (bool): Whether to convert the yaw angle to the target
+                coordinate. Defaults to False.
 
         Returns:
-            :obj:`DepthInstance3DBoxes`:
-                The converted box of the same type in the ``dst`` mode.
+            :obj:`BaseInstance3DBoxes`: The converted box of the same type in
+            the ``dst`` mode.
         """
         from .box_3d_mode import Box3DMode
         return Box3DMode.convert(
-            box=self, src=Box3DMode.DEPTH, dst=dst, rt_mat=rt_mat)
-
-    def enlarged_box(self, extra_width):
-        """Enlarge the length, width and height boxes.
+            box=self,
+            src=Box3DMode.DEPTH,
+            dst=dst,
+            rt_mat=rt_mat,
+            correct_yaw=correct_yaw)
+
+    def enlarged_box(
+            self, extra_width: Union[float, Tensor]) -> 'DepthInstance3DBoxes':
+        """Enlarge the length, width and height of boxes.
 
         Args:
-            extra_width (float | torch.Tensor): Extra width to enlarge the box.
+            extra_width (float or Tensor): Extra width to enlarge the box.
 
         Returns:
             :obj:`DepthInstance3DBoxes`: Enlarged boxes.
         """
         enlarged_boxes = self.tensor.clone()
         enlarged_boxes[:, 3:6] += extra_width * 2
         # bottom center z minus extra_width
         enlarged_boxes[:, 2] -= extra_width
         return self.new_box(enlarged_boxes)
 
-    def get_surface_line_center(self):
+    def get_surface_line_center(self) -> Tuple[Tensor, Tensor]:
         """Compute surface and line center of bounding boxes.
 
         Returns:
-            torch.Tensor: Surface and line center of bounding boxes.
+            Tuple[Tensor, Tensor]: Surface and line center of bounding boxes.
         """
         obj_size = self.dims
         center = self.gravity_center.view(-1, 1, 3)
         batch_size = center.shape[0]
 
         rot_sin = torch.sin(-self.yaw)
         rot_cos = torch.cos(-self.yaw)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/structures/bbox_3d/lidar_box3d.py` & `mmdet3d-1.1.1/mmdet3d/structures/bbox_3d/lidar_box3d.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,113 +1,109 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Optional, Tuple, Union
+
 import numpy as np
 import torch
+from torch import Tensor
 
 from mmdet3d.structures.points import BasePoints
 from .base_box3d import BaseInstance3DBoxes
 from .utils import rotation_3d_in_axis
 
 
 class LiDARInstance3DBoxes(BaseInstance3DBoxes):
     """3D boxes of instances in LIDAR coordinates.
 
     Coordinates in LiDAR:
 
     .. code-block:: none
 
-                                up z    x front (yaw=0)
-                                   ^   ^
-                                   |  /
-                                   | /
-       (yaw=0.5*pi) left y <------ 0
+                                 up z    x front (yaw=0)
+                                    ^   ^
+                                    |  /
+                                    | /
+        (yaw=0.5*pi) left y <------ 0
 
     The relative coordinate of bottom center in a LiDAR box is (0.5, 0.5, 0),
-    and the yaw is around the z axis, thus the rotation axis=2.
-    The yaw is 0 at the positive direction of x axis, and increases from
-    the positive direction of x to the positive direction of y.
-
-    A refactor is ongoing to make the three coordinate systems
-    easier to understand and convert between each other.
+    and the yaw is around the z axis, thus the rotation axis=2. The yaw is 0 at
+    the positive direction of x axis, and increases from the positive direction
+    of x to the positive direction of y.
 
     Attributes:
-        tensor (torch.Tensor): Float matrix of N x box_dim.
-        box_dim (int): Integer indicating the dimension of a box.
-            Each row is (x, y, z, x_size, y_size, z_size, yaw, ...).
+        tensor (Tensor): Float matrix with shape (N, box_dim).
+        box_dim (int): Integer indicating the dimension of a box. Each row is
+            (x, y, z, x_size, y_size, z_size, yaw, ...).
         with_yaw (bool): If True, the value of yaw will be set to 0 as minmax
             boxes.
     """
     YAW_AXIS = 2
 
     @property
-    def gravity_center(self):
-        """torch.Tensor: A tensor with center of each box in shape (N, 3)."""
-        bottom_center = self.bottom_center
-        gravity_center = torch.zeros_like(bottom_center)
-        gravity_center[:, :2] = bottom_center[:, :2]
-        gravity_center[:, 2] = bottom_center[:, 2] + self.tensor[:, 5] * 0.5
-        return gravity_center
-
-    @property
-    def corners(self):
-        """torch.Tensor: Coordinates of corners of all the boxes
-        in shape (N, 8, 3).
-
-        Convert the boxes to corners in clockwise order, in form of
-        ``(x0y0z0, x0y0z1, x0y1z1, x0y1z0, x1y0z0, x1y0z1, x1y1z1, x1y1z0)``
+    def corners(self) -> Tensor:
+        """Convert boxes to corners in clockwise order, in the form of (x0y0z0,
+        x0y0z1, x0y1z1, x0y1z0, x1y0z0, x1y0z1, x1y1z1, x1y1z0).
 
         .. code-block:: none
 
                                            up z
                             front x           ^
                                  /            |
                                 /             |
                   (x1, y0, z1) + -----------  + (x1, y1, z1)
                               /|            / |
                              / |           /  |
                (x0, y0, z1) + ----------- +   + (x1, y1, z0)
                             |  /      .   |  /
                             | / origin    | /
-            left y<-------- + ----------- + (x0, y1, z0)
+            left y <------- + ----------- + (x0, y1, z0)
                 (x0, y0, z0)
+
+        Returns:
+            Tensor: A tensor with 8 corners of each box in shape (N, 8, 3).
         """
         if self.tensor.numel() == 0:
             return torch.empty([0, 8, 3], device=self.tensor.device)
 
         dims = self.dims
         corners_norm = torch.from_numpy(
             np.stack(np.unravel_index(np.arange(8), [2] * 3), axis=1)).to(
                 device=dims.device, dtype=dims.dtype)
 
         corners_norm = corners_norm[[0, 1, 3, 2, 4, 5, 7, 6]]
-        # use relative origin [0.5, 0.5, 0]
+        # use relative origin (0.5, 0.5, 0)
         corners_norm = corners_norm - dims.new_tensor([0.5, 0.5, 0])
         corners = dims.view([-1, 1, 3]) * corners_norm.reshape([1, 8, 3])
 
         # rotate around z axis
         corners = rotation_3d_in_axis(
             corners, self.tensor[:, 6], axis=self.YAW_AXIS)
         corners += self.tensor[:, :3].view(-1, 1, 3)
         return corners
 
-    def rotate(self, angle, points=None):
+    def rotate(
+        self,
+        angle: Union[Tensor, np.ndarray, float],
+        points: Optional[Union[Tensor, np.ndarray, BasePoints]] = None
+    ) -> Union[Tuple[Tensor, Tensor], Tuple[np.ndarray, np.ndarray], Tuple[
+            BasePoints, Tensor], None]:
         """Rotate boxes with points (optional) with the given angle or rotation
         matrix.
 
         Args:
-            angles (float | torch.Tensor | np.ndarray):
-                Rotation angle or rotation matrix.
-            points (torch.Tensor | np.ndarray | :obj:`BasePoints`, optional):
+            angle (Tensor or np.ndarray or float): Rotation angle or rotation
+                matrix.
+            points (Tensor or np.ndarray or :obj:`BasePoints`, optional):
                 Points to rotate. Defaults to None.
 
         Returns:
-            tuple or None: When ``points`` is None, the function returns
-                None, otherwise it returns the rotated points and the
-                rotation matrix ``rot_mat_T``.
+            tuple or None: When ``points`` is None, the function returns None,
+            otherwise it returns the rotated points and the rotation matrix
+            ``rot_mat_T``.
         """
-        if not isinstance(angle, torch.Tensor):
+        if not isinstance(angle, Tensor):
             angle = self.tensor.new_tensor(angle)
 
         assert angle.shape == torch.Size([3, 3]) or angle.numel() == 1, \
             f'invalid rotation angle shape {angle.shape}'
 
         if angle.numel() == 1:
             self.tensor[:, 0:3], rot_mat_T = rotation_3d_in_axis(
@@ -125,89 +121,101 @@
         self.tensor[:, 6] += angle
 
         if self.tensor.shape[1] == 9:
             # rotate velo vector
             self.tensor[:, 7:9] = self.tensor[:, 7:9] @ rot_mat_T[:2, :2]
 
         if points is not None:
-            if isinstance(points, torch.Tensor):
+            if isinstance(points, Tensor):
                 points[:, :3] = points[:, :3] @ rot_mat_T
             elif isinstance(points, np.ndarray):
                 rot_mat_T = rot_mat_T.cpu().numpy()
                 points[:, :3] = np.dot(points[:, :3], rot_mat_T)
             elif isinstance(points, BasePoints):
                 points.rotate(rot_mat_T)
             else:
                 raise ValueError
             return points, rot_mat_T
 
-    def flip(self, bev_direction='horizontal', points=None):
+    def flip(
+        self,
+        bev_direction: str = 'horizontal',
+        points: Optional[Union[Tensor, np.ndarray, BasePoints]] = None
+    ) -> Union[Tensor, np.ndarray, BasePoints, None]:
         """Flip the boxes in BEV along given BEV direction.
 
         In LIDAR coordinates, it flips the y (horizontal) or x (vertical) axis.
 
         Args:
-            bev_direction (str): Flip direction (horizontal or vertical).
-            points (torch.Tensor | np.ndarray | :obj:`BasePoints`, optional):
+            bev_direction (str): Direction by which to flip. Can be chosen from
+                'horizontal' and 'vertical'. Defaults to 'horizontal'.
+            points (Tensor or np.ndarray or :obj:`BasePoints`, optional):
                 Points to flip. Defaults to None.
 
         Returns:
-            torch.Tensor, numpy.ndarray or None: Flipped points.
+            Tensor or np.ndarray or :obj:`BasePoints` or None: When ``points``
+            is None, the function returns None, otherwise it returns the
+            flipped points.
         """
         assert bev_direction in ('horizontal', 'vertical')
         if bev_direction == 'horizontal':
             self.tensor[:, 1::7] = -self.tensor[:, 1::7]
             if self.with_yaw:
                 self.tensor[:, 6] = -self.tensor[:, 6]
         elif bev_direction == 'vertical':
             self.tensor[:, 0::7] = -self.tensor[:, 0::7]
             if self.with_yaw:
                 self.tensor[:, 6] = -self.tensor[:, 6] + np.pi
 
         if points is not None:
-            assert isinstance(points, (torch.Tensor, np.ndarray, BasePoints))
-            if isinstance(points, (torch.Tensor, np.ndarray)):
+            assert isinstance(points, (Tensor, np.ndarray, BasePoints))
+            if isinstance(points, (Tensor, np.ndarray)):
                 if bev_direction == 'horizontal':
                     points[:, 1] = -points[:, 1]
                 elif bev_direction == 'vertical':
                     points[:, 0] = -points[:, 0]
             elif isinstance(points, BasePoints):
                 points.flip(bev_direction)
             return points
 
-    def convert_to(self, dst, rt_mat=None, correct_yaw=False):
+    def convert_to(self,
+                   dst: int,
+                   rt_mat: Optional[Union[Tensor, np.ndarray]] = None,
+                   correct_yaw: bool = False) -> 'BaseInstance3DBoxes':
         """Convert self to ``dst`` mode.
 
         Args:
-            dst (:obj:`Box3DMode`): the target Box mode
-            rt_mat (np.ndarray | torch.Tensor, optional): The rotation and
+            dst (int): The target Box mode.
+            rt_mat (Tensor or np.ndarray, optional): The rotation and
                 translation matrix between different coordinates.
-                Defaults to None.
-                The conversion from ``src`` coordinates to ``dst`` coordinates
-                usually comes along the change of sensors, e.g., from camera
-                to LiDAR. This requires a transformation matrix.
-            correct_yaw (bool): If convert the yaw angle to the target
+                Defaults to None. The conversion from ``src`` coordinates to
+                ``dst`` coordinates usually comes along the change of sensors,
+                e.g., from camera to LiDAR. This requires a transformation
+                matrix.
+            correct_yaw (bool): Whether to convert the yaw angle to the target
                 coordinate. Defaults to False.
+
         Returns:
-            :obj:`BaseInstance3DBoxes`:
-                The converted box of the same type in the ``dst`` mode.
+            :obj:`BaseInstance3DBoxes`: The converted box of the same type in
+            the ``dst`` mode.
         """
         from .box_3d_mode import Box3DMode
         return Box3DMode.convert(
             box=self,
             src=Box3DMode.LIDAR,
             dst=dst,
             rt_mat=rt_mat,
             correct_yaw=correct_yaw)
 
-    def enlarged_box(self, extra_width):
-        """Enlarge the length, width and height boxes.
+    def enlarged_box(
+            self, extra_width: Union[float, Tensor]) -> 'LiDARInstance3DBoxes':
+        """Enlarge the length, width and height of boxes.
 
         Args:
-            extra_width (float | torch.Tensor): Extra width to enlarge the box.
+            extra_width (float or Tensor): Extra width to enlarge the box.
 
         Returns:
             :obj:`LiDARInstance3DBoxes`: Enlarged boxes.
         """
         enlarged_boxes = self.tensor.clone()
         enlarged_boxes[:, 3:6] += extra_width * 2
         # bottom center z minus extra_width
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/structures/bbox_3d/utils.py` & `mmdet3d-1.1.1/mmdet3d/structures/bbox_3d/utils.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,68 +1,75 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from logging import warning
+from typing import Tuple, Union
 
 import numpy as np
 import torch
+from torch import Tensor
 
-from mmdet3d.utils.array_converter import array_converter
+from mmdet3d.utils import array_converter
 
 
 @array_converter(apply_to=('val', ))
-def limit_period(val, offset=0.5, period=np.pi):
+def limit_period(val: Union[np.ndarray, Tensor],
+                 offset: float = 0.5,
+                 period: float = np.pi) -> Union[np.ndarray, Tensor]:
     """Limit the value into a period for periodic function.
 
     Args:
-        val (torch.Tensor | np.ndarray): The value to be converted.
-        offset (float, optional): Offset to set the value range.
-            Defaults to 0.5.
-        period ([type], optional): Period of the value. Defaults to np.pi.
+        val (np.ndarray or Tensor): The value to be converted.
+        offset (float): Offset to set the value range. Defaults to 0.5.
+        period (float): Period of the value. Defaults to np.pi.
 
     Returns:
-        (torch.Tensor | np.ndarray): Value in the range of
-            [-offset * period, (1-offset) * period]
+        np.ndarray or Tensor: Value in the range of
+        [-offset * period, (1-offset) * period].
     """
     limited_val = val - torch.floor(val / period + offset) * period
     return limited_val
 
 
 @array_converter(apply_to=('points', 'angles'))
-def rotation_3d_in_axis(points,
-                        angles,
-                        axis=0,
-                        return_mat=False,
-                        clockwise=False):
+def rotation_3d_in_axis(
+    points: Union[np.ndarray, Tensor],
+    angles: Union[np.ndarray, Tensor, float],
+    axis: int = 0,
+    return_mat: bool = False,
+    clockwise: bool = False
+) -> Union[Tuple[np.ndarray, np.ndarray], Tuple[Tensor, Tensor], np.ndarray,
+           Tensor]:
     """Rotate points by angles according to axis.
 
     Args:
-        points (np.ndarray | torch.Tensor | list | tuple ):
-            Points of shape (N, M, 3).
-        angles (np.ndarray | torch.Tensor | list | tuple | float):
-            Vector of angles in shape (N,)
-        axis (int, optional): The axis to be rotated. Defaults to 0.
-        return_mat: Whether or not return the rotation matrix (transposed).
-            Defaults to False.
-        clockwise: Whether the rotation is clockwise. Defaults to False.
+        points (np.ndarray or Tensor): Points with shape (N, M, 3).
+        angles (np.ndarray or Tensor or float): Vector of angles with shape
+            (N, ).
+        axis (int): The axis to be rotated. Defaults to 0.
+        return_mat (bool): Whether or not to return the rotation matrix
+            (transposed). Defaults to False.
+        clockwise (bool): Whether the rotation is clockwise. Defaults to False.
 
     Raises:
-        ValueError: when the axis is not in range [0, 1, 2], it will
-            raise value error.
+        ValueError: When the axis is not in range [-3, -2, -1, 0, 1, 2], it
+            will raise ValueError.
 
     Returns:
-        (torch.Tensor | np.ndarray): Rotated points in shape (N, M, 3).
+        Tuple[np.ndarray, np.ndarray] or Tuple[Tensor, Tensor] or np.ndarray or
+        Tensor: Rotated points with shape (N, M, 3) and rotation matrix with
+        shape (N, 3, 3).
     """
     batch_free = len(points.shape) == 2
     if batch_free:
         points = points[None]
 
     if isinstance(angles, float) or len(angles.shape) == 0:
         angles = torch.full(points.shape[:1], angles)
 
-    assert len(points.shape) == 3 and len(angles.shape) == 1 \
-        and points.shape[0] == angles.shape[0], f'Incorrect shape of points ' \
+    assert len(points.shape) == 3 and len(angles.shape) == 1 and \
+        points.shape[0] == angles.shape[0], 'Incorrect shape of points ' \
         f'angles: {points.shape}, {angles.shape}'
 
     assert points.shape[-1] in [2, 3], \
         f'Points size should be 2 or 3 instead of {points.shape[-1]}'
 
     rot_sin = torch.sin(angles)
     rot_cos = torch.cos(angles)
@@ -85,16 +92,16 @@
         elif axis == 0 or axis == -3:
             rot_mat_T = torch.stack([
                 torch.stack([ones, zeros, zeros]),
                 torch.stack([zeros, rot_cos, rot_sin]),
                 torch.stack([zeros, -rot_sin, rot_cos])
             ])
         else:
-            raise ValueError(f'axis should in range '
-                             f'[-3, -2, -1, 0, 1, 2], got {axis}')
+            raise ValueError(
+                f'axis should in range [-3, -2, -1, 0, 1, 2], got {axis}')
     else:
         rot_mat_T = torch.stack([
             torch.stack([rot_cos, rot_sin]),
             torch.stack([-rot_sin, rot_cos])
         ])
 
     if clockwise:
@@ -114,45 +121,46 @@
             rot_mat_T = rot_mat_T.squeeze(0)
         return points_new, rot_mat_T
     else:
         return points_new
 
 
 @array_converter(apply_to=('boxes_xywhr', ))
-def xywhr2xyxyr(boxes_xywhr):
+def xywhr2xyxyr(
+        boxes_xywhr: Union[Tensor, np.ndarray]) -> Union[Tensor, np.ndarray]:
     """Convert a rotated boxes in XYWHR format to XYXYR format.
 
     Args:
-        boxes_xywhr (torch.Tensor | np.ndarray): Rotated boxes in XYWHR format.
+        boxes_xywhr (Tensor or np.ndarray): Rotated boxes in XYWHR format.
 
     Returns:
-        (torch.Tensor | np.ndarray): Converted boxes in XYXYR format.
+        Tensor or np.ndarray: Converted boxes in XYXYR format.
     """
     boxes = torch.zeros_like(boxes_xywhr)
     half_w = boxes_xywhr[..., 2] / 2
     half_h = boxes_xywhr[..., 3] / 2
 
     boxes[..., 0] = boxes_xywhr[..., 0] - half_w
     boxes[..., 1] = boxes_xywhr[..., 1] - half_h
     boxes[..., 2] = boxes_xywhr[..., 0] + half_w
     boxes[..., 3] = boxes_xywhr[..., 1] + half_h
     boxes[..., 4] = boxes_xywhr[..., 4]
     return boxes
 
 
-def get_box_type(box_type):
+def get_box_type(box_type: str) -> Tuple[type, int]:
     """Get the type and mode of box structure.
 
     Args:
-        box_type (str): The type of box structure.
-            The valid value are "LiDAR", "Camera", or "Depth".
+        box_type (str): The type of box structure. The valid value are "LiDAR",
+            "Camera" and "Depth".
 
     Raises:
-        ValueError: A ValueError is raised when `box_type`
-            does not belong to the three valid types.
+        ValueError: A ValueError is raised when ``box_type`` does not belong to
+            the three valid types.
 
     Returns:
         tuple: Box type and box mode.
     """
     from .box_3d_mode import (Box3DMode, CameraInstance3DBoxes,
                               DepthInstance3DBoxes, LiDARInstance3DBoxes)
     box_type_lower = box_type.lower()
@@ -162,44 +170,47 @@
     elif box_type_lower == 'camera':
         box_type_3d = CameraInstance3DBoxes
         box_mode_3d = Box3DMode.CAM
     elif box_type_lower == 'depth':
         box_type_3d = DepthInstance3DBoxes
         box_mode_3d = Box3DMode.DEPTH
     else:
-        raise ValueError('Only "box_type" of "camera", "lidar", "depth"'
-                         f' are supported, got {box_type}')
+        raise ValueError('Only "box_type" of "camera", "lidar", "depth" are '
+                         f'supported, got {box_type}')
 
     return box_type_3d, box_mode_3d
 
 
 @array_converter(apply_to=('points_3d', 'proj_mat'))
-def points_cam2img(points_3d, proj_mat, with_depth=False):
+def points_cam2img(points_3d: Union[Tensor, np.ndarray],
+                   proj_mat: Union[Tensor, np.ndarray],
+                   with_depth: bool = False) -> Union[Tensor, np.ndarray]:
     """Project points in camera coordinates to image coordinates.
 
     Args:
-        points_3d (torch.Tensor | np.ndarray): Points in shape (N, 3)
-        proj_mat (torch.Tensor | np.ndarray):
-            Transformation matrix between coordinates.
-        with_depth (bool, optional): Whether to keep depth in the output.
+        points_3d (Tensor or np.ndarray): Points in shape (N, 3).
+        proj_mat (Tensor or np.ndarray): Transformation matrix between
+            coordinates.
+        with_depth (bool): Whether to keep depth in the output.
             Defaults to False.
 
     Returns:
-        (torch.Tensor | np.ndarray): Points in image coordinates,
-            with shape [N, 2] if `with_depth=False`, else [N, 3].
+        Tensor or np.ndarray: Points in image coordinates with shape [N, 2] if
+        ``with_depth=False``, else [N, 3].
     """
     points_shape = list(points_3d.shape)
     points_shape[-1] = 1
 
-    assert len(proj_mat.shape) == 2, 'The dimension of the projection'\
-        f' matrix should be 2 instead of {len(proj_mat.shape)}.'
+    assert len(proj_mat.shape) == 2, \
+        'The dimension of the projection matrix should be 2 ' \
+        f'instead of {len(proj_mat.shape)}.'
     d1, d2 = proj_mat.shape[:2]
-    assert (d1 == 3 and d2 == 3) or (d1 == 3 and d2 == 4) or (
-        d1 == 4 and d2 == 4), 'The shape of the projection matrix'\
-        f' ({d1}*{d2}) is not supported.'
+    assert (d1 == 3 and d2 == 3) or (d1 == 3 and d2 == 4) or \
+        (d1 == 4 and d2 == 4), 'The shape of the projection matrix ' \
+        f'({d1}*{d2}) is not supported.'
     if d1 == 3:
         proj_mat_expanded = torch.eye(
             4, device=proj_mat.device, dtype=proj_mat.dtype)
         proj_mat_expanded[:d1, :d2] = proj_mat
         proj_mat = proj_mat_expanded
 
     # previous implementation use new_zeros, new_one yields better results
@@ -211,26 +222,28 @@
     if with_depth:
         point_2d_res = torch.cat([point_2d_res, point_2d[..., 2:3]], dim=-1)
 
     return point_2d_res
 
 
 @array_converter(apply_to=('points', 'cam2img'))
-def points_img2cam(points, cam2img):
+def points_img2cam(
+        points: Union[Tensor, np.ndarray],
+        cam2img: Union[Tensor, np.ndarray]) -> Union[Tensor, np.ndarray]:
     """Project points in image coordinates to camera coordinates.
 
     Args:
-        points (torch.Tensor): 2.5D points in 2D images, [N, 3],
-            3 corresponds with x, y in the image and depth.
-        cam2img (torch.Tensor): Camera intrinsic matrix. The shape can be
-            [3, 3], [3, 4] or [4, 4].
+        points (Tensor or np.ndarray): 2.5D points in 2D images with shape
+            [N, 3], 3 corresponds with x, y in the image and depth.
+        cam2img (Tensor or np.ndarray): Camera intrinsic matrix. The shape can
+            be [3, 3], [3, 4] or [4, 4].
 
     Returns:
-        torch.Tensor: points in 3D space. [N, 3],
-            3 corresponds with x, y, z in 3D space.
+        Tensor or np.ndarray: Points in 3D space with shape [N, 3], 3
+        corresponds with x, y, z in 3D space.
     """
     assert cam2img.shape[0] <= 4
     assert cam2img.shape[1] <= 4
     assert points.shape[1] == 3
 
     xys = points[:, :2]
     depths = points[:, 2].view(-1, 1)
@@ -256,27 +269,26 @@
         2. change orientation from local yaw to global yaw
         3. convert yaw by (np.pi / 2 - yaw)
 
     After applying this function, we can project and draw it on 2D images.
 
     Args:
         cam_box (:obj:`CameraInstance3DBoxes`): 3D bbox in camera coordinate
-            system before conversion. Could be gt bbox loaded from dataset
-            or network prediction output.
+            system before conversion. Could be gt bbox loaded from dataset or
+            network prediction output.
 
     Returns:
         :obj:`CameraInstance3DBoxes`: Box after conversion.
     """
     warning.warn('DeprecationWarning: The hack of yaw and dimension in the '
                  'monocular 3D detection on nuScenes has been removed. The '
                  'function mono_cam_box2vis will be deprecated.')
-    from . import CameraInstance3DBoxes
+    from .cam_box3d import CameraInstance3DBoxes
     assert isinstance(cam_box, CameraInstance3DBoxes), \
         'input bbox should be CameraInstance3DBoxes!'
-
     loc = cam_box.gravity_center
     dim = cam_box.dims
     yaw = cam_box.yaw
     feats = cam_box.tensor[:, 7:]
     # rotate along x-axis for np.pi / 2
     # see also here: https://github.com/open-mmlab/mmdetection3d/blob/master/mmdet3d/datasets/nuscenes_mono_dataset.py#L557  # noqa
     dim[:, [1, 2]] = dim[:, [2, 1]]
@@ -290,64 +302,62 @@
     cam_box = torch.cat([loc, dim, yaw[:, None], feats], dim=1)
     cam_box = CameraInstance3DBoxes(
         cam_box, box_dim=cam_box.shape[-1], origin=(0.5, 0.5, 0.5))
 
     return cam_box
 
 
-def get_proj_mat_by_coord_type(img_meta, coord_type):
+def get_proj_mat_by_coord_type(img_meta: dict, coord_type: str) -> Tensor:
     """Obtain image features using points.
 
     Args:
-        img_meta (dict): Meta info.
-        coord_type (str): 'DEPTH' or 'CAMERA' or 'LIDAR'.
-            Can be case-insensitive.
+        img_meta (dict): Meta information.
+        coord_type (str): 'DEPTH' or 'CAMERA' or 'LIDAR'. Can be case-
+            insensitive.
 
     Returns:
-        torch.Tensor: transformation matrix.
+        Tensor: Transformation matrix.
     """
     coord_type = coord_type.upper()
     mapping = {'LIDAR': 'lidar2img', 'DEPTH': 'depth2img', 'CAMERA': 'cam2img'}
     assert coord_type in mapping.keys()
     return img_meta[mapping[coord_type]]
 
 
-def yaw2local(yaw, loc):
+def yaw2local(yaw: Tensor, loc: Tensor) -> Tensor:
     """Transform global yaw to local yaw (alpha in kitti) in camera
     coordinates, ranges from -pi to pi.
 
     Args:
-        yaw (torch.Tensor): A vector with local yaw of each box.
-            shape: (N, )
-        loc (torch.Tensor): gravity center of each box.
-            shape: (N, 3)
+        yaw (Tensor): A vector with local yaw of each box in shape (N, ).
+        loc (Tensor): Gravity center of each box in shape (N, 3).
 
     Returns:
-        torch.Tensor: local yaw (alpha in kitti).
+        Tensor: Local yaw (alpha in kitti).
     """
     local_yaw = yaw - torch.atan2(loc[:, 0], loc[:, 2])
     larger_idx = (local_yaw > np.pi).nonzero(as_tuple=False)
     small_idx = (local_yaw < -np.pi).nonzero(as_tuple=False)
     if len(larger_idx) != 0:
         local_yaw[larger_idx] -= 2 * np.pi
     if len(small_idx) != 0:
         local_yaw[small_idx] += 2 * np.pi
 
     return local_yaw
 
 
-def get_lidar2img(cam2img, lidar2cam):
+def get_lidar2img(cam2img: Tensor, lidar2cam: Tensor) -> Tensor:
     """Get the projection matrix of lidar2img.
 
     Args:
         cam2img (torch.Tensor): A 3x3 or 4x4 projection matrix.
         lidar2cam (torch.Tensor): A 3x3 or 4x4 projection matrix.
 
     Returns:
-        torch.Tensor: transformation matrix with shape 4x4.
+        Tensor: Transformation matrix with shape 4x4.
     """
     if cam2img.shape == (3, 3):
         temp = cam2img.new_zeros(4, 4)
         temp[:3, :3] = cam2img
         cam2img = temp
 
     if lidar2cam.shape == (3, 3):
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/structures/det3d_data_sample.py` & `mmdet3d-1.1.1/mmdet3d/structures/det3d_data_sample.py`

 * *Files 10% similar despite different names*

```diff
@@ -10,227 +10,204 @@
 
 class Det3DDataSample(DetDataSample):
     """A data structure interface of MMDetection3D. They are used as interfaces
     between different components.
 
     The attributes in ``Det3DDataSample`` are divided into several parts:
 
-        - ``proposals``(InstanceData): Region proposals used in two-stage
-            detectors.
-        - ``ignored_instances``(InstanceData): Instances to be ignored during
-            training/testing.
-        - ``gt_instances_3d``(InstanceData): Ground truth of 3D instance
-            annotations.
-        - ``gt_instances``(InstanceData): Ground truth of 2D instance
-            annotations.
-        - ``pred_instances_3d``(InstanceData): 3D instances of model
-            predictions.
-            - For point-cloud 3d object detection task whose input modality
-            is `use_lidar=True, use_camera=False`, the 3D predictions results
-            are saved in `pred_instances_3d`.
-            - For vision-only(monocular/multi-view) 3D object detection task
+        - ``proposals`` (InstanceData): Region proposals used in two-stage
+          detectors.
+        - ``ignored_instances`` (InstanceData): Instances to be ignored during
+          training/testing.
+        - ``gt_instances_3d`` (InstanceData): Ground truth of 3D instance
+          annotations.
+        - ``gt_instances`` (InstanceData): Ground truth of 2D instance
+          annotations.
+        - ``pred_instances_3d`` (InstanceData): 3D instances of model
+          predictions.
+          - For point-cloud 3D object detection task whose input modality is
+            `use_lidar=True, use_camera=False`, the 3D predictions results are
+            saved in `pred_instances_3d`.
+          - For vision-only (monocular/multi-view) 3D object detection task
             whose input modality is `use_lidar=False, use_camera=True`, the 3D
             predictions are saved in `pred_instances_3d`.
-        - ``pred_instances``(InstanceData): 2D instances of model
-            predictions.
-            -  For multi-modality 3D detection task whose input modality is
-            `use_lidar=True, use_camera=True`, the 2D predictions
-            are saved in `pred_instances`.
-        - ``pts_pred_instances_3d``(InstanceData): 3D instances of model
-            predictions based on point cloud.
-            -  For multi-modality 3D detection task whose input modality is
+        - ``pred_instances`` (InstanceData): 2D instances of model predictions.
+          - For multi-modality 3D detection task whose input modality is
+            `use_lidar=True, use_camera=True`, the 2D predictions are saved in
+            `pred_instances`.
+        - ``pts_pred_instances_3d`` (InstanceData): 3D instances of model
+          predictions based on point cloud.
+          - For multi-modality 3D detection task whose input modality is
             `use_lidar=True, use_camera=True`, the 3D predictions based on
             point cloud are saved in `pts_pred_instances_3d` to distinguish
             with `img_pred_instances_3d` which based on image.
-        - ``img_pred_instances_3d``(InstanceData): 3D instances of model
-            predictions based on image.
-            -  For multi-modality 3D detection task whose input modality is
+        - ``img_pred_instances_3d`` (InstanceData): 3D instances of model
+          predictions based on image.
+          - For multi-modality 3D detection task whose input modality is
             `use_lidar=True, use_camera=True`, the 3D predictions based on
             image are saved in `img_pred_instances_3d` to distinguish with
             `pts_pred_instances_3d` which based on point cloud.
-        - ``gt_pts_seg``(PointData): Ground truth of point cloud
-            segmentation.
-        - ``pred_pts_seg``(PointData): Prediction of point cloud
-            segmentation.
-        - ``eval_ann_info``(dict): Raw annotation, which will be passed to
-            evaluator and do the online evaluation.
+        - ``gt_pts_seg`` (PointData): Ground truth of point cloud segmentation.
+        - ``pred_pts_seg`` (PointData): Prediction of point cloud segmentation.
+        - ``eval_ann_info`` (dict or None): Raw annotation, which will be
+          passed to evaluator and do the online evaluation.
 
     Examples:
-    >>> from mmengine.structures import InstanceData
+        >>> import torch
+        >>> from mmengine.structures import InstanceData
 
-    >>> from mmdet3d.structures import Det3DDataSample
-    >>> from mmdet3d.structures import BaseInstance3DBoxes
-
-    >>> data_sample = Det3DDataSample()
-    >>> meta_info = dict(img_shape=(800, 1196, 3),
-    ...     pad_shape=(800, 1216, 3))
-    >>> gt_instances_3d = InstanceData(metainfo=meta_info)
-    >>> gt_instances_3d.bboxes = BaseInstance3DBoxes(torch.rand((5, 7)))
-    >>> gt_instances_3d.labels = torch.randint(0,3,(5, ))
-    >>> data_sample.gt_instances_3d = gt_instances_3d
-    >>> assert 'img_shape' in data_sample.gt_instances_3d.metainfo_keys()
-    >>> print(data_sample)
-    <Det3DDataSample(
-
-        META INFORMATION
-
-        DATA FIELDS
-        _gt_instances_3d: <InstanceData(
+        >>> from mmdet3d.structures import Det3DDataSample
+        >>> from mmdet3d.structures.bbox_3d import BaseInstance3DBoxes
 
+        >>> data_sample = Det3DDataSample()
+        >>> meta_info = dict(
+        ...     img_shape=(800, 1196, 3),
+        ...     pad_shape=(800, 1216, 3))
+        >>> gt_instances_3d = InstanceData(metainfo=meta_info)
+        >>> gt_instances_3d.bboxes_3d = BaseInstance3DBoxes(torch.rand((5, 7)))
+        >>> gt_instances_3d.labels_3d = torch.randint(0, 3, (5,))
+        >>> data_sample.gt_instances_3d = gt_instances_3d
+        >>> assert 'img_shape' in data_sample.gt_instances_3d.metainfo_keys()
+        >>> len(data_sample.gt_instances_3d)
+        5
+        >>> print(data_sample)
+        <Det3DDataSample(
             META INFORMATION
-            pad_shape: (800, 1216, 3)
-            img_shape: (800, 1196, 3)
-
             DATA FIELDS
-            labels: tensor([0, 0, 1, 0, 2])
-            bboxes: BaseInstance3DBoxes(
-            tensor([[0.2874, 0.3078, 0.8368, 0.2326, 0.9845, 0.6199, 0.9944],
-                    [0.6222, 0.8778, 0.7306, 0.3320, 0.3973, 0.7662, 0.7326],
-                    [0.8547, 0.6082, 0.1660, 0.1676, 0.9810, 0.3092, 0.0917],
-                    [0.4686, 0.7007, 0.4428, 0.0672, 0.3319, 0.3033, 0.8519],
-                    [0.9693, 0.5315, 0.4642, 0.9079, 0.2481, 0.1781, 0.9557]]))
-        ) at 0x7fb0d9354280>
-        gt_instances_3d: <InstanceData(
-
+            gt_instances_3d: <InstanceData(
+                    META INFORMATION
+                    img_shape: (800, 1196, 3)
+                    pad_shape: (800, 1216, 3)
+                    DATA FIELDS
+                    labels_3d: tensor([1, 0, 2, 0, 1])
+                    bboxes_3d: BaseInstance3DBoxes(
+                            tensor([[1.9115e-01, 3.6061e-01, 6.7707e-01, 5.2902e-01, 8.0736e-01, 8.2759e-01,
+                                2.4328e-01],
+                                [5.6272e-01, 2.7508e-01, 5.7966e-01, 9.2410e-01, 3.0456e-01, 1.8912e-01,
+                                3.3176e-01],
+                                [8.1069e-01, 2.8684e-01, 7.7689e-01, 9.2397e-02, 5.5849e-01, 3.8007e-01,
+                                4.6719e-01],
+                                [6.6346e-01, 4.8005e-01, 5.2318e-02, 4.4137e-01, 4.1163e-01, 8.9339e-01,
+                                7.2847e-01],
+                                [2.4800e-01, 7.1944e-01, 3.4766e-01, 7.8583e-01, 8.5507e-01, 6.3729e-02,
+                                7.5161e-05]]))
+                ) at 0x7f7e29de3a00>
+        ) at 0x7f7e2a0e8640>
+        >>> pred_instances = InstanceData(metainfo=meta_info)
+        >>> pred_instances.bboxes = torch.rand((5, 4))
+        >>> pred_instances.scores = torch.rand((5, ))
+        >>> data_sample = Det3DDataSample(pred_instances=pred_instances)
+        >>> assert 'pred_instances' in data_sample
+
+        >>> pred_instances_3d = InstanceData(metainfo=meta_info)
+        >>> pred_instances_3d.bboxes_3d = BaseInstance3DBoxes(
+        ...     torch.rand((5, 7)))
+        >>> pred_instances_3d.scores_3d = torch.rand((5, ))
+        >>> pred_instances_3d.labels_3d = torch.rand((5, ))
+        >>> data_sample = Det3DDataSample(pred_instances_3d=pred_instances_3d)
+        >>> assert 'pred_instances_3d' in data_sample
+
+        >>> data_sample = Det3DDataSample()
+        >>> gt_instances_3d_data = dict(
+        ...     bboxes_3d=BaseInstance3DBoxes(torch.rand((2, 7))),
+        ...     labels_3d=torch.rand(2))
+        >>> gt_instances_3d = InstanceData(**gt_instances_3d_data)
+        >>> data_sample.gt_instances_3d = gt_instances_3d
+        >>> assert 'gt_instances_3d' in data_sample
+        >>> assert 'bboxes_3d' in data_sample.gt_instances_3d
+
+        >>> from mmdet3d.structures import PointData
+        >>> data_sample = Det3DDataSample()
+        >>> gt_pts_seg_data = dict(
+        ...     pts_instance_mask=torch.rand(2),
+        ...     pts_semantic_mask=torch.rand(2))
+        >>> data_sample.gt_pts_seg = PointData(**gt_pts_seg_data)
+        >>> print(data_sample)
+        <Det3DDataSample(
             META INFORMATION
-            pad_shape: (800, 1216, 3)
-            img_shape: (800, 1196, 3)
-
             DATA FIELDS
-            labels: tensor([0, 0, 1, 0, 2])
-            bboxes: BaseInstance3DBoxes(
-            tensor([[0.2874, 0.3078, 0.8368, 0.2326, 0.9845, 0.6199, 0.9944],
-                    [0.6222, 0.8778, 0.7306, 0.3320, 0.3973, 0.7662, 0.7326],
-                    [0.8547, 0.6082, 0.1660, 0.1676, 0.9810, 0.3092, 0.0917],
-                    [0.4686, 0.7007, 0.4428, 0.0672, 0.3319, 0.3033, 0.8519],
-                    [0.9693, 0.5315, 0.4642, 0.9079, 0.2481, 0.1781, 0.9557]]))
-        ) at 0x7fb0d9354280>
-    ) at 0x7fb0d93543d0>
-    >>> pred_instances = InstanceData(metainfo=meta_info)
-    >>> pred_instances.bboxes = torch.rand((5, 4))
-    >>> pred_instances.scores = torch.rand((5, ))
-    >>> data_sample = Det3DDataSample(pred_instances=pred_instances)
-    >>> assert 'pred_instances' in data_sample
-
-    >>> pred_instances_3d = InstanceData(metainfo=meta_info)
-    >>> pred_instances_3d.bboxes_3d = BaseInstance3DBoxes(torch.rand((5, 7)))
-    >>> pred_instances_3d.scores_3d = torch.rand((5, ))
-    >>> pred_instances_3d.labels_3d = torch.rand((5, ))
-    >>> data_sample = Det3DDataSample(pred_instances_3d=pred_instances_3d)
-    >>> assert 'pred_instances_3d' in data_sample
-
-    >>> data_sample = Det3DDataSample()
-    >>> gt_instances_3d_data = dict(
-    ...    bboxes=BaseInstance3DBoxes(torch.rand((2, 7))),
-    ...    labels=torch.rand(2))
-    >>> gt_instances_3d = InstanceData(**gt_instances_3d_data)
-    >>> data_sample.gt_instances_3d = gt_instances_3d
-    >>> assert 'gt_instances_3d' in data_sample
-    >>> assert 'bboxes' in data_sample.gt_instances_3d
-
-    >>> data_sample = Det3DDataSample()
-    ... gt_pts_seg_data = dict(
-    ...    pts_instance_mask=torch.rand(2),
-    ...    pts_semantic_mask=torch.rand(2))
-    >>> data_sample.gt_pts_seg = PointData(**gt_pts_seg_data)
-    >>> print(data_sample)
-    <Det3DDataSample(
-
-        META INFORMATION
-
-        DATA FIELDS
-        gt_pts_seg: <PointData(
-
-                META INFORMATION
-
-                DATA FIELDS
-                pts_instance_mask: tensor([0.0576, 0.3067])
-                pts_semantic_mask: tensor([0.9267, 0.7455])
-            ) at 0x7f654a9c1590>
-        _gt_pts_seg: <PointData(
-
-                META INFORMATION
-
-                DATA FIELDS
-                pts_instance_mask: tensor([0.0576, 0.3067])
-                pts_semantic_mask: tensor([0.9267, 0.7455])
-            ) at 0x7f654a9c1590>
-    ) at 0x7f654a9c1550>
-    """
+            gt_pts_seg: <PointData(
+                    META INFORMATION
+                    DATA FIELDS
+                    pts_semantic_mask: tensor([0.7199, 0.4006])
+                    pts_instance_mask: tensor([0.7363, 0.8096])
+                ) at 0x7f7e2962cc40>
+        ) at 0x7f7e29ff0d60>
+    """  # noqa: E501
 
     @property
     def gt_instances_3d(self) -> InstanceData:
         return self._gt_instances_3d
 
     @gt_instances_3d.setter
-    def gt_instances_3d(self, value: InstanceData):
+    def gt_instances_3d(self, value: InstanceData) -> None:
         self.set_field(value, '_gt_instances_3d', dtype=InstanceData)
 
     @gt_instances_3d.deleter
-    def gt_instances_3d(self):
+    def gt_instances_3d(self) -> None:
         del self._gt_instances_3d
 
     @property
     def pred_instances_3d(self) -> InstanceData:
         return self._pred_instances_3d
 
     @pred_instances_3d.setter
-    def pred_instances_3d(self, value: InstanceData):
+    def pred_instances_3d(self, value: InstanceData) -> None:
         self.set_field(value, '_pred_instances_3d', dtype=InstanceData)
 
     @pred_instances_3d.deleter
-    def pred_instances_3d(self):
+    def pred_instances_3d(self) -> None:
         del self._pred_instances_3d
 
     @property
     def pts_pred_instances_3d(self) -> InstanceData:
         return self._pts_pred_instances_3d
 
     @pts_pred_instances_3d.setter
-    def pts_pred_instances_3d(self, value: InstanceData):
+    def pts_pred_instances_3d(self, value: InstanceData) -> None:
         self.set_field(value, '_pts_pred_instances_3d', dtype=InstanceData)
 
     @pts_pred_instances_3d.deleter
-    def pts_pred_instances_3d(self):
+    def pts_pred_instances_3d(self) -> None:
         del self._pts_pred_instances_3d
 
     @property
     def img_pred_instances_3d(self) -> InstanceData:
         return self._img_pred_instances_3d
 
     @img_pred_instances_3d.setter
-    def img_pred_instances_3d(self, value: InstanceData):
+    def img_pred_instances_3d(self, value: InstanceData) -> None:
         self.set_field(value, '_img_pred_instances_3d', dtype=InstanceData)
 
     @img_pred_instances_3d.deleter
-    def img_pred_instances_3d(self):
+    def img_pred_instances_3d(self) -> None:
         del self._img_pred_instances_3d
 
     @property
     def gt_pts_seg(self) -> PointData:
         return self._gt_pts_seg
 
     @gt_pts_seg.setter
-    def gt_pts_seg(self, value: PointData):
+    def gt_pts_seg(self, value: PointData) -> None:
         self.set_field(value, '_gt_pts_seg', dtype=PointData)
 
     @gt_pts_seg.deleter
-    def gt_pts_seg(self):
+    def gt_pts_seg(self) -> None:
         del self._gt_pts_seg
 
     @property
     def pred_pts_seg(self) -> PointData:
         return self._pred_pts_seg
 
     @pred_pts_seg.setter
-    def pred_pts_seg(self, value: PointData):
+    def pred_pts_seg(self, value: PointData) -> None:
         self.set_field(value, '_pred_pts_seg', dtype=PointData)
 
     @pred_pts_seg.deleter
-    def pred_pts_seg(self):
+    def pred_pts_seg(self) -> None:
         del self._pred_pts_seg
 
 
 SampleList = List[Det3DDataSample]
 OptSampleList = Optional[SampleList]
 ForwardResults = Union[Dict[str, torch.Tensor], List[Det3DDataSample],
                        Tuple[torch.Tensor], torch.Tensor]
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/structures/ops/__init__.py` & `mmdet3d-1.1.1/mmdet3d/structures/ops/__init__.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/structures/ops/box_np_ops.py` & `mmdet3d-1.1.1/mmdet3d/structures/ops/box_np_ops.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/structures/ops/iou3d_calculator.py` & `mmdet3d-1.1.1/mmdet3d/structures/ops/iou3d_calculator.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/structures/ops/transforms.py` & `mmdet3d-1.1.1/mmdet3d/structures/ops/transforms.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/structures/point_data.py` & `mmdet3d-1.1.1/mmdet3d/structures/point_data.py`

 * *Files 6% similar despite different names*

```diff
@@ -8,119 +8,124 @@
 
 IndexType = Union[str, slice, int, list, torch.LongTensor,
                   torch.cuda.LongTensor, torch.BoolTensor,
                   torch.cuda.BoolTensor, np.ndarray]
 
 
 class PointData(BaseDataElement):
-    """Data structure for point-level annnotations or predictions.
+    """Data structure for point-level annotations or predictions.
 
     All data items in ``data_fields`` of ``PointData`` meet the following
     requirements:
 
     - They are all one dimension.
     - They should have the same length.
 
     `PointData` is used to save point-level semantic and instance mask,
     it also can save `instances_labels` and `instances_scores` temporarily.
     In the future, we would consider to move the instance-level info into
     `gt_instances_3d` and `pred_instances_3d`.
 
     Examples:
         >>> metainfo = dict(
-        ...     sample_id=random.randint(0, 100))
+        ...     sample_idx=random.randint(0, 100))
         >>> points = np.random.randint(0, 255, (100, 3))
         >>> point_data = PointData(metainfo=metainfo,
         ...                        points=points)
         >>> print(len(point_data))
-        >>> (100)
+        100
 
         >>> # slice
-        >>> slice_data = pixel_data[10:60]
-        >>> assert slice_data.shape == (50,)
+        >>> slice_data = point_data[10:60]
+        >>> assert len(slice_data) == 50
 
         >>> # set
-        >>> point_data.pts_semantic_mask = torch.randint(0, 255, (100))
-        >>> point_data.pts_instance_mask = torch.randint(0, 255, (100))
-        >>> assert tuple(point_data.pts_semantic_mask.shape) == (100)
-        >>> assert tuple(point_data.pts_instance_mask.shape) == (100)
+        >>> point_data.pts_semantic_mask = torch.randint(0, 255, (100,))
+        >>> point_data.pts_instance_mask = torch.randint(0, 255, (100,))
+        >>> assert tuple(point_data.pts_semantic_mask.shape) == (100,)
+        >>> assert tuple(point_data.pts_instance_mask.shape) == (100,)
     """
 
-    def __setattr__(self, name: str, value: Sized):
+    def __setattr__(self, name: str, value: Sized) -> None:
         """setattr is only used to set data.
 
-        the value must have the attribute of `__len__` and have the same length
-        of PointData.
+        The value must have the attribute of `__len__` and have the same length
+        of `PointData`.
         """
         if name in ('_metainfo_fields', '_data_fields'):
             if not hasattr(self, name):
                 super().__setattr__(name, value)
             else:
-                raise AttributeError(
-                    f'{name} has been used as a '
-                    f'private attribute, which is immutable. ')
+                raise AttributeError(f'{name} has been used as a '
+                                     'private attribute, which is immutable.')
 
         else:
             assert isinstance(value,
-                              Sized), 'value must contain `_len__` attribute'
+                              Sized), 'value must contain `__len__` attribute'
+            # TODO: make sure the input value share the same length
             super().__setattr__(name, value)
 
     __setitem__ = __setattr__
 
     def __getitem__(self, item: IndexType) -> 'PointData':
         """
         Args:
-            item (str, obj:`slice`,
-                obj`torch.LongTensor`, obj:`torch.BoolTensor`):
-                get the corresponding values according to item.
+            item (str, int, list, :obj:`slice`, :obj:`numpy.ndarray`,
+                :obj:`torch.LongTensor`, :obj:`torch.BoolTensor`):
+                Get the corresponding values according to item.
 
         Returns:
-            obj:`PointData`: Corresponding values.
+            :obj:`PointData`: Corresponding values.
         """
         if isinstance(item, list):
             item = np.array(item)
         if isinstance(item, np.ndarray):
+            # The default int type of numpy is platform dependent, int32 for
+            # windows and int64 for linux. `torch.Tensor` requires the index
+            # should be int64, therefore we simply convert it to int64 here.
+            # Mode details in https://github.com/numpy/numpy/issues/9464
+            item = item.astype(np.int64) if item.dtype == np.int32 else item
             item = torch.from_numpy(item)
         assert isinstance(
             item, (str, slice, int, torch.LongTensor, torch.cuda.LongTensor,
                    torch.BoolTensor, torch.cuda.BoolTensor))
 
         if isinstance(item, str):
             return getattr(self, item)
 
-        if type(item) == int:
-            if item >= len(self) or item < -len(self):  # type:ignore
+        if isinstance(item, int):
+            if item >= len(self) or item < -len(self):  # type: ignore
                 raise IndexError(f'Index {item} out of range!')
             else:
                 # keep the dimension
                 item = slice(item, None, len(self))
 
         new_data = self.__class__(metainfo=self.metainfo)
         if isinstance(item, torch.Tensor):
             assert item.dim() == 1, 'Only support to get the' \
                                     ' values along the first dimension.'
             if isinstance(item, (torch.BoolTensor, torch.cuda.BoolTensor)):
-                assert len(item) == len(self), f'The shape of the' \
-                                               f' input(BoolTensor)) ' \
+                assert len(item) == len(self), 'The shape of the ' \
+                                               'input(BoolTensor) ' \
                                                f'{len(item)} ' \
-                                               f' does not match the shape ' \
-                                               f'of the indexed tensor ' \
-                                               f'in results_filed ' \
+                                               'does not match the shape ' \
+                                               'of the indexed tensor ' \
+                                               'in results_field ' \
                                                f'{len(self)} at ' \
-                                               f'first dimension. '
+                                               'first dimension.'
 
             for k, v in self.items():
                 if isinstance(v, torch.Tensor):
                     new_data[k] = v[item]
                 elif isinstance(v, np.ndarray):
                     new_data[k] = v[item.cpu().numpy()]
                 elif isinstance(
                         v, (str, list, tuple)) or (hasattr(v, '__getitem__')
                                                    and hasattr(v, 'cat')):
-                    # convert to indexes from boolTensor
+                    # convert to indexes from BoolTensor
                     if isinstance(item,
                                   (torch.BoolTensor, torch.cuda.BoolTensor)):
                         indexes = torch.nonzero(item).view(
                             -1).cpu().numpy().tolist()
                     else:
                         indexes = item.cpu().numpy().tolist()
                     slice_list = []
@@ -137,21 +142,20 @@
                     else:
                         new_value = v.cat(r_list)
                     new_data[k] = new_value
                 else:
                     raise ValueError(
                         f'The type of `{k}` is `{type(v)}`, which has no '
                         'attribute of `cat`, so it does not '
-                        f'support slice with `bool`')
-
+                        'support slice with `bool`')
         else:
             # item is a slice
             for k, v in self.items():
                 new_data[k] = v[item]
-        return new_data  # type:ignore
+        return new_data  # type: ignore
 
     def __len__(self) -> int:
-        """int: the length of PointData"""
+        """int: The length of `PointData`."""
         if len(self._data_fields) > 0:
             return len(self.values()[0])
         else:
             return 0
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/structures/points/__init__.py` & `mmdet3d-1.1.1/mmdet3d/structures/points/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -3,28 +3,29 @@
 from .cam_points import CameraPoints
 from .depth_points import DepthPoints
 from .lidar_points import LiDARPoints
 
 __all__ = ['BasePoints', 'CameraPoints', 'DepthPoints', 'LiDARPoints']
 
 
-def get_points_type(points_type):
+def get_points_type(points_type: str) -> type:
     """Get the class of points according to coordinate type.
 
     Args:
-        points_type (str): The type of points coordinate.
-            The valid value are "CAMERA", "LIDAR", or "DEPTH".
+        points_type (str): The type of points coordinate. The valid value are
+            "CAMERA", "LIDAR" and "DEPTH".
 
     Returns:
-        class: Points type.
+        type: Points type.
     """
-    if points_type == 'CAMERA':
+    points_type_upper = points_type.upper()
+    if points_type_upper == 'CAMERA':
         points_cls = CameraPoints
-    elif points_type == 'LIDAR':
+    elif points_type_upper == 'LIDAR':
         points_cls = LiDARPoints
-    elif points_type == 'DEPTH':
+    elif points_type_upper == 'DEPTH':
         points_cls = DepthPoints
     else:
-        raise ValueError('Only "points_type" of "CAMERA", "LIDAR", or "DEPTH"'
-                         f' are supported, got {points_type}')
+        raise ValueError('Only "points_type" of "CAMERA", "LIDAR" and "DEPTH" '
+                         f'are supported, got {points_type}')
 
     return points_cls
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/structures/points/base_points.py` & `mmdet3d-1.1.1/mmdet3d/structures/points/base_points.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,122 +1,144 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import warnings
 from abc import abstractmethod
+from typing import Iterator, Optional, Sequence, Union
 
 import numpy as np
 import torch
+from torch import Tensor
 
-from ..bbox_3d.utils import rotation_3d_in_axis
+from mmdet3d.structures.bbox_3d.utils import rotation_3d_in_axis
 
 
-class BasePoints(object):
+class BasePoints:
     """Base class for Points.
 
     Args:
-        tensor (torch.Tensor | np.ndarray | list): a N x points_dim matrix.
-        points_dim (int, optional): Number of the dimension of a point.
-            Each row is (x, y, z). Defaults to 3.
-        attribute_dims (dict, optional): Dictionary to indicate the
-            meaning of extra dimension. Defaults to None.
+        tensor (Tensor or np.ndarray or Sequence[Sequence[float]]): The points
+            data with shape (N, points_dim).
+        points_dim (int): Integer indicating the dimension of a point. Each row
+            is (x, y, z, ...). Defaults to 3.
+        attribute_dims (dict, optional): Dictionary to indicate the meaning of
+            extra dimension. Defaults to None.
 
     Attributes:
-        tensor (torch.Tensor): Float matrix of N x points_dim.
-        points_dim (int): Integer indicating the dimension of a point.
-            Each row is (x, y, z, ...).
-        attribute_dims (bool): Dictionary to indicate the meaning of extra
-            dimension. Defaults to None.
+        tensor (Tensor): Float matrix with shape (N, points_dim).
+        points_dim (int): Integer indicating the dimension of a point. Each row
+            is (x, y, z, ...).
+        attribute_dims (dict, optional): Dictionary to indicate the meaning of
+            extra dimension. Defaults to None.
         rotation_axis (int): Default rotation axis for points rotation.
     """
 
-    def __init__(self, tensor, points_dim=3, attribute_dims=None):
-        if isinstance(tensor, torch.Tensor):
+    def __init__(self,
+                 tensor: Union[Tensor, np.ndarray, Sequence[Sequence[float]]],
+                 points_dim: int = 3,
+                 attribute_dims: Optional[dict] = None) -> None:
+        if isinstance(tensor, Tensor):
             device = tensor.device
         else:
             device = torch.device('cpu')
         tensor = torch.as_tensor(tensor, dtype=torch.float32, device=device)
         if tensor.numel() == 0:
-            # Use reshape, so we don't end up creating a new tensor that
-            # does not depend on the inputs (and consequently confuses jit)
-            tensor = tensor.reshape((0, points_dim)).to(
-                dtype=torch.float32, device=device)
-        assert tensor.dim() == 2 and tensor.size(-1) == \
-            points_dim, tensor.size()
+            # Use reshape, so we don't end up creating a new tensor that does
+            # not depend on the inputs (and consequently confuses jit)
+            tensor = tensor.reshape((-1, points_dim))
+        assert tensor.dim() == 2 and tensor.size(-1) == points_dim, \
+            ('The points dimension must be 2 and the length of the last '
+             f'dimension must be {points_dim}, but got points with shape '
+             f'{tensor.shape}.')
 
-        self.tensor = tensor
+        self.tensor = tensor.clone()
         self.points_dim = points_dim
         self.attribute_dims = attribute_dims
         self.rotation_axis = 0
 
     @property
-    def coord(self):
-        """torch.Tensor: Coordinates of each point in shape (N, 3)."""
+    def coord(self) -> Tensor:
+        """Tensor: Coordinates of each point in shape (N, 3)."""
         return self.tensor[:, :3]
 
     @coord.setter
-    def coord(self, tensor):
-        """Set the coordinates of each point."""
+    def coord(self, tensor: Union[Tensor, np.ndarray]) -> None:
+        """Set the coordinates of each point.
+
+        Args:
+            tensor (Tensor or np.ndarray): Coordinates of each point with shape
+                (N, 3).
+        """
         try:
             tensor = tensor.reshape(self.shape[0], 3)
         except (RuntimeError, ValueError):  # for torch.Tensor and np.ndarray
             raise ValueError(f'got unexpected shape {tensor.shape}')
-        if not isinstance(tensor, torch.Tensor):
+        if not isinstance(tensor, Tensor):
             tensor = self.tensor.new_tensor(tensor)
         self.tensor[:, :3] = tensor
 
     @property
-    def height(self):
-        """torch.Tensor:
-            A vector with height of each point in shape (N, 1), or None."""
+    def height(self) -> Union[Tensor, None]:
+        """Tensor or None: Returns a vector with height of each point in shape
+        (N, )."""
         if self.attribute_dims is not None and \
                 'height' in self.attribute_dims.keys():
             return self.tensor[:, self.attribute_dims['height']]
         else:
             return None
 
     @height.setter
-    def height(self, tensor):
-        """Set the height of each point."""
+    def height(self, tensor: Union[Tensor, np.ndarray]) -> None:
+        """Set the height of each point.
+
+        Args:
+            tensor (Tensor or np.ndarray): Height of each point with shape
+                (N, ).
+        """
         try:
             tensor = tensor.reshape(self.shape[0])
         except (RuntimeError, ValueError):  # for torch.Tensor and np.ndarray
             raise ValueError(f'got unexpected shape {tensor.shape}')
-        if not isinstance(tensor, torch.Tensor):
+        if not isinstance(tensor, Tensor):
             tensor = self.tensor.new_tensor(tensor)
         if self.attribute_dims is not None and \
                 'height' in self.attribute_dims.keys():
             self.tensor[:, self.attribute_dims['height']] = tensor
         else:
             # add height attribute
             if self.attribute_dims is None:
                 self.attribute_dims = dict()
             attr_dim = self.shape[1]
             self.tensor = torch.cat([self.tensor, tensor.unsqueeze(1)], dim=1)
             self.attribute_dims.update(dict(height=attr_dim))
             self.points_dim += 1
 
     @property
-    def color(self):
-        """torch.Tensor:
-            A vector with color of each point in shape (N, 3), or None."""
+    def color(self) -> Union[Tensor, None]:
+        """Tensor or None: Returns a vector with color of each point in shape
+        (N, 3)."""
         if self.attribute_dims is not None and \
                 'color' in self.attribute_dims.keys():
             return self.tensor[:, self.attribute_dims['color']]
         else:
             return None
 
     @color.setter
-    def color(self, tensor):
-        """Set the color of each point."""
+    def color(self, tensor: Union[Tensor, np.ndarray]) -> None:
+        """Set the color of each point.
+
+        Args:
+            tensor (Tensor or np.ndarray): Color of each point with shape
+                (N, 3).
+        """
         try:
             tensor = tensor.reshape(self.shape[0], 3)
         except (RuntimeError, ValueError):  # for torch.Tensor and np.ndarray
             raise ValueError(f'got unexpected shape {tensor.shape}')
         if tensor.max() >= 256 or tensor.min() < 0:
             warnings.warn('point got color value beyond [0, 255]')
-        if not isinstance(tensor, torch.Tensor):
+        if not isinstance(tensor, Tensor):
             tensor = self.tensor.new_tensor(tensor)
         if self.attribute_dims is not None and \
                 'color' in self.attribute_dims.keys():
             self.tensor[:, self.attribute_dims['color']] = tensor
         else:
             # add color attribute
             if self.attribute_dims is None:
@@ -124,40 +146,44 @@
             attr_dim = self.shape[1]
             self.tensor = torch.cat([self.tensor, tensor], dim=1)
             self.attribute_dims.update(
                 dict(color=[attr_dim, attr_dim + 1, attr_dim + 2]))
             self.points_dim += 3
 
     @property
-    def shape(self):
-        """torch.Shape: Shape of points."""
+    def shape(self) -> torch.Size:
+        """torch.Size: Shape of points."""
         return self.tensor.shape
 
-    def shuffle(self):
+    def shuffle(self) -> Tensor:
         """Shuffle the points.
 
         Returns:
-            torch.Tensor: The shuffled index.
+            Tensor: The shuffled index.
         """
         idx = torch.randperm(self.__len__(), device=self.tensor.device)
         self.tensor = self.tensor[idx]
         return idx
 
-    def rotate(self, rotation, axis=None):
+    def rotate(self,
+               rotation: Union[Tensor, np.ndarray, float],
+               axis: Optional[int] = None) -> Tensor:
         """Rotate points with the given rotation matrix or angle.
 
         Args:
-            rotation (float | np.ndarray | torch.Tensor): Rotation matrix
-                or angle.
+            rotation (Tensor or np.ndarray or float): Rotation matrix or angle.
             axis (int, optional): Axis to rotate at. Defaults to None.
+
+        Returns:
+            Tensor: Rotation matrix.
         """
-        if not isinstance(rotation, torch.Tensor):
+        if not isinstance(rotation, Tensor):
             rotation = self.tensor.new_tensor(rotation)
-        assert rotation.shape == torch.Size([3, 3]) or \
-            rotation.numel() == 1, f'invalid rotation shape {rotation.shape}'
+        assert rotation.shape == torch.Size([3, 3]) or rotation.numel() == 1, \
+            f'invalid rotation shape {rotation.shape}'
 
         if axis is None:
             axis = self.rotation_axis
 
         if rotation.numel() == 1:
             rotated_points, rot_mat_T = rotation_3d_in_axis(
                 self.tensor[:, :3][None], rotation, axis=axis, return_mat=True)
@@ -167,149 +193,164 @@
             # rotation.numel() == 9
             self.tensor[:, :3] = self.tensor[:, :3] @ rotation
             rot_mat_T = rotation
 
         return rot_mat_T
 
     @abstractmethod
-    def flip(self, bev_direction='horizontal'):
+    def flip(self, bev_direction: str = 'horizontal') -> None:
         """Flip the points along given BEV direction.
 
         Args:
             bev_direction (str): Flip direction (horizontal or vertical).
+                Defaults to 'horizontal'.
         """
         pass
 
-    def translate(self, trans_vector):
+    def translate(self, trans_vector: Union[Tensor, np.ndarray]) -> None:
         """Translate points with the given translation vector.
 
         Args:
-            trans_vector (np.ndarray, torch.Tensor): Translation
-                vector of size 3 or nx3.
+            trans_vector (Tensor or np.ndarray): Translation vector of size 3
+                or nx3.
         """
-        if not isinstance(trans_vector, torch.Tensor):
+        if not isinstance(trans_vector, Tensor):
             trans_vector = self.tensor.new_tensor(trans_vector)
         trans_vector = trans_vector.squeeze(0)
         if trans_vector.dim() == 1:
             assert trans_vector.shape[0] == 3
         elif trans_vector.dim() == 2:
             assert trans_vector.shape[0] == self.tensor.shape[0] and \
                 trans_vector.shape[1] == 3
         else:
             raise NotImplementedError(
                 f'Unsupported translation vector of shape {trans_vector.shape}'
             )
         self.tensor[:, :3] += trans_vector
 
-    def in_range_3d(self, point_range):
+    def in_range_3d(
+            self, point_range: Union[Tensor, np.ndarray,
+                                     Sequence[float]]) -> Tensor:
         """Check whether the points are in the given range.
 
         Args:
-            point_range (list | torch.Tensor): The range of point
-                (x_min, y_min, z_min, x_max, y_max, z_max)
+            point_range (Tensor or np.ndarray or Sequence[float]): The range of
+                point (x_min, y_min, z_min, x_max, y_max, z_max).
 
         Note:
-            In the original implementation of SECOND, checking whether
-            a box in the range checks whether the points are in a convex
-            polygon, we try to reduce the burden for simpler cases.
+            In the original implementation of SECOND, checking whether a box in
+            the range checks whether the points are in a convex polygon, we try
+            to reduce the burden for simpler cases.
 
         Returns:
-            torch.Tensor: A binary vector indicating whether each point is
-                inside the reference range.
+            Tensor: A binary vector indicating whether each point is inside the
+            reference range.
         """
         in_range_flags = ((self.tensor[:, 0] > point_range[0])
                           & (self.tensor[:, 1] > point_range[1])
                           & (self.tensor[:, 2] > point_range[2])
                           & (self.tensor[:, 0] < point_range[3])
                           & (self.tensor[:, 1] < point_range[4])
                           & (self.tensor[:, 2] < point_range[5]))
         return in_range_flags
 
     @property
-    def bev(self):
-        """torch.Tensor: BEV of the points in shape (N, 2)."""
+    def bev(self) -> Tensor:
+        """Tensor: BEV of the points in shape (N, 2)."""
         return self.tensor[:, [0, 1]]
 
-    def in_range_bev(self, point_range):
+    def in_range_bev(
+            self, point_range: Union[Tensor, np.ndarray,
+                                     Sequence[float]]) -> Tensor:
         """Check whether the points are in the given range.
 
         Args:
-            point_range (list | torch.Tensor): The range of point
-                in order of (x_min, y_min, x_max, y_max).
+            point_range (Tensor or np.ndarray or Sequence[float]): The range of
+                point in order of (x_min, y_min, x_max, y_max).
 
         Returns:
-            torch.Tensor: Indicating whether each point is inside
-                the reference range.
+            Tensor: A binary vector indicating whether each point is inside the
+            reference range.
         """
         in_range_flags = ((self.bev[:, 0] > point_range[0])
                           & (self.bev[:, 1] > point_range[1])
                           & (self.bev[:, 0] < point_range[2])
                           & (self.bev[:, 1] < point_range[3]))
         return in_range_flags
 
     @abstractmethod
-    def convert_to(self, dst, rt_mat=None):
+    def convert_to(self,
+                   dst: int,
+                   rt_mat: Optional[Union[Tensor,
+                                          np.ndarray]] = None) -> 'BasePoints':
         """Convert self to ``dst`` mode.
 
         Args:
-            dst (:obj:`CoordMode`): The target Box mode.
-            rt_mat (np.ndarray | torch.Tensor, optional): The rotation and
+            dst (int): The target Point mode.
+            rt_mat (Tensor or np.ndarray, optional): The rotation and
                 translation matrix between different coordinates.
-                Defaults to None.
-                The conversion from `src` coordinates to `dst` coordinates
-                usually comes along the change of sensors, e.g., from camera
-                to LiDAR. This requires a transformation matrix.
+                Defaults to None. The conversion from ``src`` coordinates to
+                ``dst`` coordinates usually comes along the change of sensors,
+                e.g., from camera to LiDAR. This requires a transformation
+                matrix.
 
         Returns:
-            :obj:`BasePoints`: The converted box of the same type
-                in the `dst` mode.
+            :obj:`BasePoints`: The converted point of the same type in the
+            ``dst`` mode.
         """
         pass
 
-    def scale(self, scale_factor):
+    def scale(self, scale_factor: float) -> None:
         """Scale the points with horizontal and vertical scaling factors.
 
         Args:
             scale_factors (float): Scale factors to scale the points.
         """
         self.tensor[:, :3] *= scale_factor
 
-    def __getitem__(self, item):
+    def __getitem__(
+            self, item: Union[int, tuple, slice, np.ndarray,
+                              Tensor]) -> 'BasePoints':
         """
+        Args:
+            item (int or tuple or slice or np.ndarray or Tensor): Index of
+                points.
+
         Note:
             The following usage are allowed:
-            1. `new_points = points[3]`:
-                return a `Points` that contains only one point.
-            2. `new_points = points[2:10]`:
-                return a slice of points.
-            3. `new_points = points[vector]`:
-                where vector is a torch.BoolTensor with `length = len(points)`.
-                Nonzero elements in the vector will be selected.
-            4. `new_points = points[3:11, vector]`:
-                return a slice of points and attribute dims.
-            5. `new_points = points[4:12, 2]`:
-                return a slice of points with single attribute.
+
+            1. `new_points = points[3]`: Return a `Points` that contains only
+               one point.
+            2. `new_points = points[2:10]`: Return a slice of points.
+            3. `new_points = points[vector]`: Whether vector is a
+               torch.BoolTensor with `length = len(points)`. Nonzero elements
+               in the vector will be selected.
+            4. `new_points = points[3:11, vector]`: Return a slice of points
+               and attribute dims.
+            5. `new_points = points[4:12, 2]`: Return a slice of points with
+               single attribute.
+
             Note that the returned Points might share storage with this Points,
-            subject to Pytorch's indexing semantics.
+            subject to PyTorch's indexing semantics.
 
         Returns:
-            :obj:`BasePoints`: A new object of
-                :class:`BasePoints` after indexing.
+            :obj:`BasePoints`: A new object of :class:`BasePoints` after
+            indexing.
         """
         original_type = type(self)
         if isinstance(item, int):
             return original_type(
                 self.tensor[item].view(1, -1),
                 points_dim=self.points_dim,
                 attribute_dims=self.attribute_dims)
         elif isinstance(item, tuple) and len(item) == 2:
             if isinstance(item[1], slice):
                 start = 0 if item[1].start is None else item[1].start
-                stop = self.tensor.shape[1] if \
-                    item[1].stop is None else item[1].stop
+                stop = self.tensor.shape[1] \
+                    if item[1].stop is None else item[1].stop
                 step = 1 if item[1].step is None else item[1].step
                 item = list(item)
                 item[1] = list(range(start, stop, step))
                 item = tuple(item)
             elif isinstance(item[1], int):
                 item = list(item)
                 item[1] = [item[1]]
@@ -330,111 +371,153 @@
                         attribute_dims[key] = intersect_attr[0]
                     elif len(intersect_attr) > 1:
                         attribute_dims[key] = intersect_attr
                     else:
                         attribute_dims.pop(key)
             else:
                 attribute_dims = None
-        elif isinstance(item, (slice, np.ndarray, torch.Tensor)):
+        elif isinstance(item, (slice, np.ndarray, Tensor)):
             p = self.tensor[item]
             attribute_dims = self.attribute_dims
         else:
             raise NotImplementedError(f'Invalid slice {item}!')
 
         assert p.dim() == 2, \
             f'Indexing on Points with {item} failed to return a matrix!'
         return original_type(
             p, points_dim=p.shape[1], attribute_dims=attribute_dims)
 
-    def __len__(self):
+    def __len__(self) -> int:
         """int: Number of points in the current object."""
         return self.tensor.shape[0]
 
-    def __repr__(self):
-        """str: Return a strings that describes the object."""
+    def __repr__(self) -> str:
+        """str: Return a string that describes the object."""
         return self.__class__.__name__ + '(\n    ' + str(self.tensor) + ')'
 
     @classmethod
-    def cat(cls, points_list):
+    def cat(cls, points_list: Sequence['BasePoints']) -> 'BasePoints':
         """Concatenate a list of Points into a single Points.
 
         Args:
-            points_list (list[:obj:`BasePoints`]): List of points.
+            points_list (Sequence[:obj:`BasePoints`]): List of points.
 
         Returns:
-            :obj:`BasePoints`: The concatenated Points.
+            :obj:`BasePoints`: The concatenated points.
         """
         assert isinstance(points_list, (list, tuple))
         if len(points_list) == 0:
             return cls(torch.empty(0))
         assert all(isinstance(points, cls) for points in points_list)
 
         # use torch.cat (v.s. layers.cat)
         # so the returned points never share storage with input
         cat_points = cls(
             torch.cat([p.tensor for p in points_list], dim=0),
-            points_dim=points_list[0].tensor.shape[1],
+            points_dim=points_list[0].points_dim,
             attribute_dims=points_list[0].attribute_dims)
         return cat_points
 
-    def to(self, device):
+    def numpy(self) -> np.ndarray:
+        """Reload ``numpy`` from self.tensor."""
+        return self.tensor.numpy()
+
+    def to(self, device: Union[str, torch.device], *args,
+           **kwargs) -> 'BasePoints':
         """Convert current points to a specific device.
 
         Args:
-            device (str | :obj:`torch.device`): The name of the device.
+            device (str or :obj:`torch.device`): The name of the device.
+
+        Returns:
+            :obj:`BasePoints`: A new points object on the specific device.
+        """
+        original_type = type(self)
+        return original_type(
+            self.tensor.to(device, *args, **kwargs),
+            points_dim=self.points_dim,
+            attribute_dims=self.attribute_dims)
+
+    def cpu(self) -> 'BasePoints':
+        """Convert current points to cpu device.
+
+        Returns:
+            :obj:`BasePoints`: A new points object on the cpu device.
+        """
+        original_type = type(self)
+        return original_type(
+            self.tensor.cpu(),
+            points_dim=self.points_dim,
+            attribute_dims=self.attribute_dims)
+
+    def cuda(self, *args, **kwargs) -> 'BasePoints':
+        """Convert current points to cuda device.
 
         Returns:
-            :obj:`BasePoints`: A new boxes object on the
-                specific device.
+            :obj:`BasePoints`: A new points object on the cuda device.
         """
         original_type = type(self)
         return original_type(
-            self.tensor.to(device),
+            self.tensor.cuda(*args, **kwargs),
             points_dim=self.points_dim,
             attribute_dims=self.attribute_dims)
 
-    def clone(self):
-        """Clone the Points.
+    def clone(self) -> 'BasePoints':
+        """Clone the points.
 
         Returns:
-            :obj:`BasePoints`: Box object with the same properties
-                as self.
+            :obj:`BasePoints`: Point object with the same properties as self.
         """
         original_type = type(self)
         return original_type(
             self.tensor.clone(),
             points_dim=self.points_dim,
             attribute_dims=self.attribute_dims)
 
+    def detach(self) -> 'BasePoints':
+        """Detach the points.
+
+        Returns:
+            :obj:`BasePoints`: Point object with the same properties as self.
+        """
+        original_type = type(self)
+        return original_type(
+            self.tensor.detach(),
+            points_dim=self.points_dim,
+            attribute_dims=self.attribute_dims)
+
     @property
-    def device(self):
-        """str: The device of the points are on."""
+    def device(self) -> torch.device:
+        """torch.device: The device of the points are on."""
         return self.tensor.device
 
-    def __iter__(self):
-        """Yield a point as a Tensor of shape (4,) at a time.
+    def __iter__(self) -> Iterator[Tensor]:
+        """Yield a point as a Tensor at a time.
 
         Returns:
-            torch.Tensor: A point of shape (4,).
+            Iterator[Tensor]: A point of shape (points_dim, ).
         """
         yield from self.tensor
 
-    def new_point(self, data):
+    def new_point(
+        self, data: Union[Tensor, np.ndarray, Sequence[Sequence[float]]]
+    ) -> 'BasePoints':
         """Create a new point object with data.
 
-        The new point and its tensor has the similar properties
-            as self and self.tensor, respectively.
+        The new point and its tensor has the similar properties as self and
+        self.tensor, respectively.
 
         Args:
-            data (torch.Tensor | numpy.array | list): Data to be copied.
+            data (Tensor or np.ndarray or Sequence[Sequence[float]]): Data to
+                be copied.
 
         Returns:
-            :obj:`BasePoints`: A new point object with ``data``,
-                the object's other properties are similar to ``self``.
+            :obj:`BasePoints`: A new point object with ``data``, the object's
+            other properties are similar to ``self``.
         """
         new_tensor = self.tensor.new_tensor(data) \
-            if not isinstance(data, torch.Tensor) else data.to(self.device)
+            if not isinstance(data, Tensor) else data.to(self.device)
         original_type = type(self)
         return original_type(
             new_tensor,
             points_dim=self.points_dim,
             attribute_dims=self.attribute_dims)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/structures/points/cam_points.py` & `mmdet3d-1.1.1/mmdet3d/structures/points/depth_points.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,63 +1,72 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Optional, Sequence, Union
+
+import numpy as np
+from torch import Tensor
+
 from .base_points import BasePoints
 
 
-class CameraPoints(BasePoints):
-    """Points of instances in CAM coordinates.
+class DepthPoints(BasePoints):
+    """Points of instances in DEPTH coordinates.
 
     Args:
-        tensor (torch.Tensor | np.ndarray | list): a N x points_dim matrix.
-        points_dim (int, optional): Number of the dimension of a point.
-            Each row is (x, y, z). Defaults to 3.
-        attribute_dims (dict, optional): Dictionary to indicate the
-            meaning of extra dimension. Defaults to None.
+        tensor (Tensor or np.ndarray or Sequence[Sequence[float]]): The points
+            data with shape (N, points_dim).
+        points_dim (int): Integer indicating the dimension of a point. Each row
+            is (x, y, z, ...). Defaults to 3.
+        attribute_dims (dict, optional): Dictionary to indicate the meaning of
+            extra dimension. Defaults to None.
 
     Attributes:
-        tensor (torch.Tensor): Float matrix of N x points_dim.
-        points_dim (int): Integer indicating the dimension of a point.
-            Each row is (x, y, z, ...).
-        attribute_dims (bool): Dictionary to indicate the meaning of extra
-            dimension. Defaults to None.
+        tensor (Tensor): Float matrix with shape (N, points_dim).
+        points_dim (int): Integer indicating the dimension of a point. Each row
+            is (x, y, z, ...).
+        attribute_dims (dict, optional): Dictionary to indicate the meaning of
+            extra dimension. Defaults to None.
         rotation_axis (int): Default rotation axis for points rotation.
     """
 
-    def __init__(self, tensor, points_dim=3, attribute_dims=None):
-        super(CameraPoints, self).__init__(
+    def __init__(self,
+                 tensor: Union[Tensor, np.ndarray, Sequence[Sequence[float]]],
+                 points_dim: int = 3,
+                 attribute_dims: Optional[dict] = None) -> None:
+        super(DepthPoints, self).__init__(
             tensor, points_dim=points_dim, attribute_dims=attribute_dims)
-        self.rotation_axis = 1
+        self.rotation_axis = 2
 
-    def flip(self, bev_direction='horizontal'):
+    def flip(self, bev_direction: str = 'horizontal') -> None:
         """Flip the points along given BEV direction.
 
         Args:
             bev_direction (str): Flip direction (horizontal or vertical).
+                Defaults to 'horizontal'.
         """
+        assert bev_direction in ('horizontal', 'vertical')
         if bev_direction == 'horizontal':
             self.tensor[:, 0] = -self.tensor[:, 0]
         elif bev_direction == 'vertical':
-            self.tensor[:, 2] = -self.tensor[:, 2]
-
-    @property
-    def bev(self):
-        """torch.Tensor: BEV of the points in shape (N, 2)."""
-        return self.tensor[:, [0, 2]]
+            self.tensor[:, 1] = -self.tensor[:, 1]
 
-    def convert_to(self, dst, rt_mat=None):
+    def convert_to(self,
+                   dst: int,
+                   rt_mat: Optional[Union[Tensor,
+                                          np.ndarray]] = None) -> 'BasePoints':
         """Convert self to ``dst`` mode.
 
         Args:
-            dst (:obj:`CoordMode`): The target Point mode.
-            rt_mat (np.ndarray | torch.Tensor, optional): The rotation and
+            dst (int): The target Point mode.
+            rt_mat (Tensor or np.ndarray, optional): The rotation and
                 translation matrix between different coordinates.
-                Defaults to None.
-                The conversion from `src` coordinates to `dst` coordinates
-                usually comes along the change of sensors, e.g., from camera
-                to LiDAR. This requires a transformation matrix.
+                Defaults to None. The conversion from ``src`` coordinates to
+                ``dst`` coordinates usually comes along the change of sensors,
+                e.g., from camera to LiDAR. This requires a transformation
+                matrix.
 
         Returns:
-            :obj:`BasePoints`: The converted point of the same type
-                in the `dst` mode.
+            :obj:`BasePoints`: The converted point of the same type in the
+            ``dst`` mode.
         """
-        from mmdet3d.structures import Coord3DMode
+        from mmdet3d.structures.bbox_3d import Coord3DMode
         return Coord3DMode.convert_point(
-            point=self, src=Coord3DMode.CAM, dst=dst, rt_mat=rt_mat)
+            point=self, src=Coord3DMode.DEPTH, dst=dst, rt_mat=rt_mat)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/structures/points/depth_points.py` & `mmdet3d-1.1.1/mmdet3d/structures/points/cam_points.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,58 +1,77 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Optional, Sequence, Union
+
+import numpy as np
+from torch import Tensor
+
 from .base_points import BasePoints
 
 
-class DepthPoints(BasePoints):
-    """Points of instances in DEPTH coordinates.
+class CameraPoints(BasePoints):
+    """Points of instances in CAM coordinates.
 
     Args:
-        tensor (torch.Tensor | np.ndarray | list): a N x points_dim matrix.
-        points_dim (int, optional): Number of the dimension of a point.
-            Each row is (x, y, z). Defaults to 3.
-        attribute_dims (dict, optional): Dictionary to indicate the
-            meaning of extra dimension. Defaults to None.
+        tensor (Tensor or np.ndarray or Sequence[Sequence[float]]): The points
+            data with shape (N, points_dim).
+        points_dim (int): Integer indicating the dimension of a point. Each row
+            is (x, y, z, ...). Defaults to 3.
+        attribute_dims (dict, optional): Dictionary to indicate the meaning of
+            extra dimension. Defaults to None.
 
     Attributes:
-        tensor (torch.Tensor): Float matrix of N x points_dim.
-        points_dim (int): Integer indicating the dimension of a point.
-            Each row is (x, y, z, ...).
-        attribute_dims (bool): Dictionary to indicate the meaning of extra
-            dimension. Defaults to None.
+        tensor (Tensor): Float matrix with shape (N, points_dim).
+        points_dim (int): Integer indicating the dimension of a point. Each row
+            is (x, y, z, ...).
+        attribute_dims (dict, optional): Dictionary to indicate the meaning of
+            extra dimension. Defaults to None.
         rotation_axis (int): Default rotation axis for points rotation.
     """
 
-    def __init__(self, tensor, points_dim=3, attribute_dims=None):
-        super(DepthPoints, self).__init__(
+    def __init__(self,
+                 tensor: Union[Tensor, np.ndarray, Sequence[Sequence[float]]],
+                 points_dim: int = 3,
+                 attribute_dims: Optional[dict] = None) -> None:
+        super(CameraPoints, self).__init__(
             tensor, points_dim=points_dim, attribute_dims=attribute_dims)
-        self.rotation_axis = 2
+        self.rotation_axis = 1
 
-    def flip(self, bev_direction='horizontal'):
+    def flip(self, bev_direction: str = 'horizontal') -> None:
         """Flip the points along given BEV direction.
 
         Args:
             bev_direction (str): Flip direction (horizontal or vertical).
+                Defaults to 'horizontal'.
         """
+        assert bev_direction in ('horizontal', 'vertical')
         if bev_direction == 'horizontal':
             self.tensor[:, 0] = -self.tensor[:, 0]
         elif bev_direction == 'vertical':
-            self.tensor[:, 1] = -self.tensor[:, 1]
+            self.tensor[:, 2] = -self.tensor[:, 2]
 
-    def convert_to(self, dst, rt_mat=None):
+    @property
+    def bev(self) -> Tensor:
+        """Tensor: BEV of the points in shape (N, 2)."""
+        return self.tensor[:, [0, 2]]
+
+    def convert_to(self,
+                   dst: int,
+                   rt_mat: Optional[Union[Tensor,
+                                          np.ndarray]] = None) -> 'BasePoints':
         """Convert self to ``dst`` mode.
 
         Args:
-            dst (:obj:`CoordMode`): The target Point mode.
-            rt_mat (np.ndarray | torch.Tensor, optional): The rotation and
+            dst (int): The target Point mode.
+            rt_mat (Tensor or np.ndarray, optional): The rotation and
                 translation matrix between different coordinates.
-                Defaults to None.
-                The conversion from `src` coordinates to `dst` coordinates
-                usually comes along the change of sensors, e.g., from camera
-                to LiDAR. This requires a transformation matrix.
+                Defaults to None. The conversion from ``src`` coordinates to
+                ``dst`` coordinates usually comes along the change of sensors,
+                e.g., from camera to LiDAR. This requires a transformation
+                matrix.
 
         Returns:
-            :obj:`BasePoints`: The converted point of the same type
-                in the `dst` mode.
+            :obj:`BasePoints`: The converted point of the same type in the
+            ``dst`` mode.
         """
-        from mmdet3d.structures import Coord3DMode
+        from mmdet3d.structures.bbox_3d import Coord3DMode
         return Coord3DMode.convert_point(
-            point=self, src=Coord3DMode.DEPTH, dst=dst, rt_mat=rt_mat)
+            point=self, src=Coord3DMode.CAM, dst=dst, rt_mat=rt_mat)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/structures/points/lidar_points.py` & `mmdet3d-1.1.1/mmdet3d/structures/points/lidar_points.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,58 +1,72 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Optional, Sequence, Union
+
+import numpy as np
+from torch import Tensor
+
 from .base_points import BasePoints
 
 
 class LiDARPoints(BasePoints):
     """Points of instances in LIDAR coordinates.
 
     Args:
-        tensor (torch.Tensor | np.ndarray | list): a N x points_dim matrix.
-        points_dim (int, optional): Number of the dimension of a point.
-            Each row is (x, y, z). Defaults to 3.
-        attribute_dims (dict, optional): Dictionary to indicate the
-            meaning of extra dimension. Defaults to None.
+        tensor (Tensor or np.ndarray or Sequence[Sequence[float]]): The points
+            data with shape (N, points_dim).
+        points_dim (int): Integer indicating the dimension of a point. Each row
+            is (x, y, z, ...). Defaults to 3.
+        attribute_dims (dict, optional): Dictionary to indicate the meaning of
+            extra dimension. Defaults to None.
 
     Attributes:
-        tensor (torch.Tensor): Float matrix of N x points_dim.
-        points_dim (int): Integer indicating the dimension of a point.
-            Each row is (x, y, z, ...).
-        attribute_dims (bool): Dictionary to indicate the meaning of extra
-            dimension. Defaults to None.
+        tensor (Tensor): Float matrix with shape (N, points_dim).
+        points_dim (int): Integer indicating the dimension of a point. Each row
+            is (x, y, z, ...).
+        attribute_dims (dict, optional): Dictionary to indicate the meaning of
+            extra dimension. Defaults to None.
         rotation_axis (int): Default rotation axis for points rotation.
     """
 
-    def __init__(self, tensor, points_dim=3, attribute_dims=None):
+    def __init__(self,
+                 tensor: Union[Tensor, np.ndarray, Sequence[Sequence[float]]],
+                 points_dim: int = 3,
+                 attribute_dims: Optional[dict] = None) -> None:
         super(LiDARPoints, self).__init__(
             tensor, points_dim=points_dim, attribute_dims=attribute_dims)
         self.rotation_axis = 2
 
-    def flip(self, bev_direction='horizontal'):
+    def flip(self, bev_direction: str = 'horizontal') -> None:
         """Flip the points along given BEV direction.
 
         Args:
             bev_direction (str): Flip direction (horizontal or vertical).
+                Defaults to 'horizontal'.
         """
+        assert bev_direction in ('horizontal', 'vertical')
         if bev_direction == 'horizontal':
             self.tensor[:, 1] = -self.tensor[:, 1]
         elif bev_direction == 'vertical':
             self.tensor[:, 0] = -self.tensor[:, 0]
 
-    def convert_to(self, dst, rt_mat=None):
+    def convert_to(self,
+                   dst: int,
+                   rt_mat: Optional[Union[Tensor,
+                                          np.ndarray]] = None) -> 'BasePoints':
         """Convert self to ``dst`` mode.
 
         Args:
-            dst (:obj:`CoordMode`): The target Point mode.
-            rt_mat (np.ndarray | torch.Tensor, optional): The rotation and
+            dst (int): The target Point mode.
+            rt_mat (Tensor or np.ndarray, optional): The rotation and
                 translation matrix between different coordinates.
-                Defaults to None.
-                The conversion from `src` coordinates to `dst` coordinates
-                usually comes along the change of sensors, e.g., from camera
-                to LiDAR. This requires a transformation matrix.
+                Defaults to None. The conversion from ``src`` coordinates to
+                ``dst`` coordinates usually comes along the change of sensors,
+                e.g., from camera to LiDAR. This requires a transformation
+                matrix.
 
         Returns:
-            :obj:`BasePoints`: The converted point of the same type
-                in the `dst` mode.
+            :obj:`BasePoints`: The converted point of the same type in the
+            ``dst`` mode.
         """
-        from mmdet3d.structures import Coord3DMode
+        from mmdet3d.structures.bbox_3d import Coord3DMode
         return Coord3DMode.convert_point(
             point=self, src=Coord3DMode.LIDAR, dst=dst, rt_mat=rt_mat)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/testing/__init__.py` & `mmdet3d-1.1.1/mmdet3d/testing/__init__.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/testing/data_utils.py` & `mmdet3d-1.1.1/mmdet3d/testing/data_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -156,15 +156,19 @@
             0.0,
             'index':
             0,
             'group_id':
             0
         }],
         'plane':
-        None
+        None,
+        'pts_semantic_mask_path':
+        'tests/data/semantickitti/sequences/00/labels/000000.label',
+        'pts_panoptic_mask_path':
+        'tests/data/semantickitti/sequences/00/labels/000000.label',
     }
     if with_ann:
         data_info['ann_info'] = ann_info
     return data_info
 
 
 def create_data_info_after_loading():
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/testing/model_utils.py` & `mmdet3d-1.1.1/mmdet3d/testing/model_utils.py`

 * *Files 6% similar despite different names*

```diff
@@ -80,14 +80,15 @@
                            num_gt_instance=20,
                            num_points=10,
                            points_feat_dim=4,
                            num_classes=3,
                            gt_bboxes_dim=7,
                            with_pts_semantic_mask=False,
                            with_pts_instance_mask=False,
+                           with_eval_ann_info=False,
                            bboxes_3d_type='lidar'):
     setup_seed(seed)
     assert bboxes_3d_type in ('lidar', 'depth', 'cam')
     bbox_3d_class = {
         'lidar': LiDARInstance3DBoxes,
         'depth': DepthInstance3DBoxes,
         'cam': CameraInstance3DBoxes
@@ -141,9 +142,13 @@
     data_sample.gt_pts_seg = PointData()
     if with_pts_instance_mask:
         pts_instance_mask = torch.randint(0, num_gt_instance, [num_points])
         data_sample.gt_pts_seg['pts_instance_mask'] = pts_instance_mask
     if with_pts_semantic_mask:
         pts_semantic_mask = torch.randint(0, num_classes, [num_points])
         data_sample.gt_pts_seg['pts_semantic_mask'] = pts_semantic_mask
+    if with_eval_ann_info:
+        data_sample.eval_ann_info = dict()
+    else:
+        data_sample.eval_ann_info = None
 
     return dict(inputs=inputs_dict, data_samples=[data_sample])
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/utils/__init__.py` & `mmdet3d-1.1.1/mmdet3d/utils/__init__.py`

 * *Files 19% similar despite different names*

```diff
@@ -2,15 +2,16 @@
 from .array_converter import ArrayConverter, array_converter
 from .collect_env import collect_env
 from .compat_cfg import compat_cfg
 from .misc import replace_ceph_backend
 from .setup_env import register_all_modules, setup_multi_processes
 from .typing_utils import (ConfigType, InstanceList, MultiConfig,
                            OptConfigType, OptInstanceList, OptMultiConfig,
-                           OptSamplingResultList)
+                           OptSampleList, OptSamplingResultList)
 
 __all__ = [
     'collect_env', 'setup_multi_processes', 'compat_cfg',
     'register_all_modules', 'array_converter', 'ArrayConverter', 'ConfigType',
     'OptConfigType', 'MultiConfig', 'OptMultiConfig', 'InstanceList',
-    'OptInstanceList', 'OptSamplingResultList', 'replace_ceph_backend'
+    'OptInstanceList', 'OptSamplingResultList', 'replace_ceph_backend',
+    'OptSampleList'
 ]
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/utils/array_converter.py` & `mmdet3d-1.1.1/mmdet3d/utils/array_converter.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,52 +1,54 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import functools
 from inspect import getfullargspec
+from typing import Callable, Optional, Tuple, Type, Union
 
 import numpy as np
 import torch
 
+TemplateArrayType = Union[np.ndarray, torch.Tensor, list, tuple, int, float]
 
-def array_converter(to_torch=True,
-                    apply_to=tuple(),
-                    template_arg_name_=None,
-                    recover=True):
+
+def array_converter(to_torch: bool = True,
+                    apply_to: Tuple[str, ...] = tuple(),
+                    template_arg_name_: Optional[str] = None,
+                    recover: bool = True) -> Callable:
     """Wrapper function for data-type agnostic processing.
 
-    First converts input arrays to PyTorch tensors or NumPy ndarrays
-    for middle calculation, then convert output to original data-type if
-    `recover=True`.
+    First converts input arrays to PyTorch tensors or NumPy arrays for middle
+    calculation, then convert output to original data-type if `recover=True`.
 
     Args:
-        to_torch (Bool, optional): Whether convert to PyTorch tensors
-            for middle calculation. Defaults to True.
-        apply_to (tuple[str], optional): The arguments to which we apply
-            data-type conversion. Defaults to an empty tuple.
-        template_arg_name_ (str, optional): Argument serving as the template (
-            return arrays should have the same dtype and device
-            as the template). Defaults to None. If None, we will use the
-            first argument in `apply_to` as the template argument.
-        recover (Bool, optional): Whether or not recover the wrapped function
-            outputs to the `template_arg_name_` type. Defaults to True.
+        to_torch (bool): Whether to convert to PyTorch tensors for middle
+            calculation. Defaults to True.
+        apply_to (Tuple[str]): The arguments to which we apply data-type
+            conversion. Defaults to an empty tuple.
+        template_arg_name_ (str, optional): Argument serving as the template
+            (return arrays should have the same dtype and device as the
+            template). Defaults to None. If None, we will use the first
+            argument in `apply_to` as the template argument.
+        recover (bool): Whether or not to recover the wrapped function outputs
+            to the `template_arg_name_` type. Defaults to True.
 
     Raises:
-        ValueError: When template_arg_name_ is not among all args, or
-            when apply_to contains an arg which is not among all args,
-            a ValueError will be raised. When the template argument or
-            an argument to convert is a list or tuple, and cannot be
-            converted to a NumPy array, a ValueError will be raised.
-        TypeError: When the type of the template argument or
-                an argument to convert does not belong to the above range,
-                or the contents of such an list-or-tuple-type argument
-                do not share the same data type, a TypeError is raised.
+        ValueError: When template_arg_name_ is not among all args, or when
+            apply_to contains an arg which is not among all args, a ValueError
+            will be raised. When the template argument or an argument to
+            convert is a list or tuple, and cannot be converted to a NumPy
+            array, a ValueError will be raised.
+        TypeError: When the type of the template argument or an argument to
+            convert does not belong to the above range, or the contents of such
+            an list-or-tuple-type argument do not share the same data type, a
+            TypeError will be raised.
 
     Returns:
-        (function): wrapped function.
+        Callable: Wrapped function.
 
-    Example:
+    Examples:
         >>> import torch
         >>> import numpy as np
         >>>
         >>> # Use torch addition for a + b,
         >>> # and convert return values to the type of a
         >>> @array_converter(apply_to=('a', 'b'))
         >>> def simple_add(a, b):
@@ -59,15 +61,15 @@
         >>> # Use numpy addition for a + b,
         >>> # and convert return values to the type of b
         >>> @array_converter(to_torch=False, apply_to=('a', 'b'),
         >>>                  template_arg_name_='b')
         >>> def simple_add(a, b):
         >>>     return a + b
         >>>
-        >>> simple_add()
+        >>> simple_add(a, b)
         >>>
         >>> # Use torch funcs for floor(a) if flag=True else ceil(a),
         >>> # and return the torch tensor
         >>> @array_converter(apply_to=('a',), recover=False)
         >>> def floor_or_ceil(a, flag=True):
         >>>     return torch.floor(a) if flag else torch.ceil(a)
         >>>
@@ -118,16 +120,16 @@
             if template_arg_name not in all_arg_names:
                 raise ValueError(f'{template_arg_name} is not among the '
                                  f'argument list of function {func_name}')
 
             # inspect apply_to
             for arg_to_apply in apply_to:
                 if arg_to_apply not in all_arg_names:
-                    raise ValueError(f'{arg_to_apply} is not '
-                                     f'an argument of {func_name}')
+                    raise ValueError(
+                        f'{arg_to_apply} is not an argument of {func_name}')
 
             new_args = []
             new_kwargs = {}
 
             converter = ArrayConverter()
             target_type = torch.Tensor if to_torch else np.ndarray
 
@@ -196,36 +198,42 @@
 
         return new_func
 
     return array_converter_wrapper
 
 
 class ArrayConverter:
+    """Utility class for data-type agnostic processing.
 
+    Args:
+        template_array (np.ndarray or torch.Tensor or list or tuple or int or
+            float, optional): Template array. Defaults to None.
+    """
     SUPPORTED_NON_ARRAY_TYPES = (int, float, np.int8, np.int16, np.int32,
                                  np.int64, np.uint8, np.uint16, np.uint32,
                                  np.uint64, np.float16, np.float32, np.float64)
 
-    def __init__(self, template_array=None):
+    def __init__(self,
+                 template_array: Optional[TemplateArrayType] = None) -> None:
         if template_array is not None:
             self.set_template(template_array)
 
-    def set_template(self, array):
+    def set_template(self, array: TemplateArrayType) -> None:
         """Set template array.
 
         Args:
-            array (tuple | list | int | float | np.ndarray | torch.Tensor):
-                Template array.
+            array (np.ndarray or torch.Tensor or list or tuple or int or
+                float): Template array.
 
         Raises:
-            ValueError: If input is list or tuple and cannot be converted to
-                to a NumPy array, a ValueError is raised.
-            TypeError: If input type does not belong to the above range,
-                or the contents of a list or tuple do not share the
-                same data type, a TypeError is raised.
+            ValueError: If input is list or tuple and cannot be converted to a
+                NumPy array, a ValueError is raised.
+            TypeError: If input type does not belong to the above range, or the
+                contents of a list or tuple do not share the same data type, a
+                TypeError is raised.
         """
         self.array_type = type(array)
         self.is_num = False
         self.device = 'cpu'
 
         if isinstance(array, np.ndarray):
             self.dtype = array.dtype
@@ -235,54 +243,60 @@
         elif isinstance(array, (list, tuple)):
             try:
                 array = np.array(array)
                 if array.dtype not in self.SUPPORTED_NON_ARRAY_TYPES:
                     raise TypeError
                 self.dtype = array.dtype
             except (ValueError, TypeError):
-                print(f'The following list cannot be converted to'
-                      f' a numpy array of supported dtype:\n{array}')
+                print('The following list cannot be converted to a numpy '
+                      f'array of supported dtype:\n{array}')
                 raise
-        elif isinstance(array, self.SUPPORTED_NON_ARRAY_TYPES):
+        elif isinstance(array, (int, float)):
             self.array_type = np.ndarray
             self.is_num = True
             self.dtype = np.dtype(type(array))
         else:
-            raise TypeError(f'Template type {self.array_type}'
-                            f' is not supported.')
+            raise TypeError(
+                f'Template type {self.array_type} is not supported.')
 
-    def convert(self, input_array, target_type=None, target_array=None):
+    def convert(
+        self,
+        input_array: TemplateArrayType,
+        target_type: Optional[Type] = None,
+        target_array: Optional[Union[np.ndarray, torch.Tensor]] = None
+    ) -> Union[np.ndarray, torch.Tensor]:
         """Convert input array to target data type.
 
         Args:
-            input_array (tuple | list | np.ndarray |
-                torch.Tensor | int | float ):
-                Input array. Defaults to None.
-            target_type (<class 'np.ndarray'> | <class 'torch.Tensor'>,
-                optional):
-                Type to which input array is converted. Defaults to None.
-            target_array (np.ndarray | torch.Tensor, optional):
-                Template array to which input array is converted.
+            input_array (np.ndarray or torch.Tensor or list or tuple or int or
+                float): Input array.
+            target_type (Type, optional): Type to which input array is
+                converted. It should be `np.ndarray` or `torch.Tensor`.
                 Defaults to None.
+            target_array (np.ndarray or torch.Tensor, optional): Template array
+                to which input array is converted. Defaults to None.
 
         Raises:
-            ValueError: If input is list or tuple and cannot be converted to
-                to a NumPy array, a ValueError is raised.
-            TypeError: If input type does not belong to the above range,
-                or the contents of a list or tuple do not share the
-                same data type, a TypeError is raised.
+            ValueError: If input is list or tuple and cannot be converted to a
+                NumPy array, a ValueError is raised.
+            TypeError: If input type does not belong to the above range, or the
+                contents of a list or tuple do not share the same data type, a
+                TypeError is raised.
+
+        Returns:
+            np.ndarray or torch.Tensor: The converted array.
         """
         if isinstance(input_array, (list, tuple)):
             try:
                 input_array = np.array(input_array)
                 if input_array.dtype not in self.SUPPORTED_NON_ARRAY_TYPES:
                     raise TypeError
             except (ValueError, TypeError):
-                print(f'The input cannot be converted to'
-                      f' a single-type numpy array:\n{input_array}')
+                print('The input cannot be converted to a single-type numpy '
+                      f'array:\n{input_array}')
                 raise
         elif isinstance(input_array, self.SUPPORTED_NON_ARRAY_TYPES):
             input_array = np.array(input_array)
         array_type = type(input_array)
         assert target_type is not None or target_array is not None, \
             'must specify a target'
         if target_type is not None:
@@ -305,15 +319,25 @@
             elif isinstance(target_array, np.ndarray):
                 converted_array = input_array.cpu().numpy().astype(
                     target_array.dtype)
             else:
                 converted_array = target_array.new_tensor(input_array)
         return converted_array
 
-    def recover(self, input_array):
+    def recover(
+        self, input_array: Union[np.ndarray, torch.Tensor]
+    ) -> Union[np.ndarray, torch.Tensor, int, float]:
+        """Recover input type to original array type.
+
+        Args:
+            input_array (np.ndarray or torch.Tensor): Input array.
+
+        Returns:
+            np.ndarray or torch.Tensor or int or float: Converted array.
+        """
         assert isinstance(input_array, (np.ndarray, torch.Tensor)), \
             'invalid input array type'
         if isinstance(input_array, self.array_type):
             return input_array
         elif isinstance(input_array, torch.Tensor):
             converted_array = input_array.cpu().numpy().astype(self.dtype)
         else:
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/utils/collect_env.py` & `mmdet3d-1.1.1/mmdet3d/utils/collect_env.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/utils/compat_cfg.py` & `mmdet3d-1.1.1/mmdet3d/utils/compat_cfg.py`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/utils/misc.py` & `mmdet3d-1.1.1/mmdet3d/utils/misc.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 
 
 def replace_ceph_backend(cfg):
     cfg_pretty_text = cfg.pretty_text
 
     replace_strs = \
-        r'''file_client_args = dict(
+        r'''backend_args = dict(
             backend='petrel',
             path_mapping=dict({
                 './data/DATA/': 's3://openmmlab/datasets/detection3d/CEPH/',
                 'data/DATA/': 's3://openmmlab/datasets/detection3d/CEPH/'
             }))
         '''
 
@@ -45,14 +45,16 @@
 
     replace_strs = replace_strs.replace(' ', '').replace('\n', '')
 
     # use data info file from ceph
     # cfg_pretty_text = cfg_pretty_text.replace(
     #   'ann_file', replace_strs + ', ann_file')
 
+    cfg_pretty_text = cfg_pretty_text.replace('backend_args=None', '')
+
     # replace LoadImageFromFile
     cfg_pretty_text = cfg_pretty_text.replace(
         'LoadImageFromFile\'', 'LoadImageFromFile\',' + replace_strs)
 
     # replace LoadImageFromFileMono3D
     cfg_pretty_text = cfg_pretty_text.replace(
         'LoadImageFromFileMono3D\'',
@@ -76,14 +78,26 @@
     cfg_pretty_text = cfg_pretty_text.replace(
         'LoadAnnotations\'', 'LoadAnnotations\',' + replace_strs)
 
     # replace LoadAnnotations3D
     cfg_pretty_text = cfg_pretty_text.replace(
         'LoadAnnotations3D\'', 'LoadAnnotations3D\',' + replace_strs)
 
+    # replace KittiMetric
+    cfg_pretty_text = cfg_pretty_text.replace('KittiMetric\'',
+                                              'KittiMetric\',' + replace_strs)
+
+    # replace LyftMetric
+    cfg_pretty_text = cfg_pretty_text.replace('LyftMetric\'',
+                                              'LyftMetric\',' + replace_strs)
+
+    # replace NuScenesMetric
+    cfg_pretty_text = cfg_pretty_text.replace(
+        'NuScenesMetric\'', 'NuScenesMetric\',' + replace_strs)
+
     # replace WaymoMetric
     cfg_pretty_text = cfg_pretty_text.replace('WaymoMetric\'',
                                               'WaymoMetric\',' + replace_strs)
 
     # replace dbsampler
     cfg_pretty_text = cfg_pretty_text.replace('info_path',
                                               replace_strs + ', info_path')
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/utils/setup_env.py` & `mmdet3d-1.1.1/mmdet3d/utils/setup_env.py`

 * *Files 0% similar despite different names*

```diff
@@ -60,15 +60,15 @@
     """Register all modules in mmdet3d into the registries.
 
     Args:
         init_default_scope (bool): Whether initialize the mmdet default scope.
             When `init_default_scope=True`, the global default scope will be
             set to `mmdet3d`, and all registries will build modules from mmdet3d's
             registry node. To understand more about the registry, please refer
-            to https://github.com/open-mmlab/mmengine/blob/main/docs/en/tutorials/registry.md
+            to https://github.com/open-mmlab/mmengine/blob/main/docs/en/advanced_tutorials/registry.md
             Defaults to True.
     """  # noqa
     import mmdet3d.datasets  # noqa: F401,F403
     import mmdet3d.engine  # noqa: F401,F403
     import mmdet3d.evaluation.metrics  # noqa: F401,F403
     import mmdet3d.models  # noqa: F401,F403
     import mmdet3d.structures  # noqa: F401,F403
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/utils/typing_utils.py` & `mmdet3d-1.1.1/mmdet3d/utils/typing_utils.py`

 * *Files 20% similar despite different names*

```diff
@@ -19,7 +19,8 @@
 InstanceList = List[InstanceData]
 OptInstanceList = Optional[InstanceList]
 
 SamplingResultList = List[SamplingResult]
 
 OptSamplingResultList = Optional[SamplingResultList]
 SampleList = List[Det3DDataSample]
+OptSampleList = Optional[SampleList]
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/version.py` & `mmdet3d-1.1.1/mmdet3d/version.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,22 +1,22 @@
 # Copyright (c) Open-MMLab. All rights reserved.
 
-__version__ = '1.1.0rc3'
+__version__ = '1.1.1'
 short_version = __version__
 
 
-def parse_version_info(version_str):
+def parse_version_info(version_str: str) -> tuple:
     """Parse a version string into a tuple.
 
     Args:
         version_str (str): The version string.
 
     Returns:
-        tuple[int | str]: The version info, e.g., "1.3.0" is parsed into
-            (1, 3, 0), and "2.0.0rc1" is parsed into (2, 0, 0, 'rc1').
+        tuple: The version info, e.g., "1.3.0" is parsed into (1, 3, 0), and
+        "2.0.0rc4" is parsed into (2, 0, 0, 'rc4').
     """
     version_info = []
     for x in version_str.split('.'):
         if x.isdigit():
             version_info.append(int(x))
         elif x.find('rc') != -1:
             patch_version = x.split('rc')
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/visualization/local_visualizer.py` & `mmdet3d-1.1.1/mmdet3d/visualization/local_visualizer.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,29 +1,33 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import copy
-from typing import Dict, List, Optional, Tuple, Union
+import math
+import time
+from typing import List, Optional, Sequence, Tuple, Union
 
 import matplotlib.pyplot as plt
 import mmcv
 import numpy as np
 from matplotlib.collections import PatchCollection
 from matplotlib.patches import PathPatch
 from matplotlib.path import Path
-from mmdet.visualization import DetLocalVisualizer
+from mmdet.visualization import DetLocalVisualizer, get_palette
 from mmengine.dist import master_only
 from mmengine.structures import InstanceData
-from mmengine.visualization.utils import check_type, tensor2ndarray
+from mmengine.visualization import Visualizer as MMENGINE_Visualizer
+from mmengine.visualization.utils import (check_type, color_val_matplotlib,
+                                          tensor2ndarray)
 from torch import Tensor
 
 from mmdet3d.registry import VISUALIZERS
-from mmdet3d.structures import (BaseInstance3DBoxes, CameraInstance3DBoxes,
-                                Coord3DMode, DepthInstance3DBoxes,
-                                Det3DDataSample, LiDARInstance3DBoxes,
-                                PointData, points_cam2img)
-from mmdet3d.structures.bbox_3d.box_3d_mode import Box3DMode
+from mmdet3d.structures import (BaseInstance3DBoxes, Box3DMode,
+                                CameraInstance3DBoxes, Coord3DMode,
+                                DepthInstance3DBoxes, Det3DDataSample,
+                                LiDARInstance3DBoxes, PointData,
+                                points_cam2img)
 from .vis_utils import (proj_camera_bbox3d_to_img, proj_depth_bbox3d_to_img,
                         proj_lidar_bbox3d_to_img, to_depth_mode)
 
 try:
     import open3d as o3d
     from open3d import geometry
     from open3d.visualization import Visualizer
@@ -39,103 +43,121 @@
 
       - draw_bboxes_3d: draw 3D bounding boxes on point clouds
       - draw_proj_bboxes_3d: draw projected 3D bounding boxes on image
       - draw_seg_mask: draw segmentation mask via per-point colorization
 
     Args:
         name (str): Name of the instance. Defaults to 'visualizer'.
-        points (numpy.array, shape=[N, 3+C]): points to visualize.
-        image (np.ndarray, optional): the origin image to draw. The format
+        points (np.ndarray, optional): Points to visualize with shape (N, 3+C).
+            Defaults to None.
+        image (np.ndarray, optional): The origin image to draw. The format
             should be RGB. Defaults to None.
-        pcd_mode (int): The point cloud mode (coordinates):
-            0 represents LiDAR, 1 represents CAMERA, 2
-            represents Depth. Defaults to 0.
-        vis_backends (list, optional): Visual backend config list.
+        pcd_mode (int): The point cloud mode (coordinates): 0 represents LiDAR,
+            1 represents CAMERA, 2 represents Depth. Defaults to 0.
+        vis_backends (List[dict], optional): Visual backend config list.
             Defaults to None.
         save_dir (str, optional): Save file dir for all storage backends.
             If it is None, the backend storage will not save any data.
-        bbox_color (str, tuple(int), optional): Color of bbox lines.
-            The tuple of color should be in BGR order. Defaults to None.
-        text_color (str, tuple(int), optional): Color of texts.
-            The tuple of color should be in BGR order.
-            Defaults to (200, 200, 200).
-        mask_color (str, tuple(int), optional): Color of masks.
-            The tuple of color should be in BGR order.
             Defaults to None.
-        line_width (int, float): The linewidth of lines.
-            Defaults to 3.
+        bbox_color (str or Tuple[int], optional): Color of bbox lines.
+            The tuple of color should be in BGR order. Defaults to None.
+        text_color (str or Tuple[int]): Color of texts. The tuple of color
+            should be in BGR order. Defaults to (200, 200, 200).
+        mask_color (str or Tuple[int], optional): Color of masks. The tuple of
+            color should be in BGR order. Defaults to None.
+        line_width (int or float): The linewidth of lines. Defaults to 3.
         frame_cfg (dict): The coordinate frame config while Open3D
             visualization initialization.
             Defaults to dict(size=1, origin=[0, 0, 0]).
-        alpha (int, float): The transparency of bboxes or mask.
-                Defaults to 0.8.
+        alpha (int or float): The transparency of bboxes or mask.
+            Defaults to 0.8.
+        multi_imgs_col (int): The number of columns in arrangement when showing
+            multi-view images.
 
     Examples:
         >>> import numpy as np
         >>> import torch
         >>> from mmengine.structures import InstanceData
-        >>> from mmdet3d.structures import Det3DDataSample
+        >>> from mmdet3d.structures import (DepthInstance3DBoxes
+        ...                                 Det3DDataSample)
         >>> from mmdet3d.visualization import Det3DLocalVisualizer
 
         >>> det3d_local_visualizer = Det3DLocalVisualizer()
-        >>> image = np.random.randint(0, 256,
-        ...                     size=(10, 12, 3)).astype('uint8')
-        >>> points = np.random.rand((1000, ))
+        >>> image = np.random.randint(0, 256, size=(10, 12, 3)).astype('uint8')
+        >>> points = np.random.rand(1000, 3)
         >>> gt_instances_3d = InstanceData()
-        >>> gt_instances_3d.bboxes_3d = BaseInstance3DBoxes(torch.rand((5, 7)))
+        >>> gt_instances_3d.bboxes_3d = DepthInstance3DBoxes(
+        ...     torch.rand((5, 7)))
         >>> gt_instances_3d.labels_3d = torch.randint(0, 2, (5,))
         >>> gt_det3d_data_sample = Det3DDataSample()
         >>> gt_det3d_data_sample.gt_instances_3d = gt_instances_3d
         >>> data_input = dict(img=image, points=points)
         >>> det3d_local_visualizer.add_datasample('3D Scene', data_input,
-        ...                         gt_det3d_data_sample)
+        ...                                       gt_det3d_data_sample)
+
+        >>> from mmdet3d.structures import PointData
+        >>> det3d_local_visualizer = Det3DLocalVisualizer()
+        >>> points = np.random.rand(1000, 3)
+        >>> gt_pts_seg = PointData()
+        >>> gt_pts_seg.pts_semantic_mask = torch.randint(0, 10, (1000, ))
+        >>> gt_det3d_data_sample = Det3DDataSample()
+        >>> gt_det3d_data_sample.gt_pts_seg = gt_pts_seg
+        >>> data_input = dict(points=points)
+        >>> det3d_local_visualizer.add_datasample('3D Scene', data_input,
+        ...                                       gt_det3d_data_sample,
+        ...                                       vis_task='lidar_seg')
     """
 
-    def __init__(self,
-                 name: str = 'visualizer',
-                 points: Optional[np.ndarray] = None,
-                 image: Optional[np.ndarray] = None,
-                 pcd_mode: int = 0,
-                 vis_backends: Optional[Dict] = None,
-                 save_dir: Optional[str] = None,
-                 bbox_color: Optional[Union[str, Tuple[int]]] = None,
-                 text_color: Optional[Union[str,
-                                            Tuple[int]]] = (200, 200, 200),
-                 mask_color: Optional[Union[str, Tuple[int]]] = None,
-                 line_width: Union[int, float] = 3,
-                 frame_cfg: dict = dict(size=1, origin=[0, 0, 0]),
-                 alpha: float = 0.8):
+    def __init__(
+        self,
+        name: str = 'visualizer',
+        points: Optional[np.ndarray] = None,
+        image: Optional[np.ndarray] = None,
+        pcd_mode: int = 0,
+        vis_backends: Optional[List[dict]] = None,
+        save_dir: Optional[str] = None,
+        bbox_color: Optional[Union[str, Tuple[int]]] = None,
+        text_color: Union[str, Tuple[int]] = (200, 200, 200),
+        mask_color: Optional[Union[str, Tuple[int]]] = None,
+        line_width: Union[int, float] = 3,
+        frame_cfg: dict = dict(size=1, origin=[0, 0, 0]),
+        alpha: Union[int, float] = 0.8,
+        multi_imgs_col: int = 3,
+        fig_show_cfg: dict = dict(figsize=(18, 12))
+    ) -> None:
         super().__init__(
             name=name,
             image=image,
             vis_backends=vis_backends,
             save_dir=save_dir,
             bbox_color=bbox_color,
             text_color=text_color,
             mask_color=mask_color,
             line_width=line_width,
             alpha=alpha)
         if points is not None:
             self.set_points(points, pcd_mode=pcd_mode, frame_cfg=frame_cfg)
         self.pts_seg_num = 0
+        self.multi_imgs_col = multi_imgs_col
+        self.fig_show_cfg.update(fig_show_cfg)
 
     def _clear_o3d_vis(self) -> None:
         """Clear open3d vis."""
 
         if hasattr(self, 'o3d_vis'):
             del self.o3d_vis
             del self.pcd
             del self.points_colors
 
-    def _initialize_o3d_vis(self, frame_cfg) -> Visualizer:
+    def _initialize_o3d_vis(self, frame_cfg: dict) -> Visualizer:
         """Initialize open3d vis according to frame_cfg.
 
         Args:
-            frame_cfg (dict): The config to create coordinate frame
-                in open3d vis.
+            frame_cfg (dict): The config to create coordinate frame in open3d
+                vis.
 
         Returns:
             :obj:`o3d.visualization.Visualizer`: Created open3d vis.
         """
         if o3d is None or geometry is None:
             raise ImportError(
                 'Please run "pip install open3d" to install open3d first.')
@@ -148,40 +170,39 @@
 
     @master_only
     def set_points(self,
                    points: np.ndarray,
                    pcd_mode: int = 0,
                    vis_mode: str = 'replace',
                    frame_cfg: dict = dict(size=1, origin=[0, 0, 0]),
-                   points_color: Tuple = (0.5, 0.5, 0.5),
+                   points_color: Tuple[float] = (0.8, 0.8, 0.8),
                    points_size: int = 2,
                    mode: str = 'xyz') -> None:
-        """Set the points to draw.
+        """Set the point cloud to draw.
 
         Args:
-            points (numpy.array, shape=[N, 3+C]):
-                points to visualize.
-            pcd_mode (int): The point cloud mode (coordinates):
-                0 represents LiDAR, 1 represents CAMERA, 2
-                represents Depth. Defaults to 0.
+            points (np.ndarray): Points to visualize with shape (N, 3+C).
+            pcd_mode (int): The point cloud mode (coordinates): 0 represents
+                LiDAR, 1 represents CAMERA, 2 represents Depth. Defaults to 0.
             vis_mode (str): The visualization mode in Open3D:
-                'replace': Replace the existing point cloud with
-                    input point cloud.
-                'add': Add input point cloud into existing point
-                    cloud.
+
+                - 'replace': Replace the existing point cloud with input point
+                  cloud.
+                - 'add': Add input point cloud into existing point cloud.
+
                 Defaults to 'replace'.
-            frame_cfg (dict): The coordinate frame config while Open3D
+            frame_cfg (dict): The coordinate frame config for Open3D
                 visualization initialization.
                 Defaults to dict(size=1, origin=[0, 0, 0]).
-            point_color (tuple[float], optional): the color of points.
-                Default: (0.5, 0.5, 0.5).
-            points_size (int, optional): the size of points to show
-                on visualizer. Default: 2.
-            mode (str, optional):  indicate type of the input points,
-                available mode ['xyz', 'xyzrgb']. Default: 'xyz'.
+            points_color (Tuple[float]): The color of points.
+                Defaults to (1, 1, 1).
+            points_size (int): The size of points to show on visualizer.
+                Defaults to 2.
+            mode (str): Indicate type of the input points, available mode
+                ['xyz', 'xyzrgb']. Defaults to 'xyz'.
         """
         assert points is not None
         assert vis_mode in ('replace', 'add')
         check_type('points', points, np.ndarray)
 
         if not hasattr(self, 'o3d_vis'):
             self.o3d_vis = self._initialize_o3d_vis(frame_cfg)
@@ -190,15 +211,18 @@
         if pcd_mode != Coord3DMode.DEPTH:
             points = Coord3DMode.convert(points, pcd_mode, Coord3DMode.DEPTH)
 
         if hasattr(self, 'pcd') and vis_mode != 'add':
             self.o3d_vis.remove_geometry(self.pcd)
 
         # set points size in Open3D
-        self.o3d_vis.get_render_option().point_size = points_size
+        render_option = self.o3d_vis.get_render_option()
+        if render_option is not None:
+            render_option.point_size = points_size
+            render_option.background_color = np.asarray([0, 0, 0])
 
         points = points.copy()
         pcd = geometry.PointCloud()
         if mode == 'xyz':
             pcd.points = o3d.utility.Vector3dVector(points[:, :3])
             points_colors = np.tile(
                 np.array(points_color), (points.shape[0], 1))
@@ -217,36 +241,35 @@
         self.points_colors = points_colors
 
     # TODO: assign 3D Box color according to pred / GT labels
     # We draw GT / pred bboxes on the same point cloud scenes
     # for better detection performance comparison
     def draw_bboxes_3d(self,
                        bboxes_3d: BaseInstance3DBoxes,
-                       bbox_color=(0, 1, 0),
-                       points_in_box_color=(1, 0, 0),
-                       rot_axis=2,
-                       center_mode='lidar_bottom',
-                       mode='xyz'):
+                       bbox_color: Tuple[float] = (0, 1, 0),
+                       points_in_box_color: Tuple[float] = (1, 0, 0),
+                       rot_axis: int = 2,
+                       center_mode: str = 'lidar_bottom',
+                       mode: str = 'xyz') -> None:
         """Draw bbox on visualizer and change the color of points inside
         bbox3d.
 
         Args:
-            bboxes_3d (:obj:`BaseInstance3DBoxes`, shape=[M, 7]):
-                3d bbox (x, y, z, x_size, y_size, z_size, yaw) to visualize.
-            bbox_color (tuple[float], optional): the color of 3D bboxes.
-                Default: (0, 1, 0).
-            points_in_box_color (tuple[float], optional):
-                the color of points inside 3D bboxes. Default: (1, 0, 0).
-            rot_axis (int, optional): rotation axis of 3D bboxes.
-                Default: 2.
-            center_mode (bool, optional): Indicates the center of bbox is
-                bottom center or gravity center. available mode
-                ['lidar_bottom', 'camera_bottom']. Default: 'lidar_bottom'.
-            mode (str, optional):  Indicates type of input points,
-                available mode ['xyz', 'xyzrgb']. Default: 'xyz'.
+            bboxes_3d (:obj:`BaseInstance3DBoxes`): 3D bbox
+                (x, y, z, x_size, y_size, z_size, yaw) to visualize.
+            bbox_color (Tuple[float]): The color of 3D bboxes.
+                Defaults to (0, 1, 0).
+            points_in_box_color (Tuple[float]): The color of points inside 3D
+                bboxes. Defaults to (1, 0, 0).
+            rot_axis (int): Rotation axis of 3D bboxes. Defaults to 2.
+            center_mode (str): Indicates the center of bbox is bottom center or
+                gravity center. Available mode
+                ['lidar_bottom', 'camera_bottom']. Defaults to 'lidar_bottom'.
+            mode (str): Indicates the type of input points, available mode
+                ['xyz', 'xyzrgb']. Defaults to 'xyz'.
         """
         # Before visualizing the 3D Boxes in point cloud scene
         # we need to convert the boxes to Depth mode
         check_type('bboxes', bboxes_3d, BaseInstance3DBoxes)
 
         if not isinstance(bboxes_3d, DepthInstance3DBoxes):
             bboxes_3d = bboxes_3d.convert_to(Box3DMode.DEPTH)
@@ -269,15 +292,15 @@
             elif center_mode == 'camera_bottom':
                 # bottom center to gravity center
                 center[rot_axis] -= dim[rot_axis] / 2
             box3d = geometry.OrientedBoundingBox(center, rot_mat, dim)
 
             line_set = geometry.LineSet.create_from_oriented_bounding_box(
                 box3d)
-            line_set.paint_uniform_color(bbox_color)
+            line_set.paint_uniform_color(np.array(bbox_color[i]) / 255.)
             # draw bboxes on visualizer
             self.o3d_vis.add_geometry(line_set)
 
             # change the color of points which are in box
             if self.pcd is not None and mode == 'xyz':
                 indices = box3d.get_point_indices_within_bounding_box(
                     self.pcd.points)
@@ -286,15 +309,15 @@
         # update points colors
         if self.pcd is not None:
             self.pcd.colors = o3d.utility.Vector3dVector(self.points_colors)
             self.o3d_vis.update_geometry(self.pcd)
 
     def set_bev_image(self,
                       bev_image: Optional[np.ndarray] = None,
-                      bev_shape: Optional[int] = 900) -> None:
+                      bev_shape: int = 900) -> None:
         """Set the bev image to draw.
 
         Args:
             bev_image (np.ndarray, optional): The bev image to draw.
                 Defaults to None.
             bev_shape (int): The bev image shape. Defaults to 900.
         """
@@ -333,51 +356,49 @@
             markeredgecolor='red')
 
     # TODO: Support bev point cloud visualization
     @master_only
     def draw_bev_bboxes(self,
                         bboxes_3d: BaseInstance3DBoxes,
                         scale: int = 15,
-                        edge_colors: Union[str, tuple, List[str],
-                                           List[tuple]] = 'o',
+                        edge_colors: Union[str, Tuple[int],
+                                           List[Union[str, Tuple[int]]]] = 'o',
                         line_styles: Union[str, List[str]] = '-',
-                        line_widths: Union[Union[int, float],
-                                           List[Union[int, float]]] = 1,
-                        face_colors: Union[str, tuple, List[str],
-                                           List[tuple]] = 'none',
-                        alpha: Union[int, float] = 1):
+                        line_widths: Union[int, float, List[Union[int,
+                                                                  float]]] = 1,
+                        face_colors: Union[str, Tuple[int],
+                                           List[Union[str,
+                                                      Tuple[int]]]] = 'none',
+                        alpha: Union[int, float] = 1) -> MMENGINE_Visualizer:
         """Draw projected 3D boxes on the image.
 
         Args:
-            bboxes_3d (:obj:`BaseInstance3DBoxes`, shape=[M, 7]):
-                3d bbox (x, y, z, x_size, y_size, z_size, yaw) to visualize.
+            bboxes_3d (:obj:`BaseInstance3DBoxes`): 3D bbox
+                (x, y, z, x_size, y_size, z_size, yaw) to visualize.
             scale (dict): Value to scale the bev bboxes for better
                 visualization. Defaults to 15.
-            edge_colors (Union[str, tuple, List[str], List[tuple]]): The
-                colors of bboxes. ``colors`` can have the same length with
+            edge_colors (str or Tuple[int] or List[str or Tuple[int]]):
+                The colors of bboxes. ``colors`` can have the same length with
                 lines or just single value. If ``colors`` is single value, all
                 the lines will have the same colors. Refer to `matplotlib.
                 colors` for full list of formats that are accepted.
                 Defaults to 'o'.
-            line_styles (Union[str, List[str]]): The linestyle
-                of lines. ``line_styles`` can have the same length with
-                texts or just single value. If ``line_styles`` is single
-                value, all the lines will have the same linestyle.
-                Reference to
+            line_styles (str or List[str]): The linestyle of lines.
+                ``line_styles`` can have the same length with texts or just
+                single value. If ``line_styles`` is single value, all the lines
+                will have the same linestyle. Reference to
                 https://matplotlib.org/stable/api/collections_api.html?highlight=collection#matplotlib.collections.AsteriskPolygonCollection.set_linestyle
                 for more details. Defaults to '-'.
-            line_widths (Union[Union[int, float], List[Union[int, float]]]):
-                The linewidth of lines. ``line_widths`` can have
-                the same length with lines or just single value.
-                If ``line_widths`` is single value, all the lines will
-                have the same linewidth. Defaults to 2.
-            face_colors (Union[str, tuple, List[str], List[tuple]]):
-                The face colors. Default to 'none'.
-            alpha (Union[int, float]): The transparency of bboxes.
-                Defaults to 1.
+            line_widths (int or float or List[int or float]): The linewidth of
+                lines. ``line_widths`` can have the same length with lines or
+                just single value. If ``line_widths`` is single value, all the
+                lines will have the same linewidth. Defaults to 2.
+            face_colors (str or Tuple[int] or List[str or Tuple[int]]):
+                The face colors. Defaults to 'none'.
+            alpha (int or float): The transparency of bboxes. Defaults to 1.
         """
 
         check_type('bboxes', bboxes_3d, BaseInstance3DBoxes)
         bev_bboxes = tensor2ndarray(bboxes_3d.bev)
         # scale the bev bboxes for better visualization
         bev_bboxes[:, :4] *= scale
         ctr, w, h, theta = np.split(bev_bboxes, [2, 3, 4], axis=-1)
@@ -397,100 +418,121 @@
             alpha=alpha,
             edge_colors=edge_colors,
             line_styles=line_styles,
             line_widths=line_widths,
             face_colors=face_colors)
 
     @master_only
-    def draw_points_on_image(
-            self,
-            points: Union[np.ndarray, Tensor],
-            pts2img: np.ndarray,
-            sizes: Optional[Union[np.ndarray, Tensor, int]] = 10) -> None:
+    def draw_points_on_image(self,
+                             points: Union[np.ndarray, Tensor],
+                             pts2img: np.ndarray,
+                             sizes: Union[np.ndarray, int] = 3,
+                             max_depth: Optional[float] = None) -> None:
         """Draw projected points on the image.
 
         Args:
-            positions (Union[np.ndarray, torch.Tensor]): Positions to draw.
-            pts2imgs (np,ndarray): The transformatino matrix from the
-                coordinate of point cloud to image plane.
-            sizes (Optional[Union[np.ndarray, torch.Tensor, int]]): The
-                marker size. Default to 10.
+            points (np.ndarray or Tensor): Points to draw.
+            pts2img (np.ndarray): The transformation matrix from the coordinate
+                of point cloud to image plane.
+            sizes (np.ndarray or int): The marker size. Defaults to 10.
+            max_depth (float): The max depth in the color map. Defaults to
+                None.
         """
         check_type('points', points, (np.ndarray, Tensor))
         points = tensor2ndarray(points)
         assert self._image is not None, 'Please set image using `set_image`'
         projected_points = points_cam2img(points, pts2img, with_depth=True)
         depths = projected_points[:, 2]
-        colors = (depths % 20) / 20
+        # Show depth adaptively consideing different scenes
+        if max_depth is None:
+            max_depth = depths.max()
+        colors = (depths % max_depth) / max_depth
         # use colormap to obtain the render color
         color_map = plt.get_cmap('jet')
         self.ax_save.scatter(
             projected_points[:, 0],
             projected_points[:, 1],
             c=colors,
             cmap=color_map,
             s=sizes,
-            alpha=0.5,
+            alpha=0.7,
             edgecolors='none')
 
     # TODO: set bbox color according to palette
     @master_only
-    def draw_proj_bboxes_3d(self,
-                            bboxes_3d: BaseInstance3DBoxes,
-                            input_meta: dict,
-                            edge_colors: Union[str, tuple, List[str],
-                                               List[tuple]] = 'royalblue',
-                            line_styles: Union[str, List[str]] = '-',
-                            line_widths: Union[Union[int, float],
-                                               List[Union[int, float]]] = 2,
-                            face_colors: Union[str, tuple, List[str],
-                                               List[tuple]] = 'royalblue',
-                            alpha: Union[int, float] = 0.4):
+    def draw_proj_bboxes_3d(
+            self,
+            bboxes_3d: BaseInstance3DBoxes,
+            input_meta: dict,
+            edge_colors: Union[str, Tuple[int],
+                               List[Union[str, Tuple[int]]]] = 'royalblue',
+            line_styles: Union[str, List[str]] = '-',
+            line_widths: Union[int, float, List[Union[int, float]]] = 2,
+            face_colors: Union[str, Tuple[int],
+                               List[Union[str, Tuple[int]]]] = 'royalblue',
+            alpha: Union[int, float] = 0.4,
+            img_size: Optional[Tuple] = None):
         """Draw projected 3D boxes on the image.
 
         Args:
-            bbox3d (:obj:`BaseInstance3DBoxes`, shape=[M, 7]):
-                3d bbox (x, y, z, x_size, y_size, z_size, yaw) to visualize.
+            bboxes_3d (:obj:`BaseInstance3DBoxes`): 3D bbox
+                (x, y, z, x_size, y_size, z_size, yaw) to visualize.
             input_meta (dict): Input meta information.
-            edge_colors (Union[str, tuple, List[str], List[tuple]]): The
-                colors of bboxes. ``colors`` can have the same length with
+            edge_colors (str or Tuple[int] or List[str or Tuple[int]]):
+                The colors of bboxes. ``colors`` can have the same length with
                 lines or just single value. If ``colors`` is single value, all
                 the lines will have the same colors. Refer to `matplotlib.
                 colors` for full list of formats that are accepted.
                 Defaults to 'royalblue'.
-            line_styles (Union[str, List[str]]): The linestyle
-                of lines. ``line_styles`` can have the same length with
-                texts or just single value. If ``line_styles`` is single
-                value, all the lines will have the same linestyle.
-                Reference to
+            line_styles (str or List[str]): The linestyle of lines.
+                ``line_styles`` can have the same length with texts or just
+                single value. If ``line_styles`` is single value, all the lines
+                will have the same linestyle. Reference to
                 https://matplotlib.org/stable/api/collections_api.html?highlight=collection#matplotlib.collections.AsteriskPolygonCollection.set_linestyle
                 for more details. Defaults to '-'.
-            line_widths (Union[Union[int, float], List[Union[int, float]]]):
-                The linewidth of lines. ``line_widths`` can have
-                the same length with lines or just single value.
-                If ``line_widths`` is single value, all the lines will
-                have the same linewidth. Defaults to 2.
-            face_colors (Union[str, tuple, List[str], List[tuple]]):
-                The face colors. Default to 'royalblue'.
-            alpha (Union[int, float]): The transparency of bboxes.
-                Defaults to 0.4.
+            line_widths (int or float or List[int or float]): The linewidth of
+                lines. ``line_widths`` can have the same length with lines or
+                just single value. If ``line_widths`` is single value, all the
+                lines will have the same linewidth. Defaults to 2.
+            face_colors (str or Tuple[int] or List[str or Tuple[int]]):
+                The face colors. Defaults to 'royalblue'.
+            alpha (int or float): The transparency of bboxes. Defaults to 0.4.
+            img_size (tuple, optional): The size (w, h) of the image.
         """
 
         check_type('bboxes', bboxes_3d, BaseInstance3DBoxes)
 
         if isinstance(bboxes_3d, DepthInstance3DBoxes):
             proj_bbox3d_to_img = proj_depth_bbox3d_to_img
         elif isinstance(bboxes_3d, LiDARInstance3DBoxes):
             proj_bbox3d_to_img = proj_lidar_bbox3d_to_img
         elif isinstance(bboxes_3d, CameraInstance3DBoxes):
             proj_bbox3d_to_img = proj_camera_bbox3d_to_img
         else:
             raise NotImplementedError('unsupported box type!')
 
+        edge_colors_norm = color_val_matplotlib(edge_colors)
+
         corners_2d = proj_bbox3d_to_img(bboxes_3d, input_meta)
+        if img_size is not None:
+            # Filter out the bbox where half of stuff is outside the image.
+            # This is for the visualization of multi-view image.
+            valid_point_idx = (corners_2d[..., 0] >= 0) & \
+                        (corners_2d[..., 0] <= img_size[0]) & \
+                        (corners_2d[..., 1] >= 0) & (corners_2d[..., 1] <= img_size[1])  # noqa: E501
+            valid_bbox_idx = valid_point_idx.sum(axis=-1) >= 4
+            corners_2d = corners_2d[valid_bbox_idx]
+            filter_edge_colors = []
+            filter_edge_colors_norm = []
+            for i, color in enumerate(edge_colors):
+                if valid_bbox_idx[i]:
+                    filter_edge_colors.append(color)
+                    filter_edge_colors_norm.append(edge_colors_norm[i])
+            edge_colors = filter_edge_colors
+            edge_colors_norm = filter_edge_colors_norm
 
         lines_verts_idx = [0, 1, 2, 3, 7, 6, 5, 4, 0, 3, 7, 4, 5, 1, 2, 6]
         lines_verts = corners_2d[:, lines_verts_idx, :]
         front_polys = corners_2d[:, 4:, :]
         codes = [Path.LINETO] * lines_verts.shape[1]
         codes[0] = Path.MOVETO
         pathpatches = []
@@ -498,125 +540,196 @@
             verts = lines_verts[i]
             pth = Path(verts, codes)
             pathpatches.append(PathPatch(pth))
 
         p = PatchCollection(
             pathpatches,
             facecolors='none',
-            edgecolors=edge_colors,
+            edgecolors=edge_colors_norm,
             linewidths=line_widths,
             linestyles=line_styles)
 
         self.ax_save.add_collection(p)
 
         # draw a mask on the front of project bboxes
         front_polys = [front_poly for front_poly in front_polys]
         return self.draw_polygons(
             front_polys,
             alpha=alpha,
             edge_colors=edge_colors,
             line_styles=line_styles,
             line_widths=line_widths,
-            face_colors=face_colors)
+            face_colors=edge_colors)
 
     @master_only
-    def draw_seg_mask(self, seg_mask_colors: np.array):
+    def draw_seg_mask(self, seg_mask_colors: np.ndarray) -> None:
         """Add segmentation mask to visualizer via per-point colorization.
 
         Args:
-            seg_mask_colors (numpy.array, shape=[N, 6]):
-                The segmentation mask whose first 3 dims are point coordinates
-                and last 3 dims are converted colors.
+            seg_mask_colors (np.ndarray): The segmentation mask with shape
+                (N, 6), whose first 3 dims are point coordinates and last 3
+                dims are converted colors.
         """
         # we can't draw the colors on existing points
         # in case gt and pred mask would overlap
         # instead we set a large offset along x-axis for each seg mask
         self.pts_seg_num += 1
         offset = (np.array(self.pcd.points).max(0) -
                   np.array(self.pcd.points).min(0))[0] * 1.2 * self.pts_seg_num
         mesh_frame = geometry.TriangleMesh.create_coordinate_frame(
             size=1, origin=[offset, 0, 0])  # create coordinate frame for seg
         self.o3d_vis.add_geometry(mesh_frame)
         seg_points = copy.deepcopy(seg_mask_colors)
         seg_points[:, 0] += offset
         self.set_points(seg_points, pcd_mode=2, vis_mode='add', mode='xyzrgb')
 
-    def _draw_instances_3d(self, data_input: dict, instances: InstanceData,
-                           input_meta: dict, vis_task: str,
-                           palette: Optional[List[tuple]]):
+    def _draw_instances_3d(self,
+                           data_input: dict,
+                           instances: InstanceData,
+                           input_meta: dict,
+                           vis_task: str,
+                           palette: Optional[List[tuple]] = None) -> dict:
         """Draw 3D instances of GT or prediction.
 
         Args:
             data_input (dict): The input dict to draw.
-            instances (:obj:`InstanceData`): Data structure for
-                instance-level annotations or predictions.
-            metainfo (dict): Meta information.
-            vis_task (str): Visualiztion task, it includes:
-                'lidar_det', 'multi-modality_det', 'mono_det'.
+            instances (:obj:`InstanceData`): Data structure for instance-level
+                annotations or predictions.
+            input_meta (dict): Meta information.
+            vis_task (str): Visualization task, it includes: 'lidar_det',
+                'multi-modality_det', 'mono_det'.
+            palette (List[tuple], optional): Palette information corresponding
+                to the category. Defaults to None.
 
         Returns:
-            dict: the drawn point cloud and image which channel is RGB.
+            dict: The drawn point cloud and image whose channel is RGB.
         """
 
+        # Only visualize when there is at least one instance
+        if not len(instances) > 0:
+            return None
+
         bboxes_3d = instances.bboxes_3d  # BaseInstance3DBoxes
+        labels_3d = instances.labels_3d
 
         data_3d = dict()
 
         if vis_task in ['lidar_det', 'multi-modality_det']:
             assert 'points' in data_input
             points = data_input['points']
             check_type('points', points, (np.ndarray, Tensor))
             points = tensor2ndarray(points)
 
             if not isinstance(bboxes_3d, DepthInstance3DBoxes):
                 points, bboxes_3d_depth = to_depth_mode(points, bboxes_3d)
             else:
                 bboxes_3d_depth = bboxes_3d.clone()
 
+            max_label = int(max(labels_3d) if len(labels_3d) > 0 else 0)
+            bbox_color = palette if self.bbox_color is None \
+                else self.bbox_color
+            bbox_palette = get_palette(bbox_color, max_label + 1)
+            colors = [bbox_palette[label] for label in labels_3d]
+
             self.set_points(points, pcd_mode=2)
-            self.draw_bboxes_3d(bboxes_3d_depth)
+            self.draw_bboxes_3d(bboxes_3d_depth, bbox_color=colors)
 
             data_3d['bboxes_3d'] = tensor2ndarray(bboxes_3d_depth.tensor)
             data_3d['points'] = points
 
         if vis_task in ['mono_det', 'multi-modality_det']:
             assert 'img' in data_input
             img = data_input['img']
-            if isinstance(data_input['img'], Tensor):
-                img = img.permute(1, 2, 0).numpy()
-                img = img[..., [2, 1, 0]]  # bgr to rgb
-            self.set_image(img)
-            self.draw_proj_bboxes_3d(bboxes_3d, input_meta)
-            if vis_task == 'mono_det' and hasattr(instances, 'centers_2d'):
-                centers_2d = instances.centers_2d
-                self.draw_points(centers_2d)
-            drawn_img = self.get_image()
-            data_3d['img'] = drawn_img
+            if isinstance(img, list) or (isinstance(img, (np.ndarray, Tensor))
+                                         and len(img.shape) == 4):
+                # show multi-view images
+                img_size = img[0].shape[:2] if isinstance(
+                    img, list) else img.shape[-2:]  # noqa: E501
+                img_col = self.multi_imgs_col
+                img_row = math.ceil(len(img) / img_col)
+                composed_img = np.zeros(
+                    (img_size[0] * img_row, img_size[1] * img_col, 3),
+                    dtype=np.uint8)
+                for i, single_img in enumerate(img):
+                    # Note that we should keep the same order of elements both
+                    # in `img` and `input_meta`
+                    if isinstance(single_img, Tensor):
+                        single_img = single_img.permute(1, 2, 0).numpy()
+                        single_img = single_img[..., [2, 1, 0]]  # bgr to rgb
+                    self.set_image(single_img)
+                    single_img_meta = dict()
+                    for key, meta in input_meta.items():
+                        if isinstance(meta,
+                                      (Sequence, np.ndarray,
+                                       Tensor)) and len(meta) == len(img):
+                            single_img_meta[key] = meta[i]
+                        else:
+                            single_img_meta[key] = meta
+
+                    max_label = int(
+                        max(labels_3d) if len(labels_3d) > 0 else 0)
+                    bbox_color = palette if self.bbox_color is None \
+                        else self.bbox_color
+                    bbox_palette = get_palette(bbox_color, max_label + 1)
+                    colors = [bbox_palette[label] for label in labels_3d]
+
+                    self.draw_proj_bboxes_3d(
+                        bboxes_3d,
+                        single_img_meta,
+                        img_size=single_img.shape[:2][::-1],
+                        edge_colors=colors)
+                    if vis_task == 'mono_det' and hasattr(
+                            instances, 'centers_2d'):
+                        centers_2d = instances.centers_2d
+                        self.draw_points(centers_2d)
+                    composed_img[(i // img_col) *
+                                 img_size[0]:(i // img_col + 1) * img_size[0],
+                                 (i % img_col) *
+                                 img_size[1]:(i % img_col + 1) *
+                                 img_size[1]] = self.get_image()
+                data_3d['img'] = composed_img
+            else:
+                # show single-view image
+                # TODO: Solve the problem: some line segments of 3d bboxes are
+                # out of image by a large margin
+                if isinstance(data_input['img'], Tensor):
+                    img = img.permute(1, 2, 0).numpy()
+                    img = img[..., [2, 1, 0]]  # bgr to rgb
+                self.set_image(img)
+
+                max_label = int(max(labels_3d) if len(labels_3d) > 0 else 0)
+                bbox_color = palette if self.bbox_color is None \
+                    else self.bbox_color
+                bbox_palette = get_palette(bbox_color, max_label + 1)
+                colors = [bbox_palette[label] for label in labels_3d]
+
+                self.draw_proj_bboxes_3d(
+                    bboxes_3d, input_meta, edge_colors=colors)
+                if vis_task == 'mono_det' and hasattr(instances, 'centers_2d'):
+                    centers_2d = instances.centers_2d
+                    self.draw_points(centers_2d)
+                drawn_img = self.get_image()
+                data_3d['img'] = drawn_img
 
         return data_3d
 
     def _draw_pts_sem_seg(self,
                           points: Union[Tensor, np.ndarray],
                           pts_seg: PointData,
                           palette: Optional[List[tuple]] = None,
-                          ignore_index: Optional[int] = None):
+                          ignore_index: Optional[int] = None) -> None:
         """Draw 3D semantic mask of GT or prediction.
 
         Args:
-            points (Tensor | np.ndarray): The input point
-                cloud to draw.
-            pts_seg (:obj:`PointData`): Data structure for
-                pixel-level annotations or predictions.
-            palette (List[tuple], optional): Palette information
-                corresponding to the category. Defaults to None.
-            ignore_index (int, optional): Ignore category.
-                Defaults to None.
-
-        Returns:
-            dict: the drawn points with color.
+            points (Tensor or np.ndarray): The input point cloud to draw.
+            pts_seg (:obj:`PointData`): Data structure for pixel-level
+                annotations or predictions.
+            palette (List[tuple], optional): Palette information corresponding
+                to the category. Defaults to None.
+            ignore_index (int, optional): Ignore category. Defaults to None.
         """
         check_type('points', points, (np.ndarray, Tensor))
 
         points = tensor2ndarray(points)
         pts_sem_seg = tensor2ndarray(pts_seg.pts_semantic_mask)
         palette = np.array(palette)
 
@@ -633,91 +746,125 @@
     @master_only
     def show(self,
              save_path: Optional[str] = None,
              drawn_img_3d: Optional[np.ndarray] = None,
              drawn_img: Optional[np.ndarray] = None,
              win_name: str = 'image',
              wait_time: int = 0,
-             continue_key=' ') -> None:
+             continue_key: str = ' ',
+             vis_task: str = 'lidar_det') -> None:
         """Show the drawn point cloud/image.
 
         Args:
             save_path (str, optional): Path to save open3d visualized results.
-                Default: None.
+                Defaults to None.
+            drawn_img_3d (np.ndarray, optional): The image to show. If
+                drawn_img_3d is not None, it will show the image got by
+                Visualizer. Defaults to None.
             drawn_img (np.ndarray, optional): The image to show. If drawn_img
-                is None, it will show the image got by Visualizer. Defaults
-                to None.
-            win_name (str):  The image title. Defaults to 'image'.
-            wait_time (int): Delay in milliseconds. 0 is the special
-                value that means "forever". Defaults to 0.
-            continue_key (str): The key for users to continue. Defaults to
-                the space key.
+                is not None, it will show the image got by Visualizer.
+                Defaults to None.
+            win_name (str): The image title. Defaults to 'image'.
+            wait_time (int): Delay in milliseconds. 0 is the special value that
+                means "forever". Defaults to 0.
+            continue_key (str): The key for users to continue. Defaults to ' '.
         """
+        if vis_task == 'multi-modality_det':
+            img_wait_time = 0.5
+        else:
+            img_wait_time = wait_time
+
+        # In order to show multi-modal results at the same time, we show image
+        # firstly and then show point cloud since the running of
+        # Open3D will block the process
+        if hasattr(self, '_image'):
+            if drawn_img is None and drawn_img_3d is None:
+                # use the image got by Visualizer.get_image()
+                super().show(drawn_img_3d, win_name, img_wait_time,
+                             continue_key)
+            else:
+                if drawn_img_3d is not None:
+                    super().show(drawn_img_3d, win_name, img_wait_time,
+                                 continue_key)
+                if drawn_img is not None:
+                    super().show(drawn_img, win_name, img_wait_time,
+                                 continue_key)
+
         if hasattr(self, 'o3d_vis'):
-            self.o3d_vis.run()
+            self.o3d_vis.poll_events()
+            self.o3d_vis.update_renderer()
+            if wait_time > 0:
+                time.sleep(wait_time)
+            else:
+                self.o3d_vis.run()
             if save_path is not None:
+                if not (save_path.endswith('.png')
+                        or save_path.endswith('.jpg')):
+                    save_path += '.png'
                 self.o3d_vis.capture_screen_image(save_path)
+
+            # TODO: support more flexible window control
+            self.o3d_vis.clear_geometries()
             self.o3d_vis.destroy_window()
+            self.o3d_vis.close()
             self._clear_o3d_vis()
 
-        if hasattr(self, '_image'):
-            if drawn_img_3d is None:
-                super().show(drawn_img_3d, win_name, wait_time, continue_key)
-            super().show(drawn_img, win_name, wait_time, continue_key)
-
     # TODO: Support Visualize the 3D results from image and point cloud
     # respectively
     @master_only
     def add_datasample(self,
                        name: str,
                        data_input: dict,
-                       data_sample: Optional['Det3DDataSample'] = None,
+                       data_sample: Optional[Det3DDataSample] = None,
                        draw_gt: bool = True,
                        draw_pred: bool = True,
                        show: bool = False,
                        wait_time: float = 0,
                        out_file: Optional[str] = None,
                        o3d_save_path: Optional[str] = None,
                        vis_task: str = 'mono_det',
                        pred_score_thr: float = 0.3,
                        step: int = 0) -> None:
         """Draw datasample and save to all backends.
 
-        - If GT and prediction are plotted at the same time, they are
-        displayed in a stitched image where the left image is the
-        ground truth and the right image is the prediction.
-        - If ``show`` is True, all storage backends are ignored, and
-        the images will be displayed in a local window.
+        - If GT and prediction are plotted at the same time, they are displayed
+          in a stitched image where the left image is the ground truth and the
+          right image is the prediction.
+        - If ``show`` is True, all storage backends are ignored, and the images
+          will be displayed in a local window.
         - If ``out_file`` is specified, the drawn image will be saved to
-        ``out_file``. It is usually used when the display is not available.
+          ``out_file``. It is usually used when the display is not available.
 
         Args:
             name (str): The image identifier.
             data_input (dict): It should include the point clouds or image
                 to draw.
             data_sample (:obj:`Det3DDataSample`, optional): Prediction
                 Det3DDataSample. Defaults to None.
             draw_gt (bool): Whether to draw GT Det3DDataSample.
-                Default to True.
+                Defaults to True.
             draw_pred (bool): Whether to draw Prediction Det3DDataSample.
                 Defaults to True.
-            show (bool): Whether to display the drawn point clouds and
-                image. Default to False.
+            show (bool): Whether to display the drawn point clouds and image.
+                Defaults to False.
             wait_time (float): The interval of show (s). Defaults to 0.
-            out_file (str): Path to output file. Defaults to None.
+            out_file (str, optional): Path to output file. Defaults to None.
             o3d_save_path (str, optional): Path to save open3d visualized
-                results Default: None.
-            vis-task (str): Visualization task. Defaults to 'mono_det'.
+                results. Defaults to None.
+            vis_task (str): Visualization task. Defaults to 'mono_det'.
             pred_score_thr (float): The threshold to visualize the bboxes
                 and masks. Defaults to 0.3.
             step (int): Global step value to record. Defaults to 0.
         """
-        classes = self.dataset_meta.get('CLASSES', None)
-        # For object detection datasets, no PALETTE is saved
-        palette = self.dataset_meta.get('PALETTE', None)
+        assert vis_task in (
+            'mono_det', 'multi-view_det', 'lidar_det', 'lidar_seg',
+            'multi-modality_det'), f'got unexpected vis_task {vis_task}.'
+        classes = self.dataset_meta.get('classes', None)
+        # For object detection datasets, no palette is saved
+        palette = self.dataset_meta.get('palette', None)
         ignore_index = self.dataset_meta.get('ignore_index', None)
 
         gt_data_3d = None
         pred_data_3d = None
         gt_img_data = None
         pred_img_data = None
 
@@ -725,54 +872,56 @@
             if 'gt_instances_3d' in data_sample:
                 gt_data_3d = self._draw_instances_3d(
                     data_input, data_sample.gt_instances_3d,
                     data_sample.metainfo, vis_task, palette)
             if 'gt_instances' in data_sample:
                 if len(data_sample.gt_instances) > 0:
                     assert 'img' in data_input
+                    img = data_input['img']
                     if isinstance(data_input['img'], Tensor):
                         img = data_input['img'].permute(1, 2, 0).numpy()
                         img = img[..., [2, 1, 0]]  # bgr to rgb
                     gt_img_data = self._draw_instances(
                         img, data_sample.gt_instances, classes, palette)
-            if 'gt_pts_seg' in data_sample and vis_task == 'seg':
+            if 'gt_pts_seg' in data_sample and vis_task == 'lidar_seg':
                 assert classes is not None, 'class information is ' \
                                             'not provided when ' \
-                                            'visualizing panoptic ' \
+                                            'visualizing semantic ' \
                                             'segmentation results.'
                 assert 'points' in data_input
                 self._draw_pts_sem_seg(data_input['points'],
-                                       data_sample.pred_pts_seg, palette,
+                                       data_sample.gt_pts_seg, palette,
                                        ignore_index)
 
         if draw_pred and data_sample is not None:
             if 'pred_instances_3d' in data_sample:
                 pred_instances_3d = data_sample.pred_instances_3d
-                # .cpu can not be used for BaseInstancesBoxes3D
+                # .cpu can not be used for BaseInstance3DBoxes
                 # so we need to use .to('cpu')
                 pred_instances_3d = pred_instances_3d[
                     pred_instances_3d.scores_3d > pred_score_thr].to('cpu')
                 pred_data_3d = self._draw_instances_3d(data_input,
                                                        pred_instances_3d,
                                                        data_sample.metainfo,
                                                        vis_task, palette)
             if 'pred_instances' in data_sample:
                 if 'img' in data_input and len(data_sample.pred_instances) > 0:
                     pred_instances = data_sample.pred_instances
-                    pred_instances = pred_instances_3d[
+                    pred_instances = pred_instances[
                         pred_instances.scores > pred_score_thr].cpu()
+                    img = data_input['img']
                     if isinstance(data_input['img'], Tensor):
                         img = data_input['img'].permute(1, 2, 0).numpy()
                         img = img[..., [2, 1, 0]]  # bgr to rgb
                     pred_img_data = self._draw_instances(
                         img, pred_instances, classes, palette)
             if 'pred_pts_seg' in data_sample and vis_task == 'lidar_seg':
                 assert classes is not None, 'class information is ' \
                                             'not provided when ' \
-                                            'visualizing panoptic ' \
+                                            'visualizing semantic ' \
                                             'segmentation results.'
                 assert 'points' in data_input
                 self._draw_pts_sem_seg(data_input['points'],
                                        data_sample.pred_pts_seg, palette,
                                        ignore_index)
 
         # monocular 3d object detection image
@@ -780,14 +929,16 @@
             if gt_data_3d is not None and pred_data_3d is not None:
                 drawn_img_3d = np.concatenate(
                     (gt_data_3d['img'], pred_data_3d['img']), axis=1)
             elif gt_data_3d is not None:
                 drawn_img_3d = gt_data_3d['img']
             elif pred_data_3d is not None:
                 drawn_img_3d = pred_data_3d['img']
+            else:  # both instances of gt and pred are empty
+                drawn_img_3d = None
         else:
             drawn_img_3d = None
 
         # 2d object detection image
         if gt_img_data is not None and pred_img_data is not None:
             drawn_img = np.concatenate((gt_img_data, pred_img_data), axis=1)
         elif gt_img_data is not None:
@@ -799,16 +950,20 @@
 
         if show:
             self.show(
                 o3d_save_path,
                 drawn_img_3d,
                 drawn_img,
                 win_name=name,
-                wait_time=wait_time)
+                wait_time=wait_time,
+                vis_task=vis_task)
 
         if out_file is not None:
+            # check the suffix of the name of image file
+            if not (out_file.endswith('.png') or out_file.endswith('.jpg')):
+                out_file = f'{out_file}.png'
             if drawn_img_3d is not None:
                 mmcv.imwrite(drawn_img_3d[..., ::-1], out_file)
             if drawn_img is not None:
                 mmcv.imwrite(drawn_img[..., ::-1], out_file)
         else:
             self.add_image(name, drawn_img_3d, step)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d/visualization/vis_utils.py` & `mmdet3d-1.1.1/mmdet3d/visualization/vis_utils.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,19 +1,21 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import copy
+from typing import Tuple
 
 import numpy as np
 import torch
 import trimesh
 
-from mmdet3d.structures import (Box3DMode, CameraInstance3DBoxes, Coord3DMode,
+from mmdet3d.structures import (BaseInstance3DBoxes, Box3DMode,
+                                CameraInstance3DBoxes, Coord3DMode,
                                 DepthInstance3DBoxes, LiDARInstance3DBoxes)
 
 
-def write_obj(points, out_filename):
+def write_obj(points: np.ndarray, out_filename: str) -> None:
     """Write points into ``obj`` format for meshlab visualization.
 
     Args:
         points (np.ndarray): Points in shape (N, dim).
         out_filename (str): Filename to be saved.
     """
     N = points.shape[0]
@@ -27,34 +29,35 @@
 
         else:
             fout.write('v %f %f %f\n' %
                        (points[i, 0], points[i, 1], points[i, 2]))
     fout.close()
 
 
-def write_oriented_bbox(scene_bbox, out_filename):
+def write_oriented_bbox(scene_bbox: np.ndarray, out_filename: str) -> None:
     """Export oriented (around Z axis) scene bbox to meshes.
 
     Args:
-        scene_bbox(list[ndarray] or ndarray): xyz pos of center and
-            3 lengths (x_size, y_size, z_size) and heading angle around Z axis.
-            Y forward, X right, Z upward. heading angle of positive X is 0,
+        scene_bbox (np.ndarray): xyz pos of center and 3 lengths
+            (x_size, y_size, z_size) and heading angle around Z axis.
+            Y forward, X right, Z upward, heading angle of positive X is 0,
             heading angle of positive Y is 90 degrees.
-        out_filename(str): Filename.
+        out_filename (str): Filename.
     """
 
-    def heading2rotmat(heading_angle):
+    def heading2rotmat(heading_angle: float) -> np.ndarray:
         rotmat = np.zeros((3, 3))
         rotmat[2, 2] = 1
         cosval = np.cos(heading_angle)
         sinval = np.sin(heading_angle)
         rotmat[0:2, 0:2] = np.array([[cosval, -sinval], [sinval, cosval]])
         return rotmat
 
-    def convert_oriented_box_to_trimesh_fmt(box):
+    def convert_oriented_box_to_trimesh_fmt(
+            box: np.ndarray) -> trimesh.base.Trimesh:
         ctr = box[:3]
         lengths = box[3:6]
         trns = np.eye(4)
         trns[0:3, 3] = ctr
         trns[3, 3] = 1.0
         trns[0:3, 0:3] = heading2rotmat(box[6])
         box_trimesh_fmt = trimesh.creation.box(lengths, trns)
@@ -66,40 +69,39 @@
     for box in scene_bbox:
         scene.add_geometry(convert_oriented_box_to_trimesh_fmt(box))
 
     mesh_list = trimesh.util.concatenate(scene.dump())
     # save to obj file
     trimesh.io.export.export_mesh(mesh_list, out_filename, file_type='obj')
 
-    return
 
-
-def to_depth_mode(points, bboxes):
+def to_depth_mode(
+        points: np.ndarray,
+        bboxes: BaseInstance3DBoxes) -> Tuple[np.ndarray, BaseInstance3DBoxes]:
     """Convert points and bboxes to Depth Coord and Depth Box mode."""
     if points is not None:
         points = Coord3DMode.convert_point(points.copy(), Coord3DMode.LIDAR,
                                            Coord3DMode.DEPTH)
     if bboxes is not None:
         bboxes = Box3DMode.convert(bboxes.clone(), Box3DMode.LIDAR,
                                    Box3DMode.DEPTH)
     return points, bboxes
 
 
 # TODO: refactor lidar2img to img_meta
 def proj_lidar_bbox3d_to_img(bboxes_3d: LiDARInstance3DBoxes,
-                             input_meta: dict) -> np.array:
+                             input_meta: dict) -> np.ndarray:
     """Project the 3D bbox on 2D plane.
 
     Args:
-        bboxes_3d (:obj:`LiDARInstance3DBoxes`):
-            3d bbox in lidar coordinate system to visualize.
-        lidar2img (numpy.array, shape=[4, 4]): The projection matrix
-            according to the camera intrinsic parameters.
+        bboxes_3d (:obj:`LiDARInstance3DBoxes`): 3D bbox in lidar coordinate
+            system to visualize.
+        input_meta (dict): Meta information.
     """
-    corners_3d = bboxes_3d.corners
+    corners_3d = bboxes_3d.corners.cpu().numpy()
     num_bbox = corners_3d.shape[0]
     pts_4d = np.concatenate(
         [corners_3d.reshape(-1, 3),
          np.ones((num_bbox * 8, 1))], axis=-1)
     lidar2img = copy.deepcopy(input_meta['lidar2img']).reshape(4, 4)
     if isinstance(lidar2img, torch.Tensor):
         lidar2img = lidar2img.cpu().numpy()
@@ -111,21 +113,21 @@
     imgfov_pts_2d = pts_2d[..., :2].reshape(num_bbox, 8, 2)
 
     return imgfov_pts_2d
 
 
 # TODO: remove third parameter in all functions here in favour of img_metas
 def proj_depth_bbox3d_to_img(bboxes_3d: DepthInstance3DBoxes,
-                             input_meta: dict) -> np.array:
+                             input_meta: dict) -> np.ndarray:
     """Project the 3D bbox on 2D plane and draw on input image.
 
     Args:
-        bboxes_3d (:obj:`DepthInstance3DBoxes`, shape=[M, 7]):
-            3d bbox in depth coordinate system to visualize.
-        input_meta (dict): Used in coordinates transformation.
+        bboxes_3d (:obj:`DepthInstance3DBoxes`): 3D bbox in depth coordinate
+            system to visualize.
+        input_meta (dict): Meta information.
     """
     from mmdet3d.models import apply_3d_transformation
     from mmdet3d.structures import points_cam2img
 
     input_meta = copy.deepcopy(input_meta)
     corners_3d = bboxes_3d.corners
     num_bbox = corners_3d.shape[0]
@@ -142,22 +144,21 @@
     imgfov_pts_2d = uv_origin[..., :2].reshape(num_bbox, 8, 2).numpy()
 
     return imgfov_pts_2d
 
 
 # project the camera bboxes 3d to image
 def proj_camera_bbox3d_to_img(bboxes_3d: CameraInstance3DBoxes,
-                              input_meta: dict) -> np.array:
+                              input_meta: dict) -> np.ndarray:
     """Project the 3D bbox on 2D plane and draw on input image.
 
     Args:
-        bboxes_3d (:obj:`CameraInstance3DBoxes`, shape=[M, 7]):
-            3d bbox in camera coordinate system to visualize.
-        cam2img (np.array)): Camera intrinsic matrix,
-            denoted as `K` in depth bbox coordinate system.
+        bboxes_3d (:obj:`CameraInstance3DBoxes`): 3D bbox in camera coordinate
+            system to visualize.
+        input_meta (dict): Meta information.
     """
     from mmdet3d.structures import points_cam2img
 
     cam2img = copy.deepcopy(input_meta['cam2img'])
     corners_3d = bboxes_3d.corners
     num_bbox = corners_3d.shape[0]
     points_3d = corners_3d.reshape(-1, 3)
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d.egg-info/PKG-INFO` & `mmdet3d-1.1.1/mmdet3d.egg-info/PKG-INFO`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: mmdet3d
-Version: 1.1.0rc3
+Version: 1.1.1
 Summary: OpenMMLab's next-generation platformfor general 3D object detection.
 Home-page: https://github.com/open-mmlab/mmdetection3d
 Author: MMDetection3D Contributors
 Author-email: zwwdev@gmail.com
 License: Apache License 2.0
 Description: <div align="center">
           <img src="resources/mmdet3d-logo.png" width="600"/>
@@ -21,34 +21,58 @@
             <sup>
               <a href="https://platform.openmmlab.com">
                 <i><font size="4">TRY IT OUT</font></i>
               </a>
             </sup>
           </div>
           <div>&nbsp;</div>
-        </div>
         
-        [![docs](https://img.shields.io/badge/docs-latest-blue)](https://mmdetection3d.readthedocs.io/en/1.1/)
+        [![docs](https://img.shields.io/badge/docs-latest-blue)](https://mmdetection3d.readthedocs.io/en/latest/)
         [![badge](https://github.com/open-mmlab/mmdetection3d/workflows/build/badge.svg)](https://github.com/open-mmlab/mmdetection3d/actions)
         [![codecov](https://codecov.io/gh/open-mmlab/mmdetection3d/branch/master/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmdetection3d)
         [![license](https://img.shields.io/github/license/open-mmlab/mmdetection3d.svg)](https://github.com/open-mmlab/mmdetection3d/blob/master/LICENSE)
         
+        </div>
+        
+        </div>
+        
+        <div align="center">
+          <a href="https://openmmlab.medium.com/" style="text-decoration:none;">
+            <img src="https://user-images.githubusercontent.com/25839884/219255827-67c1a27f-f8c5-46a9-811d-5e57448c61d1.png" width="3%" alt="" /></a>
+          <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+          <a href="https://discord.com/channels/1037617289144569886/1046608014234370059" style="text-decoration:none;">
+            <img src="https://user-images.githubusercontent.com/25839884/218347213-c080267f-cbb6-443e-8532-8e1ed9a58ea9.png" width="3%" alt="" /></a>
+          <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+          <a href="https://twitter.com/OpenMMLab" style="text-decoration:none;">
+            <img src="https://user-images.githubusercontent.com/25839884/218346637-d30c8a0f-3eba-4699-8131-512fb06d46db.png" width="3%" alt="" /></a>
+          <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+          <a href="https://www.youtube.com/openmmlab" style="text-decoration:none;">
+            <img src="https://user-images.githubusercontent.com/25839884/218346691-ceb2116a-465a-40af-8424-9f30d2348ca9.png" width="3%" alt="" /></a>
+          <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+          <a href="https://space.bilibili.com/1293512903" style="text-decoration:none;">
+            <img src="https://user-images.githubusercontent.com/25839884/219026751-d7d14cce-a7c9-4e82-9942-8375fca65b99.png" width="3%" alt="" /></a>
+          <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+          <a href="https://www.zhihu.com/people/openmmlab" style="text-decoration:none;">
+            <img src="https://user-images.githubusercontent.com/25839884/219026120-ba71e48b-6e94-4bd4-b4e9-b7d175b5e362.png" width="3%" alt="" /></a>
+        </div>
+        
         **News**:
         
-        **v1.1.0rc3** was released in 7/1/2023
+        **We have renamed the branch `1.1`  to `main` and switched the default branch from `master` to `main`. We encourage
+        users to migrate to the latest version, though it comes with some cost. Please refer to [Migration Guide](docs/en/migration.md) for more details.**
         
-        The compatibilities of models are broken due to the unification and simplification of coordinate systems after v1.0.0rc0. For now, most models are benchmarked with similar performance, though few models are still being benchmarked. In the following release, we will update all the model checkpoints and benchmarks. See more details in the [Changelog](docs/en/notes/changelog.md) and [Changelog-v1.0.x](docs/en/notes/changelog_v1.0.x.md).
+        **v1.1.1** was released in 30/5/2023
         
-        Documentation: https://mmdetection3d.readthedocs.io/
+        We have constructed a comprehensive LiDAR semantic segmentation benchmark on SemanticKITTI, including Cylinder3D, MinkUNet and SPVCNN methods. Noteworthy, the improved MinkUNetv2 can achieve 70.3 mIoU on the validation set of SemanticKITTI. We have also supported the training of BEVFusion and an occupancy prediction method, TPVFomrer, in our `projects`. More new features about 3D perception are on the way. Please stay tuned!
         
         ## Introduction
         
         English | [](README_zh-CN.md)
         
-        The master branch works with **PyTorch 1.6+**.
+        The main branch works with **PyTorch 1.8+**.
         
         MMDetection3D is an open source object detection toolbox based on PyTorch, towards the next-generation platform for general 3D detection. It is
         a part of the OpenMMLab project developed by [MMLab](http://mmlab.ie.cuhk.edu.hk/).
         
         ![demo image](resources/mmdet3d_outdoor_demo.gif)
         
         ### Major features
@@ -56,41 +80,41 @@
         - **Support multi-modality/single-modality detectors out of box**
         
           It directly supports multi-modality/single-modality detectors including MVXNet, VoteNet, PointPillars, etc.
         
         - **Support indoor/outdoor 3D detection out of box**
         
           It directly supports popular indoor and outdoor 3D detection datasets, including ScanNet, SUNRGB-D, Waymo, nuScenes, Lyft, and KITTI.
-          For nuScenes dataset, we also support [nuImages dataset](https://github.com/open-mmlab/mmdetection3d/tree/1.1/configs/nuimages).
+          For nuScenes dataset, we also support [nuImages dataset](https://github.com/open-mmlab/mmdetection3d/tree/main/configs/nuimages).
         
         - **Natural integration with 2D detection**
         
           All the about **300+ models, methods of 40+ papers**, and modules supported in [MMDetection](https://github.com/open-mmlab/mmdetection/blob/3.x/docs/en/model_zoo.md) can be trained or used in this codebase.
         
         - **High efficiency**
         
-          It trains faster than other codebases. The main results are as below. Details can be found in [benchmark.md](./docs/en/notes/benchmarks.md). We compare the number of samples trained per second (the higher, the better). The models that are not supported by other codebases are marked by ``.
+          It trains faster than other codebases. The main results are as below. Details can be found in [benchmark.md](./docs/en/notes/benchmarks.md). We compare the number of samples trained per second (the higher, the better). The models that are not supported by other codebases are marked by ``.
         
           |       Methods       | MMDetection3D | [OpenPCDet](https://github.com/open-mmlab/OpenPCDet) | [votenet](https://github.com/facebookresearch/votenet) | [Det3D](https://github.com/poodarchu/Det3D) |
           | :-----------------: | :-----------: | :--------------------------------------------------: | :----------------------------------------------------: | :-----------------------------------------: |
-          |       VoteNet       |      358      |                                                     |                           77                           |                                            |
-          |  PointPillars-car   |      141      |                                                     |                                                       |                     140                     |
-          | PointPillars-3class |      107      |                          44                          |                                                       |                                            |
-          |       SECOND        |      40       |                          30                          |                                                       |                                            |
-          |       Part-A2       |      17       |                          14                          |                                                       |                                            |
+          |       VoteNet       |      358      |                                                     |                           77                           |                                            |
+          |  PointPillars-car   |      141      |                                                     |                                                       |                     140                     |
+          | PointPillars-3class |      107      |                          44                          |                                                       |                                            |
+          |       SECOND        |      40       |                          30                          |                                                       |                                            |
+          |       Part-A2       |      17       |                          14                          |                                                       |                                            |
         
         Like [MMDetection](https://github.com/open-mmlab/mmdetection) and [MMCV](https://github.com/open-mmlab/mmcv), MMDetection3D can also be used as a library to support different projects on top of it.
         
         ## License
         
         This project is released under the [Apache 2.0 license](LICENSE).
         
         ## Changelog
         
-        **1.1.0rc3** was released in 7/1/2023.
+        **1.1.0** was released in 6/4/2023.
         
         Please refer to [changelog.md](docs/en/notes/changelog.md) for details and release history.
         
         ## Benchmark and model zoo
         
         Results and models are available in the [model zoo](docs/en/model_zoo.md).
         
@@ -114,14 +138,17 @@
               <td>
               <ul>
                 <li><a href="configs/pointnet2">PointNet (CVPR'2017)</a></li>
                 <li><a href="configs/pointnet2">PointNet++ (NeurIPS'2017)</a></li>
                 <li><a href="configs/regnet">RegNet (CVPR'2020)</a></li>
                 <li><a href="configs/dgcnn">DGCNN (TOG'2019)</a></li>
                 <li>DLA (CVPR'2018)</li>
+                <li>MinkResNet (CVPR'2019)</li>
+                <li><a href="configs/minkunet">MinkUNet (CVPR'2019)</a></li>
+                <li><a href="configs/cylinder3d">Cylinder3D (CVPR'2021)</a></li>
               </ul>
               </td>
               <td>
               <ul>
                 <li><a href="configs/free_anchor">FreeAnchor (NeurIPS'2019)</a></li>
               </ul>
               </td>
@@ -182,75 +209,88 @@
                 <ul>
                   <li><a href="configs/imvoxelnet">ImVoxelNet (WACV'2022)</a></li>
                   <li><a href="configs/smoke">SMOKE (CVPRW'2020)</a></li>
                   <li><a href="configs/fcos3d">FCOS3D (ICCVW'2021)</a></li>
                   <li><a href="configs/pgd">PGD (CoRL'2021)</a></li>
                   <li><a href="configs/monoflex">MonoFlex (CVPR'2021)</a></li>
                 </ul>
+                <li><b>Indoor</b></li>
+                <ul>
+                  <li><a href="configs/imvoxelnet">ImVoxelNet (WACV'2022)</a></li>
+                </ul>
               </td>
               <td>
                 <li><b>Outdoor</b></li>
                 <ul>
                   <li><a href="configs/mvxnet">MVXNet (ICRA'2019)</a></li>
                 </ul>
                 <li><b>Indoor</b></li>
                 <ul>
                   <li><a href="configs/imvotenet">ImVoteNet (CVPR'2020)</a></li>
                 </ul>
               </td>
               <td>
+                <li><b>Outdoor</b></li>
+                <ul>
+                  <li><a href="configs/minkunet">MinkUNet (CVPR'2019)</a></li>
+                  <li><a href="configs/spvcnn">SPVCNN (ECCV'2020)</a></li>
+                  <li><a href="configs/cylinder3d">Cylinder3D (CVPR'2021)</a></li>
+                </ul>
                 <li><b>Indoor</b></li>
                 <ul>
                   <li><a href="configs/pointnet2">PointNet++ (NeurIPS'2017)</a></li>
                   <li><a href="configs/paconv">PAConv (CVPR'2021)</a></li>
                   <li><a href="configs/dgcnn">DGCNN (TOG'2019)</a></li>
                 </ul>
               </ul>
               </td>
             </tr>
         </td>
             </tr>
           </tbody>
         </table>
         
-        |               | ResNet | ResNeXt | SENet | PointNet++ | DGCNN | HRNet | RegNetX | Res2Net | DLA | MinkResNet |
-        | ------------- | :----: | :-----: | :---: | :--------: | :---: | :---: | :-----: | :-----: | :-: | :--------: |
-        | SECOND        |       |        |      |           |      |      |        |        |    |           |
-        | PointPillars  |       |        |      |           |      |      |        |        |    |           |
-        | FreeAnchor    |       |        |      |           |      |      |        |        |    |           |
-        | VoteNet       |       |        |      |           |      |      |        |        |    |           |
-        | H3DNet        |       |        |      |           |      |      |        |        |    |           |
-        | 3DSSD         |       |        |      |           |      |      |        |        |    |           |
-        | Part-A2       |       |        |      |           |      |      |        |        |    |           |
-        | MVXNet        |       |        |      |           |      |      |        |        |    |           |
-        | CenterPoint   |       |        |      |           |      |      |        |        |    |           |
-        | SSN           |       |        |      |           |      |      |        |        |    |           |
-        | ImVoteNet     |       |        |      |           |      |      |        |        |    |           |
-        | FCOS3D        |       |        |      |           |      |      |        |        |    |           |
-        | PointNet++    |       |        |      |           |      |      |        |        |    |           |
-        | Group-Free-3D |       |        |      |           |      |      |        |        |    |           |
-        | ImVoxelNet    |       |        |      |           |      |      |        |        |    |           |
-        | PAConv        |       |        |      |           |      |      |        |        |    |           |
-        | DGCNN         |       |        |      |           |      |      |        |        |    |           |
-        | SMOKE         |       |        |      |           |      |      |        |        |    |           |
-        | PGD           |       |        |      |           |      |      |        |        |    |           |
-        | MonoFlex      |       |        |      |           |      |      |        |        |    |           |
-        | SA-SSD        |       |        |      |           |      |      |        |        |    |           |
-        | FCAF3D        |       |        |      |           |      |      |        |        |    |           |
-        | PV-RCNN       |       |        |      |           |      |      |        |        |    |           |
+        |               | ResNet | PointNet++ | SECOND | DGCNN | RegNetX | DLA | MinkResNet | Cylinder3D | MinkUNet |
+        | :-----------: | :----: | :--------: | :----: | :---: | :-----: | :-: | :--------: | :--------: | :------: |
+        |    SECOND     |       |           |       |      |        |    |           |           |         |
+        | PointPillars  |       |           |       |      |        |    |           |           |         |
+        |  FreeAnchor   |       |           |       |      |        |    |           |           |         |
+        |    VoteNet    |       |           |       |      |        |    |           |           |         |
+        |    H3DNet     |       |           |       |      |        |    |           |           |         |
+        |     3DSSD     |       |           |       |      |        |    |           |           |         |
+        |    Part-A2    |       |           |       |      |        |    |           |           |         |
+        |    MVXNet     |       |           |       |      |        |    |           |           |         |
+        |  CenterPoint  |       |           |       |      |        |    |           |           |         |
+        |      SSN      |       |           |       |      |        |    |           |           |         |
+        |   ImVoteNet   |       |           |       |      |        |    |           |           |         |
+        |    FCOS3D     |       |           |       |      |        |    |           |           |         |
+        |  PointNet++   |       |           |       |      |        |    |           |           |         |
+        | Group-Free-3D |       |           |       |      |        |    |           |           |         |
+        |  ImVoxelNet   |       |           |       |      |        |    |           |           |         |
+        |    PAConv     |       |           |       |      |        |    |           |           |         |
+        |     DGCNN     |       |           |       |      |        |    |           |           |         |
+        |     SMOKE     |       |           |       |      |        |    |           |           |         |
+        |      PGD      |       |           |       |      |        |    |           |           |         |
+        |   MonoFlex    |       |           |       |      |        |    |           |           |         |
+        |    SA-SSD     |       |           |       |      |        |    |           |           |         |
+        |    FCAF3D     |       |           |       |      |        |    |           |           |         |
+        |    PV-RCNN    |       |           |       |      |        |    |           |           |         |
+        |  Cylinder3D   |       |           |       |      |        |    |           |           |         |
+        |   MinkUNet    |       |           |       |      |        |    |           |           |         |
+        |    SPVCNN     |       |           |       |      |        |    |           |           |         |
         
         **Note:** All the about **300+ models, methods of 40+ papers** in 2D detection supported by [MMDetection](https://github.com/open-mmlab/mmdetection/blob/3.x/docs/en/model_zoo.md) can be trained or used in this codebase.
         
         ## Installation
         
-        Please refer to [getting_started.md](docs/en/getting_started.md) for installation.
+        Please refer to [get_started.md](docs/en/get_started.md) for installation.
         
         ## Get Started
         
-        Please see [getting_started.md](docs/en/getting_started.md) for the basic usage of MMDetection3D. We provide guidance for quick run [with existing dataset](docs/en/user_guides/train_test.md) and [with new dataset](docs/en/user_guides/2_new_data_model.md) for beginners. There are also tutorials for [learning configuration systems](docs/en/user_guides/config.md), [customizing dataset](docs/en/advanced_guides/customize_dataset.md), [designing data pipeline](docs/en/user_guides/data_pipeline.md), [customizing models](docs/en/advanced_guides/customize_models.md), [customizing runtime settings](docs/en/advanced_guides/customize_runtime.md) and [Waymo dataset](docs/en/advanced_guides/datasets/waymo_det.md).
+        Please see [get_started.md](docs/en/get_started.md) for the basic usage of MMDetection3D. We provide guidance for quick run [with existing dataset](docs/en/user_guides/train_test.md) and [with new dataset](docs/en/user_guides/2_new_data_model.md) for beginners. There are also tutorials for [learning configuration systems](docs/en/user_guides/config.md), [customizing dataset](docs/en/advanced_guides/customize_dataset.md), [designing data pipeline](docs/en/user_guides/data_pipeline.md), [customizing models](docs/en/advanced_guides/customize_models.md), [customizing runtime settings](docs/en/advanced_guides/customize_runtime.md) and [Waymo dataset](docs/en/advanced_guides/datasets/waymo_det.md).
         
         Please refer to [FAQ](docs/en/notes/faq.md) for frequently asked questions. When updating the version of MMDetection3D, please also check the [compatibility doc](docs/en/notes/compatibility.md) to be aware of the BC-breaking updates introduced in each version.
         
         ## Citation
         
         If you find this project useful in your research, please consider cite:
         
@@ -261,53 +301,54 @@
             howpublished = {\url{https://github.com/open-mmlab/mmdetection3d}},
             year={2020}
         }
         ```
         
         ## Contributing
         
-        We appreciate all contributions to improve MMDetection3D. Please refer to [CONTRIBUTING.md](.github/CONTRIBUTING.md) for the contributing guideline.
+        We appreciate all contributions to improve MMDetection3D. Please refer to [CONTRIBUTING.md](./docs/en/notes/contribution_guides.md) for the contributing guideline.
         
         ## Acknowledgement
         
         MMDetection3D is an open source project that is contributed by researchers and engineers from various colleges and companies. We appreciate all the contributors as well as users who give valuable feedbacks.
         We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new 3D detectors.
         
         ## Projects in OpenMMLab
         
         - [MMEngine](https://github.com/open-mmlab/mmengine): OpenMMLab foundational library for training deep learning models.
         - [MMCV](https://github.com/open-mmlab/mmcv): OpenMMLab foundational library for computer vision.
         - [MMEval](https://github.com/open-mmlab/mmeval): A unified evaluation library for multiple machine learning libraries.
         - [MIM](https://github.com/open-mmlab/mim): MIM installs OpenMMLab packages.
-        - [MMClassification](https://github.com/open-mmlab/mmclassification): OpenMMLab image classification toolbox and benchmark.
+        - [MMPreTrain](https://github.com/open-mmlab/mmpretrain): OpenMMLab pre-training toolbox and benchmark.
         - [MMDetection](https://github.com/open-mmlab/mmdetection): OpenMMLab detection toolbox and benchmark.
         - [MMDetection3D](https://github.com/open-mmlab/mmdetection3d): OpenMMLab's next-generation platform for general 3D object detection.
         - [MMRotate](https://github.com/open-mmlab/mmrotate): OpenMMLab rotated object detection toolbox and benchmark.
         - [MMYOLO](https://github.com/open-mmlab/mmyolo): OpenMMLab YOLO series toolbox and benchmark.
         - [MMSegmentation](https://github.com/open-mmlab/mmsegmentation): OpenMMLab semantic segmentation toolbox and benchmark.
         - [MMOCR](https://github.com/open-mmlab/mmocr): OpenMMLab text detection, recognition, and understanding toolbox.
         - [MMPose](https://github.com/open-mmlab/mmpose): OpenMMLab pose estimation toolbox and benchmark.
         - [MMHuman3D](https://github.com/open-mmlab/mmhuman3d): OpenMMLab 3D human parametric model toolbox and benchmark.
         - [MMSelfSup](https://github.com/open-mmlab/mmselfsup): OpenMMLab self-supervised learning toolbox and benchmark.
         - [MMRazor](https://github.com/open-mmlab/mmrazor): OpenMMLab model compression toolbox and benchmark.
         - [MMFewShot](https://github.com/open-mmlab/mmfewshot): OpenMMLab fewshot learning toolbox and benchmark.
         - [MMAction2](https://github.com/open-mmlab/mmaction2): OpenMMLab's next-generation action understanding toolbox and benchmark.
         - [MMTracking](https://github.com/open-mmlab/mmtracking): OpenMMLab video perception toolbox and benchmark.
         - [MMFlow](https://github.com/open-mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark.
-        - [MMEditing](https://github.com/open-mmlab/mmediting): OpenMMLab image and video editing toolbox.
+        - [MMagic](https://github.com/open-mmlab/mmagic): Open**MM**Lab **A**dvanced, **G**enerative and **I**ntelligent **C**reation toolbox.
         - [MMGeneration](https://github.com/open-mmlab/mmgeneration): OpenMMLab image and video generative models toolbox.
         - [MMDeploy](https://github.com/open-mmlab/mmdeploy): OpenMMLab model deployment framework.
         
 Keywords: computer vision,3D object detection
 Platform: UNKNOWN
-Classifier: Development Status :: 4 - Beta
+Classifier: Development Status :: 5 - Production/Stable
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.6
 Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
 Description-Content-Type: text/markdown
 Provides-Extra: all
 Provides-Extra: tests
 Provides-Extra: build
 Provides-Extra: optional
 Provides-Extra: mim
```

#### html2text {}

```diff
@@ -1,187 +1,197 @@
-Metadata-Version: 2.1 Name: mmdet3d Version: 1.1.0rc3 Summary: OpenMMLab's
-next-generation platformfor general 3D object detection. Home-page: https://
+Metadata-Version: 2.1 Name: mmdet3d Version: 1.1.1 Summary: OpenMMLab's next-
+generation platformfor general 3D object detection. Home-page: https://
 github.com/open-mmlab/mmdetection3d Author: MMDetection3D Contributors Author-
 email: zwwdev@gmail.com License: Apache License 2.0 Description:
                          [resources/mmdet3d-logo.png]
                                        
            OpenMMLab website HOT  OpenMMLab platform TRY_IT_OUT
                                        
-[![docs](https://img.shields.io/badge/docs-latest-blue)](https://
-mmdetection3d.readthedocs.io/en/1.1/) [![badge](https://github.com/open-mmlab/
-mmdetection3d/workflows/build/badge.svg)](https://github.com/open-mmlab/
-mmdetection3d/actions) [![codecov](https://codecov.io/gh/open-mmlab/
+       [![docs](https://img.shields.io/badge/docs-latest-blue)](https://
+  mmdetection3d.readthedocs.io/en/latest/) [![badge](https://github.com/open-
+mmlab/mmdetection3d/workflows/build/badge.svg)](https://github.com/open-mmlab/
+     mmdetection3d/actions) [![codecov](https://codecov.io/gh/open-mmlab/
 mmdetection3d/branch/master/graph/badge.svg)](https://codecov.io/gh/open-mmlab/
-mmdetection3d) [![license](https://img.shields.io/github/license/open-mmlab/
-mmdetection3d.svg)](https://github.com/open-mmlab/mmdetection3d/blob/master/
-LICENSE) **News**: **v1.1.0rc3** was released in 7/1/2023 The compatibilities
-of models are broken due to the unification and simplification of coordinate
-systems after v1.0.0rc0. For now, most models are benchmarked with similar
-performance, though few models are still being benchmarked. In the following
-release, we will update all the model checkpoints and benchmarks. See more
-details in the [Changelog](docs/en/notes/changelog.md) and [Changelog-v1.0.x]
-(docs/en/notes/changelog_v1.0.x.md). Documentation: https://
-mmdetection3d.readthedocs.io/ ## Introduction English | []
-(README_zh-CN.md) The master branch works with **PyTorch 1.6+**. MMDetection3D
-is an open source object detection toolbox based on PyTorch, towards the next-
-generation platform for general 3D detection. It is a part of the OpenMMLab
-project developed by [MMLab](http://mmlab.ie.cuhk.edu.hk/). ![demo image]
-(resources/mmdet3d_outdoor_demo.gif) ### Major features - **Support multi-
-modality/single-modality detectors out of box** It directly supports multi-
-modality/single-modality detectors including MVXNet, VoteNet, PointPillars,
-etc. - **Support indoor/outdoor 3D detection out of box** It directly supports
-popular indoor and outdoor 3D detection datasets, including ScanNet, SUNRGB-D,
-Waymo, nuScenes, Lyft, and KITTI. For nuScenes dataset, we also support
-[nuImages dataset](https://github.com/open-mmlab/mmdetection3d/tree/1.1/
-configs/nuimages). - **Natural integration with 2D detection** All the about
-**300+ models, methods of 40+ papers**, and modules supported in [MMDetection]
-(https://github.com/open-mmlab/mmdetection/blob/3.x/docs/en/model_zoo.md) can
-be trained or used in this codebase. - **High efficiency** It trains faster
-than other codebases. The main results are as below. Details can be found in
-[benchmark.md](./docs/en/notes/benchmarks.md). We compare the number of samples
-trained per second (the higher, the better). The models that are not supported
-by other codebases are marked by ``. | Methods | MMDetection3D | [OpenPCDet]
-(https://github.com/open-mmlab/OpenPCDet) | [votenet](https://github.com/
-facebookresearch/votenet) | [Det3D](https://github.com/poodarchu/Det3D) | | :--
----------------: | :-----------: | :-------------------------------------------
--------: | :----------------------------------------------------: | :----------
--------------------------------: | | VoteNet | 358 |  | 77 |  | |
-PointPillars-car | 141 |  |  | 140 | | PointPillars-3class | 107 | 44 | 
-|  | | SECOND | 40 | 30 |  |  | | Part-A2 | 17 | 14 |  |  | Like
-[MMDetection](https://github.com/open-mmlab/mmdetection) and [MMCV](https://
-github.com/open-mmlab/mmcv), MMDetection3D can also be used as a library to
-support different projects on top of it. ## License This project is released
-under the [Apache 2.0 license](LICENSE). ## Changelog **1.1.0rc3** was released
-in 7/1/2023. Please refer to [changelog.md](docs/en/notes/changelog.md) for
-details and release history. ## Benchmark and model zoo Results and models are
-available in the [model zoo](docs/en/model_zoo.md).
+ mmdetection3d) [![license](https://img.shields.io/github/license/open-mmlab/
+ mmdetection3d.svg)](https://github.com/open-mmlab/mmdetection3d/blob/master/
+                                   LICENSE)
+
+**News**: **We have renamed the branch `1.1` to `main` and switched the default
+branch from `master` to `main`. We encourage users to migrate to the latest
+version, though it comes with some cost. Please refer to [Migration Guide]
+(docs/en/migration.md) for more details.** **v1.1.1** was released in 30/5/2023
+We have constructed a comprehensive LiDAR semantic segmentation benchmark on
+SemanticKITTI, including Cylinder3D, MinkUNet and SPVCNN methods. Noteworthy,
+the improved MinkUNetv2 can achieve 70.3 mIoU on the validation set of
+SemanticKITTI. We have also supported the training of BEVFusion and an
+occupancy prediction method, TPVFomrer, in our `projects`. More new features
+about 3D perception are on the way. Please stay tuned! ## Introduction English
+| [](README_zh-CN.md) The main branch works with **PyTorch 1.8+**.
+MMDetection3D is an open source object detection toolbox based on PyTorch,
+towards the next-generation platform for general 3D detection. It is a part of
+the OpenMMLab project developed by [MMLab](http://mmlab.ie.cuhk.edu.hk/). !
+[demo image](resources/mmdet3d_outdoor_demo.gif) ### Major features - **Support
+multi-modality/single-modality detectors out of box** It directly supports
+multi-modality/single-modality detectors including MVXNet, VoteNet,
+PointPillars, etc. - **Support indoor/outdoor 3D detection out of box** It
+directly supports popular indoor and outdoor 3D detection datasets, including
+ScanNet, SUNRGB-D, Waymo, nuScenes, Lyft, and KITTI. For nuScenes dataset, we
+also support [nuImages dataset](https://github.com/open-mmlab/mmdetection3d/
+tree/main/configs/nuimages). - **Natural integration with 2D detection** All
+the about **300+ models, methods of 40+ papers**, and modules supported in
+[MMDetection](https://github.com/open-mmlab/mmdetection/blob/3.x/docs/en/
+model_zoo.md) can be trained or used in this codebase. - **High efficiency** It
+trains faster than other codebases. The main results are as below. Details can
+be found in [benchmark.md](./docs/en/notes/benchmarks.md). We compare the
+number of samples trained per second (the higher, the better). The models that
+are not supported by other codebases are marked by ``. | Methods |
+MMDetection3D | [OpenPCDet](https://github.com/open-mmlab/OpenPCDet) |
+[votenet](https://github.com/facebookresearch/votenet) | [Det3D](https://
+github.com/poodarchu/Det3D) | | :-----------------: | :-----------: | :--------
+------------------------------------------: | :--------------------------------
+--------------------: | :-----------------------------------------: | | VoteNet
+| 358 |  | 77 |  | | PointPillars-car | 141 |  |  | 140 | |
+PointPillars-3class | 107 | 44 |  |  | | SECOND | 40 | 30 |  |  | |
+Part-A2 | 17 | 14 |  |  | Like [MMDetection](https://github.com/open-
+mmlab/mmdetection) and [MMCV](https://github.com/open-mmlab/mmcv),
+MMDetection3D can also be used as a library to support different projects on
+top of it. ## License This project is released under the [Apache 2.0 license]
+(LICENSE). ## Changelog **1.1.0** was released in 6/4/2023. Please refer to
+[changelog.md](docs/en/notes/changelog.md) for details and release history. ##
+Benchmark and model zoo Results and models are available in the [model zoo]
+(docs/en/model_zoo.md).
                                   Components
-              Backbones                 Heads                   Features
-      * PointNet_(CVPR'2017)     * FreeAnchor_        * Dynamic_Voxelization_
-      * PointNet++_                (NeurIPS'2019)       (CoRL'2019)
-        (NeurIPS'2017)
-      * RegNet_(CVPR'2020)
-      * DGCNN_(TOG'2019)
-      * DLA (CVPR'2018)
+               Backbones                 Heads                   Features
+     * PointNet_(CVPR'2017)       * FreeAnchor_        * Dynamic_Voxelization_
+     * PointNet++_                  (NeurIPS'2019)       (CoRL'2019)
+       (NeurIPS'2017)
+     * RegNet_(CVPR'2020)
+     * DGCNN_(TOG'2019)
+     * DLA (CVPR'2018)
+     * MinkResNet (CVPR'2019)
+     * MinkUNet_(CVPR'2019)
+     * Cylinder3D_(CVPR'2021)
                                  Architectures
 3D Object Detection  Monocular 3D Object  Multi-modal 3D         3D Semantic
                           Detection      Object Detection        Segmentation
-Outdoor              Outdoor             Outdoor           Indoor
-    * SECOND_            * ImVoxelNet_       * MVXNet_         * PointNet++_
-      (Sensor'2018)        (WACV'2022)         (ICRA'2019)       (NeurIPS'2017)
-    * PointPillars_      * SMOKE_        Indoor                * PAConv_
-      (CVPR'2019)          (CVPRW'2020)      * ImVoteNet_        (CVPR'2021)
-    * SSN_               * FCOS3D_             (CVPR'2020)     * DGCNN_
-      (ECCV'2020)          (ICCVW'2021)                          (TOG'2019)
-    * 3DSSD_             * PGD_
-      (CVPR'2020)          (CoRL'2021)
-    * SA-SSD_            * MonoFlex_
-      (CVPR'2020)          (CVPR'2021)
-    * PointRCNN_
-      (CVPR'2019)
-    * Part-A2_
+Outdoor              Outdoor             Outdoor           Outdoor
+    * SECOND_            * ImVoxelNet_       * MVXNet_         * MinkUNet_
+      (Sensor'2018)        (WACV'2022)         (ICRA'2019)       (CVPR'2019)
+    * PointPillars_      * SMOKE_        Indoor                * SPVCNN_
+      (CVPR'2019)          (CVPRW'2020)      * ImVoteNet_        (ECCV'2020)
+    * SSN_               * FCOS3D_             (CVPR'2020)     * Cylinder3D_
+      (ECCV'2020)          (ICCVW'2021)                          (CVPR'2021)
+    * 3DSSD_             * PGD_                            Indoor
+      (CVPR'2020)          (CoRL'2021)                         * PointNet++_
+    * SA-SSD_            * MonoFlex_                             (NeurIPS'2017)
+      (CVPR'2020)          (CVPR'2021)                         * PAConv_
+    * PointRCNN_     Indoor                                      (CVPR'2021)
+      (CVPR'2019)        * ImVoxelNet_                         * DGCNN_
+    * Part-A2_             (WACV'2022)                           (TOG'2019)
       (TPAMI'2020)
     * CenterPoint_
       (CVPR'2021)
     * PV-RCNN_
       (CVPR'2020)
 Indoor
     * VoteNet_
       (ICCV'2019)
     * H3DNet_
       (ECCV'2020)
     * Group-Free-3D_
       (ICCV'2021)
     * FCAF3D_
       (ECCV'2022)
-| | ResNet | ResNeXt | SENet | PointNet++ | DGCNN | HRNet | RegNetX | Res2Net |
-DLA | MinkResNet | | ------------- | :----: | :-----: | :---: | :--------: | :-
---: | :---: | :-----: | :-----: | :-: | :--------: | | SECOND |  |  | 
-|  |  |  |  |  |  |  | | PointPillars |  |  |  |
- |  |  |  |  |  |  | | FreeAnchor |  |  |  | 
-|  |  |  |  |  |  | | VoteNet |  |  |  |  |  |
- |  |  |  |  | | H3DNet |  |  |  |  |  |  |
- |  |  |  | | 3DSSD |  |  |  |  |  |  |  | 
-|  |  | | Part-A2 |  |  |  |  |  |  |  |  |  |
- | | MVXNet |  |  |  |  |  |  |  |  |  |  | |
-CenterPoint |  |  |  |  |  |  |  |  |  |  | | SSN
-|  |  |  |  |  |  |  |  |  |  | | ImVoteNet | 
-|  |  |  |  |  |  |  |  |  | | FCOS3D |  |  |
- |  |  |  |  |  |  |  | | PointNet++ |  |  | 
-|  |  |  |  |  |  |  | | Group-Free-3D |  |  |  |
- |  |  |  |  |  |  | | ImVoxelNet |  |  |  | 
-|  |  |  |  |  |  | | PAConv |  |  |  |  |  |
- |  |  |  |  | | DGCNN |  |  |  |  |  |  | 
-|  |  |  | | SMOKE |  |  |  |  |  |  |  |  |
- |  | | PGD |  |  |  |  |  |  |  |  |  |  |
-| MonoFlex |  |  |  |  |  |  |  |  |  |  | | SA-
-SSD |  |  |  |  |  |  |  |  |  |  | | FCAF3D |
- |  |  |  |  |  |  |  |  |  | | PV-RCNN |  |
- |  |  |  |  |  |  |  |  | **Note:** All the about
+| | ResNet | PointNet++ | SECOND | DGCNN | RegNetX | DLA | MinkResNet |
+Cylinder3D | MinkUNet | | :-----------: | :----: | :--------: | :----: | :---:
+| :-----: | :-: | :--------: | :--------: | :------: | | SECOND |  |  |
+ |  |  |  |  |  |  | | PointPillars |  |  |  |
+ |  |  |  |  |  | | FreeAnchor |  |  |  |  | 
+|  |  |  |  | | VoteNet |  |  |  |  |  |  |  |
+ |  | | H3DNet |  |  |  |  |  |  |  |  |  | |
+3DSSD |  |  |  |  |  |  |  |  |  | | Part-A2 |  |
+ |  |  |  |  |  |  |  | | MVXNet |  |  |  |
+ |  |  |  |  |  | | CenterPoint |  |  |  |  | 
+|  |  |  |  | | SSN |  |  |  |  |  |  |  | 
+|  | | ImVoteNet |  |  |  |  |  |  |  |  |  | |
+FCOS3D |  |  |  |  |  |  |  |  |  | | PointNet++ |
+ |  |  |  |  |  |  |  |  | | Group-Free-3D |  |
+ |  |  |  |  |  |  |  | | ImVoxelNet |  |  | 
+|  |  |  |  |  |  | | PAConv |  |  |  |  |  |
+ |  |  |  | | DGCNN |  |  |  |  |  |  |  | 
+|  | | SMOKE |  |  |  |  |  |  |  |  |  | | PGD |
+ |  |  |  |  |  |  |  |  | | MonoFlex |  |  |
+ |  |  |  |  |  |  | | SA-SSD |  |  |  |  |
+ |  |  |  |  | | FCAF3D |  |  |  |  |  |  |
+ |  |  | | PV-RCNN |  |  |  |  |  |  |  |  |
+ | | Cylinder3D |  |  |  |  |  |  |  |  |  | |
+MinkUNet |  |  |  |  |  |  |  |  |  | | SPVCNN | 
+|  |  |  |  |  |  |  |  | **Note:** All the about
 **300+ models, methods of 40+ papers** in 2D detection supported by
 [MMDetection](https://github.com/open-mmlab/mmdetection/blob/3.x/docs/en/
 model_zoo.md) can be trained or used in this codebase. ## Installation Please
-refer to [getting_started.md](docs/en/getting_started.md) for installation. ##
-Get Started Please see [getting_started.md](docs/en/getting_started.md) for the
-basic usage of MMDetection3D. We provide guidance for quick run [with existing
-dataset](docs/en/user_guides/train_test.md) and [with new dataset](docs/en/
-user_guides/2_new_data_model.md) for beginners. There are also tutorials for
-[learning configuration systems](docs/en/user_guides/config.md), [customizing
-dataset](docs/en/advanced_guides/customize_dataset.md), [designing data
-pipeline](docs/en/user_guides/data_pipeline.md), [customizing models](docs/en/
-advanced_guides/customize_models.md), [customizing runtime settings](docs/en/
-advanced_guides/customize_runtime.md) and [Waymo dataset](docs/en/
-advanced_guides/datasets/waymo_det.md). Please refer to [FAQ](docs/en/notes/
-faq.md) for frequently asked questions. When updating the version of
-MMDetection3D, please also check the [compatibility doc](docs/en/notes/
-compatibility.md) to be aware of the BC-breaking updates introduced in each
-version. ## Citation If you find this project useful in your research, please
-consider cite: ```latex @misc{mmdet3d2020, title={{MMDetection3D: OpenMMLab}
-next-generation platform for general {3D} object detection}, author=
-{MMDetection3D Contributors}, howpublished = {\url{https://github.com/open-
-mmlab/mmdetection3d}}, year={2020} } ``` ## Contributing We appreciate all
-contributions to improve MMDetection3D. Please refer to [CONTRIBUTING.md]
-(.github/CONTRIBUTING.md) for the contributing guideline. ## Acknowledgement
-MMDetection3D is an open source project that is contributed by researchers and
-engineers from various colleges and companies. We appreciate all the
-contributors as well as users who give valuable feedbacks. We wish that the
-toolbox and benchmark could serve the growing research community by providing a
-flexible toolkit to reimplement existing methods and develop their own new 3D
-detectors. ## Projects in OpenMMLab - [MMEngine](https://github.com/open-mmlab/
-mmengine): OpenMMLab foundational library for training deep learning models. -
-[MMCV](https://github.com/open-mmlab/mmcv): OpenMMLab foundational library for
-computer vision. - [MMEval](https://github.com/open-mmlab/mmeval): A unified
-evaluation library for multiple machine learning libraries. - [MIM](https://
-github.com/open-mmlab/mim): MIM installs OpenMMLab packages. -
-[MMClassification](https://github.com/open-mmlab/mmclassification): OpenMMLab
-image classification toolbox and benchmark. - [MMDetection](https://github.com/
-open-mmlab/mmdetection): OpenMMLab detection toolbox and benchmark. -
-[MMDetection3D](https://github.com/open-mmlab/mmdetection3d): OpenMMLab's next-
-generation platform for general 3D object detection. - [MMRotate](https://
-github.com/open-mmlab/mmrotate): OpenMMLab rotated object detection toolbox and
-benchmark. - [MMYOLO](https://github.com/open-mmlab/mmyolo): OpenMMLab YOLO
-series toolbox and benchmark. - [MMSegmentation](https://github.com/open-mmlab/
-mmsegmentation): OpenMMLab semantic segmentation toolbox and benchmark. -
-[MMOCR](https://github.com/open-mmlab/mmocr): OpenMMLab text detection,
-recognition, and understanding toolbox. - [MMPose](https://github.com/open-
-mmlab/mmpose): OpenMMLab pose estimation toolbox and benchmark. - [MMHuman3D]
-(https://github.com/open-mmlab/mmhuman3d): OpenMMLab 3D human parametric model
-toolbox and benchmark. - [MMSelfSup](https://github.com/open-mmlab/mmselfsup):
+refer to [get_started.md](docs/en/get_started.md) for installation. ## Get
+Started Please see [get_started.md](docs/en/get_started.md) for the basic usage
+of MMDetection3D. We provide guidance for quick run [with existing dataset]
+(docs/en/user_guides/train_test.md) and [with new dataset](docs/en/user_guides/
+2_new_data_model.md) for beginners. There are also tutorials for [learning
+configuration systems](docs/en/user_guides/config.md), [customizing dataset]
+(docs/en/advanced_guides/customize_dataset.md), [designing data pipeline](docs/
+en/user_guides/data_pipeline.md), [customizing models](docs/en/advanced_guides/
+customize_models.md), [customizing runtime settings](docs/en/advanced_guides/
+customize_runtime.md) and [Waymo dataset](docs/en/advanced_guides/datasets/
+waymo_det.md). Please refer to [FAQ](docs/en/notes/faq.md) for frequently asked
+questions. When updating the version of MMDetection3D, please also check the
+[compatibility doc](docs/en/notes/compatibility.md) to be aware of the BC-
+breaking updates introduced in each version. ## Citation If you find this
+project useful in your research, please consider cite: ```latex @misc
+{mmdet3d2020, title={{MMDetection3D: OpenMMLab} next-generation platform for
+general {3D} object detection}, author={MMDetection3D Contributors},
+howpublished = {\url{https://github.com/open-mmlab/mmdetection3d}}, year={2020}
+} ``` ## Contributing We appreciate all contributions to improve MMDetection3D.
+Please refer to [CONTRIBUTING.md](./docs/en/notes/contribution_guides.md) for
+the contributing guideline. ## Acknowledgement MMDetection3D is an open source
+project that is contributed by researchers and engineers from various colleges
+and companies. We appreciate all the contributors as well as users who give
+valuable feedbacks. We wish that the toolbox and benchmark could serve the
+growing research community by providing a flexible toolkit to reimplement
+existing methods and develop their own new 3D detectors. ## Projects in
+OpenMMLab - [MMEngine](https://github.com/open-mmlab/mmengine): OpenMMLab
+foundational library for training deep learning models. - [MMCV](https://
+github.com/open-mmlab/mmcv): OpenMMLab foundational library for computer
+vision. - [MMEval](https://github.com/open-mmlab/mmeval): A unified evaluation
+library for multiple machine learning libraries. - [MIM](https://github.com/
+open-mmlab/mim): MIM installs OpenMMLab packages. - [MMPreTrain](https://
+github.com/open-mmlab/mmpretrain): OpenMMLab pre-training toolbox and
+benchmark. - [MMDetection](https://github.com/open-mmlab/mmdetection):
+OpenMMLab detection toolbox and benchmark. - [MMDetection3D](https://
+github.com/open-mmlab/mmdetection3d): OpenMMLab's next-generation platform for
+general 3D object detection. - [MMRotate](https://github.com/open-mmlab/
+mmrotate): OpenMMLab rotated object detection toolbox and benchmark. - [MMYOLO]
+(https://github.com/open-mmlab/mmyolo): OpenMMLab YOLO series toolbox and
+benchmark. - [MMSegmentation](https://github.com/open-mmlab/mmsegmentation):
+OpenMMLab semantic segmentation toolbox and benchmark. - [MMOCR](https://
+github.com/open-mmlab/mmocr): OpenMMLab text detection, recognition, and
+understanding toolbox. - [MMPose](https://github.com/open-mmlab/mmpose):
+OpenMMLab pose estimation toolbox and benchmark. - [MMHuman3D](https://
+github.com/open-mmlab/mmhuman3d): OpenMMLab 3D human parametric model toolbox
+and benchmark. - [MMSelfSup](https://github.com/open-mmlab/mmselfsup):
 OpenMMLab self-supervised learning toolbox and benchmark. - [MMRazor](https://
 github.com/open-mmlab/mmrazor): OpenMMLab model compression toolbox and
 benchmark. - [MMFewShot](https://github.com/open-mmlab/mmfewshot): OpenMMLab
 fewshot learning toolbox and benchmark. - [MMAction2](https://github.com/open-
 mmlab/mmaction2): OpenMMLab's next-generation action understanding toolbox and
 benchmark. - [MMTracking](https://github.com/open-mmlab/mmtracking): OpenMMLab
 video perception toolbox and benchmark. - [MMFlow](https://github.com/open-
-mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark. - [MMEditing]
-(https://github.com/open-mmlab/mmediting): OpenMMLab image and video editing
-toolbox. - [MMGeneration](https://github.com/open-mmlab/mmgeneration):
-OpenMMLab image and video generative models toolbox. - [MMDeploy](https://
-github.com/open-mmlab/mmdeploy): OpenMMLab model deployment framework.
-Keywords: computer vision,3D object detection Platform: UNKNOWN Classifier:
-Development Status :: 4 - Beta Classifier: License :: OSI Approved :: Apache
-Software License Classifier: Operating System :: OS Independent Classifier:
-Programming Language :: Python :: 3 Classifier: Programming Language :: Python
-:: 3.6 Classifier: Programming Language :: Python :: 3.7 Description-Content-
-Type: text/markdown Provides-Extra: all Provides-Extra: tests Provides-Extra:
-build Provides-Extra: optional Provides-Extra: mim
+mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark. - [MMagic](https:/
+/github.com/open-mmlab/mmagic): Open**MM**Lab **A**dvanced, **G**enerative and
+**I**ntelligent **C**reation toolbox. - [MMGeneration](https://github.com/open-
+mmlab/mmgeneration): OpenMMLab image and video generative models toolbox. -
+[MMDeploy](https://github.com/open-mmlab/mmdeploy): OpenMMLab model deployment
+framework. Keywords: computer vision,3D object detection Platform: UNKNOWN
+Classifier: Development Status :: 5 - Production/Stable Classifier: License ::
+OSI Approved :: Apache Software License Classifier: Operating System :: OS
+Independent Classifier: Programming Language :: Python :: 3 Classifier:
+Programming Language :: Python :: 3.7 Classifier: Programming Language ::
+Python :: 3.8 Classifier: Programming Language :: Python :: 3.9 Description-
+Content-Type: text/markdown Provides-Extra: all Provides-Extra: tests Provides-
+Extra: build Provides-Extra: optional Provides-Extra: mim
```

### Comparing `mmdet3d-1.1.0rc3/mmdet3d.egg-info/SOURCES.txt` & `mmdet3d-1.1.1/mmdet3d.egg-info/SOURCES.txt`

 * *Files 4% similar despite different names*

```diff
@@ -23,31 +23,34 @@
 mmdet3d/.mim/configs/_base_/datasets/nuim-instance.py
 mmdet3d/.mim/configs/_base_/datasets/nus-3d.py
 mmdet3d/.mim/configs/_base_/datasets/nus-mono3d.py
 mmdet3d/.mim/configs/_base_/datasets/s3dis-3d.py
 mmdet3d/.mim/configs/_base_/datasets/s3dis-seg.py
 mmdet3d/.mim/configs/_base_/datasets/scannet-3d.py
 mmdet3d/.mim/configs/_base_/datasets/scannet-seg.py
+mmdet3d/.mim/configs/_base_/datasets/semantickitti.py
 mmdet3d/.mim/configs/_base_/datasets/sunrgbd-3d.py
 mmdet3d/.mim/configs/_base_/datasets/waymoD5-3d-3class.py
 mmdet3d/.mim/configs/_base_/datasets/waymoD5-3d-car.py
 mmdet3d/.mim/configs/_base_/datasets/waymoD5-fov-mono3d-3class.py
 mmdet3d/.mim/configs/_base_/datasets/waymoD5-mv-mono3d-3class.py
 mmdet3d/.mim/configs/_base_/datasets/waymoD5-mv3d-3class.py
 mmdet3d/.mim/configs/_base_/models/3dssd.py
 mmdet3d/.mim/configs/_base_/models/cascade-mask-rcnn_r50_fpn.py
 mmdet3d/.mim/configs/_base_/models/centerpoint_pillar02_second_secfpn_nus.py
 mmdet3d/.mim/configs/_base_/models/centerpoint_voxel01_second_secfpn_nus.py
+mmdet3d/.mim/configs/_base_/models/cylinder3d.py
 mmdet3d/.mim/configs/_base_/models/dgcnn.py
 mmdet3d/.mim/configs/_base_/models/fcaf3d.py
 mmdet3d/.mim/configs/_base_/models/fcos3d.py
 mmdet3d/.mim/configs/_base_/models/groupfree3d.py
 mmdet3d/.mim/configs/_base_/models/h3dnet.py
 mmdet3d/.mim/configs/_base_/models/imvotenet.py
 mmdet3d/.mim/configs/_base_/models/mask-rcnn_r50_fpn.py
+mmdet3d/.mim/configs/_base_/models/minkunet.py
 mmdet3d/.mim/configs/_base_/models/multiview_dfm.py
 mmdet3d/.mim/configs/_base_/models/paconv_ssg-cuda.py
 mmdet3d/.mim/configs/_base_/models/paconv_ssg.py
 mmdet3d/.mim/configs/_base_/models/parta2.py
 mmdet3d/.mim/configs/_base_/models/pgd.py
 mmdet3d/.mim/configs/_base_/models/point_rcnn.py
 mmdet3d/.mim/configs/_base_/models/pointnet2_msg.py
@@ -56,14 +59,15 @@
 mmdet3d/.mim/configs/_base_/models/pointpillars_hv_fpn_nus.py
 mmdet3d/.mim/configs/_base_/models/pointpillars_hv_fpn_range100_lyft.py
 mmdet3d/.mim/configs/_base_/models/pointpillars_hv_secfpn_kitti.py
 mmdet3d/.mim/configs/_base_/models/pointpillars_hv_secfpn_waymo.py
 mmdet3d/.mim/configs/_base_/models/second_hv_secfpn_kitti.py
 mmdet3d/.mim/configs/_base_/models/second_hv_secfpn_waymo.py
 mmdet3d/.mim/configs/_base_/models/smoke.py
+mmdet3d/.mim/configs/_base_/models/spvcnn.py
 mmdet3d/.mim/configs/_base_/models/votenet.py
 mmdet3d/.mim/configs/_base_/schedules/cosine.py
 mmdet3d/.mim/configs/_base_/schedules/cyclic-20e.py
 mmdet3d/.mim/configs/_base_/schedules/cyclic-40e.py
 mmdet3d/.mim/configs/_base_/schedules/mmdet-schedule-1x.py
 mmdet3d/.mim/configs/_base_/schedules/schedule-2x.py
 mmdet3d/.mim/configs/_base_/schedules/schedule-3x.py
@@ -87,14 +91,17 @@
 mmdet3d/.mim/configs/centerpoint/centerpoint_voxel0075_second_secfpn_head-dcn_8xb4-flip-tta-cyclic-20e_nus-3d.py
 mmdet3d/.mim/configs/centerpoint/centerpoint_voxel0075_second_secfpn_head-dcn_8xb4-tta-cyclic-20e_nus-3d.py
 mmdet3d/.mim/configs/centerpoint/centerpoint_voxel01_second_secfpn_8xb4-cyclic-20e_nus-3d.py
 mmdet3d/.mim/configs/centerpoint/centerpoint_voxel01_second_secfpn_head-circlenms_8xb4-cyclic-20e_nus-3d.py
 mmdet3d/.mim/configs/centerpoint/centerpoint_voxel01_second_secfpn_head-dcn-circlenms_8xb4-cyclic-20e_nus-3d.py
 mmdet3d/.mim/configs/centerpoint/centerpoint_voxel01_second_secfpn_head-dcn_8xb4-cyclic-20e_nus-3d.py
 mmdet3d/.mim/configs/centerpoint/metafile.yml
+mmdet3d/.mim/configs/cylinder3d/cylinder3d_4xb4-3x_semantickitti.py
+mmdet3d/.mim/configs/cylinder3d/cylinder3d_8xb2-laser-polar-mix-3x_semantickitti.py
+mmdet3d/.mim/configs/cylinder3d/metafile.yml
 mmdet3d/.mim/configs/dfm/multiview-dfm_r101-dcn_16xb2_waymoD5-3d-3class.py
 mmdet3d/.mim/configs/dfm/multiview-dfm_r101-dcn_centerhead_16xb2_waymoD5-3d-3class.py
 mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area1.py
 mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area2.py
 mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area3.py
 mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area4.py
 mmdet3d/.mim/configs/dgcnn/dgcnn_4xb32-cosine-100e_s3dis-seg_test-area5.py
@@ -127,14 +134,24 @@
 mmdet3d/.mim/configs/h3dnet/metafile.yml
 mmdet3d/.mim/configs/imvotenet/imvotenet_faster-rcnn-r50_fpn_4xb2_sunrgbd-3d.py
 mmdet3d/.mim/configs/imvotenet/imvotenet_stage2_8xb16_sunrgbd-3d.py
 mmdet3d/.mim/configs/imvotenet/metafile.yml
 mmdet3d/.mim/configs/imvoxelnet/imvoxelnet_2xb4_sunrgbd-3d-10class.py
 mmdet3d/.mim/configs/imvoxelnet/imvoxelnet_8xb4_kitti-3d-car.py
 mmdet3d/.mim/configs/imvoxelnet/metafile.yml
+mmdet3d/.mim/configs/minkunet/metafile.yml
+mmdet3d/.mim/configs/minkunet/minkunet18_w16_torchsparse_8xb2-amp-15e_semantickitti.py
+mmdet3d/.mim/configs/minkunet/minkunet18_w20_torchsparse_8xb2-amp-15e_semantickitti.py
+mmdet3d/.mim/configs/minkunet/minkunet18_w32_torchsparse_8xb2-amp-15e_semantickitti.py
+mmdet3d/.mim/configs/minkunet/minkunet34_w32_minkowski_8xb2-laser-polar-mix-3x_semantickitti.py
+mmdet3d/.mim/configs/minkunet/minkunet34_w32_spconv_8xb2-amp-laser-polar-mix-3x_semantickitti.py
+mmdet3d/.mim/configs/minkunet/minkunet34_w32_spconv_8xb2-laser-polar-mix-3x_semantickitti.py
+mmdet3d/.mim/configs/minkunet/minkunet34_w32_torchsparse_8xb2-amp-laser-polar-mix-3x_semantickitti.py
+mmdet3d/.mim/configs/minkunet/minkunet34_w32_torchsparse_8xb2-laser-polar-mix-3x_semantickitti.py
+mmdet3d/.mim/configs/minkunet/minkunet34v2_w32_torchsparse_8xb2-amp-laser-polar-mix-3x_semantickitti.py
 mmdet3d/.mim/configs/monoflex/metafile.yml
 mmdet3d/.mim/configs/mvxnet/metafile.yml
 mmdet3d/.mim/configs/mvxnet/mvxnet_fpn_dv_second_secfpn_8xb2-80e_kitti-3d-3class.py
 mmdet3d/.mim/configs/nuimages/cascade-mask-rcnn-r50-fpn_coco-20e_nuim.py
 mmdet3d/.mim/configs/nuimages/cascade-mask-rcnn_r101_fpn_1x_nuim.py
 mmdet3d/.mim/configs/nuimages/cascade-mask-rcnn_r50_fpn_1x_nuim.py
 mmdet3d/.mim/configs/nuimages/cascade-mask-rcnn_r50_fpn_coco-20e-1x_nuim.py
@@ -162,40 +179,39 @@
 mmdet3d/.mim/configs/pgd/metafile.yml
 mmdet3d/.mim/configs/pgd/pgd_r101-caffe_fpn_head-gn_16xb2-1x_nus-mono3d.py
 mmdet3d/.mim/configs/pgd/pgd_r101-caffe_fpn_head-gn_16xb2-1x_nus-mono3d_finetune.py
 mmdet3d/.mim/configs/pgd/pgd_r101-caffe_fpn_head-gn_16xb2-2x_nus-mono3d.py
 mmdet3d/.mim/configs/pgd/pgd_r101-caffe_fpn_head-gn_16xb2-2x_nus-mono3d_finetune.py
 mmdet3d/.mim/configs/pgd/pgd_r101-caffe_fpn_head-gn_4xb3-4x_kitti-mono3d.py
 mmdet3d/.mim/configs/pgd/pgd_r101_fpn-head_dcn_16xb3_waymoD5-fov-mono3d.py
-mmdet3d/.mim/configs/pgd/pgd_r101_fpn-head_dcn_16xb3_waymoD5-mono3d.py
 mmdet3d/.mim/configs/pgd/pgd_r101_fpn-head_dcn_16xb3_waymoD5-mv-mono3d.py
 mmdet3d/.mim/configs/point_rcnn/metafile.yml
 mmdet3d/.mim/configs/point_rcnn/point-rcnn_8xb2_kitti-3d-3class.py
 mmdet3d/.mim/configs/pointnet2/metafile.yml
 mmdet3d/.mim/configs/pointnet2/pointnet2_msg_2xb16-cosine-250e_scannet-seg-xyz-only.py
 mmdet3d/.mim/configs/pointnet2/pointnet2_msg_2xb16-cosine-250e_scannet-seg.py
 mmdet3d/.mim/configs/pointnet2/pointnet2_msg_2xb16-cosine-80e_s3dis-seg.py
 mmdet3d/.mim/configs/pointnet2/pointnet2_ssg_2xb16-cosine-200e_scannet-seg-xyz-only.py
 mmdet3d/.mim/configs/pointnet2/pointnet2_ssg_2xb16-cosine-200e_scannet-seg.py
 mmdet3d/.mim/configs/pointnet2/pointnet2_ssg_2xb16-cosine-50e_s3dis-seg.py
 mmdet3d/.mim/configs/pointpillars/metafile.yml
 mmdet3d/.mim/configs/pointpillars/pointpillars_hv_fpn_sbn-all_8xb2-2x_lyft-3d-range100.py
 mmdet3d/.mim/configs/pointpillars/pointpillars_hv_fpn_sbn-all_8xb2-2x_lyft-3d.py
+mmdet3d/.mim/configs/pointpillars/pointpillars_hv_fpn_sbn-all_8xb2-amp-2x_nus-3d.py
 mmdet3d/.mim/configs/pointpillars/pointpillars_hv_fpn_sbn-all_8xb4-2x_nus-3d.py
-mmdet3d/.mim/configs/pointpillars/pointpillars_hv_fpn_sbn-all_8xb4-amp-2x_nus-3d.py
 mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_8xb6-160e_kitti-3d-3class.py
 mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_8xb6-160e_kitti-3d-car.py
 mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_16xb2-2x_waymo-3d-3class.py
 mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_16xb2-2x_waymo-3d-car.py
 mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_16xb2-2x_waymoD5-3d-3class.py
 mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_16xb2-2x_waymoD5-3d-car.py
 mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_8xb2-2x_lyft-3d-range100.py
 mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_8xb2-2x_lyft-3d.py
+mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_8xb2-amp-2x_nus-3d.py
 mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_8xb4-2x_nus-3d.py
-mmdet3d/.mim/configs/pointpillars/pointpillars_hv_secfpn_sbn-all_8xb4-amp-2x_nus-3d.py
 mmdet3d/.mim/configs/pv_rcnn/metafile.yml
 mmdet3d/.mim/configs/pv_rcnn/pv_rcnn_8xb2-80e_kitti-3d-3class.py
 mmdet3d/.mim/configs/regnet/metafile.yml
 mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-1.6gf_fpn_sbn-all_8xb4-2x_nus-3d.py
 mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_fpn_sbn-all_8xb2-2x_lyft-3d.py
 mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_fpn_sbn-all_8xb4-2x_nus-3d.py
 mmdet3d/.mim/configs/regnet/pointpillars_hv_regnet-400mf_fpn_sbn-all_range100_8xb2-2x_lyft-3d.py
@@ -207,14 +223,19 @@
 mmdet3d/.mim/configs/second/second_hv_secfpn_8xb6-80e_kitti-3d-3class.py
 mmdet3d/.mim/configs/second/second_hv_secfpn_8xb6-80e_kitti-3d-car.py
 mmdet3d/.mim/configs/second/second_hv_secfpn_8xb6-amp-80e_kitti-3d-3class.py
 mmdet3d/.mim/configs/second/second_hv_secfpn_8xb6-amp-80e_kitti-3d-car.py
 mmdet3d/.mim/configs/second/second_hv_secfpn_sbn-all_16xb2-2x_waymoD5-3d-3class.py
 mmdet3d/.mim/configs/smoke/metafile.yml
 mmdet3d/.mim/configs/smoke/smoke_dla34_dlaneck_gn-all_4xb8-6x_kitti-mono3d.py
+mmdet3d/.mim/configs/spvcnn/metafile.yml
+mmdet3d/.mim/configs/spvcnn/spvcnn_w16_8xb2-amp-15e_semantickitti.py
+mmdet3d/.mim/configs/spvcnn/spvcnn_w20_8xb2-amp-15e_semantickitti.py
+mmdet3d/.mim/configs/spvcnn/spvcnn_w32_8xb2-amp-15e_semantickitti.py
+mmdet3d/.mim/configs/spvcnn/spvcnn_w32_8xb2-amp-laser-polar-mix-3x_semantickitti.py
 mmdet3d/.mim/configs/ssn/metafile.yml
 mmdet3d/.mim/configs/ssn/ssn_hv_regnet-400mf_secfpn_sbn-all_16xb1-2x_lyft-3d.py
 mmdet3d/.mim/configs/ssn/ssn_hv_regnet-400mf_secfpn_sbn-all_16xb2-2x_nus-3d.py
 mmdet3d/.mim/configs/ssn/ssn_hv_secfpn_sbn-all_16xb2-2x_lyft-3d.py
 mmdet3d/.mim/configs/ssn/ssn_hv_secfpn_sbn-all_16xb2-2x_nus-3d.py
 mmdet3d/.mim/configs/votenet/metafile.yml
 mmdet3d/.mim/configs/votenet/votenet_8xb16_sunrgbd-3d.py
@@ -239,14 +260,15 @@
 mmdet3d/.mim/tools/dataset_converters/kitti_data_utils.py
 mmdet3d/.mim/tools/dataset_converters/lyft_converter.py
 mmdet3d/.mim/tools/dataset_converters/lyft_data_fixer.py
 mmdet3d/.mim/tools/dataset_converters/nuimage_converter.py
 mmdet3d/.mim/tools/dataset_converters/nuscenes_converter.py
 mmdet3d/.mim/tools/dataset_converters/s3dis_data_utils.py
 mmdet3d/.mim/tools/dataset_converters/scannet_data_utils.py
+mmdet3d/.mim/tools/dataset_converters/semantickitti_converter.py
 mmdet3d/.mim/tools/dataset_converters/sunrgbd_data_utils.py
 mmdet3d/.mim/tools/dataset_converters/update_infos_to_v2.py
 mmdet3d/.mim/tools/dataset_converters/waymo_converter.py
 mmdet3d/.mim/tools/deployment/mmdet3d2torchserve.py
 mmdet3d/.mim/tools/deployment/mmdet3d_handler.py
 mmdet3d/.mim/tools/deployment/test_torchserver.py
 mmdet3d/.mim/tools/misc/browse_dataset.py
@@ -255,14 +277,20 @@
 mmdet3d/.mim/tools/misc/visualize_results.py
 mmdet3d/.mim/tools/model_converters/convert_h3dnet_checkpoints.py
 mmdet3d/.mim/tools/model_converters/convert_votenet_checkpoints.py
 mmdet3d/.mim/tools/model_converters/publish_model.py
 mmdet3d/.mim/tools/model_converters/regnet2mmdet.py
 mmdet3d/apis/__init__.py
 mmdet3d/apis/inference.py
+mmdet3d/apis/inferencers/__init__.py
+mmdet3d/apis/inferencers/base_3d_inferencer.py
+mmdet3d/apis/inferencers/lidar_det3d_inferencer.py
+mmdet3d/apis/inferencers/lidar_seg3d_inferencer.py
+mmdet3d/apis/inferencers/mono_det3d_inferencer.py
+mmdet3d/apis/inferencers/multi_modality_det3d_inferencer.py
 mmdet3d/datasets/__init__.py
 mmdet3d/datasets/convert_utils.py
 mmdet3d/datasets/dataset_wrappers.py
 mmdet3d/datasets/det3d_dataset.py
 mmdet3d/datasets/kitti2d_dataset.py
 mmdet3d/datasets/kitti_dataset.py
 mmdet3d/datasets/lyft_dataset.py
@@ -287,14 +315,15 @@
 mmdet3d/engine/hooks/disable_object_sample_hook.py
 mmdet3d/engine/hooks/visualization_hook.py
 mmdet3d/evaluation/__init__.py
 mmdet3d/evaluation/functional/__init__.py
 mmdet3d/evaluation/functional/indoor_eval.py
 mmdet3d/evaluation/functional/instance_seg_eval.py
 mmdet3d/evaluation/functional/lyft_eval.py
+mmdet3d/evaluation/functional/panoptic_seg_eval.py
 mmdet3d/evaluation/functional/seg_eval.py
 mmdet3d/evaluation/functional/kitti_utils/__init__.py
 mmdet3d/evaluation/functional/kitti_utils/eval.py
 mmdet3d/evaluation/functional/kitti_utils/rotate_iou.py
 mmdet3d/evaluation/functional/scannet_utils/__init__.py
 mmdet3d/evaluation/functional/scannet_utils/evaluate_semantic_instance.py
 mmdet3d/evaluation/functional/scannet_utils/util_3d.py
@@ -302,33 +331,40 @@
 mmdet3d/evaluation/functional/waymo_utils/prediction_to_waymo.py
 mmdet3d/evaluation/metrics/__init__.py
 mmdet3d/evaluation/metrics/indoor_metric.py
 mmdet3d/evaluation/metrics/instance_seg_metric.py
 mmdet3d/evaluation/metrics/kitti_metric.py
 mmdet3d/evaluation/metrics/lyft_metric.py
 mmdet3d/evaluation/metrics/nuscenes_metric.py
+mmdet3d/evaluation/metrics/panoptic_seg_metric.py
 mmdet3d/evaluation/metrics/seg_metric.py
 mmdet3d/evaluation/metrics/waymo_metric.py
 mmdet3d/models/__init__.py
 mmdet3d/models/backbones/__init__.py
 mmdet3d/models/backbones/base_pointnet.py
+mmdet3d/models/backbones/cylinder3d.py
 mmdet3d/models/backbones/dgcnn.py
 mmdet3d/models/backbones/dla.py
 mmdet3d/models/backbones/mink_resnet.py
+mmdet3d/models/backbones/minkunet_backbone.py
 mmdet3d/models/backbones/multi_backbone.py
 mmdet3d/models/backbones/nostem_regnet.py
 mmdet3d/models/backbones/pointnet2_sa_msg.py
 mmdet3d/models/backbones/pointnet2_sa_ssg.py
 mmdet3d/models/backbones/second.py
+mmdet3d/models/backbones/spvcnn_backone.py
 mmdet3d/models/data_preprocessors/__init__.py
 mmdet3d/models/data_preprocessors/data_preprocessor.py
 mmdet3d/models/data_preprocessors/utils.py
+mmdet3d/models/data_preprocessors/voxelize.py
 mmdet3d/models/decode_heads/__init__.py
+mmdet3d/models/decode_heads/cylinder3d_head.py
 mmdet3d/models/decode_heads/decode_head.py
 mmdet3d/models/decode_heads/dgcnn_head.py
+mmdet3d/models/decode_heads/minkunet_head.py
 mmdet3d/models/decode_heads/paconv_head.py
 mmdet3d/models/decode_heads/pointnet2_head.py
 mmdet3d/models/dense_heads/__init__.py
 mmdet3d/models/dense_heads/anchor3d_head.py
 mmdet3d/models/dense_heads/anchor_free_mono3d_head.py
 mmdet3d/models/dense_heads/base_3d_dense_head.py
 mmdet3d/models/dense_heads/base_conv_bbox_head.py
@@ -372,17 +408,19 @@
 mmdet3d/models/detectors/ssd3dnet.py
 mmdet3d/models/detectors/two_stage.py
 mmdet3d/models/detectors/votenet.py
 mmdet3d/models/detectors/voxelnet.py
 mmdet3d/models/layers/__init__.py
 mmdet3d/models/layers/box3d_nms.py
 mmdet3d/models/layers/edge_fusion_module.py
+mmdet3d/models/layers/minkowski_engine_block.py
 mmdet3d/models/layers/mlp.py
 mmdet3d/models/layers/norm.py
 mmdet3d/models/layers/sparse_block.py
+mmdet3d/models/layers/torchsparse_block.py
 mmdet3d/models/layers/transformer.py
 mmdet3d/models/layers/vote_module.py
 mmdet3d/models/layers/dgcnn_modules/__init__.py
 mmdet3d/models/layers/dgcnn_modules/dgcnn_fa_module.py
 mmdet3d/models/layers/dgcnn_modules/dgcnn_fp_module.py
 mmdet3d/models/layers/dgcnn_modules/dgcnn_gf_module.py
 mmdet3d/models/layers/fusion_layers/__init__.py
@@ -397,17 +435,20 @@
 mmdet3d/models/layers/pointnet_modules/paconv_sa_module.py
 mmdet3d/models/layers/pointnet_modules/point_fp_module.py
 mmdet3d/models/layers/pointnet_modules/point_sa_module.py
 mmdet3d/models/layers/pointnet_modules/stack_point_sa_module.py
 mmdet3d/models/layers/spconv/__init__.py
 mmdet3d/models/layers/spconv/overwrite_spconv/__init__.py
 mmdet3d/models/layers/spconv/overwrite_spconv/write_spconv2.py
+mmdet3d/models/layers/torchsparse/__init__.py
+mmdet3d/models/layers/torchsparse/torchsparse_wrapper.py
 mmdet3d/models/losses/__init__.py
 mmdet3d/models/losses/axis_aligned_iou_loss.py
 mmdet3d/models/losses/chamfer_distance.py
+mmdet3d/models/losses/lovasz_loss.py
 mmdet3d/models/losses/multibin_loss.py
 mmdet3d/models/losses/paconv_regularization_loss.py
 mmdet3d/models/losses/rotated_iou_loss.py
 mmdet3d/models/losses/uncertain_smooth_l1_loss.py
 mmdet3d/models/middle_encoders/__init__.py
 mmdet3d/models/middle_encoders/pillar_scatter.py
 mmdet3d/models/middle_encoders/sparse_encoder.py
@@ -435,15 +476,18 @@
 mmdet3d/models/roi_heads/mask_heads/primitive_head.py
 mmdet3d/models/roi_heads/roi_extractors/__init__.py
 mmdet3d/models/roi_heads/roi_extractors/batch_roigridpoint_extractor.py
 mmdet3d/models/roi_heads/roi_extractors/single_roiaware_extractor.py
 mmdet3d/models/roi_heads/roi_extractors/single_roipoint_extractor.py
 mmdet3d/models/segmentors/__init__.py
 mmdet3d/models/segmentors/base.py
+mmdet3d/models/segmentors/cylinder3d.py
 mmdet3d/models/segmentors/encoder_decoder.py
+mmdet3d/models/segmentors/minkunet.py
+mmdet3d/models/segmentors/seg3d_tta.py
 mmdet3d/models/task_modules/__init__.py
 mmdet3d/models/task_modules/builder.py
 mmdet3d/models/task_modules/anchor/__init__.py
 mmdet3d/models/task_modules/anchor/anchor_3d_generator.py
 mmdet3d/models/task_modules/anchor/builder.py
 mmdet3d/models/task_modules/assigners/__init__.py
 mmdet3d/models/task_modules/assigners/max_3d_iou_assigner.py
```

### Comparing `mmdet3d-1.1.0rc3/setup.cfg` & `mmdet3d-1.1.1/setup.cfg`

 * *Files identical despite different names*

### Comparing `mmdet3d-1.1.0rc3/setup.py` & `mmdet3d-1.1.1/setup.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+# Copyright (c) OpenMMLab. All rights reserved.
 import os
 import platform
 import shutil
 import sys
 import warnings
 from os import path as osp
 from setuptools import find_packages, setup
@@ -19,21 +20,15 @@
 
 version_file = 'mmdet3d/version.py'
 
 
 def get_version():
     with open(version_file, 'r') as f:
         exec(compile(f.read(), version_file, 'exec'))
-    import sys
-
-    # return short version for sdist
-    if 'sdist' in sys.argv or 'bdist_wheel' in sys.argv:
-        return locals()['short_version']
-    else:
-        return locals()['__version__']
+    return locals()['__version__']
 
 
 def make_cuda_ext(name,
                   module,
                   sources,
                   sources_cuda=[],
                   extra_args=[],
@@ -159,15 +154,15 @@
     elif 'sdist' in sys.argv or 'bdist_wheel' in sys.argv:
         # installed by `pip install .`
         # or create source distribution by `python setup.py sdist`
         mode = 'copy'
     else:
         return
 
-    filenames = ['tools', 'configs', 'model-index.yml']
+    filenames = ['tools', 'configs', 'demo', 'model-index.yml']
     repo_path = osp.dirname(__file__)
     mim_path = osp.join(repo_path, 'mmdet3d', '.mim')
     os.makedirs(mim_path, exist_ok=True)
 
     for filename in filenames:
         if osp.exists(filename):
             src_path = osp.join(repo_path, filename)
@@ -201,29 +196,30 @@
                      'for general 3D object detection.'),
         long_description=readme(),
         long_description_content_type='text/markdown',
         author='MMDetection3D Contributors',
         author_email='zwwdev@gmail.com',
         keywords='computer vision, 3D object detection',
         url='https://github.com/open-mmlab/mmdetection3d',
-        packages=find_packages(),
+        packages=find_packages(exclude=('configs', 'tools', 'demo')),
         include_package_data=True,
-        package_data={'mmdet3d.ops': ['*/*.so']},
         classifiers=[
-            'Development Status :: 4 - Beta',
+            'Development Status :: 5 - Production/Stable',
             'License :: OSI Approved :: Apache Software License',
             'Operating System :: OS Independent',
             'Programming Language :: Python :: 3',
-            'Programming Language :: Python :: 3.6',
             'Programming Language :: Python :: 3.7',
+            'Programming Language :: Python :: 3.8',
+            'Programming Language :: Python :: 3.9',
         ],
         license='Apache License 2.0',
         install_requires=parse_requirements('requirements/runtime.txt'),
         extras_require={
             'all': parse_requirements('requirements.txt'),
             'tests': parse_requirements('requirements/tests.txt'),
             'build': parse_requirements('requirements/build.txt'),
             'optional': parse_requirements('requirements/optional.txt'),
             'mim': parse_requirements('requirements/mminstall.txt'),
         },
+        ext_modules=[],
         cmdclass={'build_ext': BuildExtension},
         zip_safe=False)
```

